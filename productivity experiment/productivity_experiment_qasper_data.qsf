{
    "SurveyEntry": {
        "SurveyID": "SV_8iyc6G8rZOzeZW6",
        "SurveyName": "Qasper data",
        "SurveyDescription": null,
        "SurveyOwnerID": "UR_8eKZV4jJPWIuIfA",
        "SurveyBrandID": "uva",
        "DivisionID": "DV_2g9pK71JDa4rVYx",
        "SurveyLanguage": "EN",
        "SurveyActiveResponseSet": "RS_aeJPQ3Iu6rQ9BaK",
        "SurveyStatus": "Inactive",
        "SurveyStartDate": "0000-00-00 00:00:00",
        "SurveyExpirationDate": "0000-00-00 00:00:00",
        "SurveyCreationDate": "2023-05-02 05:47:13",
        "CreatorID": "UR_8eKZV4jJPWIuIfA",
        "LastModified": "2023-05-02 07:15:35",
        "LastAccessed": "0000-00-00 00:00:00",
        "LastActivated": "0000-00-00 00:00:00",
        "Deleted": null
    },
    "SurveyElements": [
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "BL",
            "PrimaryAttribute": "Survey Blocks",
            "SecondaryAttribute": null,
            "TertiaryAttribute": null,
            "Payload": {
                "0": {
                    "Type": "Default",
                    "Description": "Task1",
                    "ID": "BL_bl6Gf51I75N6nEW",
                    "BlockElements": [
                        {
                            "Type": "Question",
                            "QuestionID": "QID2"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID1"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID22"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID32"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID34"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID36"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID38"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID40"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID42"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID44"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID46"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID48"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID50"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID52"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID54"
                        }
                    ],
                    "Options": {
                        "BlockLocking": "false",
                        "RandomizeQuestions": "Advanced",
                        "BlockVisibility": "Expanded",
                        "Randomization": {
                            "Advanced": {
                                "FixedOrder": [
                                    "{~Randomized~}",
                                    "{~SubSet~}"
                                ],
                                "RandomizeAll": [
                                    "QID2"
                                ],
                                "RandomSubSet": [
                                    "QID1",
                                    "QID22",
                                    "QID24",
                                    "QID26",
                                    "QID28",
                                    "QID32",
                                    "QID34",
                                    "QID36",
                                    "QID38",
                                    "QID40",
                                    "QID42",
                                    "QID44",
                                    "QID46",
                                    "QID48",
                                    "QID50",
                                    "QID52",
                                    "QID54"
                                ],
                                "Undisplayed": [],
                                "TotalRandSubset": 1,
                                "QuestionsPerPage": 0
                            },
                            "EvenPresentation": true
                        }
                    }
                },
                "1": {
                    "Type": "Trash",
                    "Description": "Trash / Unused Questions",
                    "ID": "BL_cM7iktSbWEtZAs6",
                    "BlockElements": [
                        {
                            "Type": "Question",
                            "QuestionID": "QID24"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID26"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID28"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID25"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID27"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID29"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID23"
                        }
                    ]
                },
                "2": {
                    "Type": "Standard",
                    "SubType": "",
                    "Description": "Task2",
                    "ID": "BL_bsIslt16LUds9a6",
                    "BlockElements": [
                        {
                            "Type": "Question",
                            "QuestionID": "QID4"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID3"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID31"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID33"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID35"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID37"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID39"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID41"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID43"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID45"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID47"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID49"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID51"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID53"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID55"
                        }
                    ],
                    "Options": {
                        "BlockLocking": "false",
                        "RandomizeQuestions": "Advanced",
                        "BlockVisibility": "Expanded",
                        "Randomization": {
                            "Advanced": {
                                "FixedOrder": [
                                    "{~Randomized~}",
                                    "{~SubSet~}",
                                    "QID31"
                                ],
                                "RandomizeAll": [
                                    "QID4"
                                ],
                                "RandomSubSet": [
                                    "QID3",
                                    "QID33",
                                    "QID35",
                                    "QID37",
                                    "QID39",
                                    "QID41",
                                    "QID43",
                                    "QID45",
                                    "QID47",
                                    "QID49",
                                    "QID51",
                                    "QID53",
                                    "QID55"
                                ],
                                "Undisplayed": [],
                                "TotalRandSubset": 1,
                                "QuestionsPerPage": 0
                            },
                            "EvenPresentation": true
                        }
                    }
                },
                "3": {
                    "Type": "Standard",
                    "SubType": "",
                    "Description": "Pre-Task1",
                    "ID": "BL_3Dj5laZEghsJt2u",
                    "BlockElements": [
                        {
                            "Type": "Question",
                            "QuestionID": "QID6"
                        }
                    ],
                    "Options": {
                        "BlockLocking": "false",
                        "RandomizeQuestions": "false",
                        "BlockVisibility": "Collapsed"
                    }
                },
                "4": {
                    "Type": "Standard",
                    "SubType": "",
                    "Description": "Pre-Task2",
                    "ID": "BL_1OfoAq1t9TwTvng",
                    "BlockElements": [
                        {
                            "Type": "Question",
                            "QuestionID": "QID7"
                        }
                    ],
                    "Options": {
                        "BlockLocking": "false",
                        "RandomizeQuestions": "false",
                        "BlockVisibility": "Expanded"
                    }
                },
                "6": {
                    "Type": "Standard",
                    "SubType": "",
                    "Description": "Introduction",
                    "ID": "BL_1SS5gVuq5tOjQ5o",
                    "BlockElements": [
                        {
                            "Type": "Question",
                            "QuestionID": "QID1211903273"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID1211903274"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID1211903275"
                        }
                    ],
                    "Options": {
                        "BlockLocking": "false",
                        "RandomizeQuestions": "false",
                        "BlockVisibility": "Collapsed"
                    }
                },
                "8": {
                    "Type": "Standard",
                    "Description": "Demographics",
                    "ID": "BL_54EZfzGKGJQIM7k",
                    "BlockElements": [
                        {
                            "Type": "Question",
                            "QuestionID": "QID1211903276"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID1211903277"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID1211903278"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID1211903279"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID1211903283"
                        },
                        {
                            "Type": "Question",
                            "QuestionID": "QID1211903285"
                        }
                    ],
                    "Options": {
                        "BlockLocking": "false",
                        "RandomizeQuestions": "false",
                        "BlockVisibility": "Collapsed"
                    }
                },
                "10": {
                    "Type": "Standard",
                    "SubType": "",
                    "Description": "Efficacy",
                    "ID": "BL_2nloJzavYh1alwi",
                    "BlockElements": [
                        {
                            "Type": "Question",
                            "QuestionID": "QID1211903284"
                        }
                    ],
                    "Options": {
                        "BlockLocking": "false",
                        "RandomizeQuestions": "false",
                        "BlockVisibility": "Collapsed"
                    }
                },
                "12": {
                    "Type": "Standard",
                    "SubType": "",
                    "Description": "Efficacy",
                    "ID": "BL_9tnQNp2BFl5YC0u",
                    "BlockElements": [
                        {
                            "Type": "Question",
                            "QuestionID": "QID21"
                        }
                    ],
                    "Options": {
                        "BlockLocking": "false",
                        "RandomizeQuestions": "false",
                        "BlockVisibility": "Expanded"
                    }
                },
                "13": {
                    "Type": "Standard",
                    "SubType": "",
                    "Description": "emailBlock 8",
                    "ID": "BL_6ngbnMsTlzQYrpc",
                    "BlockElements": [
                        {
                            "Type": "Question",
                            "QuestionID": "QID30"
                        }
                    ],
                    "Options": {
                        "BlockLocking": "false",
                        "RandomizeQuestions": "false",
                        "BlockVisibility": "Expanded"
                    }
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "FL",
            "PrimaryAttribute": "Survey Flow",
            "SecondaryAttribute": null,
            "TertiaryAttribute": null,
            "Payload": {
                "Type": "Root",
                "FlowID": "FL_1",
                "Flow": [
                    {
                        "Type": "Standard",
                        "ID": "BL_1SS5gVuq5tOjQ5o",
                        "FlowID": "FL_2",
                        "Autofill": []
                    },
                    {
                        "Type": "Standard",
                        "ID": "BL_54EZfzGKGJQIM7k",
                        "FlowID": "FL_3",
                        "Autofill": []
                    },
                    {
                        "Type": "Standard",
                        "ID": "BL_3Dj5laZEghsJt2u",
                        "FlowID": "FL_5",
                        "Autofill": []
                    },
                    {
                        "Type": "BlockRandomizer",
                        "FlowID": "FL_14",
                        "SubSet": 2,
                        "EvenPresentation": true,
                        "Flow": [
                            {
                                "Type": "Block",
                                "ID": "BL_bl6Gf51I75N6nEW",
                                "FlowID": "FL_7",
                                "Autofill": []
                            },
                            {
                                "Type": "Branch",
                                "FlowID": "FL_16",
                                "Description": "New Branch",
                                "BranchLogic": {
                                    "0": {
                                        "0": {
                                            "LogicType": "Question",
                                            "LeftOperand": "q://undefined/undefined",
                                            "Type": "Expression",
                                            "Description": "<span class=\"Error\">Invalid Logic</span> <a href=\"javascript:void(0)\" clickcallback=\"Q_LogicEditor.edit\" instanceid=\"LE_10069129\">Click Here to Edit Logic</a>"
                                        },
                                        "Type": "If"
                                    },
                                    "Type": "BooleanExpression"
                                }
                            }
                        ]
                    },
                    {
                        "Type": "Standard",
                        "ID": "BL_2nloJzavYh1alwi",
                        "FlowID": "FL_11",
                        "Autofill": []
                    },
                    {
                        "Type": "Standard",
                        "ID": "BL_1OfoAq1t9TwTvng",
                        "FlowID": "FL_8",
                        "Autofill": []
                    },
                    {
                        "Type": "Standard",
                        "ID": "BL_bsIslt16LUds9a6",
                        "FlowID": "FL_9",
                        "Autofill": []
                    },
                    {
                        "Type": "Standard",
                        "ID": "BL_9tnQNp2BFl5YC0u",
                        "FlowID": "FL_13",
                        "Autofill": []
                    },
                    {
                        "ID": "BL_6ngbnMsTlzQYrpc",
                        "Type": "Standard",
                        "FlowID": "FL_18"
                    },
                    {
                        "Type": "EndSurvey",
                        "FlowID": "FL_15"
                    }
                ],
                "Properties": {
                    "Count": 18,
                    "RemovedFieldsets": []
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "PL",
            "PrimaryAttribute": "Preview Link",
            "SecondaryAttribute": null,
            "TertiaryAttribute": null,
            "Payload": {
                "PreviewType": "Brand",
                "PreviewID": "d47c6030-b126-477b-8457-c692c53afba5"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SO",
            "PrimaryAttribute": "Survey Options",
            "SecondaryAttribute": null,
            "TertiaryAttribute": null,
            "Payload": {
                "BackButton": "false",
                "SaveAndContinue": "true",
                "SurveyProtection": "PublicSurvey",
                "BallotBoxStuffingPrevention": "false",
                "NoIndex": "Yes",
                "SecureResponseFiles": "true",
                "SurveyExpiration": "None",
                "SurveyTermination": "DefaultMessage",
                "Header": "",
                "Footer": "",
                "ProgressBarDisplay": "None",
                "PartialData": "+1 week",
                "ValidationMessage": "",
                "PreviousButton": "",
                "NextButton": "",
                "SurveyTitle": "Qualtrics Survey | Qualtrics Experience Management",
                "SkinLibrary": "uva",
                "SkinType": "templated",
                "Skin": {
                    "brandingId": "3067498872",
                    "templateId": "*base",
                    "overrides": null
                },
                "NewScoring": 1,
                "SurveyMetaDescription": "The most powerful, simple and trusted way to gather experience data. Start your journey to experience management and try a free account today.",
                "SurveyName": "Start"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SCO",
            "PrimaryAttribute": "Scoring",
            "SecondaryAttribute": null,
            "TertiaryAttribute": null,
            "Payload": {
                "ScoringCategories": [],
                "ScoringCategoryGroups": [],
                "ScoringSummaryCategory": null,
                "ScoringSummaryAfterQuestions": 0,
                "ScoringSummaryAfterSurvey": 0,
                "DefaultScoringCategory": null,
                "AutoScoringCategory": null
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "PROJ",
            "PrimaryAttribute": "CORE",
            "SecondaryAttribute": null,
            "TertiaryAttribute": "1.1.0",
            "Payload": {
                "ProjectCategory": "CORE",
                "SchemaVersion": "1.1.0"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "STAT",
            "PrimaryAttribute": "Survey Statistics",
            "SecondaryAttribute": null,
            "TertiaryAttribute": null,
            "Payload": {
                "MobileCompatible": true,
                "ID": "Survey Statistics"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "QC",
            "PrimaryAttribute": "Survey Question Count",
            "SecondaryAttribute": "31",
            "TertiaryAttribute": null,
            "Payload": null
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "RS",
            "PrimaryAttribute": "RS_aeJPQ3Iu6rQ9BaK",
            "SecondaryAttribute": "Default Response Set",
            "TertiaryAttribute": null,
            "Payload": null
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID1211903275",
            "SecondaryAttribute": "I am at least 18 years of age.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "I am at least 18 years of age.",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "I am at least 18 years of age.",
                "Choices": {
                    "3": {
                        "Display": "No"
                    },
                    "4": {
                        "Display": "Yes"
                    }
                },
                "ChoiceOrder": [
                    "4",
                    "3"
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 1,
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "QuestionText_Unsafe": "I am at least 18 years of age.",
                "DataExportTag": "18+",
                "QuestionID": "QID1211903275"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID1211903274",
            "SecondaryAttribute": "I have been sufficiently informed about the study and agree to participate",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<div>I have been sufficiently informed about the study and agree to participate</div>",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "I have been sufficiently informed about the study and agree to participate",
                "Choices": {
                    "1": {
                        "Display": "I agree to participate in this study"
                    }
                },
                "ChoiceOrder": [
                    1
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "CustomValidation",
                        "CustomValidation": {
                            "Logic": {
                                "0": {
                                    "0": {
                                        "ChoiceLocator": "q://QID1211903274/SelectableChoice/1",
                                        "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">I have been informed about the study and I have been given the opportunity to ask questions about...</span> <span class=\"LeftOpDesc\">I agree to participate in this study</span> <span class=\"OpDesc\">Is Selected</span> ",
                                        "LeftOperand": "q://QID1211903274/SelectableChoice/1",
                                        "LogicType": "Question",
                                        "Operator": "Selected",
                                        "QuestionID": "QID1211903274",
                                        "QuestionIDFromLocator": "QID1211903274",
                                        "QuestionIsInLoop": "no",
                                        "Type": "Expression"
                                    },
                                    "Type": "If"
                                },
                                "Type": "BooleanExpression"
                            },
                            "Message": {
                                "description": "wrong answer",
                                "libraryID": "UR_06aHius8mgA1IOx",
                                "messageID": "MS_8HzEfRMyoBMasCh",
                                "subMessageID": "VE_CUSTOM_VALIDATION_0"
                            }
                        }
                    }
                },
                "Language": [],
                "NextChoiceId": 2,
                "NextAnswerId": 1,
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "QuestionText_Unsafe": "<div>I have been sufficiently informed about the study and agree to participate</div>",
                "DataExportTag": "consent",
                "QuestionID": "QID1211903274"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID21",
            "SecondaryAttribute": "On a scale of 1 to 10, rate the previous task",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "On a scale of 1 to 10, rate the previous task",
                "DefaultChoices": false,
                "QuestionType": "Slider",
                "Selector": "HSLIDER",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText",
                    "CSSliderMin": 0,
                    "CSSliderMax": 10,
                    "GridLines": 10,
                    "SnapToGrid": true,
                    "NumDecimals": "0",
                    "ShowValue": true,
                    "CustomStart": true,
                    "NotApplicable": false,
                    "MobileFirst": true,
                    "SliderStartPositions": {
                        "1": 0,
                        "2": 0
                    }
                },
                "QuestionDescription": "On a scale of 1 to 10, rate the previous task",
                "Choices": {
                    "1": {
                        "Display": "I enjoyed doing it"
                    },
                    "2": {
                        "Display": "I felt skilled/effective doing it"
                    },
                    "3": {
                        "Display": "The generated answers were helpful in finding the final answer"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "OFF",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "Labels": [],
                "Answers": {
                    "1": {
                        "Display": 0
                    },
                    "2": {
                        "Display": 1
                    },
                    "3": {
                        "Display": 2
                    },
                    "4": {
                        "Display": 3
                    },
                    "5": {
                        "Display": 4
                    },
                    "6": {
                        "Display": 5
                    },
                    "7": {
                        "Display": 6
                    },
                    "8": {
                        "Display": 7
                    },
                    "9": {
                        "Display": 8
                    },
                    "10": {
                        "Display": 9
                    },
                    "11": {
                        "Display": 10
                    }
                },
                "QuestionText_Unsafe": "On a scale of 1 to 10, rate the previous task",
                "DataExportTag": "Task2-Efficacy",
                "QuestionID": "QID21"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID1211903284",
            "SecondaryAttribute": "On a scale of 1 to 10, rate the previous task",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "On a scale of 1 to 10, rate the previous task",
                "DefaultChoices": false,
                "QuestionType": "Slider",
                "Selector": "HSLIDER",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText",
                    "CSSliderMin": 0,
                    "CSSliderMax": 10,
                    "GridLines": 10,
                    "SnapToGrid": true,
                    "NumDecimals": "0",
                    "ShowValue": true,
                    "CustomStart": true,
                    "NotApplicable": false,
                    "MobileFirst": true,
                    "SliderStartPositions": {
                        "1": 0,
                        "2": 0
                    }
                },
                "QuestionDescription": "On a scale of 1 to 10, rate the previous task",
                "Choices": {
                    "1": {
                        "Display": "I enjoyed doing it"
                    },
                    "2": {
                        "Display": "I felt skilled/effective doing it"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2"
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "OFF",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 1,
                "Labels": [],
                "Answers": {
                    "1": {
                        "Display": 0
                    },
                    "2": {
                        "Display": 1
                    },
                    "3": {
                        "Display": 2
                    },
                    "4": {
                        "Display": 3
                    },
                    "5": {
                        "Display": 4
                    },
                    "6": {
                        "Display": 5
                    },
                    "7": {
                        "Display": 6
                    },
                    "8": {
                        "Display": 7
                    },
                    "9": {
                        "Display": 8
                    },
                    "10": {
                        "Display": 9
                    },
                    "11": {
                        "Display": 10
                    }
                },
                "QuestionText_Unsafe": "On a scale of 1 to 10, rate the previous task",
                "DataExportTag": "Task1-efficacy",
                "QuestionID": "QID1211903284"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID30",
            "SecondaryAttribute": "Please provide your email if you are interested in receiving compensation.",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Please provide your email if you are interested in receiving compensation.",
                "DefaultChoices": false,
                "DataExportTag": "Email",
                "QuestionType": "TE",
                "Selector": "SL",
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "Please provide your email if you are interested in receiving compensation.",
                "Validation": {
                    "Settings": {
                        "ForceResponse": "OFF",
                        "Type": "ContentType",
                        "MinChars": "1",
                        "ContentType": "ValidEmail",
                        "ValidDateType": "DateWithFormat",
                        "ValidPhoneType": "ValidUSPhone",
                        "ValidZipType": "ValidUSZip",
                        "ValidNumber": {
                            "Min": "",
                            "Max": "",
                            "NumDecimals": ""
                        }
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                },
                "QuestionID": "QID30"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID1211903285",
            "SecondaryAttribute": "Rate the following skills in English",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Rate the following skills in English",
                "DefaultChoices": false,
                "DataExportTag": "english-skills",
                "QuestionID": "QID1211903285",
                "QuestionType": "Slider",
                "Selector": "HSLIDER",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText",
                    "CSSliderMin": 1,
                    "CSSliderMax": 5,
                    "GridLines": 4,
                    "SnapToGrid": true,
                    "NumDecimals": "0",
                    "ShowValue": true,
                    "CustomStart": true,
                    "NotApplicable": false,
                    "MobileFirst": true,
                    "SliderStartPositions": {
                        "1": 0,
                        "2": 0
                    }
                },
                "QuestionDescription": "Rate the following skills in English",
                "Choices": {
                    "1": {
                        "Display": "writing"
                    },
                    "2": {
                        "Display": "reading comprehension"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "OFF",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 3,
                "NextAnswerId": 1,
                "Labels": [],
                "Answers": {
                    "1": {
                        "Display": 1
                    },
                    "2": {
                        "Display": 2
                    },
                    "3": {
                        "Display": 3
                    },
                    "4": {
                        "Display": 4
                    },
                    "5": {
                        "Display": 5
                    }
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID3",
            "SecondaryAttribute": "what was the baseline?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> what was the baseline?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "Weka baseline"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "Unanswerable"
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "Unanswerable"
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "Hybrid \u2013 the current state-of-the-art"
                        }
                    }
                },
                "DataExportTag": "Task2-what was the baseline?",
                "QuestionID": "QID3",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "what was the baseline?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1708.05521.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity</a><br><b>Answer: </b>Weka baseline<br><strong>Evidence:</strong><br /><br />In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the <b>Weka baseline</b> BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings.<br /><br />In this paper we introduced an intra-sentence attention RNN for the of emotion intensity, which we developed for the WASSA-2017 Shared Task on Emotion Intensity. Our model does not make use of external information except for pre-trained embeddings and is able to outperform the <b>Weka baseline</b> for the development set, but not in the test set. In the shared task, it obtained the 13th place among 22 competitors.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1809.08935.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Lexical Bias In Essay Level Prediction</a><br><b>Answer: </b>Unanswerable<br><strong>Evidence:</strong><br />The question does not have an answer based on the given document.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.04491.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis</a><br><b>Answer: </b>Unanswerable<br><strong>Evidence:</strong><br />The question does not have an answer based on the given document.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1804.07445.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Sentence Simplification with Memory-Augmented Neural Networks</a><br><b>Answer: </b>Hybrid \u2013 the current state-of-the-art<br><strong>Evidence:</strong><br /><br />On WikiSmall, <b>Hybrid \u2013 the current state-of-the-art</b> \u2013 achieved best BLEU (53.94) and SARI (30.46) scores. Among neural models, NseLstm-B yielded the highest BLEU score (53.42), while NseLstm-S performed best on SARI (29.75). On WikiLarge, again, NseLstm-B had the highest BLEU score of 92.02. Sbmt-Sari \u2013 that was trained on a huge corpus of 106M sentence pairs and 2B words \u2013 scored highest on SARI with 39.96, followed by Dress-Ls (37.27), Dress (37.08), and NseLstm-S (36.88).",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID1",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID23",
            "SecondaryAttribute": "what dataset was used?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> what dataset was used?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "The training, validation and test datasets provided for the shared task BIBREF5"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "Data set A and Data set B"
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "113 GB"
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "Bing Liu's dataset"
                        }
                    }
                },
                "DataExportTag": "Task2-what dataset was used?",
                "QuestionID": "QID23",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "what dataset was used?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1708.05521.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity</a><br><b>Answer: </b>The training, validation and test datasets provided for the shared task BIBREF5<br><strong>Evidence:</strong><br /><br />to test our model, we experiment using <b>the training, validation and test datasets provided for the shared task bibref5</b> , which include tweets for four emotions: joy, sadness, fear, and anger. these were annotated using best-worst scaling (bws) to obtain very reliable scores bibref6 .",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1906.08871.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Advancing Speech Recognition With No Speech Or With Noisy Speech</a><br><b>Answer: </b>Data set A and Data set B<br><strong>Evidence:</strong><br /><br />For data set A, we used data from first 8 subjects for training the model, remaining two subjects data for validation and test set respectively.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.09749.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">KeyVec: Key-semantics Preserving Document Representations</a><br><b>Answer: </b>113 GB<br><strong>Evidence:</strong><br /><br />Table TABREF15 presents P@10, MAP and MRR results of our KeyVec model and competing embedding methods in academic paper retrieval. word2vec averaging generates an embedding for a document by averaging the word2vec vectors of its constituent words. In the experiment, we used two different versions of word2vec: one from public release, and the other one trained specifically on our own academic corpus (<b>113 GB</b>). From Table TABREF15 , we observe that as a document-embedding model, Paragraph Vector gave better retrieval results than word2vec averagings did. In contrast, our KeyVec outperforms all the competitors given its unique capability of capturing and embedding the key information of documents.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.04491.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis</a><br><b>Answer: </b>Bing Liu's dataset<br><strong>Evidence:</strong><br /><br />We used <b>Bing Liu's dataset</b> BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.<br /><br />In Table TABREF18 there are presented some examples of the results of our approach compared with the annotated data from <b>Bing Liu's dataset</b>. In the first sentence, the results of the analysis differ because we decided to treat only nouns or noun phrases as aspects, while annotators also accepted verbs. In some cases, such as sentences 2 or 4, our approach generated more valuable aspects than the annotators found, but in some cases, like sentence 5, we found fewer. This is possibly the result of our method of filtering valuable aspects - if some aspects were not frequent enough in the dataset, we can treat them as void. In cases where there is neither aspect nor sentiment in the dataset, such as sentence 6, we measure sentiment as well, as one of our analysis steps.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID22/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">Next Question</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID22/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID22",
                            "QuestionIDFromLocator": "QID22",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 11,
                "NextAnswerId": 4,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                },
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID31",
            "SecondaryAttribute": "Task 2: Answer the question given a document and a generated answer. You are given a set of 10 do...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given a document and a generated answer.</u><br>\n    You are given a set of 10 documents, a question that needs to be answered for each of the documents, and an answer generated by a language model.\n    Your task is to check whether the answer to the question is correct, based on the document's content. \n    If the answer is correct then leave the text box empty. \n    If the answer is incorrect then write the correct answer in the text box.<br><br>\n\n    What Information Retrieval datasets are used?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task2-1aaefbd0",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText",
                    "TextPosition": "inline",
                    "ChoiceColumnWidthPixels": 319,
                    "RepeatHeaders": "none",
                    "MobileFirst": true
                },
                "QuestionDescription": "Task 2: Answer the question given a document and a generated answer. You are given a set of 10 do...",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods</a><br><b>Answer: </b> ROUGE, QAEval, and BERTScore"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods</a><br><b>Answer: </b> ROUGE, QAEval, and BERTScore"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods</a><br><b>Answer: </b> ROUGE, QAEval, and BERTScore"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods</a><br><b>Answer: </b> ROUGE, QAEval, and BERTScore"
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID1",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "OFF",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": "Click to write Scale Point 1"
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false,
                "QuestionID": "QID31"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID6",
            "SecondaryAttribute": "The time spent on the next task will be timed, so please read the instructions, perform the task...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "The time spent on the next task will be timed, so please read the instructions, perform the task at a speed that seems reasonable to you, and take any breaks beforehand if necessary.<br><br>Task 1: Answer the question given the document. <br>You are given a set of 10 documents and a question that needs to be answered for each of the documents. Your task is to answer the question based on the document's content. If the answer can not be found in a document then answer with \"NA\".<br>",
                "DefaultChoices": false,
                "DataExportTag": "Task1-expl",
                "QuestionType": "DB",
                "Selector": "TB",
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "The time spent on the next task will be timed, so please read the instructions, perform the task...",
                "ChoiceOrder": [],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "QuestionID": "QID6",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID7",
            "SecondaryAttribute": "The time spent on the next task will be timed, so please read the instructions, perform the task...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "The time spent on the next task will be timed, so please read the instructions, perform the task at a speed that seems reasonable to you, and take any breaks beforehand if necessary.<br>\n<br>\n<u>Task 2: Answer the question given a document and a generated answer.</u><br>\nYou are given a set of 10 documents, a question that needs to be answered for each of the documents, and an answer generated by a language model. Your task is, again, to answer the question, based on the document's content. If the generated answer is correct then leave a '-' in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n&nbsp;",
                "DefaultChoices": false,
                "DataExportTag": "Task2-expl",
                "QuestionType": "DB",
                "Selector": "TB",
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "The time spent on the next task will be timed, so please read the instructions, perform the task...",
                "ChoiceOrder": [],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "QuestionID": "QID7",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID2",
            "SecondaryAttribute": "Timing",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Timing",
                "DefaultChoices": false,
                "DataExportTag": "Timer",
                "QuestionType": "Timing",
                "Selector": "PageTimer",
                "Configuration": {
                    "QuestionDescriptionOption": "UseText",
                    "MinSeconds": "0",
                    "MaxSeconds": "0"
                },
                "QuestionDescription": "Timing",
                "Choices": {
                    "1": {
                        "Display": "First Click"
                    },
                    "2": {
                        "Display": "Last Click"
                    },
                    "3": {
                        "Display": "Page Submit"
                    },
                    "4": {
                        "Display": "Click Count"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 48,
                "NextAnswerId": 1,
                "QuestionID": "QID2",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID4",
            "SecondaryAttribute": "Timing",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "Timing",
                "DefaultChoices": false,
                "DataExportTag": "Timer",
                "QuestionType": "Timing",
                "Selector": "PageTimer",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText",
                    "MinSeconds": "0",
                    "MaxSeconds": "0"
                },
                "QuestionDescription": "Timing",
                "Choices": {
                    "1": {
                        "Display": "First Click"
                    },
                    "2": {
                        "Display": "Last Click"
                    },
                    "3": {
                        "Display": "Page Submit"
                    },
                    "4": {
                        "Display": "Click Count"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 76,
                "NextAnswerId": 1,
                "QuestionID": "QID4"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID1211903283",
            "SecondaryAttribute": "To what extent do you have knowledge about Artificial Intelligence",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "To what extent do you have knowledge about Artificial Intelligence",
                "DefaultChoices": false,
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "To what extent do you have knowledge about Artificial Intelligence",
                "Choices": {
                    "1": {
                        "Display": "I have negligible knowledge about AI"
                    },
                    "2": {
                        "Display": "I have a little knowledge about AI"
                    },
                    "3": {
                        "Display": "I have significant knowledge about AI"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3"
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "OFF",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "QuestionText_Unsafe": "To what extent do you have knowledge about Artificial Intelligence",
                "DataExportTag": "ai-knowledge",
                "QuestionID": "QID1211903283",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID1211903273",
            "SecondaryAttribute": "Welcome to this study about Question-Answering productivity. This survey will take approximately...",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "<p><span style=\"font-size:16px;\"><span style=\"line-height: 150%; font-family: Arial, sans-serif;\"></span></span></p>Welcome to this study about Question-Answering productivity.<br><br>This survey will take approximately 1 hour and consists of reading comprehension and question-answering tasks. Please take the questions seriously. If you have any questions regarding this research you can reach us at papadopoulou@zeta-alpha.com.<br><br>The participants of this study will be compensated after completion.<br>Thank you very much for participating!<span style=\"font-size:16px;\"><span style=\"line-height: 107%; font-family: Arial, sans-serif;\"></span></span>",
                "DefaultChoices": false,
                "QuestionType": "DB",
                "Selector": "TB",
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "Welcome to this study about Question-Answering productivity. This survey will take approximately...",
                "ChoiceOrder": [],
                "Validation": {
                    "Settings": {
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "QuestionText_Unsafe": "<p><span style=\"font-size:16px;\"><span style=\"line-height: 150%; font-family: Arial, sans-serif;\">Hi and welcome to this study about added productivity using ChatGPT.</span></span></p><p><span style=\"font-size:16px;\"><span style=\"line-height: 150%; font-family: Arial, sans-serif;\">This survey will take about 1 hour and consists of information retrieval and writing tasks. Please take the tasks seriously.</span></span></p><p><span style=\"font-size:16px;\"><span style=\"line-height: 150%; font-family: Arial, sans-serif;\"><br></span></span></p>\n\n<p><span style=\"font-size:16px;\"><span style=\"line-height: 150%; font-family: Arial, sans-serif;\">Your answers will remain completely anonymous.&nbsp;</span></span><span style=\"font-family: Arial, sans-serif; font-size: 16px;\">If you have any other questions regarding the research you can reach me at ...</span></p><p><span style=\"font-size:16px;\"><span style=\"line-height: 150%; font-family: Arial, sans-serif;\"><span class=\"MsoHyperlink\"><br></span></span></span></p>\n<span style=\"font-size:16px;\"><span style=\"line-height: 107%; font-family: Arial, sans-serif;\">Thank you very much for participating</span></span>",
                "DataExportTag": "intro",
                "QuestionID": "QID1211903273"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID22",
            "SecondaryAttribute": "what dataset was used?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> what dataset was used?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-what dataset was used?",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "what dataset was used?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1708.05521.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1906.08871.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Advancing Speech Recognition With No Speech Or With Noisy Speech</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.09749.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">KeyVec: Key-semantics Preserving Document Representations</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.04491.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                },
                "QuestionID": "QID22"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID1",
            "SecondaryAttribute": "what was the baseline?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> what was the baseline?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-what was the baseline?",
                "QuestionID": "QID1",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "what was the baseline?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1708.05521.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1809.08935.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Lexical Bias In Essay Level Prediction</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.04491.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1804.07445.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Sentence Simplification with Memory-Augmented Neural Networks</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID1211903279",
            "SecondaryAttribute": "What is the highest level of education you've completed? (or equivalent in your country)",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "What is the highest level of education you've completed? (or equivalent in your country)",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "What is the highest level of education you've completed? (or equivalent in your country)",
                "Choices": {
                    "1": {
                        "Display": "none"
                    },
                    "2": {
                        "Display": "middle school"
                    },
                    "3": {
                        "Display": "high school"
                    },
                    "4": {
                        "Display": "bachelor's degree"
                    },
                    "5": {
                        "Display": "master's degree"
                    },
                    "6": {
                        "Display": "PhD or higher"
                    }
                },
                "ChoiceOrder": [
                    "1",
                    "2",
                    "3",
                    "4",
                    "5",
                    "6"
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "NextChoiceId": 8,
                "NextAnswerId": 1,
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "QuestionText_Unsafe": "What is the highest level of education you've completed? (or equivalent in your country)",
                "DataExportTag": "education",
                "QuestionID": "QID1211903279"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID24",
            "SecondaryAttribute": "What is the name of the Large Language Model and is it open source?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the document.</u><br>\n    You are given a set of 10 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content (meaning the pdf, if available). \n    If the answer can not be found in a document then answer with \"NA\".<br><br>\n\n    What is the name of the Large Language Model and is it open source?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-8420f823",
                "QuestionID": "QID24",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "What is the name of the Large Language Model and is it open source?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2302.13971\" target=\"_blank\" rel=\"noopener noreferrer\">LLaMA: Open and Efficient Foundation Language Models</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2102.02503\" target=\"_blank\" rel=\"noopener noreferrer\">Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2203.15556\" target=\"_blank\" rel=\"noopener noreferrer\">Training Compute-Optimal Large Language Models</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2302.13681\" target=\"_blank\" rel=\"noopener noreferrer\">The (ab)use of Open Source Code to Train Large Language Models</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "5": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1810.10045\" target=\"_blank\" rel=\"noopener noreferrer\">Language Modeling at Scale</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "6": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2206.07682\" target=\"_blank\" rel=\"noopener noreferrer\">Emergent Abilities of Large Language Models</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "7": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1210.8440\" target=\"_blank\" rel=\"noopener noreferrer\">Large Scale Language Modeling in Automatic Speech Recognition</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "8": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2211.09110\" target=\"_blank\" rel=\"noopener noreferrer\">Holistic Evaluation of Language Models</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "9": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2211.05100\" target=\"_blank\" rel=\"noopener noreferrer\">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "10": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2204.06745\" target=\"_blank\" rel=\"noopener noreferrer\">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "OFF"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID25",
            "SecondaryAttribute": "What is the name of the Large Language Model and is it open source?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given a document and a generated answer.</u><br>\n    You are given a set of 10 documents, a question that needs to be answered for each of the documents, and an answer generated by a language model.\n    Your task is to check whether the answer to the question is correct, based on the document's content. \n    If the answer is correct then leave the text box empty. \n    If the answer is incorrect then write the correct answer in the text box.<br><br>\n\n    What is the name of the Large Language Model and is it open source?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task2-8420f823",
                "QuestionID": "QID25",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText",
                    "TextPosition": "inline"
                },
                "QuestionDescription": "What is the name of the Large Language Model and is it open source?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2302.13971\" target=\"_blank\" rel=\"noopener noreferrer\">LLaMA: Open and Efficient Foundation Language Models</a><br><b>Answer: </b> The name of the Large Language Model is LLaMA and it is open source.",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2102.02503\" target=\"_blank\" rel=\"noopener noreferrer\">Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models</a><br><b>Answer: </b> GPT-3 and it is not open source.",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2203.15556\" target=\"_blank\" rel=\"noopener noreferrer\">Training Compute-Optimal Large Language Models</a><br><b>Answer: </b> Chinchilla; No",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2302.13681\" target=\"_blank\" rel=\"noopener noreferrer\">The (ab)use of Open Source Code to Train Large Language Models</a><br><b>Answer: </b> NA",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "5": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1810.10045\" target=\"_blank\" rel=\"noopener noreferrer\">Language Modeling at Scale</a><br><b>Answer: </b> The Large Language Model is the LSTM based SOTA model from [36]. It is not open source.",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "6": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2206.07682\" target=\"_blank\" rel=\"noopener noreferrer\">Emergent Abilities of Large Language Models</a><br><b>Answer: </b> Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks in language models. ICML, 2022. URL https://arxiv.org/abs/2202.06539.\n\nNo, it is not open source.",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "7": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1210.8440\" target=\"_blank\" rel=\"noopener noreferrer\">Large Scale Language Modeling in Automatic Speech Recognition</a><br><b>Answer: </b> The Large Language Model is Kneser-Ney 4-gram and it is not open source.",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "8": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2211.09110\" target=\"_blank\" rel=\"noopener noreferrer\">Holistic Evaluation of Language Models</a><br><b>Answer: </b> GPT-3 davinci v1 and it is limited-access.",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "9": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2211.05100\" target=\"_blank\" rel=\"noopener noreferrer\">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</a><br><b>Answer: </b> GPT-NeoX-20B and yes, it is open-source.",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "10": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2204.06745\" target=\"_blank\" rel=\"noopener noreferrer\">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</a><br><b>Answer: </b> GPT-NeoX-20B and yes, it is open source.",
                        "InputHeight": 29,
                        "InputWidth": 407
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID24",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "OFF",
                        "Type": null
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 11,
                "NextAnswerId": 4,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID1211903277",
            "SecondaryAttribute": "What is your age in years?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "What is your age in years?",
                "DefaultChoices": false,
                "QuestionType": "TE",
                "Selector": "SL",
                "Configuration": {
                    "QuestionDescriptionOption": "UseText",
                    "AllowFreeResponse": "false"
                },
                "QuestionDescription": "What is your age in years?",
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "ContentType",
                        "TotalChars": "2",
                        "ContentType": "ValidNumber",
                        "ValidDateType": "DateWithFormat",
                        "ValidPhoneType": "ValidUSPhone",
                        "ValidZipType": "ValidUSZip",
                        "ValidNumber": {
                            "Min": "0",
                            "Max": "99",
                            "NumDecimals": "0"
                        }
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "QuestionText_Unsafe": "What is your age in years?",
                "DataExportTag": "age",
                "QuestionID": "QID1211903277",
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID1211903276",
            "SecondaryAttribute": "What is your gender?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "What is your gender?",
                "QuestionType": "MC",
                "Selector": "SAVR",
                "SubSelector": "TX",
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "What is your gender?",
                "Choices": {
                    "1": {
                        "Display": "Male"
                    },
                    "2": {
                        "Display": "Female"
                    },
                    "3": {
                        "Display": "Other"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    "3"
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "QuestionText_Unsafe": "What is your gender?",
                "DataExportTag": "gender",
                "QuestionID": "QID1211903276"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID1211903278",
            "SecondaryAttribute": "What is your occupation (if student, specify the field of study)",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "What is your occupation (if student, specify the field of study)",
                "DefaultChoices": false,
                "QuestionType": "TE",
                "Selector": "SL",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "What is your occupation (if student, specify the field of study)",
                "Validation": {
                    "Settings": {
                        "ForceResponse": "OFF",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 8,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                },
                "QuestionText_Unsafe": "What is your occupation (if student, specify the field of study)",
                "DataExportTag": "occupation",
                "QuestionID": "QID1211903278"
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID28",
            "SecondaryAttribute": "What Long Form Question Answering datasets are used?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the document.</u><br>\n    You are given a set of 10 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content (meaning the pdf, if available). \n    If the answer can not be found in a document then answer with \"NA\".<br><br>\n\n    What Long Form Question Answering datasets are used?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-e3cb7330",
                "QuestionID": "QID28",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "What Long Form Question Answering datasets are used?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1907.09190\" target=\"_blank\" rel=\"noopener noreferrer\">ELI5: Long Form Question Answering</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2103.06332\" target=\"_blank\" rel=\"noopener noreferrer\">Hurdles to Progress in Long-form Question Answering</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2211.08386\" target=\"_blank\" rel=\"noopener noreferrer\">Generative Long-form Question Answering: Relevance, Faithfulness and Succinctness</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2203.00343\" target=\"_blank\" rel=\"noopener noreferrer\">Read before Generate! Faithful Long Form Question Answering with Machine Reading</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "5": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2210.17525\" target=\"_blank\" rel=\"noopener noreferrer\">Query Refinement Prompts for Closed-Book Long-Form Question Answering</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "6": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2112.08608\" target=\"_blank\" rel=\"noopener noreferrer\">QuALITY: Question Answering with Long Input Texts, Yes!</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "7": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1611.01839\" target=\"_blank\" rel=\"noopener noreferrer\">Hierarchical Question Answering for Long Documents</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "8": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1304.7157\" target=\"_blank\" rel=\"noopener noreferrer\">Question Answering Against Very-Large Text Collections</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "9": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2004.05109\" target=\"_blank\" rel=\"noopener noreferrer\">Towards Automatic Generation of Questions from Long Answers</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "10": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.03036\" target=\"_blank\" rel=\"noopener noreferrer\">Abductive Matching in Question Answering</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "OFF"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID29",
            "SecondaryAttribute": "What Long Form Question Answering datasets are used?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given a document and a generated answer.</u><br>\n    You are given a set of 10 documents, a question that needs to be answered for each of the documents, and an answer generated by a language model.\n    Your task is to check whether the answer to the question is correct, based on the document's content. \n    If the answer is correct then leave the text box empty. \n    If the answer is incorrect then write the correct answer in the text box.<br><br>\n\n    What Long Form Question Answering datasets are used?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task2-e3cb7330",
                "QuestionID": "QID29",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText",
                    "TextPosition": "inline"
                },
                "QuestionDescription": "What Long Form Question Answering datasets are used?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1907.09190\" target=\"_blank\" rel=\"noopener noreferrer\">ELI5: Long Form Question Answering</a><br><b>Answer: </b> ELI5: a Long Form Question Answering dataset",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2103.06332\" target=\"_blank\" rel=\"noopener noreferrer\">Hurdles to Progress in Long-form Question Answering</a><br><b>Answer: </b> Natural Questions (Kwiatkowski et al., 2019) and ELI5 (Fan et al., 2019)",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2211.08386\" target=\"_blank\" rel=\"noopener noreferrer\">Generative Long-form Question Answering: Relevance, Faithfulness and Succinctness</a><br><b>Answer: </b> TriviaQA [15] and WebQuestions [16]",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2203.00343\" target=\"_blank\" rel=\"noopener noreferrer\">Read before Generate! Faithful Long Form Question Answering with Machine Reading</a><br><b>Answer: </b> ELI5 and MS MARCO",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "5": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2210.17525\" target=\"_blank\" rel=\"noopener noreferrer\">Query Refinement Prompts for Closed-Book Long-Form Question Answering</a><br><b>Answer: </b> ASQA and AQuAMuSe",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "6": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2112.08608\" target=\"_blank\" rel=\"noopener noreferrer\">QuALITY: Question Answering with Long Input Texts, Yes!</a><br><b>Answer: </b> ELI5: Long form question answering (Fan et al., 2019)",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "7": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1611.01839\" target=\"_blank\" rel=\"noopener noreferrer\">Hierarchical Question Answering for Long Documents</a><br><b>Answer: </b> WIKIREADING dataset (Hewlett et al., 2016) and a new dataset",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "8": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1304.7157\" target=\"_blank\" rel=\"noopener noreferrer\">Question Answering Against Very-Large Text Collections</a><br><b>Answer: </b> TREC 2006 Question Answering Track, Information Retrieval for Question Answering Workshop, Chatbot NLU Corpus with STT error",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "9": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2004.05109\" target=\"_blank\" rel=\"noopener noreferrer\">Towards Automatic Generation of Questions from Long Answers</a><br><b>Answer: </b> SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), MS MARCO (Nguyen et al., 2016), Narrative QA (Kocisky et al., 2018), RACE (Lai et al., 2017), LearningQ (Chen et al., 2018), Google Natural Questions (Kwiatkowski et al., 2018)",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "10": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.03036\" target=\"_blank\" rel=\"noopener noreferrer\">Abductive Matching in Question Answering</a><br><b>Answer: </b> WikiTableQuestions",
                        "InputHeight": 29,
                        "InputWidth": 407
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID28",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "OFF",
                        "Type": null
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 11,
                "NextAnswerId": 4,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID26",
            "SecondaryAttribute": "What Neural Machine Translation metrics do they use and a short description of each?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the document.</u><br>\n    You are given a set of 10 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content (meaning the pdf, if available). \n    If the answer can not be found in a document then answer with \"NA\".<br><br>\n\n    What Neural Machine Translation metrics do they use and a short description of each?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-aa42082a",
                "QuestionID": "QID26",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "What Neural Machine Translation metrics do they use and a short description of each?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2212.10297\" target=\"_blank\" rel=\"noopener noreferrer\">Extrinsic Evaluation of Machine Translation Metrics</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2104.07541\" target=\"_blank\" rel=\"noopener noreferrer\">Reward Optimization for Neural Machine Translation with Learned Metrics</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1807.08945\" target=\"_blank\" rel=\"noopener noreferrer\">Otem&Utem: Over- and Under-Translation Evaluation Metric for NMT</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2203.15858\" target=\"_blank\" rel=\"noopener noreferrer\">Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "5": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.07809\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Machine Translation</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "6": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1710.02095\" target=\"_blank\" rel=\"noopener noreferrer\">Machine Translation Evaluation with Neural Networks</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "7": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2210.15615\" target=\"_blank\" rel=\"noopener noreferrer\">ACES: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "8": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2109.05016\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Machine Translation Quality and Post-Editing Performance</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "9": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2107.10821\" target=\"_blank\" rel=\"noopener noreferrer\">To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    },
                    "10": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2109.07740\" target=\"_blank\" rel=\"noopener noreferrer\">Scaling Laws for Neural Machine Translation</a>",
                        "InputHeight": 29,
                        "InputWidth": 407,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "OFF"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID27",
            "SecondaryAttribute": "What Neural Machine Translation metrics do they use and a short description of each?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given a document and a generated answer.</u><br>\n    You are given a set of 10 documents, a question that needs to be answered for each of the documents, and an answer generated by a language model.\n    Your task is to check whether the answer to the question is correct, based on the document's content. \n    If the answer is correct then leave the text box empty. \n    If the answer is incorrect then write the correct answer in the text box.<br><br>\n\n    What Neural Machine Translation metrics do they use and a short description of each?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task2-aa42082a",
                "QuestionID": "QID27",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText",
                    "TextPosition": "inline"
                },
                "QuestionDescription": "What Neural Machine Translation metrics do they use and a short description of each?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2212.10297\" target=\"_blank\" rel=\"noopener noreferrer\">Extrinsic Evaluation of Machine Translation Metrics</a><br><b>Answer: </b> They use chrF, COMET, BERTScore, etc. chrF is a character-level n-gram precision metric, COMET is a metric that uses a reference-based approach to measure the quality of machine translation, and BERTScore is a metric that uses a pre-trained BERT model to measure the quality of machine translation.",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2104.07541\" target=\"_blank\" rel=\"noopener noreferrer\">Reward Optimization for Neural Machine Translation with Learned Metrics</a><br><b>Answer: </b> They use BLEURT, which is trained with human judgment data because human perceives the translation quality in multiple criteria (e.g., adequacy, fluency and coverage) with different weights. They also use Bert as a teacher, which is a metric based on the similarity between sentence embeddings, and optimize the text generation models with the similarity scores. They also use Bleurt, which is a learning robust metrics for text generation. Lastly, they use Neural Machine Translation of rare words with subword units.",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1807.08945\" target=\"_blank\" rel=\"noopener noreferrer\">Otem&Utem: Over- and Under-Translation Evaluation Metric for NMT</a><br><b>Answer: </b> Otem and Utem: Otem is based on the proportion of mismatched n-grams between gold reference and system translation and focuses on the proportion of repeated n-grams in the candidate translation (C) over the whole candidate (A + C). Utem estimates the proportion of untranslated n-grams in the reference (B) over the whole reference (A + B). BLEU calculates the precision of matched n-grams (A) over the whole reference (A + C).",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2203.15858\" target=\"_blank\" rel=\"noopener noreferrer\">Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics</a><br><b>Answer: </b> NA",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "5": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.07809\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Machine Translation</a><br><b>Answer: </b> Match score: checks for each output if the aligned input word according to fast-align is indeed the input word that received the highest attention probability. \nProbability mass score: sums up the probability mass given to each alignment point obtained from fast-align.",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "6": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1710.02095\" target=\"_blank\" rel=\"noopener noreferrer\">Machine Translation Evaluation with Neural Networks</a><br><b>Answer: </b> NNRK (Neural Network ReranKing): Uses vector subtraction to find pairs of most dissimilar graph nodes and construct the graph only from the nodes included in such \u201canti-edges\u201d. The score for a sentence is generated by subtracting the scores for the two predictions. \nPRO: Uses vector subtraction to train in a pairwise fashion by subtracting the vectors for the two competing translations and then training to predict +1 or -1. At test time, a vector for a single translation is used, which is equivalent to subtracting a zero vector from it. \nSPEDE: Probabilistic edit distance metrics for MT evaluation. \nAMBER: MT evaluation metric. \nDeep-Syntactic Metric: Approximating a deep-syntactic metric for MT evaluation and tuning. \nKendall's \u03c4: Measurement of association between two things.",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "7": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2210.15615\" target=\"_blank\" rel=\"noopener noreferrer\">ACES: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics</a><br><b>Answer: </b> NA",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "8": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2109.05016\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Machine Translation Quality and Post-Editing Performance</a><br><b>Answer: </b> NA",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "9": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2107.10821\" target=\"_blank\" rel=\"noopener noreferrer\">To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation</a><br><b>Answer: </b> String-based metrics and metrics using pretrained models. String-based methods compare the coverage of various substrings between the human reference and MT output texts. Metrics using pretrained models use pretrained models to compare the translation quality of a pair of machine translation systems.",
                        "InputHeight": 29,
                        "InputWidth": 407
                    },
                    "10": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2109.07740\" target=\"_blank\" rel=\"noopener noreferrer\">Scaling Laws for Neural Machine Translation</a><br><b>Answer: </b> NA",
                        "InputHeight": 29,
                        "InputWidth": 407
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID26",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "OFF",
                        "Type": null
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 11,
                "NextAnswerId": 4,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID32",
            "SecondaryAttribute": "what was the baseline?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> what was the baseline?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-what was the baseline?",
                "QuestionID": "QID32",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "what was the baseline?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1708.05521.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1809.08935.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Lexical Bias In Essay Level Prediction</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.04491.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1804.07445.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Sentence Simplification with Memory-Augmented Neural Networks</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID33",
            "SecondaryAttribute": "what was the baseline?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> what was the baseline?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "Weka baseline"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "Unanswerable"
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "Unanswerable"
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "Hybrid \u2013 the current state-of-the-art"
                        }
                    }
                },
                "DataExportTag": "Task2-what was the baseline?",
                "QuestionID": "QID33",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "what was the baseline?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1708.05521.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity</a><br><b>Answer: </b>Weka baseline<br><strong>Evidence:</strong><br /><br />In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the <b>Weka baseline</b> BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings.<br /><br />In this paper we introduced an intra-sentence attention RNN for the of emotion intensity, which we developed for the WASSA-2017 Shared Task on Emotion Intensity. Our model does not make use of external information except for pre-trained embeddings and is able to outperform the <b>Weka baseline</b> for the development set, but not in the test set. In the shared task, it obtained the 13th place among 22 competitors.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1809.08935.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Lexical Bias In Essay Level Prediction</a><br><b>Answer: </b>Unanswerable<br><strong>Evidence:</strong><br />The question does not have an answer based on the given document.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.04491.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis</a><br><b>Answer: </b>Unanswerable<br><strong>Evidence:</strong><br />The question does not have an answer based on the given document.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1804.07445.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Sentence Simplification with Memory-Augmented Neural Networks</a><br><b>Answer: </b>Hybrid \u2013 the current state-of-the-art<br><strong>Evidence:</strong><br /><br />On WikiSmall, <b>Hybrid \u2013 the current state-of-the-art</b> \u2013 achieved best BLEU (53.94) and SARI (30.46) scores. Among neural models, NseLstm-B yielded the highest BLEU score (53.42), while NseLstm-S performed best on SARI (29.75). On WikiLarge, again, NseLstm-B had the highest BLEU score of 92.02. Sbmt-Sari \u2013 that was trained on a huge corpus of 106M sentence pairs and 2B words \u2013 scored highest on SARI with 39.96, followed by Dress-Ls (37.27), Dress (37.08), and NseLstm-S (36.88).",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID32",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID34",
            "SecondaryAttribute": "what dataset was used?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> what dataset was used?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-what dataset was used?",
                "QuestionID": "QID34",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "what dataset was used?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1708.05521.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1906.08871.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Advancing Speech Recognition With No Speech Or With Noisy Speech</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.09749.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">KeyVec: Key-semantics Preserving Document Representations</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.04491.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID35",
            "SecondaryAttribute": "what dataset was used?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> what dataset was used?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "The training, validation and test datasets provided for the shared task BIBREF5"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "Data set A and Data set B"
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "113 GB"
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "Bing Liu's dataset"
                        }
                    }
                },
                "DataExportTag": "Task2-what dataset was used?",
                "QuestionID": "QID35",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "what dataset was used?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1708.05521.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">EmoAtt at EmoInt-2017: Inner attention sentence embedding for Emotion Intensity</a><br><b>Answer: </b>The training, validation and test datasets provided for the shared task BIBREF5<br><strong>Evidence:</strong><br /><br />to test our model, we experiment using <b>the training, validation and test datasets provided for the shared task bibref5</b> , which include tweets for four emotions: joy, sadness, fear, and anger. these were annotated using best-worst scaling (bws) to obtain very reliable scores bibref6 .",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1906.08871.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Advancing Speech Recognition With No Speech Or With Noisy Speech</a><br><b>Answer: </b>Data set A and Data set B<br><strong>Evidence:</strong><br /><br />For data set A, we used data from first 8 subjects for training the model, remaining two subjects data for validation and test set respectively.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.09749.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">KeyVec: Key-semantics Preserving Document Representations</a><br><b>Answer: </b>113 GB<br><strong>Evidence:</strong><br /><br />Table TABREF15 presents P@10, MAP and MRR results of our KeyVec model and competing embedding methods in academic paper retrieval. word2vec averaging generates an embedding for a document by averaging the word2vec vectors of its constituent words. In the experiment, we used two different versions of word2vec: one from public release, and the other one trained specifically on our own academic corpus (<b>113 GB</b>). From Table TABREF15 , we observe that as a document-embedding model, Paragraph Vector gave better retrieval results than word2vec averagings did. In contrast, our KeyVec outperforms all the competitors given its unique capability of capturing and embedding the key information of documents.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1709.04491.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Method for Aspect-Based Sentiment Annotation Using Rhetorical Analysis</a><br><b>Answer: </b>Bing Liu's dataset<br><strong>Evidence:</strong><br /><br />We used <b>Bing Liu's dataset</b> BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.<br /><br />In Table TABREF18 there are presented some examples of the results of our approach compared with the annotated data from <b>Bing Liu's dataset</b>. In the first sentence, the results of the analysis differ because we decided to treat only nouns or noun phrases as aspects, while annotators also accepted verbs. In some cases, such as sentences 2 or 4, our approach generated more valuable aspects than the annotators found, but in some cases, like sentence 5, we found fewer. This is possibly the result of our method of filtering valuable aspects - if some aspects were not frequent enough in the dataset, we can treat them as void. In cases where there is neither aspect nor sentiment in the dataset, such as sentence 6, we measure sentiment as well, as one of our analysis steps.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID34",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID36",
            "SecondaryAttribute": "What dataset do they use?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> What dataset do they use?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-What dataset do they use?",
                "QuestionID": "QID36",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "What dataset do they use?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1603.07252.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Summarization by Extracting Sentences and Words</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1610.03955.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Dialogue Session Segmentation by Embedding-Enhanced TextTiling</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1704.04451.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Optimizing Differentiable Relaxations of Coreference Evaluation Metrics</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1911.07620.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Exploiting Token and Path-based Representations of Code for Identifying Security-Relevant Commits</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID37",
            "SecondaryAttribute": "What dataset do they use?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> What dataset do they use?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "DUC 2002 document summarization corpus and DailyMail news highlights corpus"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms)."
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "The English portion of CoNLL 2012 data"
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them"
                        }
                    }
                },
                "DataExportTag": "Task2-What dataset do they use?",
                "QuestionID": "QID37",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "What dataset do they use?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1603.07252.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Summarization by Extracting Sentences and Words</a><br><b>Answer: </b>DUC 2002 document summarization corpus and DailyMail news highlights corpus<br><strong>Evidence:</strong><br /><br />We evaluate our models both automatically (in terms of Rouge) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing hand-engineered features and sophisticated linguistic constraints.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1610.03955.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Dialogue Session Segmentation by Embedding-Enhanced TextTiling</a><br><b>Answer: </b>The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).<br><strong>Evidence:</strong><br /><br />We also leveraged an unlabeled massive dataset of conversation utterances to train our word embeddings with \u201cvirtual sentences.\u201d <b>The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).</b>",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1704.04451.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Optimizing Differentiable Relaxations of Coreference Evaluation Metrics</a><br><b>Answer: </b>The English portion of CoNLL 2012 data<br><strong>Evidence:</strong><br /><br />we run experiments on <b>the english portion of conll 2012 data</b> bibref15 which consists of 3,492 documents in various domains and formats. the split provided in the conll 2012 shared task is used. in all our resolvers, we use not the original features of p15-1137 but their slight modification described in n16-1114 (section 6.1).",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1911.07620.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Exploiting Token and Path-based Representations of Code for Identifying Security-Relevant Commits</a><br><b>Answer: </b>Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them<br><strong>Evidence:</strong><br /><br />for training our classification models, we use a <b>manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source java projects mapped to commits fixing them</b>, provided by bibref23. these repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively. in order to minimize the occurrence of duplicate commits in two of these splits (such as in both training and test), commits from no repository belong to more than one split. however, 808 commits may not be sufficient to train deep learning models. hence, in order to answer rq4, we augment the training split with commits mined using regular expression matching on the commit messages from the same set of open-source java projects. this almost doubles the number of commits in the training split to 1493. we then repeat our experiments for the first three research questions on the augmented dataset, and evaluate our trained models on the same validation and test splits.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID36",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID38",
            "SecondaryAttribute": "Which dataset do they use?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> Which dataset do they use?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-Which dataset do they use?",
                "QuestionID": "QID38",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "Which dataset do they use?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1812.00382.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Improved and Robust Controversy Detection in General Web Pages Using Semantic Approaches under Large Scale Conditions</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1710.06923.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Adapting general-purpose speech recognition engine output for domain-specific natural language question answering</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1811.04791.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Multilingual and Unsupervised Subword Modeling for Zero-Resource Languages</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1808.00957.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">SWDE : A Sub-Word And Document Embedding Based Engine for Clickbait Detection</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID39",
            "SecondaryAttribute": "Which dataset do they use?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> Which dataset do they use?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "Clueweb09 derived dataset and Wikipedia crawl data"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013"
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus"
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "BIBREF4 crowdsourced the annotation of 19538 tweets they had curated"
                        }
                    }
                },
                "DataExportTag": "Task2-Which dataset do they use?",
                "QuestionID": "QID39",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "Which dataset do they use?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1812.00382.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Improved and Robust Controversy Detection in General Web Pages Using Semantic Approaches under Large Scale Conditions</a><br><b>Answer: </b>Clueweb09 derived dataset and Wikipedia crawl data<br><strong>Evidence:</strong><br /><br />We use the Clueweb09 derived dataset of BIBREF0 for baseline comparison. For cross-temporal, cross-topic and cross-domain training & evaluation, we generate a new dataset based on Wikipedia crawl data. This dataset is gathered by using Wikipedia's `List of Contoversial articles' overview page of 2018 (time of writing) and 2009 (for comparison with baselines) . Using this as a `seed' set of controversial articles, we iteratively crawl the `See also', `References' and `External links' hyperlinks up to two hops from the seed list. The negative seed pages (i.e. non controversial) are gathered by using the random article endpoint. The snowball-sample approach includes general, non-Wikipedia, pages that are referred to from Wikipedia pages. The dataset thus extends beyond just the encyclopedia genre of texts. Labels are assumed to propagate: a page linked from a controversial issue is assumed to be controversial. The resulting dataset statistics are summarized in Table TABREF7 .",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1710.06923.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Adapting general-purpose speech recognition engine output for domain-specific natural language question answering</a><br><b>Answer: </b>U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013<br><strong>Evidence:</strong><br /><br />We present the results of our experiments with both the Evo-Devo and the Machine Learning mechanisms described earlier using the <b>U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013</b> BIBREF12 .",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1811.04791.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Multilingual and Unsupervised Subword Modeling for Zero-Resource Languages</a><br><b>Answer: </b>GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus<br><strong>Evidence:</strong><br /><br />We use the GlobalPhone corpus of speech read from news articles BIBREF20 . We chose 6 languages from different language families as zero-resource languages on which we evaluate the new feature representations. That means our models do not have any access to the transcriptions of the training data, although transcriptions still need to be available to run the evaluation. The selected languages and dataset sizes are shown in Table TABREF8 . Each GlobalPhone language has recordings from around 100 speakers, with 80% of these in the training sets and no speaker overlap between training, development, and test sets.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1808.00957.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">SWDE : A Sub-Word And Document Embedding Based Engine for Clickbait Detection</a><br><b>Answer: </b>BIBREF4 crowdsourced the annotation of 19538 tweets they had curated<br><strong>Evidence:</strong><br /><br /> <b>BIBREF4 crowdsourced the annotation of 19538 tweets they had curated</b>, into various levels of their clickbait-y nature. These tweets contained the title and text of the article and also included supplementary information such as target description, target keywords and linked images. We trained our model over 17000 records in the described dataset and test it over 2538 disjoint instances from the same. We performed our experiments with the aim of increasing the accuracy and F1 score of the model. Other metrics like mean squared error (MSE) were also considered.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID38",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID40",
            "SecondaryAttribute": "what are the baselines?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> what are the baselines?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-what are the baselines?",
                "QuestionID": "QID40",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "what are the baselines?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1810.02268.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in Neural Machine Translation</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1908.02284.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Two-stage Training for Chinese Dialect Recognition</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1908.07491.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Controversy in Context</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1802.02614.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Enhance word representation for out-of-vocabulary on Ubuntu dialogue corpus</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID41",
            "SecondaryAttribute": "what are the baselines?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> what are the baselines?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "a one-stage RNN system"
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "The methodology suggested by BIBREF1, BIBREF4"
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "RNN, CNN, LSTM, BiLSTM, Dual-Encoder"
                        }
                    }
                },
                "DataExportTag": "Task2-what are the baselines?",
                "QuestionID": "QID41",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "what are the baselines?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1810.02268.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in Neural Machine Translation</a><br><b>Answer: </b>a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline<br><strong>Evidence:</strong><br /><br />baseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus. It operates on the sentence level and does not see any additional context. The input and output embeddings of the decoder are tied, encoder embeddings are not.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1908.02284.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Two-stage Training for Chinese Dialect Recognition</a><br><b>Answer: </b>a one-stage RNN system<br><strong>Evidence:</strong><br /><br />The baseline we use for comparison is <b>a one-stage RNN system</b>, the RNN structure is the same as the last stage containing 2-layer BLSTM and directly trained to recognize dialect category. In the process of evaluation, we compute the accuracy of the two sub-tasks and the whole test set to evaluate the performance of each system.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1908.07491.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Controversy in Context</a><br><b>Answer: </b>The methodology suggested by BIBREF1, BIBREF4<br><strong>Evidence:</strong><br /><br />Naive Bayes (NB) Estimator: A Naive Bayes model was learned, with a bag-of-words feature set, using the word counts in the sentences of our training data \u2013 the contexts of the controversial and non-controversial concepts. The controversiality score of a concept $c$ for its occurrence in a sentence $s$, is taken as the posterior probability (according to the NB model) of $s$ to contain a controversial concept, given the words of $s$ excluding $c$, and taking a prior of $0.5$ for controversiality (as is the case in the datasets). The controversiality score of $c$ is then defined as the average score over all sentences referencing $c$.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1802.02614.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Enhance word representation for out-of-vocabulary on Ubuntu dialogue corpus</a><br><b>Answer: </b>RNN, CNN, LSTM, BiLSTM, Dual-Encoder<br><strong>Evidence:</strong><br /><br />On Douban conversation corpus, FastText BIBREF7 pre-trained Chinese embedding vectors are used in ESIM + enhanced word vector whereas word2vec generated on training set is used in baseline model (ESIM). It can be seen from table TABREF23 that character embedding enhances the performance of original ESIM. Enhanced Word representation in algorithm SECREF12 improves the performance further and has shown that the proposed method is effective. Most models (<b>RNN, CNN, LSTM, BiLSTM, Dual-Encoder</b>) which encode the whole context (or response) into compact vectors before matching do not perform well. INLINEFORM0 directly models sequential structure of multi utterances in context and achieves good performance whereas ESIM implicitly makes use of end-of-utterance(__eou__) and end-of-turn (__eot__) token tags as shown in subsection SECREF41 .",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID40",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID42",
            "SecondaryAttribute": "Which datasets do they evaluate on?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> Which datasets do they evaluate on?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-Which datasets do they evaluate on?",
                "QuestionID": "QID42",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "Which datasets do they evaluate on?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1909.12079.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Improving Fine-grained Entity Typing with Entity Linking</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1603.09405.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1809.03680.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Learning Scripts as Hidden Markov Models</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1907.12984.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">DuTongChuan: Context-aware Translation Model for Simultaneous Interpreting</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID43",
            "SecondaryAttribute": "Which datasets do they evaluate on?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> Which datasets do they evaluate on?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "FIGER (GOLD) and BBN"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "SICK (Sentences Involving Compositional Knowledge) dataset"
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types."
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus"
                        }
                    }
                },
                "DataExportTag": "Task2-Which datasets do they evaluate on?",
                "QuestionID": "QID43",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "Which datasets do they evaluate on?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1909.12079.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Improving Fine-grained Entity Typing with Entity Linking</a><br><b>Answer: </b>FIGER (GOLD) and BBN<br><strong>Evidence:</strong><br /><br />We conduct experiments on two commonly used FET datasets. Experimental results show that introducing information obtained through entity linking and having a deep neural model both helps to improve FET performance. Our model achieves more than 5% absolute strict accuracy improvement over the state of the art on both datasets.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1603.09405.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding</a><br><b>Answer: </b>SICK (Sentences Involving Compositional Knowledge) dataset<br><strong>Evidence:</strong><br /><br />We selected two related sentence relation modeling tasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 ( very related); and textual entailment task, which determines whether the truth of a text entails the truth of another text called hypothesis. We use standard <b>SICK (Sentences Involving Compositional Knowledge) dataset</b> for evaluation. It consists of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1809.03680.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Learning Scripts as Hidden Markov Models</a><br><b>Answer: </b>The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types.<br><strong>Evidence:</strong><br /><br />In this paper we address the following problem. Given a set of narrative texts, each of which describes a stereotypical event sequence drawn from a fixed but unknown distribution, learn the structure and parameters of a Left-to-Right HMM model that best captures the distribution of the event sequences. We evaluate the algorithm on natural datasets by how well the learned HMM can predict observations removed from the test sequences.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1907.12984.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">DuTongChuan: Context-aware Translation Model for Simultaneous Interpreting</a><br><b>Answer: </b>NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus<br><strong>Evidence:</strong><br /><br />To our knowledge, almost all of the previous related work on simultaneous translation evaluate their models upon the clean testing data without ASR errors and with explicit sentence boundaries annotated by human translators. Certainly, testing data with real ASR errors and without explicit sentence boundaries is beneficial to evaluate the robustness of translation models. To this end, we perform experiments on our proposed BSTC dataset.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID42",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID44",
            "SecondaryAttribute": "what were the evaluation metrics?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> what were the evaluation metrics?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-what were the evaluation metrics?",
                "QuestionID": "QID44",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "what were the evaluation metrics?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1809.08935.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Lexical Bias In Essay Level Prediction</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1705.02023.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Senti17 at SemEval-2017 Task 4: Ten Convolutional Neural Network Voters for Tweet Polarity Classification</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1805.04558.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">NRC-Canada at SMM4H Shared Task: Classifying Tweets Mentioning Adverse Drug Reactions and Medication Intake</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1805.09960.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Phrase Table as Recommendation Memory for Neural Machine Translation</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID45",
            "SecondaryAttribute": "what were the evaluation metrics?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> what were the evaluation metrics?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "cost measure that uses the confusion matrix of the prediction and prior knowledge"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "macro-average recall"
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0"
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)"
                        }
                    }
                },
                "DataExportTag": "Task2-what were the evaluation metrics?",
                "QuestionID": "QID45",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "what were the evaluation metrics?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1809.08935.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Lexical Bias In Essay Level Prediction</a><br><b>Answer: </b>cost measure that uses the confusion matrix of the prediction and prior knowledge<br><strong>Evidence:</strong><br /><br />In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a <b>cost measure that uses the confusion matrix of the prediction and prior knowledge</b> in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0 ",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1705.02023.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Senti17 at SemEval-2017 Task 4: Ten Convolutional Neural Network Voters for Tweet Polarity Classification</a><br><b>Answer: </b>macro-average recall<br><strong>Evidence:</strong><br /><br />Official ranking: Our system is ranked fourth over 38 systems in terms of <b>macro-average recall</b>. Table 4 shows the results of our system on the test set of 2016 and 2017.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1805.04558.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">NRC-Canada at SMM4H Shared Task: Classifying Tweets Mentioning Adverse Drug Reactions and Medication Intake</a><br><b>Answer: </b>F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0<br><strong>Evidence:</strong><br /><br />The official evaluation metric was the F-score for class 1 (ADR): INLINEFORM0 ",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1805.09960.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Phrase Table as Recommendation Memory for Neural Machine Translation</a><br><b>Answer: </b>case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)<br><strong>Evidence:</strong><br /><br />Besides the BLEU score, we also conduct a subjective evaluation to validate the benefit of incorporating a phrase table in NMT. The subjective evaluation is conducted on CH-EN translation. As our method tries to solve the problem that NMT system cannot reflect the true meaning of the source sentence, the criterion of the subjective evaluation is the faithfulness of translation results. Specifically, five human evaluators, who are native Chinese and expert in English, are asked to evaluate the translations of 500 source sentences randomly sampled from the test sets without knowing which system a translation is selected from. The score ranges from 0 to 5. For a translation result, the higher its score is, the more faithful it is. Table 2 shows the average results of five subjective evaluations on CH-EN translation. As shown in Table 2\uff0cthe faithfulness of translation results produced by our method is better than Arthur and baseline NMT system.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID44",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID46",
            "SecondaryAttribute": "How long is their dataset?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> How long is their dataset?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-How long is their dataset?",
                "QuestionID": "QID46",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "How long is their dataset?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1703.04009.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Automated Hate Speech Detection and the Problem of Offensive Language</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1802.09059.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1811.01183.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Unsupervised Identification of Study Descriptors in Toxicology Research: An Experimental Study</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2003.12660.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Towards Supervised and Unsupervised Neural Machine Translation Baselines for Nigerian Pidgin</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID47",
            "SecondaryAttribute": "How long is their dataset?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> How long is their dataset?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "85.4 million tweets"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "Unanswerable"
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "624 full text documents"
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "20214 sentence pairs"
                        }
                    }
                },
                "DataExportTag": "Task2-How long is their dataset?",
                "QuestionID": "QID47",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "How long is their dataset?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1703.04009.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Automated Hate Speech Detection and the Problem of Offensive Language</a><br><b>Answer: </b>85.4 million tweets<br><strong>Evidence:</strong><br /><br />We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of <b>85.4 million tweets</b>. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1802.09059.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data</a><br><b>Answer: </b>Unanswerable<br><strong>Evidence:</strong><br />The question does not have an answer based on the given document.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1811.01183.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Unsupervised Identification of Study Descriptors in Toxicology Research: An Experimental Study</a><br><b>Answer: </b>624 full text documents<br><strong>Evidence:</strong><br /><br />The version of the database which contains both GL and non-GL studies consists of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays. Specifically, each entry in the database describes one study, and studies are linked to publications using PubMed reference numbers (PMIDs). Each study is assigned seven 0/1 labels \u2013 one for each of the minimum criteria and one for the overall GL/non-GL label. The database also contains more detailed subcategories for each label (for example \u201cspecies\u201d label for MC 1) which were not used in this study. The publication PDFs were provided to us by the database creators. We have used the Grobid library to convert the PDF files into structured text. After removing documents with missing PDF files and documents which were not converted successfully, we were left with <b>624 full text documents</b>.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2003.12660.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Towards Supervised and Unsupervised Neural Machine Translation Baselines for Nigerian Pidgin</a><br><b>Answer: </b>20214 sentence pairs<br><strong>Evidence:</strong><br /><br />The dataset used for the supervised was obtained from the JW300 large-scale, parallel corpus for Machine Translation (MT) by BIBREF8. The train set contained <b>20214 sentence pairs</b>, while the validation contained 1000 sentence pairs. Both the supervised and unsupervised models were evaluated on a test set of 2101 sentences preprocessed by the Masakhane group. The model with the highest test BLEU score is selected as the best.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID46",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID48",
            "SecondaryAttribute": "Do they report results only on English data?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> Do they report results only on English data?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-Do they report results only on English data?",
                "QuestionID": "QID48",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "Do they report results only on English data?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1911.12559.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1905.07562.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Human-like machine thinking: Language guided imagination</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1910.07973.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Universal Text Representation from BERT: An Empirical Study</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1909.01720.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Different Absorption from the Same Sharing: Sifted Multi-task Learning for Fake News Detection</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID49",
            "SecondaryAttribute": "Do they report results only on English data?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> Do they report results only on English data?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "Yes"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "Yes"
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "Yes"
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "Yes"
                        }
                    }
                },
                "DataExportTag": "Task2-Do they report results only on English data?",
                "QuestionID": "QID49",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "Do they report results only on English data?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1911.12559.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents</a><br><b>Answer: </b>Yes<br><strong>Evidence:</strong><br /><br />Model performances for each dataset are reported in Table . Extractive baselines show the best results for KPCrowd and DUC-2001 which is not surprising given that these datasets exhibit the lowest ratio of absent keyphrases. Neural-based models obtain the greatest performance, but only for the dataset on which they were trained. We therefore see that these models do not generalize well across domains, confirming previous preliminary findings BIBREF2 and exacerbating the need for further research on this topic. Interestingly, CopyNews outperforms the other models on JPTimes and achieves very low scores for KPCrowd and DUC-2001, although all these datasets are from the same domain. This emphasizes the differences that exist between the reader- and editor-assigned gold standard. The score difference may be explained by the ratio of absent keyphrases that differs greatly between the reader-annotated datasets and JPTimes (see Table ), and thus question the use of these rather extractive datasets for evaluating keyphrase generation.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1905.07562.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Human-like machine thinking: Language guided imagination</a><br><b>Answer: </b>Yes<br><strong>Evidence:</strong><br /><br />[2] Devlin, J., Chang, M. W., Lee, K. & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1910.07973.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Universal Text Representation from BERT: An Empirical Study</a><br><b>Answer: </b>Yes<br><strong>Evidence:</strong><br /><br />Results: The comparison between BERT embeddings and other models is presented in Table TABREF5. Overall, in-domain fine-tuned BERT delivers the best performance. We report new state-of-the-art results on WikiPassageQA ($33\\%$ improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1909.01720.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Different Absorption from the Same Sharing: Sifted Multi-task Learning for Fake News Detection</a><br><b>Answer: </b>Yes<br><strong>Evidence:</strong><br /><br />We perform experiments on RumourEval and PHEME datasets to evaluate the performance of our method and the baselines. The experimental results are shown in Table TABREF27. We gain the following observations:",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID48",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID50",
            "SecondaryAttribute": "What datasets are used?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> What datasets are used?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-What datasets are used?",
                "QuestionID": "QID50",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "What datasets are used?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2004.02363.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">BERT in Negotiations: Early Prediction of Buyer-Seller Negotiation Outcomes</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1603.03876.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Variational Neural Discourse Relation Recognizer</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1610.07149.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2002.04374.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Convolutional Neural Networks and a Transfer Learning Strategy to Classify Parkinson's Disease from Speech in Three Different Languages</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID51",
            "SecondaryAttribute": "What datasets are used?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> What datasets are used?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "Craigslist Bargaining dataset (CB)"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "PDTB 2.0 and Chatbot NLU Corpus with STT error"
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs"
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "Spanish, German, and Czech"
                        }
                    }
                },
                "DataExportTag": "Task2-What datasets are used?",
                "QuestionID": "QID51",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "What datasets are used?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2004.02363.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">BERT in Negotiations: Early Prediction of Buyer-Seller Negotiation Outcomes</a><br><b>Answer: </b>Craigslist Bargaining dataset (CB)<br><strong>Evidence:</strong><br /><br />Dataset: For our explorations, we use the <b>Craigslist Bargaining dataset (CB)</b> introduced by BIBREF4. Instead of focusing on the previously studied game environments BIBREF5, BIBREF6, the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist. The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table TABREF1). In total, 1402 product ad postings were scraped from Craigslist, belonging to six categories: phones, bikes, housing, furniture, car and electronics. Each ad posting contains details such as Product Title, Category Type and a Listing Price. Moreover, a secret target price is also pre-decided for the buyer. The final price after the agreement is called the Agreed Price, which we aim to predict.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1603.03876.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Variational Neural Discourse Relation Recognizer</a><br><b>Answer: </b>PDTB 2.0 and Chatbot NLU Corpus with STT error<br><strong>Evidence:</strong><br /><br />We used the largest hand-annotated discourse corpus PDTB 2.0 BIBREF12 (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work BIBREF6 , BIBREF13 , BIBREF14 , BIBREF9 , we used sections 2-20 as our training set, sections 21-22 as the test set. Sections 0-1 were used as the development set for hyperparameter optimization.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1610.07149.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems</a><br><b>Answer: </b>A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs<br><strong>Evidence:</strong><br /><br />For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs. For each query $q$ , we searched for a candidate reply $r^*$ by the retrieval component and obtained a tuple $\\langle q, r^*, r\\rangle $ . As a friendly reminder, $q$ and $r^*$ are the input of biseq2seq, whose output should approximate $r$ . We randomly selected 100k triples for validation and another 6,741 for testing. The train-val-test split remains the same for all competing models.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2002.04374.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Convolutional Neural Networks and a Transfer Learning Strategy to Classify Parkinson's Disease from Speech in Three Different Languages</a><br><b>Answer: </b>Spanish, German, and Czech<br><strong>Evidence:</strong><br /><br />Speech recordings of patients in three different languages are considered: <b>Spanish, German, and Czech</b>. All of the recordings were captured in noise controlled conditions. The speech signals were down-sampled to 16 kHz. The patients in the three datasets were evaluated by a neurologist expert according to the third section of the movement disorder society, unified Parkinson's disease rating scale (MDS-UPDRS-III) BIBREF16. Table TABREF5 summarizes the information about the patients and healthy speakers.<br /><br />Most of the studies in the literature to classify PD from speech are based on computing hand-crafted features and using classifiers such as support vector machines (SVMs) or K-nearest neighbors (KNN). For instance, in BIBREF3, the authors computed features related to perturbations of the fundamental frequency and amplitude of the speech signal to classify utterances from 20 PD patients and 20 HC subjects, Turkish speakers. Classifiers based on KNN and SVMs were considered, and accuracies of up to 75% were reported. Later, in BIBREF4 the authors proposed a phonation analysis based on several time frequency representations to assess tremor in the speech of PD patients. The extracted features were based on energy and entropy computed from time frequency representations. Several classifiers were used, including Gaussian mixture models (GMMs) and SVMs. Accuracies of up to 77% were reported in utterances of the PC-GITA database BIBREF5, formed with utterances from 50 PD patients and 50 HC subjects, Colombian Spanish native speakers. The authors from BIBREF6 computed features to model different articulation deficits in PD such as vowel quality, coordination of laryngeal and supra-laryngeal activity, precision of consonant articulation, tongue movement, occlusion weakening, and speech timing. The authors studied the rapid repetition of the syllables /pa-ta-ka/ pronounced by 24 Czech native speakers, and reported an accuracy of 88% discriminating between PD patients and HC speakers, using an SVM classifier. Additional articulation features were proposed in BIBREF7, where the authors modeled the difficulty of PD patients to start/stop the vocal fold vibration in continuous speech. The model was based on the energy content in the transitions between unvoiced and voiced segments. The authors classified PD patients and HC speakers with speech recordings in three different languages (<b>Spanish, German, and Czech</b>), and reported accuracies ranging from 80% to 94% depending on the language; however, the results were optimistic, since the hyper-parameters of the classifier were optimized based on the accuracy on the test set. Another articulation model was proposed in BIBREF8. The authors considered a forced alignment strategy to segment the different phonetic units in the speech utterances. The phonemes were segmented and grouped to train different GMMs. The classification was performed based on a threshold of the difference between the posterior probabilities from the models created for HC subjects and PD patients. The model was tested with Colombian Spanish utterances from the PC-GITA database BIBREF5 and with the Czech data from BIBREF9. The authors reported accuracies of up to 81% for the Spanish data, and of up to 94% for the Czech data.<br /><br />This study proposed the use of a transfer learning strategy based on fine-tuning to classify PD from speech in three different languages: <b>Spanish, German, and Czech</b>. The transfer learning among languages aimed to improve the accuracy when the models are initialized with utterances from a different language than the one used for the test set. Mel-scale spectrograms extracted from the transitions between voiced and unvoiced segments are used to train a CNN for each language. Then, the trained models are used to fine-tune a model to classify utterances in the remaining two languages.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID50",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID52",
            "SecondaryAttribute": "What are the baseline models?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> What are the baseline models?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-What are the baseline models?",
                "QuestionID": "QID52",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "What are the baseline models?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1908.11046.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Remedying BiLSTM-CNN Deficiency in Modeling Cross-Context for NER.</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1911.09753.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Reinforcing an Image Caption Generator Using Off-Line Human Feedback</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1911.10401.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">A Transformer-based approach to Irony and Sarcasm detection</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1910.02677.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Controllable Sentence Simplification</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID53",
            "SecondaryAttribute": "What are the baseline models?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> What are the baseline models?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "BiLSTM-CNN proposed by BIBREF1"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "MLE model and Baseline$+(t)$ with $t \\in [0,1]$"
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model"
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "Seq2Seq MT models"
                        }
                    }
                },
                "DataExportTag": "Task2-What are the baseline models?",
                "QuestionID": "QID53",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "What are the baseline models?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1908.11046.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Remedying BiLSTM-CNN Deficiency in Modeling Cross-Context for NER.</a><br><b>Answer: </b>BiLSTM-CNN proposed by BIBREF1<br><strong>Evidence:</strong><br /><br />Many have attempted tackling the NER task with LSTM-based sequence encoders BIBREF7, BIBREF0, BIBREF1, BIBREF8. Among these, the most sophisticated, state-of-the-art is the <b>BiLSTM-CNN proposed by BIBREF1</b>. They stack multiple layers of LSTM cells per direction and also use a CNN to compute character-level word vectors alongside pre-trained word vectors. This paper largely follows their work in constructing the Baseline-BiLSTM-CNN, including the selection of raw features, the CNN, and the multi-layer BiLSTM. A subtle difference is that they send the output of each direction through separate affine-softmax classifiers and then sum their probabilities, while this paper sum the scores from affine layers before computing softmax once. While not changing the modeling capacity regarded in this paper, the baseline model does perform better than their formulation.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1911.09753.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Reinforcing an Image Caption Generator Using Off-Line Human Feedback</a><br><b>Answer: </b>MLE model and Baseline$+(t)$ with $t \\in [0,1]$<br><strong>Evidence:</strong><br /><br />We first train an MLE model as our baseline, trained on the Conceptual Captions training split alone. We referred to this model as Baseline. For a baseline approach that utilizes (some of) the Caption-Quality data, we merge positively-rated captions from the Caption-Quality training split with the Conceptual Captions examples and finetune the baseline model. We call this model Baseline$+(t)$, where $t \\in [0,1]$ is the rating threshold for the included positive captions. We train models for two variants, $t\\in \\lbrace 0.5, 0.7\\rbrace $, which results in $\\sim $72K and $\\sim $51K additional (pseudo-)ground-truth captions, respectively. Note that the Baseline$+(t)$ approaches attempt to make use of the same additional dataset as our two reinforced models, OnPG and OffPG, but they need to exclude below-threshold captions due to the constraints in MLE.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1911.10401.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">A Transformer-based approach to Irony and Sarcasm detection</a><br><b>Answer: </b>ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model<br><strong>Evidence:</strong><br /><br />The results are summarized in the tables TABREF14-TABREF17; each table refers to the respective comparison study. All tables present the performance results of our proposed method (\u201cProposed\u201d) and contrast them to eight state-of-the-art baseline methodologies along with published results using the same dataset. Specifically, Table TABREF14 presents the results obtained using the ironic dataset used in SemEval-2018 Task 3.A, compared with recently published studies and two high performing teams from the respective SemEval shared task BIBREF98, BIBREF99. Tables TABREF15,TABREF16 summarize results obtained using Sarcastic datasets (Reddit SARC politics BIBREF97 and Riloff Twitter BIBREF96). Finally, Table TABREF17 compares the results from baseline models, from top two ranked task participants BIBREF68, BIBREF67, from our previous study with the DESC methodology BIBREF0 with the proposed RCNN-RoBERTa framework on a Sentiment Analysis task with figurative language, using the SemEval 2015 Task 11 dataset.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1910.02677.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Controllable Sentence Simplification</a><br><b>Answer: </b>Seq2Seq MT models<br><strong>Evidence:</strong><br /><br />Lately, SS has mostly been tackled using <b>Seq2Seq MT models</b> BIBREF14. Seq2Seq models were either used as-is BIBREF15 or combined with reinforcement learning thanks to a specific simplification reward BIBREF10, augmented with an external simplification database as a dynamic memory BIBREF16 or trained with multi-tasking on entailment and paraphrase generation BIBREF17.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID52",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID54",
            "SecondaryAttribute": "How long is the dataset?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 1: Answer the question given the Document.</u><br>\n    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content. \n    If the answer can not be found in the document then answer with \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> How long is the dataset?\n    ",
                "DefaultChoices": false,
                "DataExportTag": "Task1-How long is the dataset?",
                "QuestionID": "QID54",
                "QuestionType": "TE",
                "Selector": "FORM",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "QuestionDescriptionOption": "UseText"
                },
                "QuestionDescription": "How long is the dataset?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1802.09233.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">EiTAKA at SemEval-2018 Task 1: An Ensemble of N-Channels ConvNet and XGboost Regressors for Emotion Analysis of Tweets</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1611.02378.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">A Surrogate-based Generic Classifier for Chinese TV Series Reviews</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1605.04278.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Universal Dependencies for Learner English</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2001.11316.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Adversarial Training for Aspect-Based Sentiment Analysis with BERT</a>",
                        "InputHeight": 29,
                        "InputWidth": 470,
                        "TextEntry": "on"
                    }
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 4,
                "NextAnswerId": 1,
                "SearchSource": {
                    "AllowFreeResponse": "false"
                }
            }
        },
        {
            "SurveyID": "SV_8iyc6G8rZOzeZW6",
            "Element": "SQ",
            "PrimaryAttribute": "QID55",
            "SecondaryAttribute": "How long is the dataset?",
            "TertiaryAttribute": null,
            "Payload": {
                "QuestionText": "\n    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n    If the answer can not be found in the document, answer \"Unanswerable\".\n    <br><br>\n    <u>Question:</u> How long is the dataset?\n    ",
                "DefaultChoices": {
                    "1": {
                        "1": {
                            "Value": "12,284 English-language tweets and 6100 Arabic-language tweets"
                        }
                    },
                    "2": {
                        "1": {
                            "Value": "5000 for each TV series"
                        }
                    },
                    "3": {
                        "1": {
                            "Value": "5,124 sentences"
                        }
                    },
                    "4": {
                        "1": {
                            "Value": "Unanswerable"
                        }
                    }
                },
                "DataExportTag": "Task2-How long is the dataset?",
                "QuestionID": "QID55",
                "QuestionType": "Matrix",
                "Selector": "TE",
                "SubSelector": "Long",
                "DataVisibility": {
                    "Private": false,
                    "Hidden": false
                },
                "Configuration": {
                    "ChoiceColumnWidthPixels": 400
                },
                "QuestionDescription": "How long is the dataset?",
                "Choices": {
                    "1": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1802.09233.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">EiTAKA at SemEval-2018 Task 1: An Ensemble of N-Channels ConvNet and XGboost Regressors for Emotion Analysis of Tweets</a><br><b>Answer: </b>12,284 English-language tweets and 6100 Arabic-language tweets<br><strong>Evidence:</strong><br /><br />The system has been tested on <b>12,284 English-language tweets and 6100 Arabic-language tweets</b> provided by the organizers. The golden answers of all the test tweets were omitted by the organizers. The official evaluation results of our system are reported along with the top 10 systems and the baseline results in Table 2 and 3. Our system ranks 8th among 38 systems in the English-language tweets and ranks 2nd among 8 systems in the Arabic language tweets. The baselines 1, 2 and 3 stand for case when the system classify all the tweets as positive, negative and neutral respectively.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "2": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1611.02378.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">A Surrogate-based Generic Classifier for Chinese TV Series Reviews</a><br><b>Answer: </b>5000 for each TV series<br><strong>Evidence:</strong><br /><br />The purpose of this research is to classify each review into one of the above 8 categories. In order to build reasonable classifiers, first we need to obtain a labeled dataset. Each of the TV series reviews was labeled by at least two individuals, and only those reviews with the same assigned label were selected in our training and testing data. This approach ensures that reviews with human biases are filtered out. As a result, we have <b>5000 for each TV series</b> that matches the selection criteria.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "3": {
                        "Display": "<a href=\"https://arxiv.org/pdf/1605.04278.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Universal Dependencies for Learner English</a><br><b>Answer: </b>5,124 sentences<br><strong>Evidence:</strong><br /><br />To address this shortcoming, we present the Treebank of Learner English (TLE), a first of its kind resource for non-native English, containing <b>5,124 sentences</b> manually annotated with POS tags and dependency trees. The TLE sentences are drawn from the FCE dataset BIBREF1 , and authored by English learners from 10 different native language backgrounds. The treebank uses the Universal Dependencies (UD) formalism BIBREF2 , BIBREF3 , which provides a unified annotation framework across different languages and is geared towards multilingual NLP BIBREF4 . This characteristic allows our treebank to support computational analysis of ESL using not only English based but also multilingual approaches which seek to relate ESL phenomena to native language syntax.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    },
                    "4": {
                        "Display": "<a href=\"https://arxiv.org/pdf/2001.11316.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Adversarial Training for Aspect-Based Sentiment Analysis with BERT</a><br><b>Answer: </b>Unanswerable<br><strong>Evidence:</strong><br />The question does not have an answer based on the given document.",
                        "InputHeight": 29,
                        "InputWidth": 470
                    }
                },
                "DisplayLogic": {
                    "0": {
                        "0": {
                            "ChoiceLocator": "q://QID1/ChoiceTextEntryValue/1",
                            "Description": "<span class=\"ConjDesc\">If</span> <span class=\"QuestionDesc\">What 3D scene datasets are used?</span> <span class=\"LeftOpDesc\">Text Response</span> <span class=\"OpDesc\">Is Not Displayed</span> ",
                            "LeftOperand": "q://QID1/ChoiceDisplayed/1",
                            "LogicType": "Question",
                            "Operator": "NotDisplayed",
                            "QuestionID": "QID54",
                            "QuestionIDFromLocator": "QID1",
                            "QuestionIsInLoop": "no",
                            "Type": "Expression"
                        },
                        "Type": "If"
                    },
                    "Type": "BooleanExpression",
                    "inPage": false
                },
                "ChoiceOrder": [
                    1,
                    2,
                    3,
                    4
                ],
                "Validation": {
                    "Settings": {
                        "ForceResponse": "ON",
                        "ForceResponseType": "ON",
                        "Type": "None"
                    }
                },
                "GradingData": [],
                "Language": [],
                "NextChoiceId": 5,
                "NextAnswerId": 2,
                "Answers": {
                    "1": {
                        "Display": " "
                    }
                },
                "AnswerOrder": [
                    1
                ],
                "ChoiceDataExportTags": false
            }
        }
    ]
}