{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# open csv file with all the answers (for all queries) from all the chunks\n",
    "with open(\"Productivity Experiment - e3cb7330.csv\", \"r\") as f:\n",
    "    df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TREC, SQuAD, NewsQA, SearchQA, QuAC, HotpotQA, NarrativeQA MS MARCO, TriviaQA, ELI5 The first large-scale long form question answering dataset of open-ended queries with explanatory multi-sentence answers.\n",
      "5 5\n",
      "ELI5 Explain Anything Like I'm Five (https://yjernite.github.io/lfqa.html) QQP, AMT evaluation KILT Natural Questions\n",
      "6 6\n",
      "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension Natural Questions, MS MARCO Karpukhin et al. [32] HotpotQA dataset Fever, zsRE, AY2, T-REx, WoW\n",
      "5 5\n",
      "MS MARCO NaturalQuestions (Kwiatkowski et al., 2019) ELI5 TrivaQA, kwiatkowski2019naturaluestion, HotpotQA, Fever, zsRE, AY2, T-REx, WoW.\n",
      "5 5\n",
      "Natural Questions (Kwiatkowski et al., 2019), ASQA (Stelmakh et al., 2022), AQuaMuSe (Kulkarni et al., 2020) ELI5 (Fan et al., 2019), AQuaMuSe\n",
      "7 7\n",
      "NarrativeQA QuALITY QuALITY-HARD RACE TriviaQA, SearchQA, HotpotQA, QAngaroo, ComplexWebQuestions\n",
      "5 5\n",
      "TREC QA, WikiQA, SelQA WIKIREADING, WR-LONG, WIKISUGGEST NewsQA, Parallel-Hierarchical Model, Building a Question Answering Test Collection\n",
      "4 4\n",
      "TREC answer specifications TREC question sets. METEOR TREC2004, TREC2005, TREC2006\n",
      "4 4\n",
      "SQuAD, NewsQA, MS MARCO, Narrative QA, RACE, LearningQ, Google Natural Questions\n",
      "6 6\n",
      "WikiTableQuestions WikiTables SQuAD 2.0, HotpotQA\n",
      "3 3\n",
      "chrF, COMET, BERTScore - chrF is a character-level metric that uses n-grams to compare the translation to the reference; COMET is a metric that uses a neural network to compare the translation to the reference; BERTScore is a metric that uses a BERT model to compare the translation to the reference. BLEU (string-matching metric that compares the token-level n-grams of the hypothesis with the reference translation), WMT (annual shared task on developing MT models for several categories in machine translation, human evaluation of the translated outputs from the participating machine translation models is often used to determine the best-performing MT system). COMET-DA uses direct assessments from 2017 to 2019 as its training data. COMET-MQM uses direct assessments from 2017 to 2021 as its training data and is fine-tuned with MQM data from Freitag et al. (2021a). UniTE metrics (Wan et al., 2022) is a neural translation metric that proposes a multi-task setup for the three strategies of evaluation - source-hypothesis, source-hypothesis-reference, and reference-hypothesis in a single model. The pre-training stage involves training the model with synthetic data constructed using a subset of WMT evaluation data. Fine-tuning uses novel attention mechanisms and aggregate loss functions to facilitate the multi-task setup. All the above reference-based metrics have their corresponding reference-free versions which exclude encoding the reference. Multidimensional Quality Metrics (MQM): A framework for declaring and describing translation quality metrics. Roberta: A robustly optimized BERT pretraining approach. Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges. An automatic evaluation of the WMT22 general machine translation task. COMET-MQM: a scoring scheme with expert annotators for evaluating MT outputs. False negative rate of 0.796 in the round-trip translation setup compared to 0.097 when the human reference is used instead.\n",
      "8 8\n",
      "They use token-level negative log-likelihood (NLL) and BLEU as the reward, as well as the state-of-the-art model-based metric, BLEURT. NLL is a token-level metric that does not guarantee that the generated translations will be optimized for a selected sequence-level evaluation metric. BLEU is a reward used to directly improve the metric, but it has a low correlation with human judgment when dealing with state-of-the-art models. BLEURT is a model-based metric that has a much higher human correlation. Smoothed BLEU and BLEURT. Smoothed BLEU is a token-level negative log-likelihood metric that does not guarantee that the generated translations will be optimized for a selected sequence-level evaluation metric. BLEURT is a model-based evaluation metric that has a much higher human correlation. Meteor (Banerjee and Lavie, 2005): a metric that combines unigram precision and recall with a modified n-gram precision. RIBES (Isozaki et al., 2010): a metric that evaluates the quality of translations by comparing them to a set of reference translations. SBLEU (Smoothed BLEU) is a modified version of BLEU that is more robust to small changes in the translation. Kendal's τ is a metric used to measure the correlation between two variables.\n",
      "7 7\n",
      "Otem and Utem. Otem is a quantitative metric based on the proportion of mismatched n-grams between gold reference and system translation to evaluate the system performance in terms of over-translation. Utem is a quantitative metric based on the proportion of mismatched n-grams between gold reference and system translation to evaluate the system performance in terms of under-translation. Reval: A simple and effective machine translation evaluation metric based on recurrent neural networks. Linguistic features for automatic evaluation of heterogenous MT systems. Pairwise neural machine translation evaluation. Neural Machine Translation metrics used are Red (Reference Dependency based MT Evaluation Metric) and GRU-Gated Attention Model. Red is a metric that evaluates the quality of machine translation by comparing the syntactic dependencies of the source and target sentences. GRU-Gated Attention Model is a model that uses a gated recurrent unit (GRU) to control the attention mechanism in a neural machine translation system. Neural Machine Translation metrics used are METEOR (an automatic metric for MT evaluation with improved correlation with human judgments) Towards Automatic Error Analysis of Machine Translation Output: A metric which uses a combination of precision, recall, and F-measure to measure the quality of a machine translation system. \n",
      "Neural Machine Translation of Rare Words with Subword Units: A metric which uses subword units to measure the quality of a machine translation system. Minimum Risk Training (MRT): a metric that measures the risk of a translation model by taking into account the cost of incorrect translations. Translation Edit Rate (TER): a metric that measures the number of edits required to transform a machine-generated translation into a human-generated translation. Bidirectional Recurrent Neural Networks (BRNN): a type of neural network that uses both forward and backward recurrent connections to process input sequences.\n",
      "9 9\n",
      "BERT: Pre-training of deep bidirectional transformers for language understanding. WMT21 Metrics Shared Task: Evaluating metrics with expert-based human evaluations on TED and News domain. BLEU (n-grams macro), WER (Levenshtein distance macro), TER (edit distance macro), PER (edit distance macro), chrF (character n-grams micro), chrF+ (character n-grams micro), BEER (char. n-grams, trees micro), CharacTER (char. edit distance micro), BERTScore (neural representations micro), MoverScore (neural representations micro).\n",
      "5 5\n",
      "They use word translation probabilities (obtained from the softmax), extend each partial translation with subsequent word predictions and accumulate these scores. They also use beam size and sentence level model scores normalized by length of the output. Kalchbrenner and Blunsom (2013) use a convolutional neural network to encode the source sentence and then generate the target sentence by reversing the process. Gehring et al. (2017) use multiple convolutional layers in the encoder and the decoder that do not reduce the length of the encoded sequence but incorporate wider context with each layer. Vaswani et al. (2017) replace the recurrent neural networks used in attentional sequence-to-sequence models with multiple self-attention layers, both for the encoder as well as the decoder. Bojar et al. (2016) and Wu et al. (2017) use public benchmarks to measure the performance of Neural Machine Translation. BLEU (Bilingual Evaluation Understudy) is a metric used to measure the quality of text which has been machine-translated from one natural language to another. It compares an automatically produced translation against a professional human translation and assigns a score based on how many words in the machine-generated translation match words in the human-generated translation. Match Score: Checks for each output if the aligned input word according to fast-align is indeed the input word that received the highest attention probability. \n",
      "Probability Mass Score: Sums up the probability mass given to each alignment point obtained from fast-align. Coverage Models: The vector that accumulates coverage of input words is used to inform the attention model. Over-Generation: The sum of coverage of input words is compared to 1, and if it is greater than 1, the difference is the over-generation metric. Under-Generation: The sum of coverage of input words is compared to 1, and if it is less than 1, the difference is the under-generation metric.\n",
      "6 6\n",
      "[24] Recurrent Neural Network based Language Model - a model based on complex recurrent neural networks in an encoder-decoder configuration.\n",
      "[25] Fast and Robust Neural Network Joint Models - a model based on neural networks in a joint model configuration.\n",
      "[26] Recurrent Continuous Translation Models - a model based on recurrent neural networks for translation.\n",
      "[27] Sequence to Sequence Learning with Neural Networks - a model based on neural networks for sequence to sequence learning.\n",
      "[28] Multi-level Evaluation for Machine Translation - a model based on evaluation metrics for machine translation.\n",
      "[29] Representation Based Translation Evaluation Metrics - a model based on representation-based evaluation metrics for machine translation.\n",
      "[30] Machine Translation Evaluation using Recurrent Neural Networks - a model based on recurrent neural networks for machine translation evaluation. ReVal. ReVal is a neural evaluation metric that performs significantly better at the segment level. NNRK (Neural Network ReranKing) - uses a pairwise fashion by subtracting the vectors for the two competing translations and then training to predict +1 or -1. At test time, a vector for a single translation is used, which is equivalent to subtracting a zero vector from it, i.e., to predicting whether the translation would win against an empty translation, and by what margin. NNRK (Absolute) - uses absolute scores instead of pairwise comparisons. The metrics used are BLEU, ROUGE, and METEOR. BLEU stands for Bilingual Evaluation Understudy and is a precision-based metric that compares an automatically generated translation against a set of reference translations. ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation and is a recall-based metric that measures the overlap between the automatically generated translation and the reference translations. METEOR stands for Metric for Evaluation of Translation with Explicit Ordering and is a harmonic mean of precision and recall that also takes into account the order of words in the translation. NIST: National Institute of Standards and Technology, a score that measures the similarity between the predicted and reference translations. TER: Translation Error Rate, a score that measures the number of edits needed to transform the predicted translation into the reference translation. Meteor: Metric for Evaluation of Translation with Explicit Ordering, a score that measures the similarity between the predicted and reference translations. Dropout: A technique used to prevent neural networks from overfitting by randomly dropping out neurons during training. Long Short-Term Memory (LSTM): A recurrent neural network architecture that uses gated cells to store and access information over long periods of time. Bidirectional Recurrent Neural Networks (BRNN): A type of recurrent neural network that processes data in both forward and backward directions. Spede: A probabilistic edit distance metric for Machine Translation evaluation. Amber: A Machine Translation evaluation metric. Deep-Syntactic Metric: A metric for Machine Translation evaluation and tuning. Spearman's Correlation Coefficient: A measure of the strength of the association between two variables. NNRKMean: multi-layer NN, mean vector; NNRKZero: multi-layer NN, zero vector; DiscoTK: Best on the WMT12 dataset; SEMPOS: 1st at the WMT12 competition; AMBER: 2nd at the WMT12 competition; Meteor: 3rd at the WMT12 competition.\n",
      "8 8\n",
      "UNITE-SRC, UNITE-REF, and UNITE: metric approach where the model-based metrics can possess the ability of evaluating translation outputs following all three evaluation scenarios, i.e. source-only, reference-only, and source-reference-combined. COMET-Kiwi: ensembles two QE models similarly to COMET-22. The first model follows the classic Predictor-Estimator QE architecture where MT and source are encoded together. The second model is the same multitask model used in the COMET-22 submission but without access to a reference translation. Huawei: submitted several metrics to the shared task. HW-TSC 2022 Submission: A model based on partial translation which is proposed to be better than whole translation. \n",
      "YiSi: A unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources. \n",
      "MQM: A framework for declaring and describing translation quality metrics. MS-COMET-QE-22 - reference-free metrics that have a high correlation on sentence-level untranslated text, suggesting they have learnt language-dependent representations. Most reference-based metrics have good to almost perfect correlation and can identify the copied source quite easily.\n",
      "3 3\n",
      "BLEU (Bilingual Evaluation Understudy): a metric used to measure the quality of machine translation by comparing the output of the machine translation system to a reference translation. TER (Translation Error Rate): a metric used to measure the quality of machine translation by comparing the output of the machine translation system to a reference translation. WER (Word Error Rate): a metric used to measure the quality of machine translation by comparing the output of the machine translation system to a reference translation. TER. TER is a metric that measures the accuracy of a machine translation system by comparing it to a reference translation, but it also takes into account the number of words that were moved within the sentence.\n",
      "7 7\n",
      "String-based metrics: compare the coverage of various substrings between the human reference and MT output texts. Pretrained models: use pretrained models to compare the MT output and the human reference. BLEURT (fine-tuned on human annotations from WMT on the news domain), Prism (gains for Prism for the “from English” directions). NIST 2008 Metrics: A set of metrics for machine translation challenge that includes an overview, methodology, metrics, and results. COMET: A Neural Framework for MT Evaluation that uses a neural network to evaluate machine translation. BLEU: A metric for machine translation that uses precision and recall to measure the quality of a translation. The metrics used are BLEU, ROUGE, METEOR, and TER. BLEU is a precision-based metric that measures the n-gram overlap between the machine translation output and the reference translation. ROUGE is a recall-based metric that measures the longest common subsequence between the machine translation output and the reference translation. METEOR is a harmonic mean of precision and recall that uses an alignment model to identify matching words between the machine translation output and the reference translation. TER is a word-level edit distance metric that measures the number of edits required to transform the machine translation output into the reference translation. COMET: highest accuracy, suited for ranking system pairs. COMET-src: quality estimation metric, does not use a human reference. COMET: smallest deviation and highest correlation with human judgements. WMT20 (WMT20 Metrics Shared Task): a metric used to measure the quality of machine translation by comparing the output of the machine translation system to a reference translation. SacreBLEU, pretrained methods, string-based methods. SacreBLEU is a metric that is recommended in recent years and is used in later evaluation studies. Pretrained methods are methods that are trained on a large dataset and are used to improve the accuracy of the translation. String-based methods are methods that use strings of words to generate translations.\n",
      "9 9\n",
      "BLEU (Bilingual Evaluation Understudy): a metric used to evaluate the quality of machine translation by comparing the output of the machine translation system to a reference translation. It is calculated by comparing the number of words in the reference translation that are also present in the machine translation output.\n",
      "\n",
      "ROUGE (Recall-Oriented Understudy for Gisting Evaluation): a metric used to evaluate the quality of machine translation by comparing the output of the machine translation system to a reference translation. It is calculated by comparing the number of words in the reference translation that are also present in the machine translation output, as well as the number of words in the reference translation that are not present in the machine translation output.\n",
      "\n",
      "AdaFactor (Adaptive Learning Rates with Sublinear Memory Cost): a metric used to evaluate the quality of machine translation by adjusting the learning rate of the model based on the amount of data available. It is calculated by adjusting the learning rate of the model based on the amount of data available, as well as the amount of memory required to store the model. Cross-entropy loss. Cross-entropy loss is a measure of the difference between the predicted probability distribution and the true probability distribution.\n",
      "6 6\n",
      "GPT-J-6B and yes, it is open source. PaLM (Chowdhery et al., 2022) and NA (open source status is not specified). LLaMA and yes, it is open source. BERT (Devlin et al., open source) Megatron-Turing NLG 530B and it is open source. Codegen, Yes.\n",
      "7 7\n",
      "GPT-3 and it is publicly-disclosed. GPT-3 and it is not open source. GPT-3 and yes, it is open source.\n",
      "3 3\n",
      "Efficient Large Scale Language Modeling with Mixtures of Experts; open source status unknown. Chinchilla, Yes Chinchilla and it is not open source. Glam and it is open source.\n",
      "4 4\n",
      "NA\n",
      "10 1\n",
      "N-gram Counts and Language Models from the Common Crawl and it is open source.\n",
      "1 1\n",
      "Efficient Large Scale Language Modeling with Mixtures of Experts and it is open source. GPT-2 and yes, it is open source. GLaM and Switch Transformers, NA (not open source) OpenAI's GPT-3 API, No.\n",
      "4 4\n",
      "Interpolated Kneser-Ney 4-gram, NA LARGE, pruned 5-gram; NA\n",
      "2 2\n",
      "YaLM (100B) and it is not open source. GPT-3 davinci v1 and it is limited-access. TNLG v2 (530B) and it is open source. GPT-NeoX (20B) and yes, it is open source. TNLG v2 (530B) (closed) GPT-J, No\n",
      "7 7\n",
      "BLOOM (BigScience Large Open-science Open-access Multilingual Language Model) and yes, it is open source. Training Compute-Optimal Large Language Model, NA. The Pile: An 800GB dataset of diverse text for language modeling. It is open source.\n",
      "5 5\n",
      "GPT-Neo (2.7B parameters) (Black et al., 2021) and GPT-J-6B (Wang and Komatsuzaki, 2021) are open source. GPT-NeoX-20B, Yes GPT-NeoX-20B, No HyperCLOVA: Billions-scale Korean generative pre-trained transformers. NA (open source status not specified)\n",
      "7 7\n",
      "Numerous IR datasets and benchmarks. A variety of datasets and benchmarks for search engines. IR_DATASETS DCAT, schema.org, Capreolus [93], PyTerrier [58], OpenNIR [55], Anserini [91], ir_datasets (IRDS) Anserini [91] and its Python interface Pyserini [53] MS-MARCO, Capreolus, PyTerrier, OpenNIR Cranfield collection, NPL collection CORD-19, TREC COVID NTCIR-14 We Want Web Task, Terrier information retrieval platform, CLEF 2017 Task Overview: The IR Task at the eHealth Evaluation Lab - Evaluating Retrieval Methods for Consumer Health Search.\n",
      "9 9\n",
      "NA\n",
      "8 1\n",
      "WIKIR Robust04, ClueWeb09, wikIR78k, wikIRS78k MQ2007 and MQ2008 Wikipedia-based English Information Retrieval Dataset\n",
      "6 6\n",
      "TIPSTER corpus (Disks 2 and 3), TREC Disks 4 and 5, and the AQUAINT I corpus TREC 2005 Robust track, TREC 7 Title-only Retrieval (Disks 4&5) TREC collections TREC 2005 HARD/Robust NIST 1995, ACM Transactions on Information Systems 2002 TREC 4 topics 201 - 250, TREC 2005 HARD and Robust topics\n",
      "6 6\n",
      "Books, C4, Wikipedia, RealNews, Amazon Reviews Robust04, Robust05, MQ 2007, MQ 2008, ClueWeb09-CatB, TREC Web Track 99-2001, TREC Deep Learning Track 2019-2021, AOL 6M Queries, Sogou-QL, Sogou-SR, Sogou-QC, Tiangong-ST. TREC web track, TREC Deep Learning Track, AOL, Sogou-QCL, Sogou-SRR, Tiangong-ST\n",
      "3 3\n",
      "NA\n",
      "10 1\n",
      "NA\n",
      "10 1\n",
      "TIPSTER, TREC 4 & 5, AQUAINT, AQUAINT-2, ClueWeb cat. B. Websites in English (2009), Websites in English (2012), iSearch, MSN Commercial query log, Excite Commercial query log, Google books Syntactic unigrams TREC collections TREC disks 1 & 2, TREC disks 4& 5, WT10g and .GOV2 TREC collections.\n",
      "4 4\n",
      "Bajaj et al., 2016, Craswell et al., 2019\n",
      "1 1\n",
      "Version January 24, 2023 submitted to Journal Not Specified 7 of 26 \"DeepImpact\" and \"COIL\" (Continuous Optimization-based Information Retrieval) TREC collections\n",
      "2 2\n",
      "ROUGE, QAEval, and BERTScore. ROUGE, QAEval, BERTScore ROUGE, BERTScore BLEU, Learning to Score System Summaries, Ranking Human and Machine Summarization Systems Williams' test, Kendall's τ, paired t-tests, Wilcoxon signed-rank tests BLEU and ROUGE. N-Gram Graphs, NPowER-ed, N-Gram Graphs\n",
      "7 7\n",
      "SacreROUGE Pearson, Spearman, and Kendall. ROUGE and Automatically evaluating content selection in summarization without human models.\n",
      "3 3\n",
      "ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, MoverScore, JS-2 Inter-metric correlation BERTScore (BScore), MoverScore (MS), JS divergence (JS-2), ROUGE-1 (R1), ROUGE-2 (R2), ROUGE-L ROUGE, BERTScore, MoverScore\n",
      "4 4\n",
      "Automatic summarization evaluation metrics. Automatic evaluation metrics ROUGE Automatic metrics Automatic evaluation metrics. ROUGE-1, BERTScore, QAEval Pyramid Method, Studying Summarization Evaluation Metrics, Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation ROUGE-1\n",
      "8 8\n",
      "ROUGE Manual evaluations using the lightweight pyramids method (Shapira et al., 2019), automated metrics, MoverScore, ROUGE-2. Weighted macro F1 score, ROUGE based metrics, R-2, JS-2 ROUGE, JS-2, S3, BERTScore, MoverScore ROUGE-2, ROUGE-L, ROUGE-WE, JS-2, S3. Pyramid, lightweight and crowdsourceable version of Pyramid\n",
      "6 6\n",
      "ROUGE and BERTScore. ROUGE, BERTScore ROUGE (Lin, 2004) or BERTScore (Zhang et al., 2019) ROUGE, BERTScore, other summarization evaluation metrics ROUGE and BERTScore, 10 other evaluation metrics. ROUGE Automated pyramid summarization evaluation, Autosummeng and memog, Summary evaluation: Together we stand npower-ed.\n",
      "7 7\n",
      "Kendall's tau rank correlations Consistency, Fluency, Relevance 14 automatic evaluation metrics ROUGE and its variants. ROUGE-L, SummaQA, BLANC, SuPERT Coherence, DUC quality question (Dang, 2005) of structure and coherence.\n",
      "6 6\n",
      "Precision and recall, in the form of focus and coverage. BLEU scores ROUGE (Lin, 2004), METEOR (Lavie and Agarwal, 2007), BLEU (Papineni et al., 2002), MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020b) ROUGE, METEOR, MoverScore, BERTScore Highlight-based reference-less evaluation ROUGE, manual and automatic evaluation BERTScore 19 summarization evaluation metrics ROUGE\n",
      "9 9\n",
      "Automatic evaluation metrics such as ROUGE, BLEU, METEOR, and CIDEr. Faithfulness, recall, precision, relevance, coherence, and fluency. ROUGE (Lin, 2004), QAGS question answering-based framework (Wang et al., 2020) Faithfulness, precision, recall Faithfulness, focus, coverage, and inter-sentential coherence (FFCI) ROUGE, METEOR, BLEU, BERTScore, MoverScore ROUGE, METEOR, and BLEU ROUGE, METEOR, BLEU, QAGS, STS-Score (EDU), STS-Score (sentence), STS-Score (doc), BERTScore, Nayeem and Chali (2017), NSP Faithfulness, Relevance, Coherence, Fluency.\n",
      "9 9\n",
      "BLEU, METEOR, ROUGE, ROUGE-WE, SUM-QE, APES, Summa-QA BLEU, BLANC ROUGE, BERTScore, JS, SUM-QE, BLANC BLANC, crowd and expert ratings, commonly used automatic metrics Mean Opinion Score (MOS) scale, summary usefulness, post usefulness, and summary informativeness. Automatic Quality Evaluation Automatic evaluation metric for news article summarization, meta evaluation of factuality in summarization, unsupervised evaluation metrics for multi-document summarization. ROUGE\n",
      "8 8\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.DataFrame(columns=[\"guid\", \"title\", \"url\", \"extract_query\", \"Passages\", \"Answers\", \"edit_answer\"])\n",
    "passages = []\n",
    "answers = []\n",
    "guids = []\n",
    "titles = []\n",
    "urls = []\n",
    "extract_queries = []\n",
    "edit_answers = []\n",
    "annotators = []\n",
    "\n",
    "for guid, title, url, extract_query, edit_answer, passages_list, answers_list, annotator in zip(df[\"guid\"], df[\"Title\"], df[\"URL\"], df[\"Extract Query\"], df[\"edit_answer\"], df[\"Passages (top-10 chunks retrieved knn)\"], df[\"Αnswers\"], df[\"Annotator\"]):\n",
    "    passages_list = eval(passages_list)\n",
    "    answers_list = eval(answers_list)\n",
    "    edit_answer = eval(edit_answer)\n",
    "    edit_answer = \", \".join(edit_answer)\n",
    "    print(edit_answer)\n",
    "    print(len(passages_list), len(answers_list))\n",
    "\n",
    "    for passage, answer in zip(passages_list, answers_list):\n",
    "        passages.append(passage)\n",
    "        answers.append(answer)\n",
    "        guids.append(guid)\n",
    "        titles.append(title)\n",
    "        urls.append(url)\n",
    "        extract_queries.append(extract_query)\n",
    "        edit_answers.append(edit_answer)\n",
    "        annotators.append(annotator)\n",
    "\n",
    "final_df[\"guid\"] = guids\n",
    "final_df[\"title\"] = titles\n",
    "final_df[\"url\"] = urls\n",
    "final_df[\"extract_query\"] = extract_queries\n",
    "final_df[\"Passages\"] = passages\n",
    "final_df[\"Answers\"] = answers\n",
    "final_df[\"edit_answer\"] = edit_answers\n",
    "final_df[\"Annotator\"] = annotators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"Productivity Experiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"QA Productivity Experiment - Productivity Experiment Final.csv\", \"r\") as f:\n",
    "    df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = df[\"Edited Answers (manually removed duplicates within one answer, NA answers and duplicate answers were removed programatically)\"]\n",
    "refs = df[\"Final Answer (here provide the final answer according to the passages)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      TREC, SQuAD, NewsQA, SearchQA, QuAC, HotpotQA,...\n",
       "1                                                    NaN\n",
       "2                                                    NaN\n",
       "3                                                    NaN\n",
       "4                                                    NaN\n",
       "                             ...                        \n",
       "255                                                  NaN\n",
       "256                                                  NaN\n",
       "257                                                  NaN\n",
       "258                                                  NaN\n",
       "259                                                  NaN\n",
       "Name: Edited Answers (manually removed duplicates within one answer, NA answers and duplicate answers were removed programatically), Length: 260, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      TREC, SQuAD, NewsQA, SearchQA, QuAC, HotpotQA,...\n",
       "1                                                    NaN\n",
       "2                                                    NaN\n",
       "3                                                    NaN\n",
       "4                                                    NaN\n",
       "                             ...                        \n",
       "255                                                  NaN\n",
       "256                                                  NaN\n",
       "257                                                  NaN\n",
       "258                                                  NaN\n",
       "259                                                  NaN\n",
       "Name: Final Answer (here provide the final answer according to the passages), Length: 260, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TREC, SQuAD, NewsQA, SearchQA, QuAC, HotpotQA, NarrativeQA MS MARCO, TriviaQA, ELI5 The first large-scale long form question answering dataset of open-ended queries with explanatory multi-sentence answers.\n",
      "TREC, SQuAD, NewsQA, SearchQA, QuAC, HotpotQA, NarrativeQA MS MARCO, TriviaQA, ELI5 The first large-scale long form question answering dataset of open-ended queries with explanatory multi-sentence answers.\n",
      "ELI5 Explain Anything Like I'm Five (https://yjernite.github.io/lfqa.html) QQP, AMT evaluation KILT Natural Questions\n",
      "ELI5 Explain Anything Like I'm Five (https://yjernite.github.io/lfqa.html) QQP, AMT evaluation KILT Natural Questions\n",
      "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension Natural Questions, MS MARCO Karpukhin et al. [32] HotpotQA dataset Fever, zsRE, AY2, T-REx, WoW\n",
      "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension Natural Questions, MS MARCO Karpukhin et al. [32] HotpotQA dataset Fever, zsRE, AY2, T-REx, WoW\n",
      "MS MARCO NaturalQuestions (Kwiatkowski et al., 2019) ELI5 TrivaQA, kwiatkowski2019naturaluestion, HotpotQA, Fever, zsRE, AY2, T-REx, WoW.\n",
      "MS MARCO NaturalQuestions (Kwiatkowski et al., 2019) ELI5 TrivaQA, kwiatkowski2019naturaluestion, HotpotQA, Fever, zsRE, AY2, T-REx, WoW.\n",
      "Natural Questions (Kwiatkowski et al., 2019), ASQA (Stelmakh et al., 2022), AQuaMuSe (Kulkarni et al., 2020) ELI5 (Fan et al., 2019), AQuaMuSe\n",
      "Natural Questions (Kwiatkowski et al., 2019), ASQA (Stelmakh et al., 2022), AQuaMuSe (Kulkarni et al., 2020) ELI5 (Fan et al., 2019), AQuaMuSe\n",
      "NarrativeQA QuALITY QuALITY-HARD RACE TriviaQA, SearchQA, HotpotQA, QAngaroo, ComplexWebQuestions\n",
      "NarrativeQA QuALITY QuALITY-HARD RACE TriviaQA, SearchQA, HotpotQA, QAngaroo, ComplexWebQuestions\n",
      "TREC QA, WikiQA, SelQA WIKIREADING, WR-LONG, WIKISUGGEST NewsQA, Parallel-Hierarchical Model, Building a Question Answering Test Collection\n",
      "TREC QA, WikiQA, SelQA WIKIREADING, WR-LONG, WIKISUGGEST NewsQA, Parallel-Hierarchical Model, Building a Question Answering Test Collection\n",
      "TREC answer specifications TREC question sets. METEOR TREC2004, TREC2005, TREC2006\n",
      "TREC answer specifications TREC question sets. METEOR TREC2004, TREC2005, TREC2006\n",
      "SQuAD, NewsQA, MS MARCO, Narrative QA, RACE, LearningQ, Google Natural Questions\n",
      "SQuAD, NewsQA, MS MARCO, Narrative QA, RACE, LearningQ, Google Natural Questions\n",
      "WikiTableQuestions WikiTables SQuAD 2.0, HotpotQA\n",
      "WikiTableQuestions WikiTables SQuAD 2.0, HotpotQA\n",
      "chrF, COMET, BERTScore - chrF is a character-level metric that uses n-grams to compare the translation to the reference; COMET is a metric that uses a neural network to compare the translation to the reference; BERTScore is a metric that uses a BERT model to compare the translation to the reference. BLEU (string-matching metric that compares the token-level n-grams of the hypothesis with the reference translation), WMT (annual shared task on developing MT models for several categories in machine translation, human evaluation of the translated outputs from the participating machine translation models is often used to determine the best-performing MT system). COMET-DA uses direct assessments from 2017 to 2019 as its training data. COMET-MQM uses direct assessments from 2017 to 2021 as its training data and is fine-tuned with MQM data from Freitag et al. (2021a). UniTE metrics (Wan et al., 2022) is a neural translation metric that proposes a multi-task setup for the three strategies of evaluation - source-hypothesis, source-hypothesis-reference, and reference-hypothesis in a single model. The pre-training stage involves training the model with synthetic data constructed using a subset of WMT evaluation data. Fine-tuning uses novel attention mechanisms and aggregate loss functions to facilitate the multi-task setup. All the above reference-based metrics have their corresponding reference-free versions which exclude encoding the reference. Multidimensional Quality Metrics (MQM): A framework for declaring and describing translation quality metrics. Roberta: A robustly optimized BERT pretraining approach. Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges. An automatic evaluation of the WMT22 general machine translation task. COMET-MQM: a scoring scheme with expert annotators for evaluating MT outputs. False negative rate of 0.796 in the round-trip translation setup compared to 0.097 when the human reference is used instead.\n",
      "chrF, COMET, BERTScore - chrF is a character-level metric that uses n-grams to compare the translation to the reference; COMET is a metric that uses a neural network to compare the translation to the reference; BERTScore is a metric that uses a BERT model to compare the translation to the reference. BLEU (string-matching metric that compares the token-level n-grams of the hypothesis with the reference translation), WMT (annual shared task on developing MT models for several categories in machine translation, human evaluation of the translated outputs from the participating machine translation models is often used to determine the best-performing MT system). COMET-DA uses direct assessments from 2017 to 2019 as its training data. COMET-MQM uses direct assessments from 2017 to 2021 as its training data and is fine-tuned with MQM data from Freitag et al. (2021a). UniTE metrics (Wan et al., 2022) is a neural translation metric that proposes a multi-task setup for the three strategies of evaluation - source-hypothesis, source-hypothesis-reference, and reference-hypothesis in a single model. The pre-training stage involves training the model with synthetic data constructed using a subset of WMT evaluation data. Fine-tuning uses novel attention mechanisms and aggregate loss functions to facilitate the multi-task setup. All the above reference-based metrics have their corresponding reference-free versions which exclude encoding the reference. Multidimensional Quality Metrics (MQM): A framework for declaring and describing translation quality metrics. Roberta: A robustly optimized BERT pretraining approach. Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges. An automatic evaluation of the WMT22 general machine translation task. COMET-MQM: a scoring scheme with expert annotators for evaluating MT outputs. False negative rate of 0.796 in the round-trip translation setup compared to 0.097 when the human reference is used instead.\n",
      "They use token-level negative log-likelihood (NLL) and BLEU as the reward, as well as the state-of-the-art model-based metric, BLEURT. NLL is a token-level metric that does not guarantee that the generated translations will be optimized for a selected sequence-level evaluation metric. BLEU is a reward used to directly improve the metric, but it has a low correlation with human judgment when dealing with state-of-the-art models. BLEURT is a model-based metric that has a much higher human correlation. Smoothed BLEU and BLEURT. Smoothed BLEU is a token-level negative log-likelihood metric that does not guarantee that the generated translations will be optimized for a selected sequence-level evaluation metric. BLEURT is a model-based evaluation metric that has a much higher human correlation. Meteor (Banerjee and Lavie, 2005): a metric that combines unigram precision and recall with a modified n-gram precision. RIBES (Isozaki et al., 2010): a metric that evaluates the quality of translations by comparing them to a set of reference translations. SBLEU (Smoothed BLEU) is a modified version of BLEU that is more robust to small changes in the translation. Kendal's τ is a metric used to measure the correlation between two variables.\n",
      "They use token-level negative log-likelihood (NLL) and BLEU as the reward, as well as the state-of-the-art model-based metric, BLEURT. NLL is a token-level metric that does not guarantee that the generated translations will be optimized for a selected sequence-level evaluation metric. BLEU is a reward used to directly improve the metric, but it has a low correlation with human judgment when dealing with state-of-the-art models. BLEURT is a model-based metric that has a much higher human correlation. Smoothed BLEU and BLEURT. Smoothed BLEU is a token-level negative log-likelihood metric that does not guarantee that the generated translations will be optimized for a selected sequence-level evaluation metric. BLEURT is a model-based evaluation metric that has a much higher human correlation. Meteor (Banerjee and Lavie, 2005): a metric that combines unigram precision and recall with a modified n-gram precision. RIBES (Isozaki et al., 2010): a metric that evaluates the quality of translations by comparing them to a set of reference translations. SBLEU (Smoothed BLEU) is a modified version of BLEU that is more robust to small changes in the translation. Kendal's τ is a metric used to measure the correlation between two variables.\n",
      "Otem and Utem. Otem is a quantitative metric based on the proportion of mismatched n-grams between gold reference and system translation to evaluate the system performance in terms of over-translation. Utem is a quantitative metric based on the proportion of mismatched n-grams between gold reference and system translation to evaluate the system performance in terms of under-translation. Reval: A simple and effective machine translation evaluation metric based on recurrent neural networks. Linguistic features for automatic evaluation of heterogenous MT systems. Pairwise neural machine translation evaluation. Neural Machine Translation metrics used are Red (Reference Dependency based MT Evaluation Metric) and GRU-Gated Attention Model. Red is a metric that evaluates the quality of machine translation by comparing the syntactic dependencies of the source and target sentences. GRU-Gated Attention Model is a model that uses a gated recurrent unit (GRU) to control the attention mechanism in a neural machine translation system. Neural Machine Translation metrics used are METEOR (an automatic metric for MT evaluation with improved correlation with human judgments) Towards Automatic Error Analysis of Machine Translation Output: A metric which uses a combination of precision, recall, and F-measure to measure the quality of a machine translation system. \n",
      "Neural Machine Translation of Rare Words with Subword Units: A metric which uses subword units to measure the quality of a machine translation system. Minimum Risk Training (MRT): a metric that measures the risk of a translation model by taking into account the cost of incorrect translations. Translation Edit Rate (TER): a metric that measures the number of edits required to transform a machine-generated translation into a human-generated translation. Bidirectional Recurrent Neural Networks (BRNN): a type of neural network that uses both forward and backward recurrent connections to process input sequences.\n",
      "Otem and Utem. Otem is a quantitative metric based on the proportion of mismatched n-grams between gold reference and system translation to evaluate the system performance in terms of over-translation. Utem is a quantitative metric based on the proportion of mismatched n-grams between gold reference and system translation to evaluate the system performance in terms of under-translation. Reval: A simple and effective machine translation evaluation metric based on recurrent neural networks. Linguistic features for automatic evaluation of heterogenous MT systems. Pairwise neural machine translation evaluation. Neural Machine Translation metrics used are Red (Reference Dependency based MT Evaluation Metric) and GRU-Gated Attention Model. Red is a metric that evaluates the quality of machine translation by comparing the syntactic dependencies of the source and target sentences. GRU-Gated Attention Model is a model that uses a gated recurrent unit (GRU) to control the attention mechanism in a neural machine translation system. Neural Machine Translation metrics used are METEOR (an automatic metric for MT evaluation with improved correlation with human judgments) Towards Automatic Error Analysis of Machine Translation Output: A metric which uses a combination of precision, recall, and F-measure to measure the quality of a machine translation system. \n",
      "Neural Machine Translation of Rare Words with Subword Units: A metric which uses subword units to measure the quality of a machine translation system. Minimum Risk Training (MRT): a metric that measures the risk of a translation model by taking into account the cost of incorrect translations. Translation Edit Rate (TER): a metric that measures the number of edits required to transform a machine-generated translation into a human-generated translation. Bidirectional Recurrent Neural Networks (BRNN): a type of neural network that uses both forward and backward recurrent connections to process input sequences.\n",
      "BERT: Pre-training of deep bidirectional transformers for language understanding. WMT21 Metrics Shared Task: Evaluating metrics with expert-based human evaluations on TED and News domain. BLEU (n-grams macro), WER (Levenshtein distance macro), TER (edit distance macro), PER (edit distance macro), chrF (character n-grams micro), chrF+ (character n-grams micro), BEER (char. n-grams, trees micro), CharacTER (char. edit distance micro), BERTScore (neural representations micro), MoverScore (neural representations micro).\n",
      "BERT: Pre-training of deep bidirectional transformers for language understanding. WMT21 Metrics Shared Task: Evaluating metrics with expert-based human evaluations on TED and News domain. BLEU (n-grams macro), WER (Levenshtein distance macro), TER (edit distance macro), PER (edit distance macro), chrF (character n-grams micro), chrF+ (character n-grams micro), BEER (char. n-grams, trees micro), CharacTER (char. edit distance micro), BERTScore (neural representations micro), MoverScore (neural representations micro).\n",
      "They use word translation probabilities (obtained from the softmax), extend each partial translation with subsequent word predictions and accumulate these scores. They also use beam size and sentence level model scores normalized by length of the output. Kalchbrenner and Blunsom (2013) use a convolutional neural network to encode the source sentence and then generate the target sentence by reversing the process. Gehring et al. (2017) use multiple convolutional layers in the encoder and the decoder that do not reduce the length of the encoded sequence but incorporate wider context with each layer. Vaswani et al. (2017) replace the recurrent neural networks used in attentional sequence-to-sequence models with multiple self-attention layers, both for the encoder as well as the decoder. Bojar et al. (2016) and Wu et al. (2017) use public benchmarks to measure the performance of Neural Machine Translation. BLEU (Bilingual Evaluation Understudy) is a metric used to measure the quality of text which has been machine-translated from one natural language to another. It compares an automatically produced translation against a professional human translation and assigns a score based on how many words in the machine-generated translation match words in the human-generated translation. Match Score: Checks for each output if the aligned input word according to fast-align is indeed the input word that received the highest attention probability. \n",
      "Probability Mass Score: Sums up the probability mass given to each alignment point obtained from fast-align. Coverage Models: The vector that accumulates coverage of input words is used to inform the attention model. Over-Generation: The sum of coverage of input words is compared to 1, and if it is greater than 1, the difference is the over-generation metric. Under-Generation: The sum of coverage of input words is compared to 1, and if it is less than 1, the difference is the under-generation metric.\n",
      "They use word translation probabilities (obtained from the softmax), extend each partial translation with subsequent word predictions and accumulate these scores. They also use beam size and sentence level model scores normalized by length of the output. Kalchbrenner and Blunsom (2013) use a convolutional neural network to encode the source sentence and then generate the target sentence by reversing the process. Gehring et al. (2017) use multiple convolutional layers in the encoder and the decoder that do not reduce the length of the encoded sequence but incorporate wider context with each layer. Vaswani et al. (2017) replace the recurrent neural networks used in attentional sequence-to-sequence models with multiple self-attention layers, both for the encoder as well as the decoder. Bojar et al. (2016) and Wu et al. (2017) use public benchmarks to measure the performance of Neural Machine Translation. BLEU (Bilingual Evaluation Understudy) is a metric used to measure the quality of text which has been machine-translated from one natural language to another. It compares an automatically produced translation against a professional human translation and assigns a score based on how many words in the machine-generated translation match words in the human-generated translation. Match Score: Checks for each output if the aligned input word according to fast-align is indeed the input word that received the highest attention probability. \n",
      "Probability Mass Score: Sums up the probability mass given to each alignment point obtained from fast-align. Coverage Models: The vector that accumulates coverage of input words is used to inform the attention model. Over-Generation: The sum of coverage of input words is compared to 1, and if it is greater than 1, the difference is the over-generation metric. Under-Generation: The sum of coverage of input words is compared to 1, and if it is less than 1, the difference is the under-generation metric.\n",
      "[24] Recurrent Neural Network based Language Model - a model based on complex recurrent neural networks in an encoder-decoder configuration.\n",
      "[25] Fast and Robust Neural Network Joint Models - a model based on neural networks in a joint model configuration.\n",
      "[26] Recurrent Continuous Translation Models - a model based on recurrent neural networks for translation.\n",
      "[27] Sequence to Sequence Learning with Neural Networks - a model based on neural networks for sequence to sequence learning.\n",
      "[28] Multi-level Evaluation for Machine Translation - a model based on evaluation metrics for machine translation.\n",
      "[29] Representation Based Translation Evaluation Metrics - a model based on representation-based evaluation metrics for machine translation.\n",
      "[30] Machine Translation Evaluation using Recurrent Neural Networks - a model based on recurrent neural networks for machine translation evaluation. ReVal. ReVal is a neural evaluation metric that performs significantly better at the segment level. NNRK (Neural Network ReranKing) - uses a pairwise fashion by subtracting the vectors for the two competing translations and then training to predict +1 or -1. At test time, a vector for a single translation is used, which is equivalent to subtracting a zero vector from it, i.e., to predicting whether the translation would win against an empty translation, and by what margin. NNRK (Absolute) - uses absolute scores instead of pairwise comparisons. The metrics used are BLEU, ROUGE, and METEOR. BLEU stands for Bilingual Evaluation Understudy and is a precision-based metric that compares an automatically generated translation against a set of reference translations. ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation and is a recall-based metric that measures the overlap between the automatically generated translation and the reference translations. METEOR stands for Metric for Evaluation of Translation with Explicit Ordering and is a harmonic mean of precision and recall that also takes into account the order of words in the translation. NIST: National Institute of Standards and Technology, a score that measures the similarity between the predicted and reference translations. TER: Translation Error Rate, a score that measures the number of edits needed to transform the predicted translation into the reference translation. Meteor: Metric for Evaluation of Translation with Explicit Ordering, a score that measures the similarity between the predicted and reference translations. Dropout: A technique used to prevent neural networks from overfitting by randomly dropping out neurons during training. Long Short-Term Memory (LSTM): A recurrent neural network architecture that uses gated cells to store and access information over long periods of time. Bidirectional Recurrent Neural Networks (BRNN): A type of recurrent neural network that processes data in both forward and backward directions. Spede: A probabilistic edit distance metric for Machine Translation evaluation. Amber: A Machine Translation evaluation metric. Deep-Syntactic Metric: A metric for Machine Translation evaluation and tuning. Spearman's Correlation Coefficient: A measure of the strength of the association between two variables. NNRKMean: multi-layer NN, mean vector; NNRKZero: multi-layer NN, zero vector; DiscoTK: Best on the WMT12 dataset; SEMPOS: 1st at the WMT12 competition; AMBER: 2nd at the WMT12 competition; Meteor: 3rd at the WMT12 competition.\n",
      "[24] Recurrent Neural Network based Language Model - a model based on complex recurrent neural networks in an encoder-decoder configuration.\n",
      "[25] Fast and Robust Neural Network Joint Models - a model based on neural networks in a joint model configuration.\n",
      "[26] Recurrent Continuous Translation Models - a model based on recurrent neural networks for translation.\n",
      "[27] Sequence to Sequence Learning with Neural Networks - a model based on neural networks for sequence to sequence learning.\n",
      "[28] Multi-level Evaluation for Machine Translation - a model based on evaluation metrics for machine translation.\n",
      "[29] Representation Based Translation Evaluation Metrics - a model based on representation-based evaluation metrics for machine translation.\n",
      "[30] Machine Translation Evaluation using Recurrent Neural Networks - a model based on recurrent neural networks for machine translation evaluation. ReVal. ReVal is a neural evaluation metric that performs significantly better at the segment level. NNRK (Neural Network ReranKing) - uses a pairwise fashion by subtracting the vectors for the two competing translations and then training to predict +1 or -1. At test time, a vector for a single translation is used, which is equivalent to subtracting a zero vector from it, i.e., to predicting whether the translation would win against an empty translation, and by what margin. NNRK (Absolute) - uses absolute scores instead of pairwise comparisons. The metrics used are BLEU, ROUGE, and METEOR. BLEU stands for Bilingual Evaluation Understudy and is a precision-based metric that compares an automatically generated translation against a set of reference translations. ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation and is a recall-based metric that measures the overlap between the automatically generated translation and the reference translations. METEOR stands for Metric for Evaluation of Translation with Explicit Ordering and is a harmonic mean of precision and recall that also takes into account the order of words in the translation. NIST: National Institute of Standards and Technology, a score that measures the similarity between the predicted and reference translations. TER: Translation Error Rate, a score that measures the number of edits needed to transform the predicted translation into the reference translation. Meteor: Metric for Evaluation of Translation with Explicit Ordering, a score that measures the similarity between the predicted and reference translations. Dropout: A technique used to prevent neural networks from overfitting by randomly dropping out neurons during training. Long Short-Term Memory (LSTM): A recurrent neural network architecture that uses gated cells to store and access information over long periods of time. Bidirectional Recurrent Neural Networks (BRNN): A type of recurrent neural network that processes data in both forward and backward directions. Spede: A probabilistic edit distance metric for Machine Translation evaluation. Amber: A Machine Translation evaluation metric. Deep-Syntactic Metric: A metric for Machine Translation evaluation and tuning. Spearman's Correlation Coefficient: A measure of the strength of the association between two variables. NNRKMean: multi-layer NN, mean vector; NNRKZero: multi-layer NN, zero vector; DiscoTK: Best on the WMT12 dataset; SEMPOS: 1st at the WMT12 competition; AMBER: 2nd at the WMT12 competition; Meteor: 3rd at the WMT12 competition.\n",
      "UNITE-SRC, UNITE-REF, and UNITE: metric approach where the model-based metrics can possess the ability of evaluating translation outputs following all three evaluation scenarios, i.e. source-only, reference-only, and source-reference-combined. COMET-Kiwi: ensembles two QE models similarly to COMET-22. The first model follows the classic Predictor-Estimator QE architecture where MT and source are encoded together. The second model is the same multitask model used in the COMET-22 submission but without access to a reference translation. Huawei: submitted several metrics to the shared task. HW-TSC 2022 Submission: A model based on partial translation which is proposed to be better than whole translation. \n",
      "YiSi: A unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources. \n",
      "MQM: A framework for declaring and describing translation quality metrics. MS-COMET-QE-22 - reference-free metrics that have a high correlation on sentence-level untranslated text, suggesting they have learnt language-dependent representations. Most reference-based metrics have good to almost perfect correlation and can identify the copied source quite easily.\n",
      "UNITE-SRC, UNITE-REF, and UNITE: metric approach where the model-based metrics can possess the ability of evaluating translation outputs following all three evaluation scenarios, i.e. source-only, reference-only, and source-reference-combined. COMET-Kiwi: ensembles two QE models similarly to COMET-22. The first model follows the classic Predictor-Estimator QE architecture where MT and source are encoded together. The second model is the same multitask model used in the COMET-22 submission but without access to a reference translation. Huawei: submitted several metrics to the shared task. HW-TSC 2022 Submission: A model based on partial translation which is proposed to be better than whole translation. \n",
      "YiSi: A unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources. \n",
      "MQM: A framework for declaring and describing translation quality metrics. MS-COMET-QE-22 - reference-free metrics that have a high correlation on sentence-level untranslated text, suggesting they have learnt language-dependent representations. Most reference-based metrics have good to almost perfect correlation and can identify the copied source quite easily.\n",
      "BLEU (Bilingual Evaluation Understudy): a metric used to measure the quality of machine translation by comparing the output of the machine translation system to a reference translation. TER (Translation Error Rate): a metric used to measure the quality of machine translation by comparing the output of the machine translation system to a reference translation. WER (Word Error Rate): a metric used to measure the quality of machine translation by comparing the output of the machine translation system to a reference translation. TER. TER is a metric that measures the accuracy of a machine translation system by comparing it to a reference translation, but it also takes into account the number of words that were moved within the sentence.\n",
      "BLEU (Bilingual Evaluation Understudy): a metric used to measure the quality of machine translation by comparing the output of the machine translation system to a reference translation. TER (Translation Error Rate): a metric used to measure the quality of machine translation by comparing the output of the machine translation system to a reference translation. WER (Word Error Rate): a metric used to measure the quality of machine translation by comparing the output of the machine translation system to a reference translation. TER. TER is a metric that measures the accuracy of a machine translation system by comparing it to a reference translation, but it also takes into account the number of words that were moved within the sentence.\n",
      "String-based metrics: compare the coverage of various substrings between the human reference and MT output texts. Pretrained models: use pretrained models to compare the MT output and the human reference. BLEURT (fine-tuned on human annotations from WMT on the news domain), Prism (gains for Prism for the “from English” directions). NIST 2008 Metrics: A set of metrics for machine translation challenge that includes an overview, methodology, metrics, and results. COMET: A Neural Framework for MT Evaluation that uses a neural network to evaluate machine translation. BLEU: A metric for machine translation that uses precision and recall to measure the quality of a translation. The metrics used are BLEU, ROUGE, METEOR, and TER. BLEU is a precision-based metric that measures the n-gram overlap between the machine translation output and the reference translation. ROUGE is a recall-based metric that measures the longest common subsequence between the machine translation output and the reference translation. METEOR is a harmonic mean of precision and recall that uses an alignment model to identify matching words between the machine translation output and the reference translation. TER is a word-level edit distance metric that measures the number of edits required to transform the machine translation output into the reference translation. COMET: highest accuracy, suited for ranking system pairs. COMET-src: quality estimation metric, does not use a human reference. COMET: smallest deviation and highest correlation with human judgements. WMT20 (WMT20 Metrics Shared Task): a metric used to measure the quality of machine translation by comparing the output of the machine translation system to a reference translation. SacreBLEU, pretrained methods, string-based methods. SacreBLEU is a metric that is recommended in recent years and is used in later evaluation studies. Pretrained methods are methods that are trained on a large dataset and are used to improve the accuracy of the translation. String-based methods are methods that use strings of words to generate translations.\n",
      "String-based metrics: compare the coverage of various substrings between the human reference and MT output texts. Pretrained models: use pretrained models to compare the MT output and the human reference. BLEURT (fine-tuned on human annotations from WMT on the news domain), Prism (gains for Prism for the “from English” directions). NIST 2008 Metrics: A set of metrics for machine translation challenge that includes an overview, methodology, metrics, and results. COMET: A Neural Framework for MT Evaluation that uses a neural network to evaluate machine translation. BLEU: A metric for machine translation that uses precision and recall to measure the quality of a translation. The metrics used are BLEU, ROUGE, METEOR, and TER. BLEU is a precision-based metric that measures the n-gram overlap between the machine translation output and the reference translation. ROUGE is a recall-based metric that measures the longest common subsequence between the machine translation output and the reference translation. METEOR is a harmonic mean of precision and recall that uses an alignment model to identify matching words between the machine translation output and the reference translation. TER is a word-level edit distance metric that measures the number of edits required to transform the machine translation output into the reference translation. COMET: highest accuracy, suited for ranking system pairs. COMET-src: quality estimation metric, does not use a human reference. COMET: smallest deviation and highest correlation with human judgements. WMT20 (WMT20 Metrics Shared Task): a metric used to measure the quality of machine translation by comparing the output of the machine translation system to a reference translation. SacreBLEU, pretrained methods, string-based methods. SacreBLEU is a metric that is recommended in recent years and is used in later evaluation studies. Pretrained methods are methods that are trained on a large dataset and are used to improve the accuracy of the translation. String-based methods are methods that use strings of words to generate translations.\n",
      "BLEU (Bilingual Evaluation Understudy): a metric used to evaluate the quality of machine translation by comparing the output of the machine translation system to a reference translation. It is calculated by comparing the number of words in the reference translation that are also present in the machine translation output.\n",
      "\n",
      "ROUGE (Recall-Oriented Understudy for Gisting Evaluation): a metric used to evaluate the quality of machine translation by comparing the output of the machine translation system to a reference translation. It is calculated by comparing the number of words in the reference translation that are also present in the machine translation output, as well as the number of words in the reference translation that are not present in the machine translation output.\n",
      "\n",
      "AdaFactor (Adaptive Learning Rates with Sublinear Memory Cost): a metric used to evaluate the quality of machine translation by adjusting the learning rate of the model based on the amount of data available. It is calculated by adjusting the learning rate of the model based on the amount of data available, as well as the amount of memory required to store the model. Cross-entropy loss. Cross-entropy loss is a measure of the difference between the predicted probability distribution and the true probability distribution.\n",
      "BLEU (Bilingual Evaluation Understudy): a metric used to evaluate the quality of machine translation by comparing the output of the machine translation system to a reference translation. It is calculated by comparing the number of words in the reference translation that are also present in the machine translation output.\n",
      "\n",
      "ROUGE (Recall-Oriented Understudy for Gisting Evaluation): a metric used to evaluate the quality of machine translation by comparing the output of the machine translation system to a reference translation. It is calculated by comparing the number of words in the reference translation that are also present in the machine translation output, as well as the number of words in the reference translation that are not present in the machine translation output.\n",
      "\n",
      "AdaFactor (Adaptive Learning Rates with Sublinear Memory Cost): a metric used to evaluate the quality of machine translation by adjusting the learning rate of the model based on the amount of data available. It is calculated by adjusting the learning rate of the model based on the amount of data available, as well as the amount of memory required to store the model. Cross-entropy loss. Cross-entropy loss is a measure of the difference between the predicted probability distribution and the true probability distribution.\n",
      "GPT-J-6B and yes, it is open source. PaLM (Chowdhery et al., 2022) and NA (open source status is not specified). LLaMA and yes, it is open source. BERT (Devlin et al., open source) Megatron-Turing NLG 530B and it is open source. Codegen, Yes.\n",
      "GPT-J-6B and yes, it is open source. PaLM (Chowdhery et al., 2022) and NA (open source status is not specified). LLaMA and yes, it is open source. BERT (Devlin et al., open source) Megatron-Turing NLG 530B and it is open source. Codegen, Yes.\n",
      "GPT-3 and it is publicly-disclosed. GPT-3 and it is not open source. GPT-3 and yes, it is open source.\n",
      "GPT-3 and it is publicly-disclosed. GPT-3 and it is not open source. GPT-3 and yes, it is open source.\n",
      "Efficient Large Scale Language Modeling with Mixtures of Experts; open source status unknown. Chinchilla, Yes Chinchilla and it is not open source. Glam and it is open source.\n",
      "Efficient Large Scale Language Modeling with Mixtures of Experts; open source status unknown. Chinchilla, Yes Chinchilla and it is not open source. Glam and it is open source.\n",
      "N-gram Counts and Language Models from the Common Crawl and it is open source.\n",
      "N-gram Counts and Language Models from the Common Crawl and it is open source.\n",
      "Efficient Large Scale Language Modeling with Mixtures of Experts and it is open source. GPT-2 and yes, it is open source. GLaM and Switch Transformers, NA (not open source) OpenAI's GPT-3 API, No.\n",
      "Efficient Large Scale Language Modeling with Mixtures of Experts and it is open source. GPT-2 and yes, it is open source. GLaM and Switch Transformers, NA (not open source) OpenAI's GPT-3 API, No.\n",
      "Interpolated Kneser-Ney 4-gram, NA LARGE, pruned 5-gram; NA\n",
      "Interpolated Kneser-Ney 4-gram, NA LARGE, pruned 5-gram; NA\n",
      "YaLM (100B) and it is not open source. GPT-3 davinci v1 and it is limited-access. TNLG v2 (530B) and it is open source. GPT-NeoX (20B) and yes, it is open source. TNLG v2 (530B) (closed) GPT-J, No\n",
      "YaLM (100B) and it is not open source. GPT-3 davinci v1 and it is limited-access. TNLG v2 (530B) and it is open source. GPT-NeoX (20B) and yes, it is open source. TNLG v2 (530B) (closed) GPT-J, No\n",
      "BLOOM (BigScience Large Open-science Open-access Multilingual Language Model) and yes, it is open source. Training Compute-Optimal Large Language Model, NA. The Pile: An 800GB dataset of diverse text for language modeling. It is open source.\n",
      "BLOOM (BigScience Large Open-science Open-access Multilingual Language Model) and yes, it is open source. Training Compute-Optimal Large Language Model, NA. The Pile: An 800GB dataset of diverse text for language modeling. It is open source.\n",
      "GPT-Neo (2.7B parameters) (Black et al., 2021) and GPT-J-6B (Wang and Komatsuzaki, 2021) are open source. GPT-NeoX-20B, Yes GPT-NeoX-20B, No HyperCLOVA: Billions-scale Korean generative pre-trained transformers. NA (open source status not specified)\n",
      "GPT-Neo (2.7B parameters) (Black et al., 2021) and GPT-J-6B (Wang and Komatsuzaki, 2021) are open source. GPT-NeoX-20B, Yes GPT-NeoX-20B, No HyperCLOVA: Billions-scale Korean generative pre-trained transformers. NA (open source status not specified)\n",
      "Numerous IR datasets and benchmarks. A variety of datasets and benchmarks for search engines. IR_DATASETS DCAT, schema.org, Capreolus [93], PyTerrier [58], OpenNIR [55], Anserini [91], ir_datasets (IRDS) Anserini [91] and its Python interface Pyserini [53] MS-MARCO, Capreolus, PyTerrier, OpenNIR Cranfield collection, NPL collection CORD-19, TREC COVID NTCIR-14 We Want Web Task, Terrier information retrieval platform, CLEF 2017 Task Overview: The IR Task at the eHealth Evaluation Lab - Evaluating Retrieval Methods for Consumer Health Search.\n",
      "Numerous IR datasets and benchmarks. A variety of datasets and benchmarks for search engines. IR_DATASETS DCAT, schema.org, Capreolus [93], PyTerrier [58], OpenNIR [55], Anserini [91], ir_datasets (IRDS) Anserini [91] and its Python interface Pyserini [53] MS-MARCO, Capreolus, PyTerrier, OpenNIR Cranfield collection, NPL collection CORD-19, TREC COVID NTCIR-14 We Want Web Task, Terrier information retrieval platform, CLEF 2017 Task Overview: The IR Task at the eHealth Evaluation Lab - Evaluating Retrieval Methods for Consumer Health Search.\n",
      "WIKIR Robust04, ClueWeb09, wikIR78k, wikIRS78k MQ2007 and MQ2008 Wikipedia-based English Information Retrieval Dataset\n",
      "WIKIR Robust04, ClueWeb09, wikIR78k, wikIRS78k MQ2007 and MQ2008 Wikipedia-based English Information Retrieval Dataset\n",
      "TIPSTER corpus (Disks 2 and 3), TREC Disks 4 and 5, and the AQUAINT I corpus TREC 2005 Robust track, TREC 7 Title-only Retrieval (Disks 4&5) TREC collections TREC 2005 HARD/Robust NIST 1995, ACM Transactions on Information Systems 2002 TREC 4 topics 201 - 250, TREC 2005 HARD and Robust topics\n",
      "TIPSTER corpus (Disks 2 and 3), TREC Disks 4 and 5, and the AQUAINT I corpus TREC 2005 Robust track, TREC 7 Title-only Retrieval (Disks 4&5) TREC collections TREC 2005 HARD/Robust NIST 1995, ACM Transactions on Information Systems 2002 TREC 4 topics 201 - 250, TREC 2005 HARD and Robust topics\n",
      "Books, C4, Wikipedia, RealNews, Amazon Reviews Robust04, Robust05, MQ 2007, MQ 2008, ClueWeb09-CatB, TREC Web Track 99-2001, TREC Deep Learning Track 2019-2021, AOL 6M Queries, Sogou-QL, Sogou-SR, Sogou-QC, Tiangong-ST. TREC web track, TREC Deep Learning Track, AOL, Sogou-QCL, Sogou-SRR, Tiangong-ST\n",
      "Books, C4, Wikipedia, RealNews, Amazon Reviews Robust04, Robust05, MQ 2007, MQ 2008, ClueWeb09-CatB, TREC Web Track 99-2001, TREC Deep Learning Track 2019-2021, AOL 6M Queries, Sogou-QL, Sogou-SR, Sogou-QC, Tiangong-ST. TREC web track, TREC Deep Learning Track, AOL, Sogou-QCL, Sogou-SRR, Tiangong-ST\n",
      "TIPSTER, TREC 4 & 5, AQUAINT, AQUAINT-2, ClueWeb cat. B. Websites in English (2009), Websites in English (2012), iSearch, MSN Commercial query log, Excite Commercial query log, Google books Syntactic unigrams TREC collections TREC disks 1 & 2, TREC disks 4& 5, WT10g and .GOV2 TREC collections.\n",
      "TIPSTER, TREC 4 & 5, AQUAINT, AQUAINT-2, ClueWeb cat. B. Websites in English (2009), Websites in English (2012), iSearch, MSN Commercial query log, Excite Commercial query log, Google books Syntactic unigrams TREC collections TREC disks 1 & 2, TREC disks 4& 5, WT10g and .GOV2 TREC collections.\n",
      "Bajaj et al., 2016, Craswell et al., 2019\n",
      "Bajaj et al., 2016, Craswell et al., 2019\n",
      "Version January 24, 2023 submitted to Journal Not Specified 7 of 26 \"DeepImpact\" and \"COIL\" (Continuous Optimization-based Information Retrieval) TREC collections\n",
      "Version January 24, 2023 submitted to Journal Not Specified 7 of 26 \"DeepImpact\" and \"COIL\" (Continuous Optimization-based Information Retrieval) TREC collections\n",
      "ROUGE, QAEval, and BERTScore. ROUGE, QAEval, BERTScore ROUGE, BERTScore BLEU, Learning to Score System Summaries, Ranking Human and Machine Summarization Systems Williams' test, Kendall's τ, paired t-tests, Wilcoxon signed-rank tests BLEU and ROUGE. N-Gram Graphs, NPowER-ed, N-Gram Graphs\n",
      "ROUGE, QAEval, and BERTScore. ROUGE, QAEval, BERTScore ROUGE, BERTScore BLEU, Learning to Score System Summaries, Ranking Human and Machine Summarization Systems Williams' test, Kendall's τ, paired t-tests, Wilcoxon signed-rank tests BLEU and ROUGE. N-Gram Graphs, NPowER-ed, N-Gram Graphs\n",
      "SacreROUGE Pearson, Spearman, and Kendall. ROUGE and Automatically evaluating content selection in summarization without human models.\n",
      "SacreROUGE Pearson, Spearman, and Kendall. ROUGE and Automatically evaluating content selection in summarization without human models.\n",
      "ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, MoverScore, JS-2 Inter-metric correlation BERTScore (BScore), MoverScore (MS), JS divergence (JS-2), ROUGE-1 (R1), ROUGE-2 (R2), ROUGE-L ROUGE, BERTScore, MoverScore\n",
      "ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, MoverScore, JS-2 Inter-metric correlation BERTScore (BScore), MoverScore (MS), JS divergence (JS-2), ROUGE-1 (R1), ROUGE-2 (R2), ROUGE-L ROUGE, BERTScore, MoverScore\n",
      "Automatic summarization evaluation metrics. Automatic evaluation metrics ROUGE Automatic metrics Automatic evaluation metrics. ROUGE-1, BERTScore, QAEval Pyramid Method, Studying Summarization Evaluation Metrics, Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation ROUGE-1\n",
      "Automatic summarization evaluation metrics. Automatic evaluation metrics ROUGE Automatic metrics Automatic evaluation metrics. ROUGE-1, BERTScore, QAEval Pyramid Method, Studying Summarization Evaluation Metrics, Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation ROUGE-1\n",
      "ROUGE Manual evaluations using the lightweight pyramids method (Shapira et al., 2019), automated metrics, MoverScore, ROUGE-2. Weighted macro F1 score, ROUGE based metrics, R-2, JS-2 ROUGE, JS-2, S3, BERTScore, MoverScore ROUGE-2, ROUGE-L, ROUGE-WE, JS-2, S3. Pyramid, lightweight and crowdsourceable version of Pyramid\n",
      "ROUGE Manual evaluations using the lightweight pyramids method (Shapira et al., 2019), automated metrics, MoverScore, ROUGE-2. Weighted macro F1 score, ROUGE based metrics, R-2, JS-2 ROUGE, JS-2, S3, BERTScore, MoverScore ROUGE-2, ROUGE-L, ROUGE-WE, JS-2, S3. Pyramid, lightweight and crowdsourceable version of Pyramid\n",
      "ROUGE and BERTScore. ROUGE, BERTScore ROUGE (Lin, 2004) or BERTScore (Zhang et al., 2019) ROUGE, BERTScore, other summarization evaluation metrics ROUGE and BERTScore, 10 other evaluation metrics. ROUGE Automated pyramid summarization evaluation, Autosummeng and memog, Summary evaluation: Together we stand npower-ed.\n",
      "ROUGE and BERTScore. ROUGE, BERTScore ROUGE (Lin, 2004) or BERTScore (Zhang et al., 2019) ROUGE, BERTScore, other summarization evaluation metrics ROUGE and BERTScore, 10 other evaluation metrics. ROUGE Automated pyramid summarization evaluation, Autosummeng and memog, Summary evaluation: Together we stand npower-ed.\n",
      "Kendall's tau rank correlations Consistency, Fluency, Relevance 14 automatic evaluation metrics ROUGE and its variants. ROUGE-L, SummaQA, BLANC, SuPERT Coherence, DUC quality question (Dang, 2005) of structure and coherence.\n",
      "Kendall's tau rank correlations Consistency, Fluency, Relevance 14 automatic evaluation metrics ROUGE and its variants. ROUGE-L, SummaQA, BLANC, SuPERT Coherence, DUC quality question (Dang, 2005) of structure and coherence.\n",
      "Precision and recall, in the form of focus and coverage. BLEU scores ROUGE (Lin, 2004), METEOR (Lavie and Agarwal, 2007), BLEU (Papineni et al., 2002), MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020b) ROUGE, METEOR, MoverScore, BERTScore Highlight-based reference-less evaluation ROUGE, manual and automatic evaluation BERTScore 19 summarization evaluation metrics ROUGE\n",
      "Precision and recall, in the form of focus and coverage. BLEU scores ROUGE (Lin, 2004), METEOR (Lavie and Agarwal, 2007), BLEU (Papineni et al., 2002), MoverScore (Zhao et al., 2019), BERTScore (Zhang et al., 2020b) ROUGE, METEOR, MoverScore, BERTScore Highlight-based reference-less evaluation ROUGE, manual and automatic evaluation BERTScore 19 summarization evaluation metrics ROUGE\n",
      "Automatic evaluation metrics such as ROUGE, BLEU, METEOR, and CIDEr. Faithfulness, recall, precision, relevance, coherence, and fluency. ROUGE (Lin, 2004), QAGS question answering-based framework (Wang et al., 2020) Faithfulness, precision, recall Faithfulness, focus, coverage, and inter-sentential coherence (FFCI) ROUGE, METEOR, BLEU, BERTScore, MoverScore ROUGE, METEOR, and BLEU ROUGE, METEOR, BLEU, QAGS, STS-Score (EDU), STS-Score (sentence), STS-Score (doc), BERTScore, Nayeem and Chali (2017), NSP Faithfulness, Relevance, Coherence, Fluency.\n",
      "Automatic evaluation metrics such as ROUGE, BLEU, METEOR, and CIDEr. Faithfulness, recall, precision, relevance, coherence, and fluency. ROUGE (Lin, 2004), QAGS question answering-based framework (Wang et al., 2020) Faithfulness, precision, recall Faithfulness, focus, coverage, and inter-sentential coherence (FFCI) ROUGE, METEOR, BLEU, BERTScore, MoverScore ROUGE, METEOR, and BLEU ROUGE, METEOR, BLEU, QAGS, STS-Score (EDU), STS-Score (sentence), STS-Score (doc), BERTScore, Nayeem and Chali (2017), NSP Faithfulness, Relevance, Coherence, Fluency.\n",
      "BLEU, METEOR, ROUGE, ROUGE-WE, SUM-QE, APES, Summa-QA BLEU, BLANC ROUGE, BERTScore, JS, SUM-QE, BLANC BLANC, crowd and expert ratings, commonly used automatic metrics Mean Opinion Score (MOS) scale, summary usefulness, post usefulness, and summary informativeness. Automatic Quality Evaluation Automatic evaluation metric for news article summarization, meta evaluation of factuality in summarization, unsupervised evaluation metrics for multi-document summarization. ROUGE\n",
      "BLEU, METEOR, ROUGE, ROUGE-WE, SUM-QE, APES, Summa-QA BLEU, BLANC ROUGE, BERTScore, JS, SUM-QE, BLANC BLANC, crowd and expert ratings, commonly used automatic metrics Mean Opinion Score (MOS) scale, summary usefulness, post usefulness, and summary informativeness. Automatic Quality Evaluation Automatic evaluation metric for news article summarization, meta evaluation of factuality in summarization, unsupervised evaluation metrics for multi-document summarization. ROUGE\n"
     ]
    }
   ],
   "source": [
    "# only keep rows that are not NAN\n",
    "preds = list(preds[~preds.isna()])\n",
    "refs = list(refs[~refs.isna()])\n",
    "\n",
    "for pred, ref in zip(preds, refs):\n",
    "    print(pred)\n",
    "    print(ref)\n",
    "    assert pred == ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "results = exact_match.compute(references=refs, predictions=preds)\n",
    "print(results)\n",
    "# print(\"Query: \", query_dict[query_ids[0]][\"extract_query\"])\n",
    "# print(\"Exact-Match: \", round(results[\"exact_match\"], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0489772436b17a7398828cf658e622302372e5cbea7306ec070702a3e6380b80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
