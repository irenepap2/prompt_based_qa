{"question_id": "4137a82d7752be7a6c142ceb48ce784fd475fb06", "predicted_answer": "Weka", "predicted_evidence": []}
{"question_id": "7fba61426737394304e307cdc7537225f6253150", "predicted_answer": "Numerical Features", "predicted_evidence": []}
{"question_id": "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521", "predicted_answer": "Unanswerable", "predicted_evidence": []}
{"question_id": "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b", "predicted_answer": "PBMT-R, SBMT", "predicted_evidence": []}
{"question_id": "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02", "predicted_answer": "Various emotions dataset", "predicted_evidence": []}
{"question_id": "d36a6447bfe58204e0d29f9213d84be04d875624", "predicted_answer": "A self made linked noise and EEG", "predicted_evidence": []}
{"question_id": "1237b6fcc64b43901415f3ded17cc210a54ab698", "predicted_answer": "Published IEEE papers", "predicted_evidence": []}
{"question_id": "4748a50c96acb1aa03f7efd1b43376c193b2450a", "predicted_answer": "Snap Amazon dataset", "predicted_evidence": []}
{"question_id": "0fe49431db5ffaa24372919daf24d8f84117bfda", "predicted_answer": "They use 2 datasets. The first one is for sentence extraction which labels sentences from news articles that must appear in the corresponding summary and the other one is for word extraction with words that appear in both the article and the highlight", "predicted_evidence": []}
{"question_id": "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f", "predicted_answer": "They use 2 datasets. 1 dataset where chat sessions are manually annotated as the ground truth and 1 more unlabeled dataset to test their framework on", "predicted_evidence": []}
{"question_id": "9489b0ecb643c1fc95c001c65d4e9771315989aa", "predicted_answer": "The English portion of CoNLL 2012 data", "predicted_evidence": []}
{"question_id": "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763", "predicted_answer": "A dataset containing different vulnerabilities in code commits", "predicted_evidence": []}
{"question_id": "1ecbbb60dc44a701e9c57c22167dd412711bb0be", "predicted_answer": "Clueweb09 and a generated one crawling Wikipedia", "predicted_evidence": []}
{"question_id": "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e", "predicted_answer": "Data from a survey and handcrafted questions", "predicted_evidence": []}
{"question_id": "8c46a26f9b0b41c656b5b55142d491600663defa", "predicted_answer": "GlobalPhone", "predicted_evidence": []}
{"question_id": "a398c9b061f28543bc77c2951d0dfc5d1bee9e87", "predicted_answer": "Tweets annotated by Potthast et al", "predicted_evidence": []}
{"question_id": "ebe1084a06abdabefffc66f029eeb0b69f114fd9", "predicted_answer": "standard bididrectional RNN model with attention, a standard context agnostic Transformer.", "predicted_evidence": []}
{"question_id": "eb653a5c59851eda313ece0bcd8c589b6155d73e", "predicted_answer": "one-stage RNN system", "predicted_evidence": []}
{"question_id": "634a071b13eb7139e77872ecfdc135a2eb2f89da", "predicted_answer": "the methods suggested in Dori-Hacohen et al. (2016); Rad and Barbosa (2012)", "predicted_evidence": []}
{"question_id": "8cf52ba480d372fc15024b3db704952f10fdca27", "predicted_answer": "The original ESIM model", "predicted_evidence": []}
{"question_id": "3f717e6eceab0a066af65ddf782c1ebc502c28c0", "predicted_answer": "FIGER (GOLD) and BBN", "predicted_evidence": []}
{"question_id": "420862798054f736128a6f0c4393c7f9cc648b40", "predicted_answer": "SICK (Sentences Involving Compositional Knowledge)", "predicted_evidence": []}
{"question_id": "bc6ad5964f444cf414b661a4b942dafb7640c564", "predicted_answer": "Open Minds Indoor Common Sense (OMICS) corpus", "predicted_evidence": []}
{"question_id": "f3b4e52ba962a0004064132d123fd9b78d9e12e2", "predicted_answer": "NIST 2006 (NIST06)", "predicted_evidence": []}
{"question_id": "50bda708293532f07a3193aaea0519d433fcc040", "predicted_answer": "Accuracy and the custom error metric defined in equation (1)", "predicted_evidence": []}
{"question_id": "8908d1b865137bc309dde10a93735ec76037e5f9", "predicted_answer": "Avg. Recall, Accuracy and F-score", "predicted_evidence": []}
{"question_id": "301a453abaa3bc15976817fefce7a41f3b779907", "predicted_answer": "F-score, micro-averaged F-score", "predicted_evidence": []}
{"question_id": "74a17eb3bf1d4f36e2db1459a342c529b9785f6e", "predicted_answer": "4-gram BLEU score", "predicted_evidence": []}
{"question_id": "4a91432abe3f54fcbdd00bb85dc0df95b16edf42", "predicted_answer": "Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets", "predicted_evidence": []}
{"question_id": "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b", "predicted_answer": "Unanswerable", "predicted_evidence": []}
{"question_id": "0b5c599195973c563c4b1a0fe5d8fc77204d71a0", "predicted_answer": "After removing documents with missing PDF files and documents which were not converted successfully, we were left with 624 full text documents", "predicted_evidence": []}
{"question_id": "80bb07e553449bde9ac0ff35fcc718d7c161f2d4", "predicted_answer": "The train set contained 20214 sentence pairs, while the validation contained 1000 sentence pairs", "predicted_evidence": []}
{"question_id": "bc7081aaa207de2362e0bea7bc8108d338aee36f", "predicted_answer": "Yes", "predicted_evidence": []}
{"question_id": "693cdb9978749db04ba34d9c168e71534f00a226", "predicted_answer": "Unanswerable", "predicted_evidence": []}
{"question_id": "a17fc7b96753f85aee1d2036e2627570f4b50c30", "predicted_answer": "No", "predicted_evidence": []}
{"question_id": "2f142cd11731d29d0c3fa426e26ef80d997862e0", "predicted_answer": "Yes", "predicted_evidence": []}
{"question_id": "90aba75508aa145475d7cc9a501bbe987c0e8413", "predicted_answer": "Craigslist Bargaining dataset (CB)", "predicted_evidence": []}
{"question_id": "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c", "predicted_answer": "PDTB 2.05", "predicted_evidence": []}
{"question_id": "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8", "predicted_answer": "(1) human-human utterances from massive online forums, microblogs, and question-answering communities, such as Sina Weibo Baidu Zhidao and Baidu Tieba, (2) dataset from various resources in public websites comprising 1,606,741 query-reply pairs.", "predicted_evidence": []}
{"question_id": "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e", "predicted_answer": "Speech recording of patients in (1) Spanish (PC-GITA corpus), (2) German, (3) Czech", "predicted_evidence": []}
{"question_id": "77bbe1698e001c5889217be3164982ea36e85752", "predicted_answer": "Baseline-BiLSTM, CRF-BiLSTM, Cross-BiLSTM-CNN and AttBiLSTM-CNN", "predicted_evidence": []}
{"question_id": "fe6181ab0aecf5bc8c3def843f82e530347d918b", "predicted_answer": "MLE model and Baseline+(t), where t \u00e2\u02c6\u02c6 [0, 1]", "predicted_evidence": []}
{"question_id": "8e113fd9661bc8af97e30c75a20712f01fc4520a", "predicted_answer": "ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT in two setups: BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model", "predicted_evidence": []}
{"question_id": "23cbf6ab365c1eb760b565d8ba51fb3f06257d62", "predicted_answer": "Transformer baseline", "predicted_evidence": []}
{"question_id": "e801b6a6048175d3b1f3440852386adb220bcb36", "predicted_answer": "330M", "predicted_evidence": []}
{"question_id": "0a736e0e3305a50d771dfc059c7d94b8bd27032e", "predicted_answer": "6381+7527+5424 = 19332 reviews", "predicted_evidence": []}
{"question_id": "576a3ed6e4faa4c3893db632e97a52ac6e864aac", "predicted_answer": "98976 tokens", "predicted_evidence": []}
{"question_id": "16b816925567deb734049416c149747118e13963", "predicted_answer": "3045+2000 = 5045 sentences in training set and 800+676 = 1476 in test set", "predicted_evidence": []}
