{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open json file\n",
    "import json\n",
    "with open(\"start_template.qsf\", \"r\") as f:\n",
    "    survey = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./qasper_data.json\", \"r\") as f:\n",
    "    qasper_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in qasper_data.items():\n",
    "    evidence = v[\"evidence\"]\n",
    "    answers = v[\"answers\"]\n",
    "    evidence_HTML = []\n",
    "    for answer, passages in zip(answers, evidence):\n",
    "        HTML_string = \"\"\n",
    "        for passage in passages:\n",
    "            if answer in passage:\n",
    "                passage = passage.replace(answer, \"<b>\" + answer + \"</b>\")\n",
    "                HTML_string += f\"<br /><br />{passage}\"\n",
    "            elif answer.lower() in passage.lower():\n",
    "                temp = passage.lower()\n",
    "                passage_lower = temp.replace(answer.lower(), \"<b>\" + answer.lower() + \"</b>\")\n",
    "                # passage = passage.replace(answer.lower(), \"<b>\" + answer.lower() + \"</b>\")\n",
    "                HTML_string += f\"<br /><br />{passage_lower}\"\n",
    "            elif answer == \"Unanswerable\":\n",
    "                HTML_string = \"<br />The question does not have an answer based on the given document.\"\n",
    "        if HTML_string == \"\":\n",
    "            HTML_string += f\"<br /><br />{passages[0]}\"\n",
    "        evidence_HTML.append(HTML_string)\n",
    "        \n",
    "    qasper_data[k][\"evidence_HTMLs\"] = evidence_HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_Q(element, q_key):\n",
    "    query = qasper_data[q_key]\n",
    "    titles = query[\"titles\"]\n",
    "    urls = query[\"URLs\"]\n",
    "\n",
    "    element[\"SecondaryAttribute\"] = q_key\n",
    "    element[\"Payload\"][\"QuestionText\"] = f\"\"\"\n",
    "    <u>Task 1: Answer the question given the Document.</u><br>\n",
    "    You are given a set of 4 documents and a question that needs to be answered for each of the documents. \n",
    "    Your task is to answer the question based on the document's content. \n",
    "    If the answer can not be found in the document then answer with \"Unanswerable\".\n",
    "    <br><br>\n",
    "    <u>Question:</u> {q_key}\n",
    "    \"\"\"\n",
    "    element[\"Payload\"][\"DataExportTag\"] = \"Task1-\"+q_key\n",
    "    element[\"Payload\"][\"Choices\"] = {k+1 : {\n",
    "        \"Display\": f\"<a href=\\\"{url}\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\">{title}</a>\",\n",
    "        \"InputHeight\": 29,\n",
    "        \"InputWidth\": 470,\n",
    "        \"TextEntry\": \"on\"\n",
    "    } for k, (title, url) in enumerate(zip(titles, urls))}\n",
    "    element[\"Payload\"][\"ChoiceOrder\"] = list(range(1,len(titles)+1))\n",
    "    element[\"Payload\"][\"QuestionDescription\"] = q_key\n",
    "    element[\"Payload\"][\"Validation\"] = {\n",
    "        \"Settings\": {\n",
    "            \"ForceResponse\": \"ON\",\n",
    "            \"ForceResponseType\": \"ON\",\n",
    "            \"Type\": \"None\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def task2_Q(element, q_key, task1ID):\n",
    "    query = qasper_data[q_key]\n",
    "    titles = query[\"titles\"]\n",
    "    urls = query[\"URLs\"]\n",
    "    gen_answers = query[\"answers\"]\n",
    "    evidence_HTMLs = query[\"evidence_HTMLs\"]\n",
    "\n",
    "    element[\"SecondaryAttribute\"] = q_key\n",
    "    element[\"Payload\"][\"QuestionText\"] = f\"\"\"\n",
    "    <u>Task 2: Answer the question given the Document, a generated Answer and the corresponding Evidence.</u><br>\n",
    "    Same as in Task 1, you are given a set of 4 documents and a question that needs to be answered for each of the documents.\n",
    "    Now, you are also given an answer generated by a language model and evidence passages, meaning passages from the document in which the answer is found.<br>  \n",
    "    Your task is, again, to answer the question, based on the document's content now with the help of the generated answer and the evidence passages.<br>\n",
    "    Only open the document if the answer is incorrect and cannot be found in the supporting evidence.<br>\n",
    "    If the generated answer is correct then leave it as is in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n",
    "    If the answer can not be found in the document, answer \"Unanswerable\".\n",
    "    <br><br>\n",
    "    <u>Question:</u> {q_key}\n",
    "    \"\"\"\n",
    "    element[\"Payload\"][\"DataExportTag\"] = \"Task2-\"+q_key\n",
    "    element[\"Payload\"][\"Choices\"] = {k+1 : {\n",
    "        \"Display\": f\"<a href=\\\"{url}\\\" target=\\\"_blank\\\" rel=\\\"noopener noreferrer\\\">{title}</a><br><b>Answer: </b>{gen_answer}<br><strong>Evidence:</strong>{evidence_HTML}\",\n",
    "        \"InputHeight\": 29,\n",
    "        \"InputWidth\": 470,\n",
    "    } for k, (title, url, gen_answer, evidence_HTML) in enumerate(zip(titles, urls, gen_answers, evidence_HTMLs))}\n",
    "    element[\"Payload\"][\"DefaultChoices\"] = {k+1: {\n",
    "        \"1\": {\n",
    "            \"Value\": gen_answer\n",
    "        }\n",
    "    } for k, gen_answer in enumerate(gen_answers)}\n",
    "    element[\"Payload\"][\"ChoiceOrder\"] = list(range(1,len(titles)+1))\n",
    "    element[\"Payload\"][\"QuestionDescription\"] = q_key\n",
    "    element[\"Payload\"][\"DisplayLogic\"][\"0\"][\"0\"][\"QuestionID\"] = task1ID\n",
    "    element[\"Payload\"][\"Validation\"] = {\n",
    "        \"Settings\": {\n",
    "            \"ForceResponse\": \"ON\",\n",
    "            \"ForceResponseType\": \"ON\",\n",
    "            \"Type\": \"None\"\n",
    "        }\n",
    "    }\n",
    "    element[\"Payload\"][\"Configuration\"]= {\n",
    "        \"ChoiceColumnWidthPixels\": 400,\n",
    "    }\n",
    "    element[\"Payload\"][\"Answers\"] = {\n",
    "        \"1\": {\n",
    "            \"Display\": \" \"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_elements = survey[\"SurveyElements\"]\n",
    "# # Overwrite the values of the four base questions, two for each task\n",
    "for i, element in enumerate(survey_elements):\n",
    "    q_key = \"what was the baseline?\"\n",
    "\n",
    "    task1_q1_ID = \"QID1\"\n",
    "    task2_q1_ID = \"QID3\"\n",
    "    if element[\"PrimaryAttribute\"] == task1_q1_ID:\n",
    "        default_task1_element = element.copy()\n",
    "        task1_Q(element, q_key)\n",
    "\n",
    "    elif element[\"PrimaryAttribute\"] == task2_q1_ID:\n",
    "        default_task2_element = element.copy()\n",
    "        task2_Q(element, q_key, task1_q1_ID)\n",
    "\n",
    "    q_key = \"what dataset was used?\"\n",
    "\n",
    "    task1_q2_ID = \"QID22\"\n",
    "    task2_q2_ID = \"QID23\"\n",
    "    if element[\"PrimaryAttribute\"] == task1_q2_ID:\n",
    "        task1_Q(element, q_key)\n",
    "\n",
    "    elif element[\"PrimaryAttribute\"] == task2_q2_ID:\n",
    "        task2_Q(element, q_key, task1_q2_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In survey_elements, element where element[\"Element\"] == \"BL\" is the survey blocks\n",
    "# then element[\"Payload\"] is the list of blocks where element[\"Payload\"][\"0\"]\n",
    "# is the block for task1 and element[\"Payload\"][\"2\"] is the block for task2\n",
    "# in a block, for the list block[\"BlockElements\"] append\n",
    "# {\n",
    "#     \"Type\": \"Question\",\n",
    "#     \"QuestionID\": \"the question's id\",\n",
    "# }\n",
    "# for each added question.\n",
    "# Then also for the list block[\"Options\"][\"Randomization\"][\"Advanced\"][\"RandomSubSet\"]\n",
    "# append the question's id\n",
    "\n",
    "survey_elements = survey[\"SurveyElements\"]\n",
    "survey[\"SurveyEntry\"][\"SurveyName\"] = \"Qasper data\"\n",
    "\n",
    "question_counter = int([e for e in survey_elements if e[\"Element\"] == \"QC\"][0][\"SecondaryAttribute\"])\n",
    "\n",
    "# Create new question elements\n",
    "for q_key in qasper_data.keys():\n",
    "    # print(q_key)\n",
    "    # if q_key in [\"8d44bd26\", \"1aaefbd0\"]:\n",
    "    #     continue\n",
    "    # Add task1 question\n",
    "    # element1_id = \"QID-T1-\"+q_key\n",
    "    question_counter += 1\n",
    "    element1_id = f\"QID{question_counter}\"\n",
    "\n",
    "    element1 = json.loads(json.dumps(default_task1_element)) # deep copy\n",
    "    task1_Q(element1, q_key)\n",
    "    element1[\"PrimaryAttribute\"] = element1_id\n",
    "    element1[\"Payload\"][\"QuestionID\"] = element1_id\n",
    "    length = len(survey[\"SurveyElements\"])\n",
    "    survey[\"SurveyElements\"].append(element1)\n",
    "    \n",
    "    # display(survey[\"SurveyElements\"][length-2])\n",
    "    # display(survey[\"SurveyElements\"][length])\n",
    "\n",
    "    # Add task2 question\n",
    "    # element2_id = \"QID-T2-\"+q_key\n",
    "    question_counter += 1\n",
    "    element2_id = f\"QID{question_counter}\"\n",
    "    \n",
    "    element2 = json.loads(json.dumps(default_task2_element)) # deep copy\n",
    "    task2_Q(element2, q_key, element1_id)\n",
    "    element2[\"PrimaryAttribute\"] = element2_id\n",
    "    element2[\"Payload\"][\"QuestionID\"] = element2_id\n",
    "    survey[\"SurveyElements\"].append(element2)\n",
    "\n",
    "    # Update blocks\n",
    "    blocks = [e for e in survey[\"SurveyElements\"] if e[\"Element\"]==\"BL\"][0]\n",
    "    blocks[\"Payload\"][\"0\"][\"BlockElements\"].append({\n",
    "        \"Type\": \"Question\",\n",
    "        \"QuestionID\": element1_id,\n",
    "    })\n",
    "    blocks[\"Payload\"][\"2\"][\"BlockElements\"].append({\n",
    "        \"Type\": \"Question\",\n",
    "        \"QuestionID\": element2_id,\n",
    "    })\n",
    "    blocks[\"Payload\"][\"0\"][\"Options\"][\"Randomization\"][\"Advanced\"][\"RandomSubSet\"].append(element1_id)\n",
    "    blocks[\"Payload\"][\"2\"][\"Options\"][\"Randomization\"][\"Advanced\"][\"RandomSubSet\"].append(element2_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save json\n",
    "with open(\"productivity_experiment_qasper_data.qsf\", \"w\") as f:\n",
    "    json.dump(survey, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
