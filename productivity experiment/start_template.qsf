{"SurveyEntry":{"SurveyID":"SV_8iyc6G8rZOzeZW6","SurveyName":"Start","SurveyDescription":null,"SurveyOwnerID":"UR_8eKZV4jJPWIuIfA","SurveyBrandID":"uva","DivisionID":"DV_2g9pK71JDa4rVYx","SurveyLanguage":"EN","SurveyActiveResponseSet":"RS_aeJPQ3Iu6rQ9BaK","SurveyStatus":"Inactive","SurveyStartDate":"0000-00-00 00:00:00","SurveyExpirationDate":"0000-00-00 00:00:00","SurveyCreationDate":"2023-05-02 05:47:13","CreatorID":"UR_8eKZV4jJPWIuIfA","LastModified":"2023-05-02 07:15:35","LastAccessed":"0000-00-00 00:00:00","LastActivated":"0000-00-00 00:00:00","Deleted":null},"SurveyElements":[{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"BL","PrimaryAttribute":"Survey Blocks","SecondaryAttribute":null,"TertiaryAttribute":null,"Payload":{"0":{"Type":"Default","Description":"Task1","ID":"BL_bl6Gf51I75N6nEW","BlockElements":[{"Type":"Question","QuestionID":"QID2"},{"Type":"Question","QuestionID":"QID1"},{"Type":"Question","QuestionID":"QID22"}],"Options":{"BlockLocking":"false","RandomizeQuestions":"Advanced","BlockVisibility":"Expanded","Randomization":{"Advanced":{"FixedOrder":["{~Randomized~}","{~SubSet~}"],"RandomizeAll":["QID2"],"RandomSubSet":["QID1","QID22","QID24","QID26","QID28"],"Undisplayed":[],"TotalRandSubset":1,"QuestionsPerPage":0},"EvenPresentation":true}}},"1":{"Type":"Trash","Description":"Trash \/ Unused Questions","ID":"BL_cM7iktSbWEtZAs6","BlockElements":[{"Type":"Question","QuestionID":"QID24"},{"Type":"Question","QuestionID":"QID26"},{"Type":"Question","QuestionID":"QID28"},{"Type":"Question","QuestionID":"QID25"},{"Type":"Question","QuestionID":"QID27"},{"Type":"Question","QuestionID":"QID29"},{"Type":"Question","QuestionID":"QID23"}]},"2":{"Type":"Standard","SubType":"","Description":"Task2","ID":"BL_bsIslt16LUds9a6","BlockElements":[{"Type":"Question","QuestionID":"QID4"},{"Type":"Question","QuestionID":"QID3"},{"Type":"Question","QuestionID":"QID31"}],"Options":{"BlockLocking":"false","RandomizeQuestions":"Advanced","BlockVisibility":"Expanded","Randomization":{"Advanced":{"FixedOrder":["{~Randomized~}","{~SubSet~}","QID31"],"RandomizeAll":["QID4"],"RandomSubSet":["QID3"],"Undisplayed":[],"TotalRandSubset":1,"QuestionsPerPage":0},"EvenPresentation":true}}},"3":{"Type":"Standard","SubType":"","Description":"Pre-Task1","ID":"BL_3Dj5laZEghsJt2u","BlockElements":[{"Type":"Question","QuestionID":"QID6"}],"Options":{"BlockLocking":"false","RandomizeQuestions":"false","BlockVisibility":"Collapsed"}},"4":{"Type":"Standard","SubType":"","Description":"Pre-Task2","ID":"BL_1OfoAq1t9TwTvng","BlockElements":[{"Type":"Question","QuestionID":"QID7"}],"Options":{"BlockLocking":"false","RandomizeQuestions":"false","BlockVisibility":"Expanded"}},"6":{"Type":"Standard","SubType":"","Description":"Introduction","ID":"BL_1SS5gVuq5tOjQ5o","BlockElements":[{"Type":"Question","QuestionID":"QID1211903273"},{"Type":"Question","QuestionID":"QID1211903274"},{"Type":"Question","QuestionID":"QID1211903275"}],"Options":{"BlockLocking":"false","RandomizeQuestions":"false","BlockVisibility":"Collapsed"}},"8":{"Type":"Standard","Description":"Demographics","ID":"BL_54EZfzGKGJQIM7k","BlockElements":[{"Type":"Question","QuestionID":"QID1211903276"},{"Type":"Question","QuestionID":"QID1211903277"},{"Type":"Question","QuestionID":"QID1211903278"},{"Type":"Question","QuestionID":"QID1211903279"},{"Type":"Question","QuestionID":"QID1211903283"},{"Type":"Question","QuestionID":"QID1211903285"}],"Options":{"BlockLocking":"false","RandomizeQuestions":"false","BlockVisibility":"Collapsed"}},"10":{"Type":"Standard","SubType":"","Description":"Efficacy","ID":"BL_2nloJzavYh1alwi","BlockElements":[{"Type":"Question","QuestionID":"QID1211903284"}],"Options":{"BlockLocking":"false","RandomizeQuestions":"false","BlockVisibility":"Collapsed"}},"12":{"Type":"Standard","SubType":"","Description":"Efficacy","ID":"BL_9tnQNp2BFl5YC0u","BlockElements":[{"Type":"Question","QuestionID":"QID21"}],"Options":{"BlockLocking":"false","RandomizeQuestions":"false","BlockVisibility":"Expanded"}},"13":{"Type":"Standard","SubType":"","Description":"emailBlock 8","ID":"BL_6ngbnMsTlzQYrpc","BlockElements":[{"Type":"Question","QuestionID":"QID30"}],"Options":{"BlockLocking":"false","RandomizeQuestions":"false","BlockVisibility":"Expanded"}}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"FL","PrimaryAttribute":"Survey Flow","SecondaryAttribute":null,"TertiaryAttribute":null,"Payload":{"Type":"Root","FlowID":"FL_1","Flow":[{"Type":"Standard","ID":"BL_1SS5gVuq5tOjQ5o","FlowID":"FL_2","Autofill":[]},{"Type":"Standard","ID":"BL_54EZfzGKGJQIM7k","FlowID":"FL_3","Autofill":[]},{"Type":"Standard","ID":"BL_3Dj5laZEghsJt2u","FlowID":"FL_5","Autofill":[]},{"Type":"BlockRandomizer","FlowID":"FL_14","SubSet":2,"EvenPresentation":true,"Flow":[{"Type":"Block","ID":"BL_bl6Gf51I75N6nEW","FlowID":"FL_7","Autofill":[]},{"Type":"Branch","FlowID":"FL_16","Description":"New Branch","BranchLogic":{"0":{"0":{"LogicType":"Question","LeftOperand":"q:\/\/undefined\/undefined","Type":"Expression","Description":"<span class=\"Error\">Invalid Logic<\/span> <a href=\"javascript:void(0)\" clickcallback=\"Q_LogicEditor.edit\" instanceid=\"LE_10069129\">Click Here to Edit Logic<\/a>"},"Type":"If"},"Type":"BooleanExpression"}}]},{"Type":"Standard","ID":"BL_2nloJzavYh1alwi","FlowID":"FL_11","Autofill":[]},{"Type":"Standard","ID":"BL_1OfoAq1t9TwTvng","FlowID":"FL_8","Autofill":[]},{"Type":"Standard","ID":"BL_bsIslt16LUds9a6","FlowID":"FL_9","Autofill":[]},{"Type":"Standard","ID":"BL_9tnQNp2BFl5YC0u","FlowID":"FL_13","Autofill":[]},{"ID":"BL_6ngbnMsTlzQYrpc","Type":"Standard","FlowID":"FL_18"},{"Type":"EndSurvey","FlowID":"FL_15"}],"Properties":{"Count":18,"RemovedFieldsets":[]}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"PL","PrimaryAttribute":"Preview Link","SecondaryAttribute":null,"TertiaryAttribute":null,"Payload":{"PreviewType":"Brand","PreviewID":"d47c6030-b126-477b-8457-c692c53afba5"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SO","PrimaryAttribute":"Survey Options","SecondaryAttribute":null,"TertiaryAttribute":null,"Payload":{"BackButton":"false","SaveAndContinue":"true","SurveyProtection":"PublicSurvey","BallotBoxStuffingPrevention":"false","NoIndex":"Yes","SecureResponseFiles":"true","SurveyExpiration":"None","SurveyTermination":"DefaultMessage","Header":"","Footer":"","ProgressBarDisplay":"None","PartialData":"+1 week","ValidationMessage":"","PreviousButton":"","NextButton":"","SurveyTitle":"Qualtrics Survey | Qualtrics Experience Management","SkinLibrary":"uva","SkinType":"templated","Skin":{"brandingId":"3067498872","templateId":"*base","overrides":null},"NewScoring":1,"SurveyMetaDescription":"The most powerful, simple and trusted way to gather experience data. Start your journey to experience management and try a free account today.","SurveyName":"Start"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SCO","PrimaryAttribute":"Scoring","SecondaryAttribute":null,"TertiaryAttribute":null,"Payload":{"ScoringCategories":[],"ScoringCategoryGroups":[],"ScoringSummaryCategory":null,"ScoringSummaryAfterQuestions":0,"ScoringSummaryAfterSurvey":0,"DefaultScoringCategory":null,"AutoScoringCategory":null}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"PROJ","PrimaryAttribute":"CORE","SecondaryAttribute":null,"TertiaryAttribute":"1.1.0","Payload":{"ProjectCategory":"CORE","SchemaVersion":"1.1.0"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"STAT","PrimaryAttribute":"Survey Statistics","SecondaryAttribute":null,"TertiaryAttribute":null,"Payload":{"MobileCompatible":true,"ID":"Survey Statistics"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"QC","PrimaryAttribute":"Survey Question Count","SecondaryAttribute":"31","TertiaryAttribute":null,"Payload":null},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"RS","PrimaryAttribute":"RS_aeJPQ3Iu6rQ9BaK","SecondaryAttribute":"Default Response Set","TertiaryAttribute":null,"Payload":null},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID1211903275","SecondaryAttribute":"I am at least 18 years of age.","TertiaryAttribute":null,"Payload":{"QuestionText":"I am at least 18 years of age.","QuestionType":"MC","Selector":"SAVR","SubSelector":"TX","Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"I am at least 18 years of age.","Choices":{"3":{"Display":"No"},"4":{"Display":"Yes"}},"ChoiceOrder":["4","3"],"Validation":{"Settings":{"ForceResponse":"ON","ForceResponseType":"ON","Type":"None"}},"Language":[],"NextChoiceId":5,"NextAnswerId":1,"DataVisibility":{"Private":false,"Hidden":false},"QuestionText_Unsafe":"I am at least 18 years of age.","DataExportTag":"18+","QuestionID":"QID1211903275"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID1211903274","SecondaryAttribute":"I have been sufficiently informed about the study and agree to participate","TertiaryAttribute":null,"Payload":{"QuestionText":"<div>I have been sufficiently informed about the study and agree to participate<\/div>","QuestionType":"MC","Selector":"SAVR","SubSelector":"TX","Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"I have been sufficiently informed about the study and agree to participate","Choices":{"1":{"Display":"I agree to participate in this study"}},"ChoiceOrder":[1],"Validation":{"Settings":{"ForceResponse":"ON","ForceResponseType":"ON","Type":"CustomValidation","CustomValidation":{"Logic":{"0":{"0":{"ChoiceLocator":"q:\/\/QID1211903274\/SelectableChoice\/1","Description":"<span class=\"ConjDesc\">If<\/span> <span class=\"QuestionDesc\">I have been informed about the study and I have been given the opportunity to ask questions about...<\/span> <span class=\"LeftOpDesc\">I agree to participate in this study<\/span> <span class=\"OpDesc\">Is Selected<\/span> ","LeftOperand":"q:\/\/QID1211903274\/SelectableChoice\/1","LogicType":"Question","Operator":"Selected","QuestionID":"QID1211903274","QuestionIDFromLocator":"QID1211903274","QuestionIsInLoop":"no","Type":"Expression"},"Type":"If"},"Type":"BooleanExpression"},"Message":{"description":"wrong answer","libraryID":"UR_06aHius8mgA1IOx","messageID":"MS_8HzEfRMyoBMasCh","subMessageID":"VE_CUSTOM_VALIDATION_0"}}}},"Language":[],"NextChoiceId":2,"NextAnswerId":1,"DataVisibility":{"Private":false,"Hidden":false},"QuestionText_Unsafe":"<div>I have been sufficiently informed about the study and agree to participate<\/div>","DataExportTag":"consent","QuestionID":"QID1211903274"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID21","SecondaryAttribute":"On a scale of 1 to 10, rate the previous task","TertiaryAttribute":null,"Payload":{"QuestionText":"On a scale of 1 to 10, rate the previous task","DefaultChoices":false,"QuestionType":"Slider","Selector":"HSLIDER","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText","CSSliderMin":0,"CSSliderMax":10,"GridLines":10,"SnapToGrid":true,"NumDecimals":"0","ShowValue":true,"CustomStart":true,"NotApplicable":false,"MobileFirst":true,"SliderStartPositions":{"1":0,"2":0}},"QuestionDescription":"On a scale of 1 to 10, rate the previous task","Choices":{"1":{"Display":"I enjoyed doing it"},"2":{"Display":"I felt skilled\/effective doing it"},"3":{"Display":"The generated answers were helpful in finding the final answer"}},"ChoiceOrder":[1,2,3],"Validation":{"Settings":{"ForceResponse":"OFF","Type":"None"}},"GradingData":[],"Language":[],"NextChoiceId":4,"NextAnswerId":1,"Labels":[],"Answers":{"1":{"Display":0},"2":{"Display":1},"3":{"Display":2},"4":{"Display":3},"5":{"Display":4},"6":{"Display":5},"7":{"Display":6},"8":{"Display":7},"9":{"Display":8},"10":{"Display":9},"11":{"Display":10}},"QuestionText_Unsafe":"On a scale of 1 to 10, rate the previous task","DataExportTag":"Task2-Efficacy","QuestionID":"QID21"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID1211903284","SecondaryAttribute":"On a scale of 1 to 10, rate the previous task","TertiaryAttribute":null,"Payload":{"QuestionText":"On a scale of 1 to 10, rate the previous task","DefaultChoices":false,"QuestionType":"Slider","Selector":"HSLIDER","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText","CSSliderMin":0,"CSSliderMax":10,"GridLines":10,"SnapToGrid":true,"NumDecimals":"0","ShowValue":true,"CustomStart":true,"NotApplicable":false,"MobileFirst":true,"SliderStartPositions":{"1":0,"2":0}},"QuestionDescription":"On a scale of 1 to 10, rate the previous task","Choices":{"1":{"Display":"I enjoyed doing it"},"2":{"Display":"I felt skilled\/effective doing it"}},"ChoiceOrder":["1","2"],"Validation":{"Settings":{"ForceResponse":"OFF","Type":"None"}},"GradingData":[],"Language":[],"NextChoiceId":5,"NextAnswerId":1,"Labels":[],"Answers":{"1":{"Display":0},"2":{"Display":1},"3":{"Display":2},"4":{"Display":3},"5":{"Display":4},"6":{"Display":5},"7":{"Display":6},"8":{"Display":7},"9":{"Display":8},"10":{"Display":9},"11":{"Display":10}},"QuestionText_Unsafe":"On a scale of 1 to 10, rate the previous task","DataExportTag":"Task1-efficacy","QuestionID":"QID1211903284"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID30","SecondaryAttribute":"Please provide your email if you are interested in receiving compensation.","TertiaryAttribute":null,"Payload":{"QuestionText":"Please provide your email if you are interested in receiving compensation.","DefaultChoices":false,"DataExportTag":"Email","QuestionType":"TE","Selector":"SL","Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"Please provide your email if you are interested in receiving compensation.","Validation":{"Settings":{"ForceResponse":"OFF","Type":"ContentType","MinChars":"1","ContentType":"ValidEmail","ValidDateType":"DateWithFormat","ValidPhoneType":"ValidUSPhone","ValidZipType":"ValidUSZip","ValidNumber":{"Min":"","Max":"","NumDecimals":""}}},"GradingData":[],"Language":[],"NextChoiceId":4,"NextAnswerId":1,"SearchSource":{"AllowFreeResponse":"false"},"QuestionID":"QID30"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID1211903285","SecondaryAttribute":"Rate the following skills in English","TertiaryAttribute":null,"Payload":{"QuestionText":"Rate the following skills in English","DefaultChoices":false,"DataExportTag":"english-skills","QuestionID":"QID1211903285","QuestionType":"Slider","Selector":"HSLIDER","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText","CSSliderMin":1,"CSSliderMax":5,"GridLines":4,"SnapToGrid":true,"NumDecimals":"0","ShowValue":true,"CustomStart":true,"NotApplicable":false,"MobileFirst":true,"SliderStartPositions":{"1":0,"2":0}},"QuestionDescription":"Rate the following skills in English","Choices":{"1":{"Display":"writing"},"2":{"Display":"reading comprehension"}},"ChoiceOrder":[1,2],"Validation":{"Settings":{"ForceResponse":"OFF","Type":"None"}},"GradingData":[],"Language":[],"NextChoiceId":3,"NextAnswerId":1,"Labels":[],"Answers":{"1":{"Display":1},"2":{"Display":2},"3":{"Display":3},"4":{"Display":4},"5":{"Display":5}}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID3","SecondaryAttribute":"Task 2: Answer the question given a document and a generated answer. You are given a set of 10 do...","TertiaryAttribute":null,"Payload":{"QuestionText":"\n    <u>Task 2: Answer the question given a document and a generated answer.<\/u><br>\n    You are given a set of 10 documents, a question that needs to be answered for each of the documents, and an answer generated by a language model.\n    Your task is to check whether the answer to the question is correct, based on the document's content. \n    If the answer is correct then leave the text box empty. \n    If the answer is incorrect then write the correct answer in the text box.<br><br>\n\n    What Information Retrieval datasets are used?\n    ","DefaultChoices":false,"DataExportTag":"Task2-8d44bd26","QuestionID":"QID3","QuestionType":"Matrix","Selector":"TE","SubSelector":"Long","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText","TextPosition":"inline","ChoiceColumnWidth":25,"MobileFirst":true,"ChoiceColumnWidthPixels":319},"QuestionDescription":"Task 2: Answer the question given a document and a generated answer. You are given a set of 10 do...","Choices":{"1":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods<\/a><br><b>Answer: <\/b> ROUGE, QAEval, and BERTScore"},"2":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods<\/a><br><b>Answer: <\/b> ROUGE, QAEval, and BERTScore"},"3":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods<\/a><br><b>Answer: <\/b> ROUGE, QAEval, and BERTScore"},"4":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods<\/a><br><b>Answer: <\/b> ROUGE, QAEval, and BERTScore"}},"DisplayLogic":{"0":{"0":{"ChoiceLocator":"q:\/\/QID1\/ChoiceTextEntryValue\/1","Description":"<span class=\"ConjDesc\">If<\/span> <span class=\"QuestionDesc\">What 3D scene datasets are used?<\/span> <span class=\"LeftOpDesc\">Text Response<\/span> <span class=\"OpDesc\">Is Not Displayed<\/span> ","LeftOperand":"q:\/\/QID1\/ChoiceDisplayed\/1","LogicType":"Question","Operator":"NotDisplayed","QuestionID":"QID1","QuestionIDFromLocator":"QID1","QuestionIsInLoop":"no","Type":"Expression"},"Type":"If"},"Type":"BooleanExpression","inPage":false},"ChoiceOrder":[1,2,3,4],"Validation":{"Settings":{"ForceResponse":"OFF","Type":"None"}},"GradingData":[],"Language":[],"NextChoiceId":5,"NextAnswerId":2,"Answers":{"1":{"Display":"Click to write Scale Point 1"}},"AnswerOrder":[1],"ChoiceDataExportTags":false}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID23","SecondaryAttribute":"Task 2: Answer the question given a document and a generated answer. You are given a set of 10 do...","TertiaryAttribute":null,"Payload":{"QuestionText":"\n    <u>Task 2: Answer the question given a document and a generated answer.<\/u><br>\n    You are given a set of 10 documents, a question that needs to be answered for each of the documents, and an answer generated by a language model.\n    Your task is to check whether the answer to the question is correct, based on the document's content. \n    If the answer is correct then leave the text box empty. \n    If the answer is incorrect then write the correct answer in the text box.<br><br>\n\n    What are the evaluation metrics used for summarization?\n    ","DefaultChoices":false,"DataExportTag":"Task2-1aaefbd0","QuestionID":"QID23","QuestionType":"TE","Selector":"FORM","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText","TextPosition":"inline"},"QuestionDescription":"Task 2: Answer the question given a document and a generated answer. You are given a set of 10 do...","Choices":{"1":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods<\/a><br><b>Answer: <\/b> ROUGE, QAEval, and BERTScore","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"2":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2007.05374\" target=\"_blank\" rel=\"noopener noreferrer\">SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics<\/a><br><b>Answer: <\/b> The library provides Python wrappers around the official implementations of existing evaluation metrics.","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"3":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2011.04096\" target=\"_blank\" rel=\"noopener noreferrer\">Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics<\/a><br><b>Answer: <\/b> matching metrics like ROUGE as well as recently popular semantic matching metrics like BERTScore and MoverScore.","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"4":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2204.10216\" target=\"_blank\" rel=\"noopener noreferrer\">Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics<\/a><br><b>Answer: <\/b> Automatic evaluation metrics","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"5":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2010.07100\" target=\"_blank\" rel=\"noopener noreferrer\">Re-evaluating Evaluation in Text Summarization<\/a><br><b>Answer: <\/b> ROUGE, JS-2, S3, BERTScore, MoverScore","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"6":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2010.12495\" target=\"_blank\" rel=\"noopener noreferrer\">Understanding the Extent to which Summarization Evaluation Metrics Measure the Information Quality of Summaries<\/a><br><b>Answer: <\/b> ROUGE and BERTScore","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"7":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2007.12626\" target=\"_blank\" rel=\"noopener noreferrer\">SummEval: Re-evaluating Summarization Evaluation<\/a><br><b>Answer: <\/b> ROUGE, standard machine translation metrics, and other miscellaneous performance statistics.","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"8":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2106.01478\" target=\"_blank\" rel=\"noopener noreferrer\">Evaluating the Efficacy of Summarization Evaluation across Languages<\/a><br><b>Answer: <\/b> 19 summarization evaluation metrics, including multilingual BERT within BERTScore","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"9":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2011.13662\" target=\"_blank\" rel=\"noopener noreferrer\">FFCI: A Framework for Interpretable Automatic Evaluation of Summarization<\/a><br><b>Answer: <\/b> Traditional string overlap-based evaluation metrics, with a particular focus on ROUGE (Lin, 2004), QAGS question answering-based framework for evaluating faithfulness (Wang et al., 2020), semantic textual similarity (STS), next-sentence prediction (NSP), and scores derived from 19 pre-trained language models.","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"10":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2105.06027\" target=\"_blank\" rel=\"noopener noreferrer\">Towards Human-Free Automatic Quality Evaluation of German Summarization<\/a><br><b>Answer: <\/b> BLEU, ROUGE, BERTScore, JS, SUM-QE, and BLANC","InputHeight":29,"InputWidth":407,"TextEntry":"on"}},"DisplayLogic":{"0":{"0":{"ChoiceLocator":"q:\/\/QID22\/ChoiceTextEntryValue\/1","Description":"<span class=\"ConjDesc\">If<\/span> <span class=\"QuestionDesc\">Next Question<\/span> <span class=\"LeftOpDesc\">Text Response<\/span> <span class=\"OpDesc\">Is Not Displayed<\/span> ","LeftOperand":"q:\/\/QID22\/ChoiceDisplayed\/1","LogicType":"Question","Operator":"NotDisplayed","QuestionID":"QID22","QuestionIDFromLocator":"QID22","QuestionIsInLoop":"no","Type":"Expression"},"Type":"If"},"Type":"BooleanExpression","inPage":false},"ChoiceOrder":[1,2,3,4,5,6,7,8,9,10],"Validation":{"Settings":{"ForceResponse":"OFF"}},"GradingData":[],"Language":[],"NextChoiceId":11,"NextAnswerId":4,"SearchSource":{"AllowFreeResponse":"false"}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID31","SecondaryAttribute":"Task 2: Answer the question given a document and a generated answer. You are given a set of 10 do...","TertiaryAttribute":null,"Payload":{"QuestionText":"\n    <u>Task 2: Answer the question given a document and a generated answer.<\/u><br>\n    You are given a set of 10 documents, a question that needs to be answered for each of the documents, and an answer generated by a language model.\n    Your task is to check whether the answer to the question is correct, based on the document's content. \n    If the answer is correct then leave the text box empty. \n    If the answer is incorrect then write the correct answer in the text box.<br><br>\n\n    What Information Retrieval datasets are used?\n    ","DefaultChoices":false,"DataExportTag":"Task2-1aaefbd0","QuestionType":"Matrix","Selector":"TE","SubSelector":"Long","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText","TextPosition":"inline","ChoiceColumnWidthPixels":319,"RepeatHeaders":"none","MobileFirst":true},"QuestionDescription":"Task 2: Answer the question given a document and a generated answer. You are given a set of 10 do...","Choices":{"1":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods<\/a><br><b>Answer: <\/b> ROUGE, QAEval, and BERTScore"},"2":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods<\/a><br><b>Answer: <\/b> ROUGE, QAEval, and BERTScore"},"3":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods<\/a><br><b>Answer: <\/b> ROUGE, QAEval, and BERTScore"},"4":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods<\/a><br><b>Answer: <\/b> ROUGE, QAEval, and BERTScore"}},"DisplayLogic":{"0":{"0":{"ChoiceLocator":"q:\/\/QID1\/ChoiceTextEntryValue\/1","Description":"<span class=\"ConjDesc\">If<\/span> <span class=\"QuestionDesc\">What 3D scene datasets are used?<\/span> <span class=\"LeftOpDesc\">Text Response<\/span> <span class=\"OpDesc\">Is Not Displayed<\/span> ","LeftOperand":"q:\/\/QID1\/ChoiceDisplayed\/1","LogicType":"Question","Operator":"NotDisplayed","QuestionID":"QID1","QuestionIDFromLocator":"QID1","QuestionIsInLoop":"no","Type":"Expression"},"Type":"If"},"Type":"BooleanExpression","inPage":false},"ChoiceOrder":[1,2,3,4],"Validation":{"Settings":{"ForceResponse":"OFF","Type":"None"}},"GradingData":[],"Language":[],"NextChoiceId":5,"NextAnswerId":2,"Answers":{"1":{"Display":"Click to write Scale Point 1"}},"AnswerOrder":[1],"ChoiceDataExportTags":false,"QuestionID":"QID31"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID6","SecondaryAttribute":"The time spent on the next task will be timed, so please read the instructions, perform the task...","TertiaryAttribute":null,"Payload":{"QuestionText":"The time spent on the next task will be timed, so please read the instructions, perform the task at a speed that seems reasonable to you, and take any breaks beforehand if necessary.<br><br>Task 1: Answer the question given the document. <br>You are given a set of 10 documents and a question that needs to be answered for each of the documents. Your task is to answer the question based on the document's content. If the answer can not be found in a document then answer with \"NA\".<br>","DefaultChoices":false,"DataExportTag":"Task1-expl","QuestionType":"DB","Selector":"TB","Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"The time spent on the next task will be timed, so please read the instructions, perform the task...","ChoiceOrder":[],"Validation":{"Settings":{"Type":"None"}},"GradingData":[],"Language":[],"NextChoiceId":4,"NextAnswerId":1,"QuestionID":"QID6","DataVisibility":{"Private":false,"Hidden":false}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID7","SecondaryAttribute":"The time spent on the next task will be timed, so please read the instructions, perform the task...","TertiaryAttribute":null,"Payload":{"QuestionText":"The time spent on the next task will be timed, so please read the instructions, perform the task at a speed that seems reasonable to you, and take any breaks beforehand if necessary.<br>\n<br>\n<u>Task 2: Answer the question given a document and a generated answer.<\/u><br>\nYou are given a set of 10 documents, a question that needs to be answered for each of the documents, and an answer generated by a language model. Your task is, again, to answer the question, based on the document's content. If the generated answer is correct then leave a '-' in the text box. If the generated answer is incorrect then write the correct answer in the text box.<br>\n&nbsp;","DefaultChoices":false,"DataExportTag":"Task2-expl","QuestionType":"DB","Selector":"TB","Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"The time spent on the next task will be timed, so please read the instructions, perform the task...","ChoiceOrder":[],"Validation":{"Settings":{"Type":"None"}},"GradingData":[],"Language":[],"NextChoiceId":4,"NextAnswerId":1,"QuestionID":"QID7","DataVisibility":{"Private":false,"Hidden":false}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID2","SecondaryAttribute":"Timing","TertiaryAttribute":null,"Payload":{"QuestionText":"Timing","DefaultChoices":false,"DataExportTag":"Timer","QuestionType":"Timing","Selector":"PageTimer","Configuration":{"QuestionDescriptionOption":"UseText","MinSeconds":"0","MaxSeconds":"0"},"QuestionDescription":"Timing","Choices":{"1":{"Display":"First Click"},"2":{"Display":"Last Click"},"3":{"Display":"Page Submit"},"4":{"Display":"Click Count"}},"GradingData":[],"Language":[],"NextChoiceId":48,"NextAnswerId":1,"QuestionID":"QID2","DataVisibility":{"Private":false,"Hidden":false}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID4","SecondaryAttribute":"Timing","TertiaryAttribute":null,"Payload":{"QuestionText":"Timing","DefaultChoices":false,"DataExportTag":"Timer","QuestionType":"Timing","Selector":"PageTimer","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText","MinSeconds":"0","MaxSeconds":"0"},"QuestionDescription":"Timing","Choices":{"1":{"Display":"First Click"},"2":{"Display":"Last Click"},"3":{"Display":"Page Submit"},"4":{"Display":"Click Count"}},"GradingData":[],"Language":[],"NextChoiceId":76,"NextAnswerId":1,"QuestionID":"QID4"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID1211903283","SecondaryAttribute":"To what extent do you have knowledge about Artificial Intelligence","TertiaryAttribute":null,"Payload":{"QuestionText":"To what extent do you have knowledge about Artificial Intelligence","DefaultChoices":false,"QuestionType":"MC","Selector":"SAVR","SubSelector":"TX","Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"To what extent do you have knowledge about Artificial Intelligence","Choices":{"1":{"Display":"I have negligible knowledge about AI"},"2":{"Display":"I have a little knowledge about AI"},"3":{"Display":"I have significant knowledge about AI"}},"ChoiceOrder":["1","2","3"],"Validation":{"Settings":{"ForceResponse":"OFF","Type":"None"}},"GradingData":[],"Language":[],"NextChoiceId":4,"NextAnswerId":1,"QuestionText_Unsafe":"To what extent do you have knowledge about Artificial Intelligence","DataExportTag":"ai-knowledge","QuestionID":"QID1211903283","DataVisibility":{"Private":false,"Hidden":false}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID1211903273","SecondaryAttribute":"Welcome to this study about Question-Answering productivity. This survey will take approximately...","TertiaryAttribute":null,"Payload":{"QuestionText":"<p><span style=\"font-size:16px;\"><span style=\"line-height: 150%; font-family: Arial, sans-serif;\"><\/span><\/span><\/p>Welcome to this study about Question-Answering productivity.<br><br>This survey will take approximately 1 hour and consists of reading comprehension and question-answering tasks. Please take the questions seriously. If you have any questions regarding this research you can reach us at papadopoulou@zeta-alpha.com.<br><br>The participants of this study will be compensated after completion.<br>Thank you very much for participating!<span style=\"font-size:16px;\"><span style=\"line-height: 107%; font-family: Arial, sans-serif;\"><\/span><\/span>","DefaultChoices":false,"QuestionType":"DB","Selector":"TB","Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"Welcome to this study about Question-Answering productivity. This survey will take approximately...","ChoiceOrder":[],"Validation":{"Settings":{"Type":"None"}},"GradingData":[],"Language":[],"NextChoiceId":4,"NextAnswerId":1,"DataVisibility":{"Private":false,"Hidden":false},"QuestionText_Unsafe":"<p><span style=\"font-size:16px;\"><span style=\"line-height: 150%; font-family: Arial, sans-serif;\">Hi and welcome to this study about added productivity using ChatGPT.<\/span><\/span><\/p><p><span style=\"font-size:16px;\"><span style=\"line-height: 150%; font-family: Arial, sans-serif;\">This survey will take about 1 hour and consists of information retrieval and writing tasks. Please take the tasks seriously.<\/span><\/span><\/p><p><span style=\"font-size:16px;\"><span style=\"line-height: 150%; font-family: Arial, sans-serif;\"><br><\/span><\/span><\/p>\n\n<p><span style=\"font-size:16px;\"><span style=\"line-height: 150%; font-family: Arial, sans-serif;\">Your answers will remain completely anonymous.&nbsp;<\/span><\/span><span style=\"font-family: Arial, sans-serif; font-size: 16px;\">If you have any other questions regarding the research you can reach me at ...<\/span><\/p><p><span style=\"font-size:16px;\"><span style=\"line-height: 150%; font-family: Arial, sans-serif;\"><span class=\"MsoHyperlink\"><br><\/span><\/span><\/span><\/p>\n<span style=\"font-size:16px;\"><span style=\"line-height: 107%; font-family: Arial, sans-serif;\">Thank you very much for participating<\/span><\/span>","DataExportTag":"intro","QuestionID":"QID1211903273"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID22","SecondaryAttribute":"What are the evaluation metrics used for summarization?","TertiaryAttribute":null,"Payload":{"QuestionText":"\n    <u>Task 1: Answer the question given the document.<\/u><br>\n    You are given a set of 10 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content (meaning the pdf, if available). \n    If the answer can not be found in a document then answer with \"NA\".<br><br>\n\n    What are the evaluation metrics used for summarization?\n    ","DefaultChoices":false,"DataExportTag":"Task1-1aaefbd0","QuestionType":"TE","Selector":"FORM","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"What are the evaluation metrics used for summarization?","Choices":{"1":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2104.00054\" target=\"_blank\" rel=\"noopener noreferrer\">A Statistical Analysis of Summarization Evaluation Metrics using Resampling Methods<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"2":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2007.05374\" target=\"_blank\" rel=\"noopener noreferrer\">SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"3":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2011.04096\" target=\"_blank\" rel=\"noopener noreferrer\">Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"4":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2204.10216\" target=\"_blank\" rel=\"noopener noreferrer\">Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"5":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2010.07100\" target=\"_blank\" rel=\"noopener noreferrer\">Re-evaluating Evaluation in Text Summarization<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"6":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2010.12495\" target=\"_blank\" rel=\"noopener noreferrer\">Understanding the Extent to which Summarization Evaluation Metrics Measure the Information Quality of Summaries<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"7":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2007.12626\" target=\"_blank\" rel=\"noopener noreferrer\">SummEval: Re-evaluating Summarization Evaluation<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"8":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2106.01478\" target=\"_blank\" rel=\"noopener noreferrer\">Evaluating the Efficacy of Summarization Evaluation across Languages<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"9":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2011.13662\" target=\"_blank\" rel=\"noopener noreferrer\">FFCI: A Framework for Interpretable Automatic Evaluation of Summarization<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"10":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2105.06027\" target=\"_blank\" rel=\"noopener noreferrer\">Towards Human-Free Automatic Quality Evaluation of German Summarization<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"}},"ChoiceOrder":[1,2,3,4,5,6,7,8,9,10],"Validation":{"Settings":{"ForceResponse":"OFF"}},"GradingData":[],"Language":[],"NextChoiceId":4,"NextAnswerId":1,"SearchSource":{"AllowFreeResponse":"false"},"QuestionID":"QID22"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID1","SecondaryAttribute":"What Information Retrieval datasets are used?","TertiaryAttribute":null,"Payload":{"QuestionText":"\n    <u>Task 1: Answer the question given the document.<\/u><br>\n    You are given a set of 10 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content (meaning the pdf, if available). \n    If the answer can not be found in a document then answer with \"NA\".<br><br>\n\n    What Information Retrieval datasets are used?\n    ","DefaultChoices":false,"DataExportTag":"Task1-8d44bd26","QuestionID":"QID1","QuestionType":"TE","Selector":"FORM","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"What Information Retrieval datasets are used?","Choices":{"1":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2103.02280\" target=\"_blank\" rel=\"noopener noreferrer\">Simplified Data Wrangling with ir_datasets<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"2":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1912.02346\" target=\"_blank\" rel=\"noopener noreferrer\">Information Retrieval and Its Sister Disciplines<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"3":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1912.01901\" target=\"_blank\" rel=\"noopener noreferrer\">WIKIR: A Python toolkit for building a large-scale Wikipedia-based English Information Retrieval Dataset<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"4":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1205.0312\" target=\"_blank\" rel=\"noopener noreferrer\">Least Information Modeling for Information Retrieval<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"5":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2111.13853\" target=\"_blank\" rel=\"noopener noreferrer\">Pre-training Methods in Information Retrieval<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"6":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2205.01230\" target=\"_blank\" rel=\"noopener noreferrer\">Retrieval-Enhanced Machine Learning<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"7":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2006.06894\" target=\"_blank\" rel=\"noopener noreferrer\">Google Dataset Search by the Numbers<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"8":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1904.00289\" target=\"_blank\" rel=\"noopener noreferrer\">On the Estimation and Use of Statistical Modelling in Information   Retrieval<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"9":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1912.09910\" target=\"_blank\" rel=\"noopener noreferrer\">Report on the First HIPstIR Workshop on the Future of Information   Retrieval<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"10":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2301.08801\" target=\"_blank\" rel=\"noopener noreferrer\">Information Retrieval: Recent Advances and Beyond<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"}},"ChoiceOrder":[1,2,3,4,5,6,7,8,9,10],"Validation":{"Settings":{"ForceResponse":"OFF"}},"GradingData":[],"Language":[],"NextChoiceId":4,"NextAnswerId":1,"SearchSource":{"AllowFreeResponse":"false"}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID1211903279","SecondaryAttribute":"What is the highest level of education you've completed? (or equivalent in your country)","TertiaryAttribute":null,"Payload":{"QuestionText":"What is the highest level of education you've completed? (or equivalent in your country)","QuestionType":"MC","Selector":"SAVR","SubSelector":"TX","Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"What is the highest level of education you've completed? (or equivalent in your country)","Choices":{"1":{"Display":"none"},"2":{"Display":"middle school"},"3":{"Display":"high school"},"4":{"Display":"bachelor's degree"},"5":{"Display":"master's degree"},"6":{"Display":"PhD or higher"}},"ChoiceOrder":["1","2","3","4","5","6"],"Validation":{"Settings":{"ForceResponse":"ON","ForceResponseType":"ON","Type":"None"}},"Language":[],"NextChoiceId":8,"NextAnswerId":1,"DataVisibility":{"Private":false,"Hidden":false},"QuestionText_Unsafe":"What is the highest level of education you've completed? (or equivalent in your country)","DataExportTag":"education","QuestionID":"QID1211903279"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID24","SecondaryAttribute":"What is the name of the Large Language Model and is it open source?","TertiaryAttribute":null,"Payload":{"QuestionText":"\n    <u>Task 1: Answer the question given the document.<\/u><br>\n    You are given a set of 10 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content (meaning the pdf, if available). \n    If the answer can not be found in a document then answer with \"NA\".<br><br>\n\n    What is the name of the Large Language Model and is it open source?\n    ","DefaultChoices":false,"DataExportTag":"Task1-8420f823","QuestionID":"QID24","QuestionType":"TE","Selector":"FORM","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"What is the name of the Large Language Model and is it open source?","Choices":{"1":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2302.13971\" target=\"_blank\" rel=\"noopener noreferrer\">LLaMA: Open and Efficient Foundation Language Models<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"2":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2102.02503\" target=\"_blank\" rel=\"noopener noreferrer\">Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"3":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2203.15556\" target=\"_blank\" rel=\"noopener noreferrer\">Training Compute-Optimal Large Language Models<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"4":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2302.13681\" target=\"_blank\" rel=\"noopener noreferrer\">The (ab)use of Open Source Code to Train Large Language Models<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"5":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1810.10045\" target=\"_blank\" rel=\"noopener noreferrer\">Language Modeling at Scale<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"6":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2206.07682\" target=\"_blank\" rel=\"noopener noreferrer\">Emergent Abilities of Large Language Models<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"7":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1210.8440\" target=\"_blank\" rel=\"noopener noreferrer\">Large Scale Language Modeling in Automatic Speech Recognition<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"8":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2211.09110\" target=\"_blank\" rel=\"noopener noreferrer\">Holistic Evaluation of Language Models<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"9":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2211.05100\" target=\"_blank\" rel=\"noopener noreferrer\">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"10":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2204.06745\" target=\"_blank\" rel=\"noopener noreferrer\">GPT-NeoX-20B: An Open-Source Autoregressive Language Model<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"}},"ChoiceOrder":[1,2,3,4,5,6,7,8,9,10],"Validation":{"Settings":{"ForceResponse":"OFF"}},"GradingData":[],"Language":[],"NextChoiceId":4,"NextAnswerId":1,"SearchSource":{"AllowFreeResponse":"false"}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID25","SecondaryAttribute":"What is the name of the Large Language Model and is it open source?","TertiaryAttribute":null,"Payload":{"QuestionText":"\n    <u>Task 2: Answer the question given a document and a generated answer.<\/u><br>\n    You are given a set of 10 documents, a question that needs to be answered for each of the documents, and an answer generated by a language model.\n    Your task is to check whether the answer to the question is correct, based on the document's content. \n    If the answer is correct then leave the text box empty. \n    If the answer is incorrect then write the correct answer in the text box.<br><br>\n\n    What is the name of the Large Language Model and is it open source?\n    ","DefaultChoices":false,"DataExportTag":"Task2-8420f823","QuestionID":"QID25","QuestionType":"TE","Selector":"FORM","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText","TextPosition":"inline"},"QuestionDescription":"What is the name of the Large Language Model and is it open source?","Choices":{"1":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2302.13971\" target=\"_blank\" rel=\"noopener noreferrer\">LLaMA: Open and Efficient Foundation Language Models<\/a><br><b>Answer: <\/b> The name of the Large Language Model is LLaMA and it is open source.","InputHeight":29,"InputWidth":407},"2":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2102.02503\" target=\"_blank\" rel=\"noopener noreferrer\">Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models<\/a><br><b>Answer: <\/b> GPT-3 and it is not open source.","InputHeight":29,"InputWidth":407},"3":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2203.15556\" target=\"_blank\" rel=\"noopener noreferrer\">Training Compute-Optimal Large Language Models<\/a><br><b>Answer: <\/b> Chinchilla; No","InputHeight":29,"InputWidth":407},"4":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2302.13681\" target=\"_blank\" rel=\"noopener noreferrer\">The (ab)use of Open Source Code to Train Large Language Models<\/a><br><b>Answer: <\/b> NA","InputHeight":29,"InputWidth":407},"5":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1810.10045\" target=\"_blank\" rel=\"noopener noreferrer\">Language Modeling at Scale<\/a><br><b>Answer: <\/b> The Large Language Model is the LSTM based SOTA model from [36]. It is not open source.","InputHeight":29,"InputWidth":407},"6":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2206.07682\" target=\"_blank\" rel=\"noopener noreferrer\">Emergent Abilities of Large Language Models<\/a><br><b>Answer: <\/b> Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks in language models. ICML, 2022. URL https:\/\/arxiv.org\/abs\/2202.06539.\n\nNo, it is not open source.","InputHeight":29,"InputWidth":407},"7":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1210.8440\" target=\"_blank\" rel=\"noopener noreferrer\">Large Scale Language Modeling in Automatic Speech Recognition<\/a><br><b>Answer: <\/b> The Large Language Model is Kneser-Ney 4-gram and it is not open source.","InputHeight":29,"InputWidth":407},"8":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2211.09110\" target=\"_blank\" rel=\"noopener noreferrer\">Holistic Evaluation of Language Models<\/a><br><b>Answer: <\/b> GPT-3 davinci v1 and it is limited-access.","InputHeight":29,"InputWidth":407},"9":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2211.05100\" target=\"_blank\" rel=\"noopener noreferrer\">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model<\/a><br><b>Answer: <\/b> GPT-NeoX-20B and yes, it is open-source.","InputHeight":29,"InputWidth":407},"10":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2204.06745\" target=\"_blank\" rel=\"noopener noreferrer\">GPT-NeoX-20B: An Open-Source Autoregressive Language Model<\/a><br><b>Answer: <\/b> GPT-NeoX-20B and yes, it is open source.","InputHeight":29,"InputWidth":407}},"DisplayLogic":{"0":{"0":{"ChoiceLocator":"q:\/\/QID1\/ChoiceTextEntryValue\/1","Description":"<span class=\"ConjDesc\">If<\/span> <span class=\"QuestionDesc\">What 3D scene datasets are used?<\/span> <span class=\"LeftOpDesc\">Text Response<\/span> <span class=\"OpDesc\">Is Not Displayed<\/span> ","LeftOperand":"q:\/\/QID1\/ChoiceDisplayed\/1","LogicType":"Question","Operator":"NotDisplayed","QuestionID":"QID24","QuestionIDFromLocator":"QID1","QuestionIsInLoop":"no","Type":"Expression"},"Type":"If"},"Type":"BooleanExpression","inPage":false},"ChoiceOrder":[1,2,3,4,5,6,7,8,9,10],"Validation":{"Settings":{"ForceResponse":"OFF","Type":null}},"GradingData":[],"Language":[],"NextChoiceId":11,"NextAnswerId":4,"SearchSource":{"AllowFreeResponse":"false"}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID1211903277","SecondaryAttribute":"What is your age in years?","TertiaryAttribute":null,"Payload":{"QuestionText":"What is your age in years?","DefaultChoices":false,"QuestionType":"TE","Selector":"SL","Configuration":{"QuestionDescriptionOption":"UseText","AllowFreeResponse":"false"},"QuestionDescription":"What is your age in years?","Validation":{"Settings":{"ForceResponse":"ON","ForceResponseType":"ON","Type":"ContentType","TotalChars":"2","ContentType":"ValidNumber","ValidDateType":"DateWithFormat","ValidPhoneType":"ValidUSPhone","ValidZipType":"ValidUSZip","ValidNumber":{"Min":"0","Max":"99","NumDecimals":"0"}}},"GradingData":[],"Language":[],"NextChoiceId":4,"NextAnswerId":1,"DataVisibility":{"Private":false,"Hidden":false},"QuestionText_Unsafe":"What is your age in years?","DataExportTag":"age","QuestionID":"QID1211903277","SearchSource":{"AllowFreeResponse":"false"}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID1211903276","SecondaryAttribute":"What is your gender?","TertiaryAttribute":null,"Payload":{"QuestionText":"What is your gender?","QuestionType":"MC","Selector":"SAVR","SubSelector":"TX","Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"What is your gender?","Choices":{"1":{"Display":"Male"},"2":{"Display":"Female"},"3":{"Display":"Other"}},"ChoiceOrder":[1,2,"3"],"Validation":{"Settings":{"ForceResponse":"ON","ForceResponseType":"ON","Type":"None"}},"Language":[],"NextChoiceId":4,"NextAnswerId":1,"DataVisibility":{"Private":false,"Hidden":false},"QuestionText_Unsafe":"What is your gender?","DataExportTag":"gender","QuestionID":"QID1211903276"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID1211903278","SecondaryAttribute":"What is your occupation (if student, specify the field of study)","TertiaryAttribute":null,"Payload":{"QuestionText":"What is your occupation (if student, specify the field of study)","DefaultChoices":false,"QuestionType":"TE","Selector":"SL","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"What is your occupation (if student, specify the field of study)","Validation":{"Settings":{"ForceResponse":"OFF","Type":"None"}},"GradingData":[],"Language":[],"NextChoiceId":8,"NextAnswerId":1,"SearchSource":{"AllowFreeResponse":"false"},"QuestionText_Unsafe":"What is your occupation (if student, specify the field of study)","DataExportTag":"occupation","QuestionID":"QID1211903278"}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID28","SecondaryAttribute":"What Long Form Question Answering datasets are used?","TertiaryAttribute":null,"Payload":{"QuestionText":"\n    <u>Task 1: Answer the question given the document.<\/u><br>\n    You are given a set of 10 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content (meaning the pdf, if available). \n    If the answer can not be found in a document then answer with \"NA\".<br><br>\n\n    What Long Form Question Answering datasets are used?\n    ","DefaultChoices":false,"DataExportTag":"Task1-e3cb7330","QuestionID":"QID28","QuestionType":"TE","Selector":"FORM","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"What Long Form Question Answering datasets are used?","Choices":{"1":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1907.09190\" target=\"_blank\" rel=\"noopener noreferrer\">ELI5: Long Form Question Answering<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"2":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2103.06332\" target=\"_blank\" rel=\"noopener noreferrer\">Hurdles to Progress in Long-form Question Answering<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"3":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2211.08386\" target=\"_blank\" rel=\"noopener noreferrer\">Generative Long-form Question Answering: Relevance, Faithfulness and Succinctness<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"4":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2203.00343\" target=\"_blank\" rel=\"noopener noreferrer\">Read before Generate! Faithful Long Form Question Answering with Machine Reading<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"5":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2210.17525\" target=\"_blank\" rel=\"noopener noreferrer\">Query Refinement Prompts for Closed-Book Long-Form Question Answering<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"6":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2112.08608\" target=\"_blank\" rel=\"noopener noreferrer\">QuALITY: Question Answering with Long Input Texts, Yes!<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"7":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1611.01839\" target=\"_blank\" rel=\"noopener noreferrer\">Hierarchical Question Answering for Long Documents<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"8":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1304.7157\" target=\"_blank\" rel=\"noopener noreferrer\">Question Answering Against Very-Large Text Collections<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"9":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2004.05109\" target=\"_blank\" rel=\"noopener noreferrer\">Towards Automatic Generation of Questions from Long Answers<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"10":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1709.03036\" target=\"_blank\" rel=\"noopener noreferrer\">Abductive Matching in Question Answering<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"}},"ChoiceOrder":[1,2,3,4,5,6,7,8,9,10],"Validation":{"Settings":{"ForceResponse":"OFF"}},"GradingData":[],"Language":[],"NextChoiceId":4,"NextAnswerId":1,"SearchSource":{"AllowFreeResponse":"false"}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID29","SecondaryAttribute":"What Long Form Question Answering datasets are used?","TertiaryAttribute":null,"Payload":{"QuestionText":"\n    <u>Task 2: Answer the question given a document and a generated answer.<\/u><br>\n    You are given a set of 10 documents, a question that needs to be answered for each of the documents, and an answer generated by a language model.\n    Your task is to check whether the answer to the question is correct, based on the document's content. \n    If the answer is correct then leave the text box empty. \n    If the answer is incorrect then write the correct answer in the text box.<br><br>\n\n    What Long Form Question Answering datasets are used?\n    ","DefaultChoices":false,"DataExportTag":"Task2-e3cb7330","QuestionID":"QID29","QuestionType":"TE","Selector":"FORM","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText","TextPosition":"inline"},"QuestionDescription":"What Long Form Question Answering datasets are used?","Choices":{"1":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1907.09190\" target=\"_blank\" rel=\"noopener noreferrer\">ELI5: Long Form Question Answering<\/a><br><b>Answer: <\/b> ELI5: a Long Form Question Answering dataset","InputHeight":29,"InputWidth":407},"2":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2103.06332\" target=\"_blank\" rel=\"noopener noreferrer\">Hurdles to Progress in Long-form Question Answering<\/a><br><b>Answer: <\/b> Natural Questions (Kwiatkowski et al., 2019) and ELI5 (Fan et al., 2019)","InputHeight":29,"InputWidth":407},"3":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2211.08386\" target=\"_blank\" rel=\"noopener noreferrer\">Generative Long-form Question Answering: Relevance, Faithfulness and Succinctness<\/a><br><b>Answer: <\/b> TriviaQA [15] and WebQuestions [16]","InputHeight":29,"InputWidth":407},"4":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2203.00343\" target=\"_blank\" rel=\"noopener noreferrer\">Read before Generate! Faithful Long Form Question Answering with Machine Reading<\/a><br><b>Answer: <\/b> ELI5 and MS MARCO","InputHeight":29,"InputWidth":407},"5":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2210.17525\" target=\"_blank\" rel=\"noopener noreferrer\">Query Refinement Prompts for Closed-Book Long-Form Question Answering<\/a><br><b>Answer: <\/b> ASQA and AQuAMuSe","InputHeight":29,"InputWidth":407},"6":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2112.08608\" target=\"_blank\" rel=\"noopener noreferrer\">QuALITY: Question Answering with Long Input Texts, Yes!<\/a><br><b>Answer: <\/b> ELI5: Long form question answering (Fan et al., 2019)","InputHeight":29,"InputWidth":407},"7":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1611.01839\" target=\"_blank\" rel=\"noopener noreferrer\">Hierarchical Question Answering for Long Documents<\/a><br><b>Answer: <\/b> WIKIREADING dataset (Hewlett et al., 2016) and a new dataset","InputHeight":29,"InputWidth":407},"8":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1304.7157\" target=\"_blank\" rel=\"noopener noreferrer\">Question Answering Against Very-Large Text Collections<\/a><br><b>Answer: <\/b> TREC 2006 Question Answering Track, Information Retrieval for Question Answering Workshop, Chatbot NLU Corpus with STT error","InputHeight":29,"InputWidth":407},"9":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2004.05109\" target=\"_blank\" rel=\"noopener noreferrer\">Towards Automatic Generation of Questions from Long Answers<\/a><br><b>Answer: <\/b> SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), MS MARCO (Nguyen et al., 2016), Narrative QA (Kocisky et al., 2018), RACE (Lai et al., 2017), LearningQ (Chen et al., 2018), Google Natural Questions (Kwiatkowski et al., 2018)","InputHeight":29,"InputWidth":407},"10":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1709.03036\" target=\"_blank\" rel=\"noopener noreferrer\">Abductive Matching in Question Answering<\/a><br><b>Answer: <\/b> WikiTableQuestions","InputHeight":29,"InputWidth":407}},"DisplayLogic":{"0":{"0":{"ChoiceLocator":"q:\/\/QID1\/ChoiceTextEntryValue\/1","Description":"<span class=\"ConjDesc\">If<\/span> <span class=\"QuestionDesc\">What 3D scene datasets are used?<\/span> <span class=\"LeftOpDesc\">Text Response<\/span> <span class=\"OpDesc\">Is Not Displayed<\/span> ","LeftOperand":"q:\/\/QID1\/ChoiceDisplayed\/1","LogicType":"Question","Operator":"NotDisplayed","QuestionID":"QID28","QuestionIDFromLocator":"QID1","QuestionIsInLoop":"no","Type":"Expression"},"Type":"If"},"Type":"BooleanExpression","inPage":false},"ChoiceOrder":[1,2,3,4,5,6,7,8,9,10],"Validation":{"Settings":{"ForceResponse":"OFF","Type":null}},"GradingData":[],"Language":[],"NextChoiceId":11,"NextAnswerId":4,"SearchSource":{"AllowFreeResponse":"false"}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID26","SecondaryAttribute":"What Neural Machine Translation metrics do they use and a short description of each?","TertiaryAttribute":null,"Payload":{"QuestionText":"\n    <u>Task 1: Answer the question given the document.<\/u><br>\n    You are given a set of 10 documents and a question that needs to be answered for each of the documents. \n    Your task is to answer the question based on the document's content (meaning the pdf, if available). \n    If the answer can not be found in a document then answer with \"NA\".<br><br>\n\n    What Neural Machine Translation metrics do they use and a short description of each?\n    ","DefaultChoices":false,"DataExportTag":"Task1-aa42082a","QuestionID":"QID26","QuestionType":"TE","Selector":"FORM","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText"},"QuestionDescription":"What Neural Machine Translation metrics do they use and a short description of each?","Choices":{"1":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2212.10297\" target=\"_blank\" rel=\"noopener noreferrer\">Extrinsic Evaluation of Machine Translation Metrics<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"2":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2104.07541\" target=\"_blank\" rel=\"noopener noreferrer\">Reward Optimization for Neural Machine Translation with Learned Metrics<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"3":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1807.08945\" target=\"_blank\" rel=\"noopener noreferrer\">Otem&Utem: Over- and Under-Translation Evaluation Metric for NMT<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"4":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2203.15858\" target=\"_blank\" rel=\"noopener noreferrer\">Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"5":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1709.07809\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Machine Translation<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"6":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1710.02095\" target=\"_blank\" rel=\"noopener noreferrer\">Machine Translation Evaluation with Neural Networks<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"7":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2210.15615\" target=\"_blank\" rel=\"noopener noreferrer\">ACES: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"8":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2109.05016\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Machine Translation Quality and Post-Editing Performance<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"9":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2107.10821\" target=\"_blank\" rel=\"noopener noreferrer\">To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"},"10":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2109.07740\" target=\"_blank\" rel=\"noopener noreferrer\">Scaling Laws for Neural Machine Translation<\/a>","InputHeight":29,"InputWidth":407,"TextEntry":"on"}},"ChoiceOrder":[1,2,3,4,5,6,7,8,9,10],"Validation":{"Settings":{"ForceResponse":"OFF"}},"GradingData":[],"Language":[],"NextChoiceId":4,"NextAnswerId":1,"SearchSource":{"AllowFreeResponse":"false"}}},{"SurveyID":"SV_8iyc6G8rZOzeZW6","Element":"SQ","PrimaryAttribute":"QID27","SecondaryAttribute":"What Neural Machine Translation metrics do they use and a short description of each?","TertiaryAttribute":null,"Payload":{"QuestionText":"\n    <u>Task 2: Answer the question given a document and a generated answer.<\/u><br>\n    You are given a set of 10 documents, a question that needs to be answered for each of the documents, and an answer generated by a language model.\n    Your task is to check whether the answer to the question is correct, based on the document's content. \n    If the answer is correct then leave the text box empty. \n    If the answer is incorrect then write the correct answer in the text box.<br><br>\n\n    What Neural Machine Translation metrics do they use and a short description of each?\n    ","DefaultChoices":false,"DataExportTag":"Task2-aa42082a","QuestionID":"QID27","QuestionType":"TE","Selector":"FORM","DataVisibility":{"Private":false,"Hidden":false},"Configuration":{"QuestionDescriptionOption":"UseText","TextPosition":"inline"},"QuestionDescription":"What Neural Machine Translation metrics do they use and a short description of each?","Choices":{"1":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2212.10297\" target=\"_blank\" rel=\"noopener noreferrer\">Extrinsic Evaluation of Machine Translation Metrics<\/a><br><b>Answer: <\/b> They use chrF, COMET, BERTScore, etc. chrF is a character-level n-gram precision metric, COMET is a metric that uses a reference-based approach to measure the quality of machine translation, and BERTScore is a metric that uses a pre-trained BERT model to measure the quality of machine translation.","InputHeight":29,"InputWidth":407},"2":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2104.07541\" target=\"_blank\" rel=\"noopener noreferrer\">Reward Optimization for Neural Machine Translation with Learned Metrics<\/a><br><b>Answer: <\/b> They use BLEURT, which is trained with human judgment data because human perceives the translation quality in multiple criteria (e.g., adequacy, fluency and coverage) with different weights. They also use Bert as a teacher, which is a metric based on the similarity between sentence embeddings, and optimize the text generation models with the similarity scores. They also use Bleurt, which is a learning robust metrics for text generation. Lastly, they use Neural Machine Translation of rare words with subword units.","InputHeight":29,"InputWidth":407},"3":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1807.08945\" target=\"_blank\" rel=\"noopener noreferrer\">Otem&Utem: Over- and Under-Translation Evaluation Metric for NMT<\/a><br><b>Answer: <\/b> Otem and Utem: Otem is based on the proportion of mismatched n-grams between gold reference and system translation and focuses on the proportion of repeated n-grams in the candidate translation (C) over the whole candidate (A + C). Utem estimates the proportion of untranslated n-grams in the reference (B) over the whole reference (A + B). BLEU calculates the precision of matched n-grams (A) over the whole reference (A + C).","InputHeight":29,"InputWidth":407},"4":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2203.15858\" target=\"_blank\" rel=\"noopener noreferrer\">Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics<\/a><br><b>Answer: <\/b> NA","InputHeight":29,"InputWidth":407},"5":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1709.07809\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Machine Translation<\/a><br><b>Answer: <\/b> Match score: checks for each output if the aligned input word according to fast-align is indeed the input word that received the highest attention probability. \nProbability mass score: sums up the probability mass given to each alignment point obtained from fast-align.","InputHeight":29,"InputWidth":407},"6":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/1710.02095\" target=\"_blank\" rel=\"noopener noreferrer\">Machine Translation Evaluation with Neural Networks<\/a><br><b>Answer: <\/b> NNRK (Neural Network ReranKing): Uses vector subtraction to find pairs of most dissimilar graph nodes and construct the graph only from the nodes included in such \u201canti-edges\u201d. The score for a sentence is generated by subtracting the scores for the two predictions. \nPRO: Uses vector subtraction to train in a pairwise fashion by subtracting the vectors for the two competing translations and then training to predict +1 or -1. At test time, a vector for a single translation is used, which is equivalent to subtracting a zero vector from it. \nSPEDE: Probabilistic edit distance metrics for MT evaluation. \nAMBER: MT evaluation metric. \nDeep-Syntactic Metric: Approximating a deep-syntactic metric for MT evaluation and tuning. \nKendall's \u03c4: Measurement of association between two things.","InputHeight":29,"InputWidth":407},"7":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2210.15615\" target=\"_blank\" rel=\"noopener noreferrer\">ACES: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics<\/a><br><b>Answer: <\/b> NA","InputHeight":29,"InputWidth":407},"8":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2109.05016\" target=\"_blank\" rel=\"noopener noreferrer\">Neural Machine Translation Quality and Post-Editing Performance<\/a><br><b>Answer: <\/b> NA","InputHeight":29,"InputWidth":407},"9":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2107.10821\" target=\"_blank\" rel=\"noopener noreferrer\">To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation<\/a><br><b>Answer: <\/b> String-based metrics and metrics using pretrained models. String-based methods compare the coverage of various substrings between the human reference and MT output texts. Metrics using pretrained models use pretrained models to compare the translation quality of a pair of machine translation systems.","InputHeight":29,"InputWidth":407},"10":{"Display":"<a href=\"https:\/\/arxiv.org\/pdf\/2109.07740\" target=\"_blank\" rel=\"noopener noreferrer\">Scaling Laws for Neural Machine Translation<\/a><br><b>Answer: <\/b> NA","InputHeight":29,"InputWidth":407}},"DisplayLogic":{"0":{"0":{"ChoiceLocator":"q:\/\/QID1\/ChoiceTextEntryValue\/1","Description":"<span class=\"ConjDesc\">If<\/span> <span class=\"QuestionDesc\">What 3D scene datasets are used?<\/span> <span class=\"LeftOpDesc\">Text Response<\/span> <span class=\"OpDesc\">Is Not Displayed<\/span> ","LeftOperand":"q:\/\/QID1\/ChoiceDisplayed\/1","LogicType":"Question","Operator":"NotDisplayed","QuestionID":"QID26","QuestionIDFromLocator":"QID1","QuestionIsInLoop":"no","Type":"Expression"},"Type":"If"},"Type":"BooleanExpression","inPage":false},"ChoiceOrder":[1,2,3,4,5,6,7,8,9,10],"Validation":{"Settings":{"ForceResponse":"OFF","Type":null}},"GradingData":[],"Language":[],"NextChoiceId":11,"NextAnswerId":4,"SearchSource":{"AllowFreeResponse":"false"}}}]}