{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt comparison results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pipeline.constants import TEST_SET_PATH\n",
    "\n",
    "# model = \"code-davinci-002\"\n",
    "model = \"text-davinci-003\"\n",
    "# model = \"gpt-3.5-turbo\"\n",
    "retriever = \"monot5-base-msmarco-10k\"\n",
    "# retriever = \"monot5-3b-msmarco-10k\"\n",
    "# retriever = \"random\"\n",
    "# retriever = \"closed-book\"\n",
    "# retriever = \"gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate QASPER results\n",
    "\n",
    "for root, dirs, files in os.walk(f\"results/{retriever}/{model}\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jsonl\"):\n",
    "            predictions_path = os.path.join(root, file)\n",
    "            os.system(f\"python qasper_evaluator.py --predictions {predictions_path} --gold {TEST_SET_PATH} --text_evidence_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "for root, dirs, files in os.walk(f\"results/{retriever}/{model}\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\"results.json\"):\n",
    "            results = json.load(open(os.path.join(root, file)))\n",
    "            # remove \"results.json\" from file name\n",
    "            model_name = file[:-13]\n",
    "            retrieval_name = root.split(\"/\")[-1]\n",
    "            results_dict[os.path.join(retrieval_name, model_name)] = {\n",
    "                \"Extractive\": results[\"Answer F1 by type\"][\"extractive\"] * 100,\n",
    "                \"Abstractive\": results[\"Answer F1 by type\"][\"abstractive\"] * 100,\n",
    "                \"Yes/No\": results[\"Answer F1 by type\"][\"boolean\"] * 100,\n",
    "                \"Unanswerable\": results[\"Answer F1 by type\"][\"none\"] * 100,\n",
    "                \"Answer F1\": results[\"Answer F1\"] * 100,\n",
    "                \"Evidence F1\": results[\"Evidence F1\"] * 100,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extractive</th>\n",
       "      <th>Abstractive</th>\n",
       "      <th>Yes/No</th>\n",
       "      <th>Unanswerable</th>\n",
       "      <th>Answer F1</th>\n",
       "      <th>Evidence F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>code-davinci-002\\qasper_zeroshot_prompt</th>\n",
       "      <td>40.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>82.8</td>\n",
       "      <td>51.7</td>\n",
       "      <td>42.3</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code-davinci-002\\qasper_fewshot_qc</th>\n",
       "      <td>50.5</td>\n",
       "      <td>26.3</td>\n",
       "      <td>70.7</td>\n",
       "      <td>9.0</td>\n",
       "      <td>44.6</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code-davinci-002\\qasper_cot_prompt</th>\n",
       "      <td>51.6</td>\n",
       "      <td>25.2</td>\n",
       "      <td>75.5</td>\n",
       "      <td>30.1</td>\n",
       "      <td>46.9</td>\n",
       "      <td>41.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code-davinci-002\\qasper_document_fewshot_prompt</th>\n",
       "      <td>54.1</td>\n",
       "      <td>27.4</td>\n",
       "      <td>80.9</td>\n",
       "      <td>11.2</td>\n",
       "      <td>48.7</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code-davinci-002\\qasper_fewshot_prompt</th>\n",
       "      <td>56.5</td>\n",
       "      <td>27.1</td>\n",
       "      <td>81.4</td>\n",
       "      <td>20.4</td>\n",
       "      <td>50.7</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Extractive  Abstractive  \\\n",
       "code-davinci-002\\qasper_zeroshot_prompt                40.0         17.1   \n",
       "code-davinci-002\\qasper_fewshot_qc                     50.5         26.3   \n",
       "code-davinci-002\\qasper_cot_prompt                     51.6         25.2   \n",
       "code-davinci-002\\qasper_document_fewshot_prompt        54.1         27.4   \n",
       "code-davinci-002\\qasper_fewshot_prompt                 56.5         27.1   \n",
       "\n",
       "                                                 Yes/No  Unanswerable  \\\n",
       "code-davinci-002\\qasper_zeroshot_prompt            82.8          51.7   \n",
       "code-davinci-002\\qasper_fewshot_qc                 70.7           9.0   \n",
       "code-davinci-002\\qasper_cot_prompt                 75.5          30.1   \n",
       "code-davinci-002\\qasper_document_fewshot_prompt    80.9          11.2   \n",
       "code-davinci-002\\qasper_fewshot_prompt             81.4          20.4   \n",
       "\n",
       "                                                 Answer F1  Evidence F1  \n",
       "code-davinci-002\\qasper_zeroshot_prompt               42.3         26.8  \n",
       "code-davinci-002\\qasper_fewshot_qc                    44.6         26.8  \n",
       "code-davinci-002\\qasper_cot_prompt                    46.9         41.9  \n",
       "code-davinci-002\\qasper_document_fewshot_prompt       48.7         26.8  \n",
       "code-davinci-002\\qasper_fewshot_prompt                50.7         26.8  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code-davinci-002 results\n",
    "df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "df = df.round(1)\n",
    "df = df.sort_values(by=\"Answer F1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extractive</th>\n",
       "      <th>Abstractive</th>\n",
       "      <th>Yes/No</th>\n",
       "      <th>Unanswerable</th>\n",
       "      <th>Answer F1</th>\n",
       "      <th>Evidence F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text-davinci-003\\qasper_fewshot_qc</th>\n",
       "      <td>40.6</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.4</td>\n",
       "      <td>61.6</td>\n",
       "      <td>38.8</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-003\\qasper_zeroshot_prompt</th>\n",
       "      <td>40.7</td>\n",
       "      <td>22.4</td>\n",
       "      <td>44.9</td>\n",
       "      <td>61.0</td>\n",
       "      <td>38.9</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-003\\qasper_cot_prompt</th>\n",
       "      <td>48.6</td>\n",
       "      <td>24.2</td>\n",
       "      <td>72.3</td>\n",
       "      <td>70.1</td>\n",
       "      <td>48.6</td>\n",
       "      <td>49.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-003\\qasper_fewshot_prompt</th>\n",
       "      <td>56.6</td>\n",
       "      <td>26.9</td>\n",
       "      <td>80.1</td>\n",
       "      <td>55.6</td>\n",
       "      <td>53.5</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Extractive  Abstractive  Yes/No  \\\n",
       "text-davinci-003\\qasper_fewshot_qc             40.6         22.0    44.4   \n",
       "text-davinci-003\\qasper_zeroshot_prompt        40.7         22.4    44.9   \n",
       "text-davinci-003\\qasper_cot_prompt             48.6         24.2    72.3   \n",
       "text-davinci-003\\qasper_fewshot_prompt         56.6         26.9    80.1   \n",
       "\n",
       "                                         Unanswerable  Answer F1  Evidence F1  \n",
       "text-davinci-003\\qasper_fewshot_qc               61.6       38.8         26.8  \n",
       "text-davinci-003\\qasper_zeroshot_prompt          61.0       38.9         26.8  \n",
       "text-davinci-003\\qasper_cot_prompt               70.1       48.6         49.5  \n",
       "text-davinci-003\\qasper_fewshot_prompt           55.6       53.5         26.8  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text-davinci-003 results\n",
    "df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "df = df.round(1)\n",
    "df = df.sort_values(by=\"Answer F1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extractive</th>\n",
       "      <th>Abstractive</th>\n",
       "      <th>Yes/No</th>\n",
       "      <th>Unanswerable</th>\n",
       "      <th>Answer F1</th>\n",
       "      <th>Evidence F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo\\qasper_zeroshot_prompt</th>\n",
       "      <td>36.7</td>\n",
       "      <td>20.9</td>\n",
       "      <td>62.4</td>\n",
       "      <td>79.6</td>\n",
       "      <td>41.1</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo\\qasper_cot_prompt</th>\n",
       "      <td>41.9</td>\n",
       "      <td>19.7</td>\n",
       "      <td>55.2</td>\n",
       "      <td>92.2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>51.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo\\qasper_cot_prompt_user_assistant</th>\n",
       "      <td>45.2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>58.4</td>\n",
       "      <td>81.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>51.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo\\qasper_fewshot_prompt</th>\n",
       "      <td>46.0</td>\n",
       "      <td>23.1</td>\n",
       "      <td>62.3</td>\n",
       "      <td>87.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Extractive  Abstractive  \\\n",
       "gpt-3.5-turbo\\qasper_zeroshot_prompt                  36.7         20.9   \n",
       "gpt-3.5-turbo\\qasper_cot_prompt                       41.9         19.7   \n",
       "gpt-3.5-turbo\\qasper_cot_prompt_user_assistant        45.2         23.0   \n",
       "gpt-3.5-turbo\\qasper_fewshot_prompt                   46.0         23.1   \n",
       "\n",
       "                                                Yes/No  Unanswerable  \\\n",
       "gpt-3.5-turbo\\qasper_zeroshot_prompt              62.4          79.6   \n",
       "gpt-3.5-turbo\\qasper_cot_prompt                   55.2          92.2   \n",
       "gpt-3.5-turbo\\qasper_cot_prompt_user_assistant    58.4          81.0   \n",
       "gpt-3.5-turbo\\qasper_fewshot_prompt               62.3          87.0   \n",
       "\n",
       "                                                Answer F1  Evidence F1  \n",
       "gpt-3.5-turbo\\qasper_zeroshot_prompt                 41.1         26.8  \n",
       "gpt-3.5-turbo\\qasper_cot_prompt                      45.0         51.5  \n",
       "gpt-3.5-turbo\\qasper_cot_prompt_user_assistant       46.0         51.6  \n",
       "gpt-3.5-turbo\\qasper_fewshot_prompt                  48.0         26.8  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt-3.5-turbo results\n",
    "df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "df = df.round(1)\n",
    "df = df.sort_values(by=\"Answer F1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extractive</th>\n",
       "      <th>Abstractive</th>\n",
       "      <th>Yes/No</th>\n",
       "      <th>Unanswerable</th>\n",
       "      <th>Answer F1</th>\n",
       "      <th>Evidence F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>unifiedqa\\unifiedqa-t5-small</th>\n",
       "      <td>28.1</td>\n",
       "      <td>12.8</td>\n",
       "      <td>48.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unifiedqa\\unifiedqa-v2-t5-large-1251000</th>\n",
       "      <td>37.4</td>\n",
       "      <td>18.2</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unifiedqa\\unifiedqa-v2-t5-3b-1251000</th>\n",
       "      <td>43.6</td>\n",
       "      <td>18.6</td>\n",
       "      <td>71.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>38.7</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unifiedqa\\unifiedqa-v2-t5-11b-1251000</th>\n",
       "      <td>44.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>80.6</td>\n",
       "      <td>19.3</td>\n",
       "      <td>41.7</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Extractive  Abstractive  Yes/No  \\\n",
       "unifiedqa\\unifiedqa-t5-small                   28.1         12.8    48.4   \n",
       "unifiedqa\\unifiedqa-v2-t5-large-1251000        37.4         18.2    66.7   \n",
       "unifiedqa\\unifiedqa-v2-t5-3b-1251000           43.6         18.6    71.7   \n",
       "unifiedqa\\unifiedqa-v2-t5-11b-1251000          44.0         18.9    80.6   \n",
       "\n",
       "                                         Unanswerable  Answer F1  Evidence F1  \n",
       "unifiedqa\\unifiedqa-t5-small                      0.8       25.0         26.8  \n",
       "unifiedqa\\unifiedqa-v2-t5-large-1251000           0.0       34.5         26.8  \n",
       "unifiedqa\\unifiedqa-v2-t5-3b-1251000              2.6       38.7         26.8  \n",
       "unifiedqa\\unifiedqa-v2-t5-11b-1251000            19.3       41.7         26.8  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unifiedQA results\n",
    "df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "df = df.round(1)\n",
    "df = df.sort_values(by=\"Answer F1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extractive</th>\n",
       "      <th>Abstractive</th>\n",
       "      <th>Yes/No</th>\n",
       "      <th>Unanswerable</th>\n",
       "      <th>Answer F1</th>\n",
       "      <th>Evidence F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text-davinci-003\\qasper_zeroshot_prompt</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>17.8</td>\n",
       "      <td>98.3</td>\n",
       "      <td>18.1</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-003\\qasper_fewshot_prompt</th>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>24.5</td>\n",
       "      <td>90.5</td>\n",
       "      <td>18.4</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Extractive  Abstractive  Yes/No  \\\n",
       "text-davinci-003\\qasper_zeroshot_prompt         0.4          0.5    17.8   \n",
       "text-davinci-003\\qasper_fewshot_prompt          1.7          1.8    24.5   \n",
       "\n",
       "                                         Unanswerable  Answer F1  Evidence F1  \n",
       "text-davinci-003\\qasper_zeroshot_prompt          98.3       18.1        100.0  \n",
       "text-davinci-003\\qasper_fewshot_prompt           90.5       18.4        100.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RANDOM\n",
    "df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "df = df.round(1)\n",
    "df = df.sort_values(by=\"Answer F1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extractive</th>\n",
       "      <th>Abstractive</th>\n",
       "      <th>Yes/No</th>\n",
       "      <th>Unanswerable</th>\n",
       "      <th>Answer F1</th>\n",
       "      <th>Evidence F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text-davinci-003\\qasper_question_only_prompt</th>\n",
       "      <td>12.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>68.7</td>\n",
       "      <td>66.7</td>\n",
       "      <td>26.5</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Extractive  Abstractive  Yes/No  \\\n",
       "text-davinci-003\\qasper_question_only_prompt        12.4         10.0    68.7   \n",
       "\n",
       "                                              Unanswerable  Answer F1  \\\n",
       "text-davinci-003\\qasper_question_only_prompt          66.7       26.5   \n",
       "\n",
       "                                              Evidence F1  \n",
       "text-davinci-003\\qasper_question_only_prompt        100.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QUESTION\n",
    "df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "df = df.round(1)\n",
    "df = df.sort_values(by=\"Answer F1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extractive</th>\n",
       "      <th>Abstractive</th>\n",
       "      <th>Yes/No</th>\n",
       "      <th>Unanswerable</th>\n",
       "      <th>Answer F1</th>\n",
       "      <th>Evidence F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text-davinci-003\\qasper_zeroshot_prompt</th>\n",
       "      <td>47.5</td>\n",
       "      <td>24.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>97.4</td>\n",
       "      <td>47.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci-003\\qasper_fewshot_prompt</th>\n",
       "      <td>64.1</td>\n",
       "      <td>24.3</td>\n",
       "      <td>65.7</td>\n",
       "      <td>97.7</td>\n",
       "      <td>59.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Extractive  Abstractive  Yes/No  \\\n",
       "text-davinci-003\\qasper_zeroshot_prompt        47.5         24.5    39.3   \n",
       "text-davinci-003\\qasper_fewshot_prompt         64.1         24.3    65.7   \n",
       "\n",
       "                                         Unanswerable  Answer F1  Evidence F1  \n",
       "text-davinci-003\\qasper_zeroshot_prompt          97.4       47.0        100.0  \n",
       "text-davinci-003\\qasper_fewshot_prompt           97.7       59.0        100.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GOLD\n",
    "df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "df = df.round(1)\n",
    "df = df.sort_values(by=\"Answer F1\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different k - Evidence F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval_name = \"all-mpnet-base-v2\"\n",
    "retrieval_name = \"monoT5-3b-msmarco-10k\"\n",
    "\n",
    "with open(f'retrieval_passages/{retrieval_name}_contents.json', 'r') as f:\n",
    "    retrieval_psgs = json.load(f)\n",
    "\n",
    "for k in range(1,6):\n",
    "    predictions = []\n",
    "    for q_id, passages in retrieval_psgs.items():\n",
    "        predictions.append({\n",
    "            \"question_id\": q_id,\n",
    "            \"predicted_answer\": \"\",\n",
    "            \"predicted_evidence\": passages[:k]\n",
    "        })\n",
    "        #make dir if it doesn't exist\n",
    "    if not os.path.exists(f\"results/{retrieval_name}/passages\"):\n",
    "        os.makedirs(f\"results/{retrieval_name}/passages\")\n",
    "    with open(f\"results/{retrieval_name}/passages/{k}.jsonl\", \"w\") as f:\n",
    "        for prediction in predictions:\n",
    "            f.write(json.dumps(prediction) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(f\"results/{retrieval_name}/passages\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jsonl\"):\n",
    "            predictions_path = os.path.join(root, file)\n",
    "            os.system(f\"python qasper_evaluator.py --predictions {predictions_path} --gold {TEST_SET_PATH} --text_evidence_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "for root, dirs, files in os.walk(f\"results/{retrieval_name}/passages/\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\"results.json\"):\n",
    "            results = json.load(open(os.path.join(root, file)))\n",
    "            model_name = file[:-13]\n",
    "            results_dict[os.path.join(retrieval_name, model_name)] = {\n",
    "                \"Evidence F1\": results[\"Evidence F1\"] * 100,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>monoT5-3b-msmarco-10k\\1</th>\n",
       "      <th>monoT5-3b-msmarco-10k\\2</th>\n",
       "      <th>monoT5-3b-msmarco-10k\\3</th>\n",
       "      <th>monoT5-3b-msmarco-10k\\4</th>\n",
       "      <th>monoT5-3b-msmarco-10k\\5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evidence F1</th>\n",
       "      <td>40.8</td>\n",
       "      <td>38.5</td>\n",
       "      <td>34.4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>28.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             monoT5-3b-msmarco-10k\\1  monoT5-3b-msmarco-10k\\2  \\\n",
       "Evidence F1                     40.8                     38.5   \n",
       "\n",
       "             monoT5-3b-msmarco-10k\\3  monoT5-3b-msmarco-10k\\4  \\\n",
       "Evidence F1                     34.4                     31.0   \n",
       "\n",
       "             monoT5-3b-msmarco-10k\\5  \n",
       "Evidence F1                     28.2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages_df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "passages_df = passages_df.round(1)\n",
    "passages_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>monoT5-base-msmarco-10k\\1</th>\n",
       "      <th>monoT5-base-msmarco-10k\\2</th>\n",
       "      <th>monoT5-base-msmarco-10k\\3</th>\n",
       "      <th>monoT5-base-msmarco-10k\\4</th>\n",
       "      <th>monoT5-base-msmarco-10k\\5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evidence F1</th>\n",
       "      <td>37.1</td>\n",
       "      <td>35.6</td>\n",
       "      <td>32.4</td>\n",
       "      <td>29.2</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             monoT5-base-msmarco-10k\\1  monoT5-base-msmarco-10k\\2  \\\n",
       "Evidence F1                       37.1                       35.6   \n",
       "\n",
       "             monoT5-base-msmarco-10k\\3  monoT5-base-msmarco-10k\\4  \\\n",
       "Evidence F1                       32.4                       29.2   \n",
       "\n",
       "             monoT5-base-msmarco-10k\\5  \n",
       "Evidence F1                       26.8  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages_df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "passages_df = passages_df.round(1)\n",
    "passages_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bm25\\1</th>\n",
       "      <th>bm25\\2</th>\n",
       "      <th>bm25\\3</th>\n",
       "      <th>bm25\\4</th>\n",
       "      <th>bm25\\5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evidence F1</th>\n",
       "      <td>21.7</td>\n",
       "      <td>23.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             bm25\\1  bm25\\2  bm25\\3  bm25\\4  bm25\\5\n",
       "Evidence F1    21.7    23.6    23.0    22.5    21.2"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages_df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "passages_df = passages_df.round(1)\n",
    "passages_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all-mpnet-base-v2/1</th>\n",
       "      <th>all-mpnet-base-v2/2</th>\n",
       "      <th>all-mpnet-base-v2/3</th>\n",
       "      <th>all-mpnet-base-v2/4</th>\n",
       "      <th>all-mpnet-base-v2/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evidence F1</th>\n",
       "      <td>17.2</td>\n",
       "      <td>19.9</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19.4</td>\n",
       "      <td>18.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             all-mpnet-base-v2/1  all-mpnet-base-v2/2  all-mpnet-base-v2/3  \\\n",
       "Evidence F1                 17.2                 19.9                 19.6   \n",
       "\n",
       "             all-mpnet-base-v2/4  all-mpnet-base-v2/5  \n",
       "Evidence F1                 19.4                 18.2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages_df = pd.DataFrame.from_dict(results_dict, orient=\"index\")\n",
    "passages_df = passages_df.round(1)\n",
    "passages_df.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
