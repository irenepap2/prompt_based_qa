Answer the following question given the documents. Answer "Unanswerable" when not enough information is provided in the context. Answer only "Yes" or "No" in boolean questions.

Example 1:
[Document 1]: The DDI corpus contains thousands of XML files, each of which are constructed by several records. For a sentence containing INLINEFORM0 drugs, there are INLINEFORM1 drug pairs. We replace the interested two drugs with \u201cdrug1\u201d and \u201cdrug2\u201d while the other drugs are replaced by \u201cdurg0\u201d, as in BIBREF9 did. This step is called drug blinding. For example, the sentence in figure FIGREF5 generates 3 instances after drug blinding: \u201cdrug1: an increased risk of hepatitis has been reported to result from combined use of drug2 and drug0\u201d, \u201cdrug1: an increased risk of hepatitis has been reported to result from combined use of drug0 and drug2\u201d, \u201cdrug0: an increased risk of hepatitis has been reported to result from combined use of drug1 and drug2\u201d. The drug blinded sentences are the instances that are fed to our model.
question: How big is the evaluated dataset?
answer: contains thousands of XML files, each of which are constructed by several records

Example 2:
[Document 1]: Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities, for a total of 4,872 community-months.
question: Do they report results only on English data?
answer: No

Example 3:
[Document 1]: 
question: How does using NMT ensure generated reviews stay on topic?
answer: Unanswerable

Example 4:
[Document 1]: Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (Inc+Corr), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80$\\%$ for our model and 74$\\%$ for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input, in addition to the original naturally incorrect tweets, in order to better model the noisy sentences. | 
[Document 2]: Experimental results for the Intent Classification task on the Chatbot NLU Corpus with STT error can be seen in Table TABREF40. When presented with data containing STT error, our model outperforms all baseline models in both combinations of TTS-STT: gtts-witai outperforms the second placing baseline model by 0.94% with F1-score of 97.17%, and macsay-witai outperforms the next highest achieving model by 1.89% with F1-score of 96.23%.
question: By how much do they outperform other models in the sentiment in intent classification tasks?
answer: In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average

Example 5:
{% for con in context %}
[Document {{loop.index}}]: {{ con }}
{% endfor %}
question: {{question}}
answer: