{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"results/monot5-base-msmarco-10k/text-davinci-003/qasper_fewshot_prompt.jsonl\") as f:\n",
    "    few_shot_predictions = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "with open(\"results/monot5-base-msmarco-10k/text-davinci-003/qasper_cot_prompt_5.jsonl\") as f:\n",
    "    cot_predictions = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "with open(\"results/monot5-base-msmarco-10k/text-davinci-003/qasper_zeroshot_prompt.jsonl\") as f:\n",
    "    zeroshot_predictions = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/qasper/general_test_questions.json\") as f:\n",
    "    general_test_questions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1451, 1451, 1451, 51)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cot_predictions), len(few_shot_predictions), len(zeroshot_predictions), len(general_test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_predictions_dict = {}\n",
    "fewshot_predictions__evidence_dict = {}\n",
    "\n",
    "for pred in few_shot_predictions:\n",
    "    q_id = pred[\"question_id\"]\n",
    "    evidence = pred[\"predicted_evidence\"]\n",
    "    fewshot_predictions_dict[q_id] = pred\n",
    "    fewshot_predictions__evidence_dict[q_id] = evidence\n",
    "\n",
    "cot_predictions_dict = {}\n",
    "\n",
    "for pred in cot_predictions:\n",
    "    q_id = pred[\"question_id\"]\n",
    "    cot_predictions_dict[q_id] = pred\n",
    "\n",
    "zeroshot_predictions_dict = {}\n",
    "\n",
    "for pred in zeroshot_predictions:\n",
    "    q_id = pred[\"question_id\"]\n",
    "    zeroshot_predictions_dict[q_id] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  what was the baseline?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zeroshot_pred_answer</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>cot_predictions</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4137a82d7752be7a6c142ceb48ce784fd475fb06</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Weka baseline</td>\n",
       "      <td>the Weka baseline</td>\n",
       "      <td>[Weka baseline BIBREF5, Weka baseline BIBREF5, Weka,  Weka baseline BIBREF5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7fba61426737394304e307cdc7537225f6253150</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Hybrid – the current state-of-the-art</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>[Dress, Dress-Ls, Pbmt-R, Hybrid,  Sbmt-Sari, Dress,  Dress-Ls, Pbmt-R, Hybrid, Sbmt-Sari]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6bce04570d4745dcfaca5cba64075242308b65cf</th>\n",
       "      <td>The baseline model is the BiGRU+CRF model.</td>\n",
       "      <td>BiGRU+CRF</td>\n",
       "      <td>BiGRU+CRF</td>\n",
       "      <td>[BiGRU+CRF, BiGRU+CRF]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e9a0a69eacd554141f56b60ab2d1912cc33f526a</th>\n",
       "      <td>The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances.</td>\n",
       "      <td>random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively</td>\n",
       "      <td>a system that randomly assigns any of the sentiment values to each of the test instances</td>\n",
       "      <td>[Random labeling,  randomly assigning any of the sentiment values to each of the test instances]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ec54ae2f4811196fcaafa45e76130239e69995f9</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library</td>\n",
       "      <td>Logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library.</td>\n",
       "      <td>[logistic regression (LR), Support Vector Machines (SVM), LSTM network from the Keras library ,  logistic regression (LR), Support Vector Machines (SVM)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         zeroshot_pred_answer  \\\n",
       "quids                                                                                                                                                           \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                                                         Unanswerable   \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                         Unanswerable   \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                         Unanswerable   \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                                                         Unanswerable   \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                           The baseline model is the BiGRU+CRF model.   \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a  The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances.   \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9                                                                                                         Unanswerable   \n",
       "\n",
       "                                                                                                                                          fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                           \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                                                        Weka baseline   \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                         Unanswerable   \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                         Unanswerable   \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                                Hybrid – the current state-of-the-art   \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                            BiGRU+CRF   \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a  random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively   \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9                             logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library   \n",
       "\n",
       "                                                                                                                    cot_predictions  \\\n",
       "quids                                                                                                                                 \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                          the Weka baseline   \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                               Unanswerable   \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                               Unanswerable   \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                                     Hybrid   \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                  BiGRU+CRF   \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a   a system that randomly assigns any of the sentiment values to each of the test instances   \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9  Logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library.   \n",
       "\n",
       "                                                                                                                                                                                       gold_answers  \n",
       "quids                                                                                                                                                                                                \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                               [Weka baseline BIBREF5, Weka baseline BIBREF5, Weka,  Weka baseline BIBREF5]  \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                                 [Unanswerable, Unanswerable, Unanswerable]  \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                                 [Unanswerable, Unanswerable, Unanswerable]  \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                 [Dress, Dress-Ls, Pbmt-R, Hybrid,  Sbmt-Sari, Dress,  Dress-Ls, Pbmt-R, Hybrid, Sbmt-Sari]  \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                                                     [BiGRU+CRF, BiGRU+CRF]  \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a                                                           [Random labeling,  randomly assigning any of the sentiment values to each of the test instances]  \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9  [logistic regression (LR), Support Vector Machines (SVM), LSTM network from the Keras library ,  logistic regression (LR), Support Vector Machines (SVM)]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  what dataset was used?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zeroshot_pred_answer</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>cot_predictions</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0ac6fbd81e2dd95b800283dc7e59ce969d45fc02</th>\n",
       "      <td>The BIBREF5, BIBREF6, BIBREF7, BIBREF8, and BIBREF9 datasets were used.</td>\n",
       "      <td>The training, validation and test datasets provided for the shared task BIBREF5</td>\n",
       "      <td>The training, validation and test datasets provided for the shared task BIBREF5, which includes tweets for four emotions: joy, sadness, fear, and anger.</td>\n",
       "      <td>[ training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger, datasets provided for the shared task BIBREF5, Dataset of tweets provided for the shared task., Dataset from shared task BIBREF5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d36a6447bfe58204e0d29f9213d84be04d875624</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Data set A and Data set B</td>\n",
       "      <td>Datasets A and B</td>\n",
       "      <td>[ two types of simultaneous speech EEG recording databases , The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment., Speech EEG recording collected from male and female subjects under different background noises, For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237b6fcc64b43901415f3ded17cc210a54ab698</th>\n",
       "      <td>The datasets used were a training set of documents and their associated summaries, where the summary is a gold summary of the document.</td>\n",
       "      <td>113 GB</td>\n",
       "      <td>a training set of documents (e.g., body-text of research papers) and their associated summaries (e.g., abstracts)</td>\n",
       "      <td>[669 academic papers published by IEEE, 850 academic papers, 669 academic papers published by IEEE, For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748a50c96acb1aa03f7efd1b43376c193b2450a</th>\n",
       "      <td>The datasets used were Bing Liu's dataset BIBREF20, the Bag of Word vectorization method with a Logistic Regression classifier trained on 1.2 million (1, 3 and 5-star rating only) of Electronic reviews from SNAP Amazon Dataset BIBREF19, and a special library implemented for such purposes BIBREF18.</td>\n",
       "      <td>Bing Liu's dataset</td>\n",
       "      <td>Bing Liu's dataset</td>\n",
       "      <td>[SNAP Amazon Dataset , Bing Liu's dataset, Bing Liu's dataset, SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5fd112980d0dd7f7ce30e6273fe6e7b230b13225</th>\n",
       "      <td>MTNT</td>\n",
       "      <td>UGC (User Generated Content), Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet</td>\n",
       "      <td>UGC, which stands for User Generated Content and was built by combining Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet.</td>\n",
       "      <td>[WMT 2014,  UGC (User Generated Content), 11.5k French reviews from Foursquare, WMT 2014, UGC (User Generated Content)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1f7a040545c9501215d3391e267c7874f9a6004</th>\n",
       "      <td>News and Sports news</td>\n",
       "      <td>News corpus (227 texts published on December 31, 2010 by the Lusa agency) and Sports news corpus (881 sports news)</td>\n",
       "      <td>a set of 227 texts published on December 31, 2010 by the Lusa agency and a set of 881 sports news</td>\n",
       "      <td>[CoNLL_X bosque data, News data by Lusa agency, Sports news data, News, Sports news, News, Sports news]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                zeroshot_pred_answer  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                  \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                                                                                                                                                                                                                     The BIBREF5, BIBREF6, BIBREF7, BIBREF8, and BIBREF9 datasets were used.   \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624                                                                                                                                                                                                                                                                                                Unanswerable   \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                                                                                     The datasets used were a training set of documents and their associated summaries, where the summary is a gold summary of the document.   \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a  The datasets used were Bing Liu's dataset BIBREF20, the Bag of Word vectorization method with a Logistic Regression classifier trained on 1.2 million (1, 3 and 5-star rating only) of Electronic reviews from SNAP Amazon Dataset BIBREF19, and a special library implemented for such purposes BIBREF18.   \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225                                                                                                                                                                                                                                                                                                        MTNT   \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004                                                                                                                                                                                                                                                                                        News and Sports news   \n",
       "\n",
       "                                                                                                                                         fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                          \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                     The training, validation and test datasets provided for the shared task BIBREF5   \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624                                                                                           Data set A and Data set B   \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                              113 GB   \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                  Bing Liu's dataset   \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225  UGC (User Generated Content), Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet   \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004  News corpus (227 texts published on December 31, 2010 by the Lusa agency) and Sports news corpus (881 sports news)   \n",
       "\n",
       "                                                                                                                                                                                        cot_predictions  \\\n",
       "quids                                                                                                                                                                                                     \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02       The training, validation and test datasets provided for the shared task BIBREF5, which includes tweets for four emotions: joy, sadness, fear, and anger.   \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624                                                                                                                                               Datasets A and B   \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                              a training set of documents (e.g., body-text of research papers) and their associated summaries (e.g., abstracts)   \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                                                             Bing Liu's dataset   \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225  UGC, which stands for User Generated Content and was built by combining Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet.   \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004                                                              a set of 227 texts published on December 31, 2010 by the Lusa agency and a set of 881 sports news   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                                                                                                                                                                                                                                  [ training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger, datasets provided for the shared task BIBREF5, Dataset of tweets provided for the shared task., Dataset from shared task BIBREF5]  \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624  [ two types of simultaneous speech EEG recording databases , The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment., Speech EEG recording collected from male and female subjects under different background noises, For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment.]  \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                                                                                 [669 academic papers published by IEEE, 850 academic papers, 669 academic papers published by IEEE, For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation.]  \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                                                                                                                                                                                                                                                                                                                                 [SNAP Amazon Dataset , Bing Liu's dataset, Bing Liu's dataset, SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20]  \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225                                                                                                                                                                                                                                                                                                                                                                                                                   [WMT 2014,  UGC (User Generated Content), 11.5k French reviews from Foursquare, WMT 2014, UGC (User Generated Content)]  \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004                                                                                                                                                                                                                                                                                                                                                                                                                                   [CoNLL_X bosque data, News data by Lusa agency, Sports news data, News, Sports news, News, Sports news]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What dataset do they use?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zeroshot_pred_answer</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>cot_predictions</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0fe49431db5ffaa24372919daf24d8f84117bfda</th>\n",
       "      <td>DUC 2002 document summarization corpus and the DailyMail news highlights corpus.</td>\n",
       "      <td>DUC 2002 document summarization corpus and DailyMail news highlights corpus</td>\n",
       "      <td>The DUC 2002 document summarization corpus and the DailyMail news highlights corpus</td>\n",
       "      <td>[DUC 2002 document summarization corpus, our own DailyMail news highlights corpus, DUC 2002, our own Dailymail news highlights corpus, the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus, DailyMail news articles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f</th>\n",
       "      <td>The dataset used is the 3M-sentence dataset.</td>\n",
       "      <td>The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).</td>\n",
       "      <td>a massive dataset of conversation utterances crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms)</td>\n",
       "      <td>[real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances, chatting corpus from DuMi and conversation data from Douban forum, chatting corpus from DuMi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9489b0ecb643c1fc95c001c65d4e9771315989aa</th>\n",
       "      <td>CoNLL 2012 data BIBREF15</td>\n",
       "      <td>The English portion of CoNLL 2012 data</td>\n",
       "      <td>The English portion of the CoNLL 2012 data</td>\n",
       "      <td>[CoNLL 2012, English portion of CoNLL 2012 data BIBREF15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763</th>\n",
       "      <td>The dataset used is a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them.</td>\n",
       "      <td>Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them</td>\n",
       "      <td>a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects</td>\n",
       "      <td>[manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                   zeroshot_pred_answer  \\\n",
       "quids                                                                                                                                                                                                     \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda                                                                               DUC 2002 document summarization corpus and the DailyMail news highlights corpus.   \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f                                                                                                                   The dataset used is the 3M-sentence dataset.   \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                                                       CoNLL 2012 data BIBREF15   \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763  The dataset used is a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them.   \n",
       "\n",
       "                                                                                                                                                             fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                                              \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda                                                             DUC 2002 document summarization corpus and DailyMail news highlights corpus   \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f  The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).   \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                  The English portion of CoNLL 2012 data   \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763  Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them   \n",
       "\n",
       "                                                                                                                                                                                             cot_predictions  \\\n",
       "quids                                                                                                                                                                                                          \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda                                                                                 The DUC 2002 document summarization corpus and the DailyMail news highlights corpus   \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f  a massive dataset of conversation utterances crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms)   \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                                          The English portion of the CoNLL 2012 data   \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763                                                          a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                               gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                        \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda            [DUC 2002 document summarization corpus, our own DailyMail news highlights corpus, DUC 2002, our own Dailymail news highlights corpus, the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus, DailyMail news articles]  \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f                                                                         [real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances, chatting corpus from DuMi and conversation data from Douban forum, chatting corpus from DuMi]  \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                                                                                                                          [CoNLL 2012, English portion of CoNLL 2012 data BIBREF15]  \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763  [manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Which dataset do they use?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zeroshot_pred_answer</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>cot_predictions</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1ecbbb60dc44a701e9c57c22167dd412711bb0be</th>\n",
       "      <td>The Wikipedia crawl-based dataset</td>\n",
       "      <td>Clueweb09 derived dataset and Wikipedia crawl data</td>\n",
       "      <td>The Clueweb09 derived dataset of BIBREF0 and BIBREF6</td>\n",
       "      <td>[Clueweb09, Clueweb09 derived dataset, new dataset based on Wikipedia crawl data, the Clueweb09 derived dataset , dataset based on Wikipedia crawl data, Clueweb09 derived dataset, Wikipedia crawl data]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00e4c9aa87411dfc5455fc92f10e5c9266e7b95e</th>\n",
       "      <td>Microsoft N-Gram dataset</td>\n",
       "      <td>U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013</td>\n",
       "      <td>The U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013.</td>\n",
       "      <td>[Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013,  survey data and hand crafted a total of 293 textual questions BIBREF13, U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013, Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8c46a26f9b0b41c656b5b55142d491600663defa</th>\n",
       "      <td>GlobalPhone and zrsc 2015</td>\n",
       "      <td>GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[GlobalPhone corpus, GlobalPhone\\nCroatian\\nHausa\\nMandarin\\nSpanish\\nSwedish\\nTurkish\\nZRSC\\nBuckeye\\nXitsonga, GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a398c9b061f28543bc77c2951d0dfc5d1bee9e87</th>\n",
       "      <td>BIBREF4</td>\n",
       "      <td>BIBREF4 crowdsourced the annotation of 19538 tweets they had curated</td>\n",
       "      <td>The dataset curated by BIBREF4, which contains the title and text of the article, as well as supplementary information such as target description, target keywords and linked images.</td>\n",
       "      <td>[A crowdsourced twitter dataset containing 19358 tweets, BIBREF4, 19538 tweets  from BIBREF4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8de0e1fdcca81b49615a6839076f8d42226bf1fe</th>\n",
       "      <td>The dataset used is of 500 rescored intent annotations found in the lattices in cancellations and refunds domain.</td>\n",
       "      <td>500 rescored intent annotations found in the lattices in cancellations and refunds domain</td>\n",
       "      <td>A sample of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, performed on more than 70 000 US English phone conversations</td>\n",
       "      <td>[500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9d963d385bd495a7e193f8a498d64c1612e6c20c</th>\n",
       "      <td>spmrl and English Penn Treebank</td>\n",
       "      <td>spmrl datasets, English Penn Treebank, Stanford dependencies</td>\n",
       "      <td>The English Penn Treebank and the SPMRL test sets</td>\n",
       "      <td>[English Penn Treebank, spmrl datasets,  English Penn Treebank, spmrl datasets]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                       zeroshot_pred_answer  \\\n",
       "quids                                                                                                                                                         \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                                  The Wikipedia crawl-based dataset   \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e                                                                                           Microsoft N-Gram dataset   \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                                                          GlobalPhone and zrsc 2015   \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                                                                            BIBREF4   \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe  The dataset used is of 500 rescored intent annotations found in the lattices in cancellations and refunds domain.   \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                                    spmrl and English Penn Treebank   \n",
       "\n",
       "                                                                                                                                                    fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                                     \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                             Clueweb09 derived dataset and Wikipedia crawl data   \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e  U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013   \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus   \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                           BIBREF4 crowdsourced the annotation of 19538 tweets they had curated   \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                      500 rescored intent annotations found in the lattices in cancellations and refunds domain   \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                   spmrl datasets, English Penn Treebank, Stanford dependencies   \n",
       "\n",
       "                                                                                                                                                                                                                cot_predictions  \\\n",
       "quids                                                                                                                                                                                                                             \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                                                                                   The Clueweb09 derived dataset of BIBREF0 and BIBREF6   \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e                                                     The U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013.   \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                                                                                                                                           Unanswerable   \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87  The dataset curated by BIBREF4, which contains the title and text of the article, as well as supplementary information such as target description, target keywords and linked images.   \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                    A sample of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, performed on more than 70 000 US English phone conversations   \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                                                                                      The English Penn Treebank and the SPMRL test sets   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                   gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                                                                                                                                                              [Clueweb09, Clueweb09 derived dataset, new dataset based on Wikipedia crawl data, the Clueweb09 derived dataset , dataset based on Wikipedia crawl data, Clueweb09 derived dataset, Wikipedia crawl data]  \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e  [Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013,  survey data and hand crafted a total of 293 textual questions BIBREF13, U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013, Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12]  \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                                                                                                                                                                                                 [GlobalPhone corpus, GlobalPhone\\nCroatian\\nHausa\\nMandarin\\nSpanish\\nSwedish\\nTurkish\\nZRSC\\nBuckeye\\nXitsonga, GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus]  \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                                                                                                                                                                                                                                                                                          [A crowdsourced twitter dataset containing 19358 tweets, BIBREF4, 19538 tweets  from BIBREF4]  \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                                                                                                [500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain]  \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                                                                                                                                                                                                                                                                                        [English Penn Treebank, spmrl datasets,  English Penn Treebank, spmrl datasets]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  what are the baselines?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zeroshot_pred_answer</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>cot_predictions</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ebe1084a06abdabefffc66f029eeb0b69f114fd9</th>\n",
       "      <td>The baselines are a standard bidirectional RNN model with attention, trained with Nematus; and a standard context-agnostic Transformer.</td>\n",
       "      <td>a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline</td>\n",
       "      <td>a standard bidirectional RNN model with attention and a standard context-agnostic Transformer</td>\n",
       "      <td>[bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21,  standard bidirectional RNN model with attention, A standard context-agnostic Transformer, standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eb653a5c59851eda313ece0bcd8c589b6155d73e</th>\n",
       "      <td>The baseline is a one-stage RNN system.</td>\n",
       "      <td>a one-stage RNN system</td>\n",
       "      <td>a one-stage RNN system with a 2-layer BLSTM</td>\n",
       "      <td>[one-stage RNN system containing 2-layer BLSTM, one-stage RNN system, a one-stage RNN system]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634a071b13eb7139e77872ecfdc135a2eb2f89da</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>The methodology suggested by BIBREF1, BIBREF4</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Nearest neighbors (NN) Estimator, Naive Bayes (NB) Estimator, Recurrent neural network (RNN), Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016).]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8cf52ba480d372fc15024b3db704952f10fdca27</th>\n",
       "      <td>The baselines are the pre-trained word embedding vectors on a large general text corpus and the word2vec generated on the training set.</td>\n",
       "      <td>RNN, CNN, LSTM, BiLSTM, Dual-Encoder</td>\n",
       "      <td>FastText, word2vec, RNN, CNN, LSTM, BiLSTM, Dual-Encoder, gtts-witai, macsay-witai, and ESIM</td>\n",
       "      <td>[ESIM, ESIM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726c5c1b6951287f4bae22978f9a91d22d9bef61</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Reschke CRF, Reschke Noisy-OR, Reschke Best, Reschke CRF, Reschke Noisy-OR, Reschke Best]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                             zeroshot_pred_answer  \\\n",
       "quids                                                                                                                                                                               \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  The baselines are a standard bidirectional RNN model with attention, trained with Nematus; and a standard context-agnostic Transformer.   \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                  The baseline is a one-stage RNN system.   \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                                                                             Unanswerable   \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27  The baselines are the pre-trained word embedding vectors on a large general text corpus and the word2vec generated on the training set.   \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                                             Unanswerable   \n",
       "\n",
       "                                                                                                                                           fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                            \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline   \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                a one-stage RNN system   \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                         The methodology suggested by BIBREF1, BIBREF4   \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                  RNN, CNN, LSTM, BiLSTM, Dual-Encoder   \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                          Unanswerable   \n",
       "\n",
       "                                                                                                                        cot_predictions  \\\n",
       "quids                                                                                                                                     \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  a standard bidirectional RNN model with attention and a standard context-agnostic Transformer   \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                    a one-stage RNN system with a 2-layer BLSTM   \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                                   Unanswerable   \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27   FastText, word2vec, RNN, CNN, LSTM, BiLSTM, Dual-Encoder, gtts-witai, macsay-witai, and ESIM   \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                   Unanswerable   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                            gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  [bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21,  standard bidirectional RNN model with attention, A standard context-agnostic Transformer, standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8]  \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                                                                                                                                                                                                                                                                   [one-stage RNN system containing 2-layer BLSTM, one-stage RNN system, a one-stage RNN system]  \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                                                                                                                                                                                                                        [Nearest neighbors (NN) Estimator, Naive Bayes (NB) Estimator, Recurrent neural network (RNN), Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016).]  \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                                                                                                                                                                                                                                                                                                                                                                    [ESIM, ESIM]  \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                                                                                                                                                                                                                                                                      [Reschke CRF, Reschke Noisy-OR, Reschke Best, Reschke CRF, Reschke Noisy-OR, Reschke Best]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Which datasets do they evaluate on?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zeroshot_pred_answer</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>cot_predictions</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3f717e6eceab0a066af65ddf782c1ebc502c28c0</th>\n",
       "      <td>FIGER (GOLD) BIBREF0 and BBN BIBREF5</td>\n",
       "      <td>FIGER (GOLD) and BBN</td>\n",
       "      <td>FIGER (GOLD) and BBN</td>\n",
       "      <td>[FIGER (GOLD) BIBREF0, BBN BIBREF5, FIGER (GOLD) , BBN, FIGER (GOLD), BBN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420862798054f736128a6f0c4393c7f9cc648b40</th>\n",
       "      <td>The SICK dataset.</td>\n",
       "      <td>SICK (Sentences Involving Compositional Knowledge) dataset</td>\n",
       "      <td>The SICK dataset, which consists of 10,000 English sentence pairs annotated for relatedness in meaning and entailment</td>\n",
       "      <td>[SICK (Sentences Involving Compositional Knowledge) dataset , SICK (Sentences Involving Compositional Knowledge) dataset, SICK (Sentences Involving Compositional Knowledge) dataset]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bc6ad5964f444cf414b661a4b942dafb7640c564</th>\n",
       "      <td>The 84 domains with at least 50 narratives and 3 event types were used for evaluation.</td>\n",
       "      <td>The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types.</td>\n",
       "      <td>84 domains with at least 50 narratives and 3 event types</td>\n",
       "      <td>[The Open Minds Indoor Common Sense (OMICS) corpus , Open Minds Indoor Common Sense (OMICS) corpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f3b4e52ba962a0004064132d123fd9b78d9e12e2</th>\n",
       "      <td>NIST OpenMT08, NIST06, NIST02, NIST03, NIST04, NIST05, NIST08, and BSTC</td>\n",
       "      <td>NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus</td>\n",
       "      <td>NIST dataset for development and the NIST02, NIST03, NIST04, NIST05, NIST08, and BSTC datasets for testing.</td>\n",
       "      <td>[NIST02, NIST03, NIST04, NIST05, NIST08, 2008 (NIST08) datasets, Baidu Speech Translation Corpus (BSTC)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            zeroshot_pred_answer  \\\n",
       "quids                                                                                                                              \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                    FIGER (GOLD) BIBREF0 and BBN BIBREF5   \n",
       "420862798054f736128a6f0c4393c7f9cc648b40                                                                       The SICK dataset.   \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564  The 84 domains with at least 50 narratives and 3 event types were used for evaluation.   \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2                 NIST OpenMT08, NIST06, NIST02, NIST03, NIST04, NIST05, NIST08, and BSTC   \n",
       "\n",
       "                                                                                                                                                                              fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                                                               \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                                                     FIGER (GOLD) and BBN   \n",
       "420862798054f736128a6f0c4393c7f9cc648b40                                                                                               SICK (Sentences Involving Compositional Knowledge) dataset   \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                          The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types.   \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2  NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus   \n",
       "\n",
       "                                                                                                                                                cot_predictions  \\\n",
       "quids                                                                                                                                                             \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                   FIGER (GOLD) and BBN   \n",
       "420862798054f736128a6f0c4393c7f9cc648b40  The SICK dataset, which consists of 10,000 English sentence pairs annotated for relatedness in meaning and entailment   \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                                                               84 domains with at least 50 narratives and 3 event types   \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2            NIST dataset for development and the NIST02, NIST03, NIST04, NIST05, NIST08, and BSTC datasets for testing.   \n",
       "\n",
       "                                                                                                                                                                                                                   gold_answers  \n",
       "quids                                                                                                                                                                                                                            \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                             [FIGER (GOLD) BIBREF0, BBN BIBREF5, FIGER (GOLD) , BBN, FIGER (GOLD), BBN]  \n",
       "420862798054f736128a6f0c4393c7f9cc648b40  [SICK (Sentences Involving Compositional Knowledge) dataset , SICK (Sentences Involving Compositional Knowledge) dataset, SICK (Sentences Involving Compositional Knowledge) dataset]  \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                                                                                    [The Open Minds Indoor Common Sense (OMICS) corpus , Open Minds Indoor Common Sense (OMICS) corpus]  \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2                                                                               [NIST02, NIST03, NIST04, NIST05, NIST08, 2008 (NIST08) datasets, Baidu Speech Translation Corpus (BSTC)]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  what were the evaluation metrics?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zeroshot_pred_answer</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>cot_predictions</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50bda708293532f07a3193aaea0519d433fcc040</th>\n",
       "      <td>The evaluation metric was the cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system.</td>\n",
       "      <td>cost measure that uses the confusion matrix of the prediction and prior knowledge</td>\n",
       "      <td>a confusion matrix of the prediction and prior knowledge</td>\n",
       "      <td>[Accuracy metric, accuracy, Accuracy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8908d1b865137bc309dde10a93735ec76037e5f9</th>\n",
       "      <td>The evaluation metric was macro-average recall.</td>\n",
       "      <td>macro-average recall</td>\n",
       "      <td>macro-average recall</td>\n",
       "      <td>[Unanswerable, macro-average recall]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301a453abaa3bc15976817fefce7a41f3b779907</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0</td>\n",
       "      <td>F-score for class 1 (ADR) and micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)</td>\n",
       "      <td>[micro-averaged F-score of the class 1 (intake) and class 2 (possible intake), F-score for class 1 (ADR), micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74a17eb3bf1d4f36e2db1459a342c529b9785f6e</th>\n",
       "      <td>The evaluation metrics were the BLEU score and a subjective evaluation of faithfulness.</td>\n",
       "      <td>case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)</td>\n",
       "      <td>case-insensitive 4-gram BLEU score</td>\n",
       "      <td>[BLEU , BLEU]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                         zeroshot_pred_answer  \\\n",
       "quids                                                                                                                                                                                                           \n",
       "50bda708293532f07a3193aaea0519d433fcc040  The evaluation metric was the cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system.   \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                      The evaluation metric was macro-average recall.   \n",
       "301a453abaa3bc15976817fefce7a41f3b779907                                                                                                                                                         Unanswerable   \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                              The evaluation metrics were the BLEU score and a subjective evaluation of faithfulness.   \n",
       "\n",
       "                                                                                                                                                                                               fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                                                                                \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                         cost measure that uses the confusion matrix of the prediction and prior knowledge   \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                      macro-average recall   \n",
       "301a453abaa3bc15976817fefce7a41f3b779907  F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0   \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                   case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)   \n",
       "\n",
       "                                                                                                                                     cot_predictions  \\\n",
       "quids                                                                                                                                                  \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                    a confusion matrix of the prediction and prior knowledge   \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                        macro-average recall   \n",
       "301a453abaa3bc15976817fefce7a41f3b779907  F-score for class 1 (ADR) and micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)   \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                          case-insensitive 4-gram BLEU score   \n",
       "\n",
       "                                                                                                                                                                                                                     gold_answers  \n",
       "quids                                                                                                                                                                                                                              \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                                                                                    [Accuracy metric, accuracy, Accuracy]  \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                     [Unanswerable, macro-average recall]  \n",
       "301a453abaa3bc15976817fefce7a41f3b779907  [micro-averaged F-score of the class 1 (intake) and class 2 (possible intake), F-score for class 1 (ADR), micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)]  \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                                                                                                            [BLEU , BLEU]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  How long is their dataset?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zeroshot_pred_answer</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>cot_predictions</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4a91432abe3f54fcbdd00bb85dc0df95b16edf42</th>\n",
       "      <td>85.4 million</td>\n",
       "      <td>85.4 million tweets</td>\n",
       "      <td>85.4 million tweets</td>\n",
       "      <td>[85400000, 24,802 , 24,802 labeled tweets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1d255f181b18f7cf8eb3dd2369a082a2a398b7b</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0b5c599195973c563c4b1a0fe5d8fc77204d71a0</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>624 full text documents</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[670, 670 publications]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80bb07e553449bde9ac0ff35fcc718d7c161f2d4</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>20214 sentence pairs</td>\n",
       "      <td>20,214 sentence pairs in the train set and 1,000 sentence pairs in the validation set</td>\n",
       "      <td>[21214, Data used has total of 23315 sentences.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         zeroshot_pred_answer  \\\n",
       "quids                                                           \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42         85.4 million   \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b         Unanswerable   \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0         Unanswerable   \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4         Unanswerable   \n",
       "\n",
       "                                              fewshot_pred_answer  \\\n",
       "quids                                                               \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42      85.4 million tweets   \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b             Unanswerable   \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0  624 full text documents   \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4     20214 sentence pairs   \n",
       "\n",
       "                                                                                                                cot_predictions  \\\n",
       "quids                                                                                                                             \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42                                                                    85.4 million tweets   \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b                                                                           Unanswerable   \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0                                                                           Unanswerable   \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4  20,214 sentence pairs in the train set and 1,000 sentence pairs in the validation set   \n",
       "\n",
       "                                                                              gold_answers  \n",
       "quids                                                                                       \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42        [85400000, 24,802 , 24,802 labeled tweets]  \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b        [Unanswerable, Unanswerable, Unanswerable]  \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0                           [670, 670 publications]  \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4  [21214, Data used has total of 23315 sentences.]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Do they report results only on English data?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zeroshot_pred_answer</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>cot_predictions</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bc7081aaa207de2362e0bea7bc8108d338aee36f</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, No, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693cdb9978749db04ba34d9c168e71534f00a226</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Yes, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a17fc7b96753f85aee1d2036e2627570f4b50c30</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2f142cd11731d29d0c3fa426e26ef80d997862e0</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Yes, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ec5e84a1d1b12f7185183d165cbb5eae66d9833e</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Yes, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74cc0300e22f60232812019011a09df92bbec803</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37b0ee4a9d0df3ae3493e3b9114c3f385746da5c</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d662fb442d5fc332194770aac835f401c2148d9</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[No, Yes]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         zeroshot_pred_answer  \\\n",
       "quids                                                           \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f         Unanswerable   \n",
       "693cdb9978749db04ba34d9c168e71534f00a226         Unanswerable   \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30         Unanswerable   \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0         Unanswerable   \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                   No   \n",
       "74cc0300e22f60232812019011a09df92bbec803         Unanswerable   \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                   No   \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                   No   \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                   No   \n",
       "\n",
       "                                         fewshot_pred_answer cot_predictions  \\\n",
       "quids                                                                          \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                 Yes    Unanswerable   \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                 Yes    Unanswerable   \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30                 Yes    Unanswerable   \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0                 Yes    Unanswerable   \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                 Yes    Unanswerable   \n",
       "74cc0300e22f60232812019011a09df92bbec803                 Yes    Unanswerable   \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                 Yes    Unanswerable   \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                 Yes             Yes   \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                 Yes             Yes   \n",
       "\n",
       "                                                                        gold_answers  \n",
       "quids                                                                                 \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                     [Unanswerable, No, Yes]  \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                    [Unanswerable, Yes, Yes]  \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30  [Unanswerable, Unanswerable, Unanswerable]  \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0           [Yes, Unanswerable, Unanswerable]  \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                         [Yes, Unanswerable]  \n",
       "74cc0300e22f60232812019011a09df92bbec803                         [Unanswerable, Yes]  \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                [Unanswerable, Unanswerable]  \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                                  [Yes, Yes]  \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                                   [No, Yes]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What datasets are used?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zeroshot_pred_answer</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>cot_predictions</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90aba75508aa145475d7cc9a501bbe987c0e8413</th>\n",
       "      <td>The Craigslist Bargaining dataset (CB)</td>\n",
       "      <td>Craigslist Bargaining dataset (CB)</td>\n",
       "      <td>Craigslist Bargaining dataset (CB)</td>\n",
       "      <td>[Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB) ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47ffc9811b037613c9c4d1ec1e4f13c08396ed1c</th>\n",
       "      <td>The datasets used are the Wall Street Journal articles from sections 2-20, 21-22, and 0-1.</td>\n",
       "      <td>PDTB 2.0 and Chatbot NLU Corpus with STT error</td>\n",
       "      <td>The PDTB 2.0 corpus and two state-of-the-art systems.</td>\n",
       "      <td>[PDTB 2.0, PDTB 2.0 , PDTB 2.0 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3e23cc3c5e4d5cec51d158130d6aeae120e94fc8</th>\n",
       "      <td>The datasets used are 1,606,741 query-reply pairs, 100k triples for validation, and 6,741 for testing.</td>\n",
       "      <td>A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs</td>\n",
       "      <td>Two datasets comprising 1,606,741 query-reply pairs and human-human utterance pairs.</td>\n",
       "      <td>[They create their own datasets from online text., To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, the database contains 7 million query-reply pairs for retrieval, For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e</th>\n",
       "      <td>The datasets used are the PC-GITA corpus, which contains utterances from 50 PD patients and 50 HC, Colombian Spanish native speakers, and the Czech data from BIBREF9.</td>\n",
       "      <td>Spanish, German, and Czech</td>\n",
       "      <td>Spanish, German, and Czech datasets.</td>\n",
       "      <td>[PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, 100 native Czech speakers (50 PD, 50 HC) , the PC-GITA corpus BIBREF5, BIBREF17, BIBREF18]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                            zeroshot_pred_answer  \\\n",
       "quids                                                                                                                                                                                                              \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                                                                                                  The Craigslist Bargaining dataset (CB)   \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                                                              The datasets used are the Wall Street Journal articles from sections 2-20, 21-22, and 0-1.   \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8                                                                  The datasets used are 1,606,741 query-reply pairs, 100k triples for validation, and 6,741 for testing.   \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e  The datasets used are the PC-GITA corpus, which contains utterances from 50 PD patients and 50 HC, Colombian Spanish native speakers, and the Czech data from BIBREF9.   \n",
       "\n",
       "                                                                                                            fewshot_pred_answer  \\\n",
       "quids                                                                                                                             \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                     Craigslist Bargaining dataset (CB)   \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                         PDTB 2.0 and Chatbot NLU Corpus with STT error   \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8  A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs   \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                             Spanish, German, and Czech   \n",
       "\n",
       "                                                                                                               cot_predictions  \\\n",
       "quids                                                                                                                            \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                    Craigslist Bargaining dataset (CB)   \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                 The PDTB 2.0 corpus and two state-of-the-art systems.   \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8  Two datasets comprising 1,606,741 query-reply pairs and human-human utterance pairs.   \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                  Spanish, German, and Czech datasets.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                              gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                                                                                                                                                                                                                                                                                     [Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB) ]  \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                                                                                                                                                                                                                                                                                                                                                                                  [PDTB 2.0, PDTB 2.0 , PDTB 2.0 ]  \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8  [They create their own datasets from online text., To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, the database contains 7 million query-reply pairs for retrieval, For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs]  \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                                                                                                                                                                                                                                       [PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, 100 native Czech speakers (50 PD, 50 HC) , the PC-GITA corpus BIBREF5, BIBREF17, BIBREF18]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What are the baseline models?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zeroshot_pred_answer</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>cot_predictions</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77bbe1698e001c5889217be3164982ea36e85752</th>\n",
       "      <td>Baseline-BiLSTM-CNN, Cross-BiLSTM-CNN, Att-BiLSTM-CNN</td>\n",
       "      <td>BiLSTM-CNN proposed by BIBREF1</td>\n",
       "      <td>BiLSTM-CNNs that stack multiple layers of LSTM cells per direction and use a CNN to compute character-level word vectors alongside pre-trained word vectors</td>\n",
       "      <td>[BiLSTM-CNN, BiLSTM-CNN proposed by BIBREF1, Baseline-BiLSTM-CNN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fe6181ab0aecf5bc8c3def843f82e530347d918b</th>\n",
       "      <td>Baseline and Baseline$+(t)$, where $t \\in [0,1]$</td>\n",
       "      <td>MLE model and Baseline$+(t)$ with $t \\in [0,1]$</td>\n",
       "      <td>The MLE model trained on the Conceptual Captions training split alone (Baseline) and the model that merges positively-rated captions from the Caption-Quality training split with the Conceptual Captions examples and finetunes the baseline model (Baseline$+(t)$)</td>\n",
       "      <td>[ MLE model, Baseline$+(t)$, MLE model]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8e113fd9661bc8af97e30c75a20712f01fc4520a</th>\n",
       "      <td>The baseline models are ELMo, USE, NBSVM, FastText, XLnet, BERT, and RoBERTa.</td>\n",
       "      <td>ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model</td>\n",
       "      <td>ELMo, USE, NBSVM, FastText, XLnet, BERT, and RoBERTa</td>\n",
       "      <td>[ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased), BERT base uncased (BERT-Uncased), RoBERTa base model, ELMo, USE , NBSVM , FastText , XLnet base cased model (XLnet, BERT base cased (BERT-Cased) , BERT base uncased (BERT-Uncased), RoBERTa ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23cbf6ab365c1eb760b565d8ba51fb3f06257d62</th>\n",
       "      <td>Seq2Seq models were either used as-is BIBREF15 or combined with reinforcement learning thanks to a specific simplification reward BIBREF10, augmented with an external simplification database as a dynamic memory BIBREF16 or trained with multi-tasking on entailment and paraphrase generation BIBREF17.</td>\n",
       "      <td>Seq2Seq MT models</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS, BIBREF12, BIBREF33, BIBREF9, BIBREF10, BIBREF17, BIBREF15, BIBREF35, BIBREF16]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                 zeroshot_pred_answer  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                   \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                                                                                                                                        Baseline-BiLSTM-CNN, Cross-BiLSTM-CNN, Att-BiLSTM-CNN   \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                                                                                                                                                                             Baseline and Baseline$+(t)$, where $t \\in [0,1]$   \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a                                                                                                                                                                                                                                The baseline models are ELMo, USE, NBSVM, FastText, XLnet, BERT, and RoBERTa.   \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62  Seq2Seq models were either used as-is BIBREF15 or combined with reinforcement learning thanks to a specific simplification reward BIBREF10, augmented with an external simplification database as a dynamic memory BIBREF16 or trained with multi-tasking on entailment and paraphrase generation BIBREF17.   \n",
       "\n",
       "                                                                                                                                                                                   fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                                                                    \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                BiLSTM-CNN proposed by BIBREF1   \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                               MLE model and Baseline$+(t)$ with $t \\in [0,1]$   \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a  ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model   \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                                                             Seq2Seq MT models   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                               cot_predictions  \\\n",
       "quids                                                                                                                                                                                                                                                                                                            \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                           BiLSTM-CNNs that stack multiple layers of LSTM cells per direction and use a CNN to compute character-level word vectors alongside pre-trained word vectors   \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b  The MLE model trained on the Conceptual Captions training split alone (Baseline) and the model that merges positively-rated captions from the Caption-Quality training split with the Conceptual Captions examples and finetunes the baseline model (Baseline$+(t)$)   \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a                                                                                                                                                                                                                  ELMo, USE, NBSVM, FastText, XLnet, BERT, and RoBERTa   \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                                                                                                                                                                          Unanswerable   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                       gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                                                \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                                                                                                          [BiLSTM-CNN, BiLSTM-CNN proposed by BIBREF1, Baseline-BiLSTM-CNN]  \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                                                                                                                                                                    [ MLE model, Baseline$+(t)$, MLE model]  \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a  [ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased), BERT base uncased (BERT-Uncased), RoBERTa base model, ELMo, USE , NBSVM , FastText , XLnet base cased model (XLnet, BERT base cased (BERT-Cased) , BERT base uncased (BERT-Uncased), RoBERTa ]  \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                             [PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS, BIBREF12, BIBREF33, BIBREF9, BIBREF10, BIBREF17, BIBREF15, BIBREF35, BIBREF16]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  How long is the dataset?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zeroshot_pred_answer</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>cot_predictions</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e801b6a6048175d3b1f3440852386adb220bcb36</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>12,284 English-language tweets and 6100 Arabic-language tweets</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0a736e0e3305a50d771dfc059c7d94b8bd27032e</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000 for each TV series</td>\n",
       "      <td>15,000 reviews</td>\n",
       "      <td>[Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series., Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576a3ed6e4faa4c3893db632e97a52ac6e864aac</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>5,124 sentences</td>\n",
       "      <td>5,624 sentences</td>\n",
       "      <td>[5124,  5,124 sentences (97,681 tokens)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16b816925567deb734049416c149747118e13963</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences, Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10edfb9428b8a4652274c13962917662fdf84f8a</th>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>808, 265, and 264 commits</td>\n",
       "      <td>1493 commits</td>\n",
       "      <td>[almost doubles the number of commits in the training split to 1493, validation, and test splits containing 808, 265, and 264 commits, 2022]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ead7704a64447dccd504951618d3be463eba86bf</th>\n",
       "      <td>125383</td>\n",
       "      <td>125383 death certificates</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[125383, 125383 death certificates]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         zeroshot_pred_answer  \\\n",
       "quids                                                           \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36         Unanswerable   \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                 5000   \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac         Unanswerable   \n",
       "16b816925567deb734049416c149747118e13963         Unanswerable   \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a         Unanswerable   \n",
       "ead7704a64447dccd504951618d3be463eba86bf               125383   \n",
       "\n",
       "                                                                                     fewshot_pred_answer  \\\n",
       "quids                                                                                                      \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36  12,284 English-language tweets and 6100 Arabic-language tweets   \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                         5000 for each TV series   \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                 5,124 sentences   \n",
       "16b816925567deb734049416c149747118e13963                                                    Unanswerable   \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                       808, 265, and 264 commits   \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                       125383 death certificates   \n",
       "\n",
       "                                          cot_predictions  \\\n",
       "quids                                                       \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36     Unanswerable   \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e   15,000 reviews   \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac  5,624 sentences   \n",
       "16b816925567deb734049416c149747118e13963     Unanswerable   \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a     1493 commits   \n",
       "ead7704a64447dccd504951618d3be463eba86bf     Unanswerable   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                     gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                                              \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36                                                                                                                                                                                                                                                             [Unanswerable, Unanswerable]  \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                                                                                                                                                                  [Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series., Unanswerable]  \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                                                                                                                                                                                                                 [5124,  5,124 sentences (97,681 tokens)]  \n",
       "16b816925567deb734049416c149747118e13963  [SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences, Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively.]  \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                                                                                                                             [almost doubles the number of commits in the training split to 1493, validation, and test splits containing 808, 265, and 264 commits, 2022]  \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                                                                                                                                                                                                                                      [125383, 125383 death certificates]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "count = 0\n",
    "for question, q_ids in general_test_questions.items():\n",
    "    zeroshot_pred_answers = []\n",
    "    fewshot_pred_answers = []\n",
    "    cot_pred_answers = []\n",
    "    golden_answers = []\n",
    "    for q_id in q_ids:\n",
    "        # print(\"QID: \", q_id)\n",
    "        # print(\"Answer: \", predictions_dict[q_id][\"predicted_answer\"])\n",
    "        # print(\"Golden Answer: \", predictions_dict[q_id][\"golden_answers\"])\n",
    "        # print(\"Predicted evidence: \", predictions_dict[q_id][\"predicted_evidence\"])\n",
    "        zeroshot_pred_answers.append(zeroshot_predictions_dict[q_id][\"predicted_answer\"])\n",
    "        fewshot_pred_answers.append(fewshot_predictions_dict[q_id][\"predicted_answer\"])\n",
    "        cot_pred_answers.append(cot_predictions_dict[q_id][\"predicted_answer\"])\n",
    "        golden_answers.append(fewshot_predictions_dict[q_id][\"golden_answers\"])\n",
    "\n",
    "    d = {'zeroshot_pred_answer': zeroshot_pred_answers, \n",
    "         'fewshot_pred_answer': fewshot_pred_answers, \n",
    "         'cot_predictions': cot_pred_answers, \n",
    "         'gold_answers': golden_answers,\n",
    "         'quids': q_ids}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    if len(df) > 3:\n",
    "        print(\"Question: \", question)\n",
    "        count += 1\n",
    "        # df.set_index('quids')\n",
    "        display(df.set_index('quids'))\n",
    "        # display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  what was the baseline?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fewshot_evidence</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4137a82d7752be7a6c142ceb48ce784fd475fb06</th>\n",
       "      <td>In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings.</td>\n",
       "      <td>Weka baseline</td>\n",
       "      <td>[Weka baseline BIBREF5, Weka baseline BIBREF5, Weka,  Weka baseline BIBREF5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4137a82d7752be7a6c142ceb48ce784fd475fb06</th>\n",
       "      <td>In this paper we introduced an intra-sentence attention RNN for the of emotion intensity, which we developed for the WASSA-2017 Shared Task on Emotion Intensity. Our model does not make use of external information except for pre-trained embeddings and is able to outperform the Weka baseline for the development set, but not in the test set. In the shared task, it obtained the 13th place among 22 competitors.</td>\n",
       "      <td>Weka baseline</td>\n",
       "      <td>[Weka baseline BIBREF5, Weka baseline BIBREF5, Weka,  Weka baseline BIBREF5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4137a82d7752be7a6c142ceb48ce784fd475fb06</th>\n",
       "      <td>While analyzing the emotional content in text, mosts tasks are almost always framed as classification tasks, where the intention is to identify one emotion among many for a sentence or passage. However, it is often useful for applications to know the degree to which an emotion is expressed in text. To this end, the WASSA-2017 Shared Task on Emotion Intensity BIBREF0 represents the first task where systems have to automatically determine the intensity of emotions in tweets. Concretely, the objective is to given a tweet containing the emotion of joy, sadness, fear or anger, determine the intensity or degree of the emotion felt by the speaker as a real-valued score between zero and one.</td>\n",
       "      <td>Weka baseline</td>\n",
       "      <td>[Weka baseline BIBREF5, Weka baseline BIBREF5, Weka,  Weka baseline BIBREF5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4137a82d7752be7a6c142ceb48ce784fd475fb06</th>\n",
       "      <td>To avoid over-fitting, we used dropout regularization, experimenting with keep probabilities of INLINEFORM0 and INLINEFORM1 . We also added a weighed L2 regularization term to our loss function. We experimented with different values for weight INLINEFORM2 , with a minimum value of 0.01 and a maximum of 0.2.</td>\n",
       "      <td>Weka baseline</td>\n",
       "      <td>[Weka baseline BIBREF5, Weka baseline BIBREF5, Weka,  Weka baseline BIBREF5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4137a82d7752be7a6c142ceb48ce784fd475fb06</th>\n",
       "      <td>Table TABREF3 summarizes the average, maximum and minimum sentence lengths for each dataset after we processed them with Twokenizer. We can see the four corpora offer similar characteristics in terms of length, with a cross dataset maximum length of 41 tokens. We also see there is an important vocabulary gap between the dataset and GloVe, with an average coverage of only 64.3 %. To tackle this issue, we used a set of binary features derived from POS tags to capture some of the semantics of the words that are not covered by the GloVe embeddings. We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular expressions. Word elongation is very common in tweets, and is usually associated to strong sentiment. The following are the POS tag-derived rules we used to generate our binary features.</td>\n",
       "      <td>Weka baseline</td>\n",
       "      <td>[Weka baseline BIBREF5, Weka baseline BIBREF5, Weka,  Weka baseline BIBREF5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7fba61426737394304e307cdc7537225f6253150</th>\n",
       "      <td>In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7fba61426737394304e307cdc7537225f6253150</th>\n",
       "      <td>In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7fba61426737394304e307cdc7537225f6253150</th>\n",
       "      <td>Automatically predicting the level of English of non-native speakers from their written text is an interesting text mining task. Systems that perform well in the task can be useful components for online, second-language learning platforms as well as for organisations that tutor students for this purpose. In this paper I present the system balikasg that achieved the state-of-the-art performance in the CAp 2018 data science challenge among 14 systems. In order to achieve the best performance in the challenge, I decided to use a variety of features that describe an essay's readability and syntactic complexity as well as its content. For the prediction step, I found Gradient Boosted Trees, whose efficiency is proven in several data science challenges, to be the most efficient across a variety of classifiers.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7fba61426737394304e307cdc7537225f6253150</th>\n",
       "      <td>where INLINEFORM0 is a cost matrix that uses prior knowledge to calculate the misclassification errors and INLINEFORM1 is the number of observations of class INLINEFORM2 classified with category INLINEFORM3 . The cost matrix INLINEFORM4 is given in Table TABREF3 . Notice that, as expected, moving away from the diagonal (correct classification) the misclassification costs are higher. The biggest error (44) occurs when a INLINEFORM5 essay is classified as INLINEFORM6 . On the contrary, the classification error is lower (6) when the opposite happens and an INLINEFORM7 essay is classified as INLINEFORM8 . Since INLINEFORM9 is not symmetric and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7fba61426737394304e307cdc7537225f6253150</th>\n",
       "      <td>I would like to thank the organisers of the challenge and NVidia for sponsoring the prize of the challenge. The views expressed in this paper belong solely to the author, and not necessarily to the author's employer.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521</th>\n",
       "      <td>A sentiment analysis can be made at the level of (1) the whole document, (2) the individual sentences, or (what is currently seen as the most attractive approach) (3) at the level of individual fragments of text. Regarding document level analysis BIBREF8 , BIBREF9 - the task at this level is to classify whether a full opinion expresses a positive, negative or neutral attitude. For example, given a product review, the model determines whether the text shows an overall positive, negative or neutral opinion about the product. The biggest disadvantage of document level analysis is an assumption that each document expresses views on a single entity. Thus, it is not applicable to documents which evaluate or compare multiple objects. As for sentence level analysis BIBREF10 - The task at this level relates to sentences and determines whether each sentence expressed a positive, negative, or neutral opinion. This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). As a result, we can aggregate the opinions. For example, the phone display gathers positive feedback, but the battery is often rated negatively. The aspect-based level of analysis is much more complex since it requires more advanced knowledge representation than at the level of entire documents only. Also, the documents often consist of multiple sentences, so saying that the document is positive provides only partial information. In the literature, there exists some initial work related to aspects. There exist initial solutions that use SVM-based algorithms BIBREF12 or conditional random field classifiers BIBREF13 with manually engineered features. There also exist some solutions based on deep neural networks, such as connecting sentiments with the corresponding aspects based on the constituency parse tree BIBREF11 .</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521</th>\n",
       "      <td>Modern society is an information society bombarded from all sides by an increasing number of different pieces of information. The 21st century has brought us the rapid development of media, especially in the internet ecosystem. This change has caused the transfer of many areas of our lives to virtual reality. New forms of communication have been established. Their development has created the need for analysis of related data. Nowadays, unstructured information is available in digital form, but how can we analyse and summarise billions of newly created texts that appear daily on the internet? Natural language analysis techniques, statistics and machine learning have emerged as tools to help us. In recent years, particular attention has focused on sentiment analysis. This area is defined as the study of opinions expressed by people as well as attitudes and emotions about a particular topic, product, event, or person. Sentiment analysis determines the polarisation of the text. It answers the question as to whether a particular text is a positive, negative, or neutral one.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521</th>\n",
       "      <td>We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521</th>\n",
       "      <td>Rhetorical analysis seeks to uncover the coherence structure underneath the text, which has been shown to be beneficial for many Natural Language Processing (NLP) applications including text summarization and compression BIBREF1 , machine translation evaluation BIBREF2 , sentiment analysis BIBREF3 , and others. Different formal theories of discourse analysis have been proposed. Martin BIBREF4 proposed discourse relations based on discourse connectives (e.g., because, but) expressed in the text. Danlos BIBREF5 extended sentence grammar and formalize discourse structure. Rhetorical Structure Theory or RST - used in our experiments - was proposed by Mann and Thompson BIBREF6 . The method proposed by them is perhaps the most influential theory of discourse in computational linguistics. Moreover, it was initially intended to be used in text generation tasks, but it became popular for parsing the structure of a text BIBREF7 . Rhetorical Structure Theory represents texts by hierarchical structures with labels. This is a tree structure, which comprises Discourse Trees (DTs). Presented at Figure FIGREF3 this Discourse Tree is a representation of the following text:</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521</th>\n",
       "      <td>The goal of discourse analysis in our method is the segmentation of the text for the basic units of discourse structures EDU (Elementary Discourse Units) and connecting them to determine semantic relations. The analysis is performed separately for each source document, and as the output we get Discourse Trees (DT) such as in Figure FIGREF3 . At this stage, existing discourse parsers will model the structure and the labels of a DT separately. They do not take into account the sequential dependencies between the DT constituents. Then existing discourse parsers will apply greedy and sub-optimal parsing algorithms and build a Discourse Tree. During this stage, and to cope with the mentioned limitation The inferred (posterior) probabilities can be used from CRF parsing models in a probabilistic CKY-like bottom-up parsing algorithm BIBREF14 which is non-greedy and optimal. Finally, discourse parsers do not discriminate between intra-sentential parsing (i.e., building the DTs for individual sentences) and multi-sentential parsing (i.e., building a DT for the whole document) BIBREF0 . Hence, this part of the analysis will extract for us distributed information about the relationship between different EDUs from parsed texts. Then we assign sentiment orientation to each EDU.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b</th>\n",
       "      <td>INLINEFORM0 .</td>\n",
       "      <td>Hybrid – the current state-of-the-art</td>\n",
       "      <td>[Dress, Dress-Ls, Pbmt-R, Hybrid,  Sbmt-Sari, Dress,  Dress-Ls, Pbmt-R, Hybrid, Sbmt-Sari]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b</th>\n",
       "      <td>We implemented two attention-based Seq2seq models, namely: (1) LstmLstm: the encoder is implemented by two LSTM layers; (2) NseLstm: the encoder is implemented by NSE. The decoder in both cases is implemented by two LSTM layers. The computations for a single model are run on an NVIDIA Titan-X GPU. For all experiments, our models have 300-dimensional hidden states and 300-dimensional word embeddings. Parameters were initialized from a uniform distribution [-0.1, 0.1). We used the same hyperparameters across all datasets. Word embeddings were initialized either randomly or with Glove vectors BIBREF24 pre-trained on Common Crawl data (840B tokens), and fine-tuned during training. We used a vocabulary size of 20K for Newsela, and 30K for WikiSmall and WikiLarge. Our models were trained with a maximum number of 40 epochs using Adam optimizer BIBREF25 with step size INLINEFORM0 for LstmLstm, and INLINEFORM1 for NseLstm, the exponential decay rates INLINEFORM2 . The batch size is set to 32. We used dropout BIBREF26 for regularization with a dropout rate of 0.3. For beam search, we experimented with beam sizes of 5 and 10. Following BIBREF27 , we replaced each out-of-vocabulary token INLINEFORM3 with the source word INLINEFORM4 with the highest alignment score INLINEFORM5 , i.e., INLINEFORM6 .</td>\n",
       "      <td>Hybrid – the current state-of-the-art</td>\n",
       "      <td>[Dress, Dress-Ls, Pbmt-R, Hybrid,  Sbmt-Sari, Dress,  Dress-Ls, Pbmt-R, Hybrid, Sbmt-Sari]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b</th>\n",
       "      <td>INLINEFORM0</td>\n",
       "      <td>Hybrid – the current state-of-the-art</td>\n",
       "      <td>[Dress, Dress-Ls, Pbmt-R, Hybrid,  Sbmt-Sari, Dress,  Dress-Ls, Pbmt-R, Hybrid, Sbmt-Sari]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b</th>\n",
       "      <td>On WikiSmall, Hybrid – the current state-of-the-art – achieved best BLEU (53.94) and SARI (30.46) scores. Among neural models, NseLstm-B yielded the highest BLEU score (53.42), while NseLstm-S performed best on SARI (29.75). On WikiLarge, again, NseLstm-B had the highest BLEU score of 92.02. Sbmt-Sari – that was trained on a huge corpus of 106M sentence pairs and 2B words – scored highest on SARI with 39.96, followed by Dress-Ls (37.27), Dress (37.08), and NseLstm-S (36.88).</td>\n",
       "      <td>Hybrid – the current state-of-the-art</td>\n",
       "      <td>[Dress, Dress-Ls, Pbmt-R, Hybrid,  Sbmt-Sari, Dress,  Dress-Ls, Pbmt-R, Hybrid, Sbmt-Sari]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b</th>\n",
       "      <td>We compared our models, either tuned with BLEU (-B) or SARI (-S), against systems reported in BIBREF15 , namely Dress, a deep reinforcement learning model, Dress-Ls, a combination of Dress and a lexical simplification model BIBREF15 , Pbmt-R, a PBMT model with dissimilarity-based re-ranking BIBREF9 , Hybrid, a hybrid semantic-based model that combines a simplification model and a monolingual MT model BIBREF29 , and Sbmt-Sari, a SBMT model with simplification-specific components. BIBREF12 .</td>\n",
       "      <td>Hybrid – the current state-of-the-art</td>\n",
       "      <td>[Dress, Dress-Ls, Pbmt-R, Hybrid,  Sbmt-Sari, Dress,  Dress-Ls, Pbmt-R, Hybrid, Sbmt-Sari]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6bce04570d4745dcfaca5cba64075242308b65cf</th>\n",
       "      <td>The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.</td>\n",
       "      <td>BiGRU+CRF</td>\n",
       "      <td>[BiGRU+CRF, BiGRU+CRF]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6bce04570d4745dcfaca5cba64075242308b65cf</th>\n",
       "      <td>There are different kinds of structures of BERT models. We chose the BERT-base model structure. BERT-base's architecture is a multi-layer bidirectional TransformerBIBREF18. The number of layers is $L=12$, the hidden size is $H=768$, and the number of self-attention heads is $A=12$BIBREF7.</td>\n",
       "      <td>BiGRU+CRF</td>\n",
       "      <td>[BiGRU+CRF, BiGRU+CRF]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6bce04570d4745dcfaca5cba64075242308b65cf</th>\n",
       "      <td>First of all, it is shown that the deeper the layer, the better the performance. All pre-training models have 12 Transformer layers, except ERNIE2.0-tiny. Although Ernie2.0-tiny increases the number of hidden units and improves the pre-training task with continual pre-training, 3 Transformer layers can not extract semantic knowledge well. The F1 value of ERNIE-2.0-tiny is even lower than the baseline model.</td>\n",
       "      <td>BiGRU+CRF</td>\n",
       "      <td>[BiGRU+CRF, BiGRU+CRF]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6bce04570d4745dcfaca5cba64075242308b65cf</th>\n",
       "      <td>Named entity recognition (NER) is the basic task of the NLP, such as information extraction and data mining. The main goal of the NER is to extract entities (persons, places, organizations and so on) from unstructured documents. Researchers have used rule-based and dictionary-based methods for the NERBIBREF1. Because these methods have poor generalization properties, researchers have proposed machine learning methods, such as Hidden Markov Model (HMM) and Conditional Random Field (CRF)BIBREF2BIBREF10. But machine learning methods require a lot of artificial features and can not avoid costly feature engineering. In recent years, deep learning, which is driven by artificial intelligence and cognitive computing, has been widely used in multiple NLP fields. Huang $et$ $al$. BIBREF3 proposed a model that combine the Bidirectional Long Short-Term Memory (BiLSTM) with the CRF. It can use both forward and backward input features to improve the performance of the NER task. Ma and Hovy BIBREF11 used a combination of the Convolutional Neural Networks (CNN) and the LSTM-CRF to recognize entities. Chiu and Nichols BIBREF12 improved the BiLSTM-CNN model and tested it on the CoNLL-2003 corpus.</td>\n",
       "      <td>BiGRU+CRF</td>\n",
       "      <td>[BiGRU+CRF, BiGRU+CRF]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6bce04570d4745dcfaca5cba64075242308b65cf</th>\n",
       "      <td>ERNIE has the same model structure as BERT-base, which uses 12 Transformer encoder layers, 768 hidden units and 12 attention heads.</td>\n",
       "      <td>BiGRU+CRF</td>\n",
       "      <td>[BiGRU+CRF, BiGRU+CRF]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e9a0a69eacd554141f56b60ab2d1912cc33f526a</th>\n",
       "      <td>The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances. Then, similar evaluation techniques are applied to the baseline system and the results are presented in Table TABREF29 .</td>\n",
       "      <td>random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively</td>\n",
       "      <td>[Random labeling,  randomly assigning any of the sentiment values to each of the test instances]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e9a0a69eacd554141f56b60ab2d1912cc33f526a</th>\n",
       "      <td>The baseline systems achieved better scores compared to CEN@AMRIT and SVNIT teams for HI-EN dataset; and AMRITA_CEN, CEN@Amrita and SVNIT teams for BN-EN dataset. IIIT-NBP team has achieved the maximum macro average f-score of 0.569 across all the sentiment classes for HI-EN dataset. IIIT-NBP also achieved the maximum macro average f-score of 0.526 for BN-EN dataset. Two way classification of HI-EN dataset achieved the maximum macro average f-score of 0.707, 0.666, and 0.663 for positive, negative, and neutral, respectively. Similarly, the two way classification of BN-EN dataset achieved the maximum average f-score of 0.641, 0.677, and 0.621 for positive, negative, and neutral, respectively. Again, the f-measure achieved using HI-EN dataset is better than BN-EN. The obvious reason for such result is that there are more instances in HI-EN than BN-EN dataset.</td>\n",
       "      <td>random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively</td>\n",
       "      <td>[Random labeling,  randomly assigning any of the sentiment values to each of the test instances]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e9a0a69eacd554141f56b60ab2d1912cc33f526a</th>\n",
       "      <td>This paper presents the details of shared task held during the ICON 2017. The competition presents the sentiment identification task from HI-EN and BN-EN code-mixed datasets. A random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively. The best performing team obtained maximum macro average f-score of 0.569 and 0.526 for HI-EN and BN-EN datasets, respectively. The team used word and character level n-grams as features and SVM for sentiment classification. We plan to enhance the current dataset and include more data pairs in the next version of the shared task. In future, more advanced task like aspect based sentiment analysis and stance detection can be performed on code-mixed dataset.</td>\n",
       "      <td>random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively</td>\n",
       "      <td>[Random labeling,  randomly assigning any of the sentiment values to each of the test instances]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e9a0a69eacd554141f56b60ab2d1912cc33f526a</th>\n",
       "      <td>The paper is organized as following manner. Section SECREF2 describes the NLP in Indian languages mainly related to code-mixing and sentiment analysis. The detailed statistics of the dataset and evaluation are described in Section SECREF3 . The baseline systems and participant's system description are described in Section SECREF4 . Finally, conclusion and future research are drawn in Section SECREF5 .</td>\n",
       "      <td>random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively</td>\n",
       "      <td>[Random labeling,  randomly assigning any of the sentiment values to each of the test instances]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e9a0a69eacd554141f56b60ab2d1912cc33f526a</th>\n",
       "      <td>With the rise of social media and user-generated data, information extraction from user-generated text became an important research area. Social media has become the voice of many people over decades and it has special relations with real time events. The multilingual user have tendency to mix two or more languages while expressing their opinion in social media, this phenomenon leads to generate a new code-mixed language. So far, many studies have been conducted on why the code-mixing phenomena occurs and can be found in Kim kim2006reasons. Several experiments have been performed on social media texts including code-mixed data. The first step toward information gathering from these texts is to identify the languages present. Till date, several language identification experiments or tasks have been performed on several code-mixed language pairs such as Spanish-English BIBREF5 , BIBREF6 , French-English BIBREF7 , Hindi-English BIBREF0 , BIBREF1 , Hindi-English-Bengali BIBREF8 , Bengali-English BIBREF1 . Many shared tasks have also been organized for language identification of code-mixed texts. Language Identification in Code-Switched Data was one of the shared tasks which covered four language pairs such as Spanish-English, Modern Standard Arabic and Arabic dialects, Chinese-English, and Nepalese-English. In the case of Indian languages, Mixed Script Information Retrieval BIBREF9 shared task at FIRE-2015 was organized for eight code-mixed Indian languages such as Bangla, Gujarati, Hindi, Kannada, Malayalam, Marathi, Tamil, and Telugu mixed with English.</td>\n",
       "      <td>random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively</td>\n",
       "      <td>[Random labeling,  randomly assigning any of the sentiment values to each of the test instances]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ec54ae2f4811196fcaafa45e76130239e69995f9</th>\n",
       "      <td>We use logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library BIBREF28 as the baseline classification models. As a baseline RNN, the LSTM network from the Keras library was applied BIBREF29. Both LSTM and MCD LSTM networks consist of an embedding layer, LSTM layer, and a fully connected layer within the Word2Vec and ELMo embeddings. The embedding layer was not used in TF-IDF and Universal Sentence encoding.</td>\n",
       "      <td>logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library</td>\n",
       "      <td>[logistic regression (LR), Support Vector Machines (SVM), LSTM network from the Keras library ,  logistic regression (LR), Support Vector Machines (SVM)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ec54ae2f4811196fcaafa45e76130239e69995f9</th>\n",
       "      <td>Hate speech represents written or oral communication that in any way discredits a person or a group based on characteristics such as race, color, ethnicity, gender, sexual orientation, nationality, or religion BIBREF0. Hate speech targets disadvantaged social groups and harms them both directly and indirectly BIBREF1. Social networks like Twitter and Facebook, where hate speech frequently occurs, receive many critics for not doing enough to deal with it. As the connection between hate speech and the actual hate crimes is high BIBREF2, the importance of detecting and managing hate speech is not questionable. Early identification of users who promote such kind of communication can prevent an escalation from speech to action. However, automatic hate speech detection is difficult, especially when the text does not contain explicit hate speech keywords. Lexical detection methods tend to have low precision because, during classification, they do not take into account the contextual information those messages carry BIBREF3. Recently, contextual word and sentence embedding methods capture semantic and syntactic relation among the words and improve prediction accuracy.</td>\n",
       "      <td>logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library</td>\n",
       "      <td>[logistic regression (LR), Support Vector Machines (SVM), LSTM network from the Keras library ,  logistic regression (LR), Support Vector Machines (SVM)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ec54ae2f4811196fcaafa45e76130239e69995f9</th>\n",
       "      <td>Recent works on combining probabilistic Bayesian inference and neural network methodology attracted much attention in the scientific community BIBREF4. The main reason is the ability of probabilistic neural networks to quantify trustworthiness of predicted results. This information can be important, especially in tasks were decision making plays an important role BIBREF5. The areas which can significantly benefit from prediction uncertainty estimation are text classification tasks which trigger specific actions. Hate speech detection is an example of a task where reliable results are needed to remove harmful contents and possibly ban malicious users without preventing the freedom of speech. In order to assess the uncertainty of the predicted values, the neural networks require a Bayesian framework. On the other hand, Srivastava et al. BIBREF6 proposed a regularization approach, called dropout, which has a considerable impact on the generalization ability of neural networks. The approach drops some randomly selected nodes from the neural network during the training process. Dropout increases the robustness of networks and prevents overfitting. Different variants of dropout improved classification results in various areas BIBREF7. Gal and Ghahramani BIBREF8 exploited the interpretation of dropout as a Bayesian approximation and proposed a Monte Carlo dropout (MCD) approach to estimate the prediction uncertainty. In this paper, we analyze the applicability of Monte Carlo dropout in assessing the predictive uncertainty.</td>\n",
       "      <td>logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library</td>\n",
       "      <td>[logistic regression (LR), Support Vector Machines (SVM), LSTM network from the Keras library ,  logistic regression (LR), Support Vector Machines (SVM)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ec54ae2f4811196fcaafa45e76130239e69995f9</th>\n",
       "      <td>Recurrent neural networks (RNNs) are a popular choice in text mining. The dropout technique was first introduced to RNNs in 2013 BIBREF14 but further research revealed negative impact of dropout in RNNs, especially within language modeling. For example, the dropout in RNNs employed on a handwriting recognition task, disrupted the ability of recurrent layers to effectively model sequences BIBREF15. The dropout was successfully applied to language modeling by BIBREF16 who applied it only on fully connected layers. The then state-of-the-art results were explained with the fact that by using the dropout, much deeper neural networks can be constructed without danger of overfitting. Gal and Ghahramani BIBREF17 implemented the variational inference based dropout which can also regularize recurrent layers. Additionally, they provide a solution for dropout within word embeddings. The method mimics Bayesian inference by combining probabilistic parameter interpretation and deep RNNs. Authors introduce the idea of augmenting probabilistic RNN models with the prediction uncertainty estimation. Recent works further investigate how to estimate prediction uncertainty within different data frameworks using RNNs BIBREF18. Some of the first investigation of probabilistic properties of SVM prediction is described in the work of Platt BIBREF19. Also, investigation how Bayes by Backprop (BBB) method can be applied to RNNs was done by BIBREF20.</td>\n",
       "      <td>logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library</td>\n",
       "      <td>[logistic regression (LR), Support Vector Machines (SVM), LSTM network from the Keras library ,  logistic regression (LR), Support Vector Machines (SVM)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ec54ae2f4811196fcaafa45e76130239e69995f9</th>\n",
       "      <td>Deep learning received significant attention in both NLP and other machine learning applications. However, standard deep neural networks do not provide information on reliability of predictions. Bayesian neural network (BNN) methodology can overcome this issue by probabilistic interpretation of model parameters. Apart from prediction uncertainty estimation, BNNs offer robustness to overfitting and can be efficiently trained on small data sets BIBREF26. However, neural networks that apply Bayesian inference can be computationally expensive, especially the ones with the complex, deep architectures. Our work is based on Monte Carlo Dropout (MCD) method proposed by BIBREF8. The idea of this approach is to capture prediction uncertainty using the dropout as a regularization technique.</td>\n",
       "      <td>logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library</td>\n",
       "      <td>[logistic regression (LR), Support Vector Machines (SVM), LSTM network from the Keras library ,  logistic regression (LR), Support Vector Machines (SVM)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       fewshot_evidence  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In this section we report the results of the experiments we performed to test our proposed model. In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin. Moreover, the model manages to do so without any additional resources, except pre-trained word embeddings. These results are, however, reversed for the test dataset, where our model performs worse than the baseline. This shows that the model is not able to generalize well, which we think is related to the missing semantic information due to the vocabulary gap we observed between the datasets and the GloVe embeddings.   \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In this paper we introduced an intra-sentence attention RNN for the of emotion intensity, which we developed for the WASSA-2017 Shared Task on Emotion Intensity. Our model does not make use of external information except for pre-trained embeddings and is able to outperform the Weka baseline for the development set, but not in the test set. In the shared task, it obtained the 13th place among 22 competitors.   \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           While analyzing the emotional content in text, mosts tasks are almost always framed as classification tasks, where the intention is to identify one emotion among many for a sentence or passage. However, it is often useful for applications to know the degree to which an emotion is expressed in text. To this end, the WASSA-2017 Shared Task on Emotion Intensity BIBREF0 represents the first task where systems have to automatically determine the intensity of emotions in tweets. Concretely, the objective is to given a tweet containing the emotion of joy, sadness, fear or anger, determine the intensity or degree of the emotion felt by the speaker as a real-valued score between zero and one.   \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           To avoid over-fitting, we used dropout regularization, experimenting with keep probabilities of INLINEFORM0 and INLINEFORM1 . We also added a weighed L2 regularization term to our loss function. We experimented with different values for weight INLINEFORM2 , with a minimum value of 0.01 and a maximum of 0.2.   \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Table TABREF3 summarizes the average, maximum and minimum sentence lengths for each dataset after we processed them with Twokenizer. We can see the four corpora offer similar characteristics in terms of length, with a cross dataset maximum length of 41 tokens. We also see there is an important vocabulary gap between the dataset and GloVe, with an average coverage of only 64.3 %. To tackle this issue, we used a set of binary features derived from POS tags to capture some of the semantics of the words that are not covered by the GloVe embeddings. We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular expressions. Word elongation is very common in tweets, and is usually associated to strong sentiment. The following are the POS tag-derived rules we used to generate our binary features.   \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics.   \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0    \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Automatically predicting the level of English of non-native speakers from their written text is an interesting text mining task. Systems that perform well in the task can be useful components for online, second-language learning platforms as well as for organisations that tutor students for this purpose. In this paper I present the system balikasg that achieved the state-of-the-art performance in the CAp 2018 data science challenge among 14 systems. In order to achieve the best performance in the challenge, I decided to use a variety of features that describe an essay's readability and syntactic complexity as well as its content. For the prediction step, I found Gradient Boosted Trees, whose efficiency is proven in several data science challenges, to be the most efficient across a variety of classifiers.   \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        where INLINEFORM0 is a cost matrix that uses prior knowledge to calculate the misclassification errors and INLINEFORM1 is the number of observations of class INLINEFORM2 classified with category INLINEFORM3 . The cost matrix INLINEFORM4 is given in Table TABREF3 . Notice that, as expected, moving away from the diagonal (correct classification) the misclassification costs are higher. The biggest error (44) occurs when a INLINEFORM5 essay is classified as INLINEFORM6 . On the contrary, the classification error is lower (6) when the opposite happens and an INLINEFORM7 essay is classified as INLINEFORM8 . Since INLINEFORM9 is not symmetric and the costs of the lower diagonal are higher, the penalties for misclassification are worse when essays of upper languages levels (e.g., INLINEFORM10 ) are classified as essays of lower levels.   \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       I would like to thank the organisers of the challenge and NVidia for sponsoring the prize of the challenge. The views expressed in this paper belong solely to the author, and not necessarily to the author's employer.   \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521  A sentiment analysis can be made at the level of (1) the whole document, (2) the individual sentences, or (what is currently seen as the most attractive approach) (3) at the level of individual fragments of text. Regarding document level analysis BIBREF8 , BIBREF9 - the task at this level is to classify whether a full opinion expresses a positive, negative or neutral attitude. For example, given a product review, the model determines whether the text shows an overall positive, negative or neutral opinion about the product. The biggest disadvantage of document level analysis is an assumption that each document expresses views on a single entity. Thus, it is not applicable to documents which evaluate or compare multiple objects. As for sentence level analysis BIBREF10 - The task at this level relates to sentences and determines whether each sentence expressed a positive, negative, or neutral opinion. This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). As a result, we can aggregate the opinions. For example, the phone display gathers positive feedback, but the battery is often rated negatively. The aspect-based level of analysis is much more complex since it requires more advanced knowledge representation than at the level of entire documents only. Also, the documents often consist of multiple sentences, so saying that the document is positive provides only partial information. In the literature, there exists some initial work related to aspects. There exist initial solutions that use SVM-based algorithms BIBREF12 or conditional random field classifiers BIBREF13 with manually engineered features. There also exist some solutions based on deep neural networks, such as connecting sentiments with the corresponding aspects based on the constituency parse tree BIBREF11 .   \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Modern society is an information society bombarded from all sides by an increasing number of different pieces of information. The 21st century has brought us the rapid development of media, especially in the internet ecosystem. This change has caused the transfer of many areas of our lives to virtual reality. New forms of communication have been established. Their development has created the need for analysis of related data. Nowadays, unstructured information is available in digital form, but how can we analyse and summarise billions of newly created texts that appear daily on the internet? Natural language analysis techniques, statistics and machine learning have emerged as tools to help us. In recent years, particular attention has focused on sentiment analysis. This area is defined as the study of opinions expressed by people as well as attitudes and emotions about a particular topic, product, event, or person. Sentiment analysis determines the polarisation of the text. It answers the question as to whether a particular text is a positive, negative, or neutral one.   \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.   \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Rhetorical analysis seeks to uncover the coherence structure underneath the text, which has been shown to be beneficial for many Natural Language Processing (NLP) applications including text summarization and compression BIBREF1 , machine translation evaluation BIBREF2 , sentiment analysis BIBREF3 , and others. Different formal theories of discourse analysis have been proposed. Martin BIBREF4 proposed discourse relations based on discourse connectives (e.g., because, but) expressed in the text. Danlos BIBREF5 extended sentence grammar and formalize discourse structure. Rhetorical Structure Theory or RST - used in our experiments - was proposed by Mann and Thompson BIBREF6 . The method proposed by them is perhaps the most influential theory of discourse in computational linguistics. Moreover, it was initially intended to be used in text generation tasks, but it became popular for parsing the structure of a text BIBREF7 . Rhetorical Structure Theory represents texts by hierarchical structures with labels. This is a tree structure, which comprises Discourse Trees (DTs). Presented at Figure FIGREF3 this Discourse Tree is a representation of the following text:   \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The goal of discourse analysis in our method is the segmentation of the text for the basic units of discourse structures EDU (Elementary Discourse Units) and connecting them to determine semantic relations. The analysis is performed separately for each source document, and as the output we get Discourse Trees (DT) such as in Figure FIGREF3 . At this stage, existing discourse parsers will model the structure and the labels of a DT separately. They do not take into account the sequential dependencies between the DT constituents. Then existing discourse parsers will apply greedy and sub-optimal parsing algorithms and build a Discourse Tree. During this stage, and to cope with the mentioned limitation The inferred (posterior) probabilities can be used from CRF parsing models in a probabilistic CKY-like bottom-up parsing algorithm BIBREF14 which is non-greedy and optimal. Finally, discourse parsers do not discriminate between intra-sentential parsing (i.e., building the DTs for individual sentences) and multi-sentential parsing (i.e., building a DT for the whole document) BIBREF0 . Hence, this part of the analysis will extract for us distributed information about the relationship between different EDUs from parsed texts. Then we assign sentiment orientation to each EDU.   \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  INLINEFORM0 .   \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We implemented two attention-based Seq2seq models, namely: (1) LstmLstm: the encoder is implemented by two LSTM layers; (2) NseLstm: the encoder is implemented by NSE. The decoder in both cases is implemented by two LSTM layers. The computations for a single model are run on an NVIDIA Titan-X GPU. For all experiments, our models have 300-dimensional hidden states and 300-dimensional word embeddings. Parameters were initialized from a uniform distribution [-0.1, 0.1). We used the same hyperparameters across all datasets. Word embeddings were initialized either randomly or with Glove vectors BIBREF24 pre-trained on Common Crawl data (840B tokens), and fine-tuned during training. We used a vocabulary size of 20K for Newsela, and 30K for WikiSmall and WikiLarge. Our models were trained with a maximum number of 40 epochs using Adam optimizer BIBREF25 with step size INLINEFORM0 for LstmLstm, and INLINEFORM1 for NseLstm, the exponential decay rates INLINEFORM2 . The batch size is set to 32. We used dropout BIBREF26 for regularization with a dropout rate of 0.3. For beam search, we experimented with beam sizes of 5 and 10. Following BIBREF27 , we replaced each out-of-vocabulary token INLINEFORM3 with the source word INLINEFORM4 with the highest alignment score INLINEFORM5 , i.e., INLINEFORM6 .   \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   INLINEFORM0    \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                On WikiSmall, Hybrid – the current state-of-the-art – achieved best BLEU (53.94) and SARI (30.46) scores. Among neural models, NseLstm-B yielded the highest BLEU score (53.42), while NseLstm-S performed best on SARI (29.75). On WikiLarge, again, NseLstm-B had the highest BLEU score of 92.02. Sbmt-Sari – that was trained on a huge corpus of 106M sentence pairs and 2B words – scored highest on SARI with 39.96, followed by Dress-Ls (37.27), Dress (37.08), and NseLstm-S (36.88).   \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We compared our models, either tuned with BLEU (-B) or SARI (-S), against systems reported in BIBREF15 , namely Dress, a deep reinforcement learning model, Dress-Ls, a combination of Dress and a lexical simplification model BIBREF15 , Pbmt-R, a PBMT model with dissimilarity-based re-ranking BIBREF9 , Hybrid, a hybrid semantic-based model that combines a simplification model and a monolingual MT model BIBREF29 , and Sbmt-Sari, a SBMT model with simplification-specific components. BIBREF12 .   \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.   \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              There are different kinds of structures of BERT models. We chose the BERT-base model structure. BERT-base's architecture is a multi-layer bidirectional TransformerBIBREF18. The number of layers is $L=12$, the hidden size is $H=768$, and the number of self-attention heads is $A=12$BIBREF7.   \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     First of all, it is shown that the deeper the layer, the better the performance. All pre-training models have 12 Transformer layers, except ERNIE2.0-tiny. Although Ernie2.0-tiny increases the number of hidden units and improves the pre-training task with continual pre-training, 3 Transformer layers can not extract semantic knowledge well. The F1 value of ERNIE-2.0-tiny is even lower than the baseline model.   \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Named entity recognition (NER) is the basic task of the NLP, such as information extraction and data mining. The main goal of the NER is to extract entities (persons, places, organizations and so on) from unstructured documents. Researchers have used rule-based and dictionary-based methods for the NERBIBREF1. Because these methods have poor generalization properties, researchers have proposed machine learning methods, such as Hidden Markov Model (HMM) and Conditional Random Field (CRF)BIBREF2BIBREF10. But machine learning methods require a lot of artificial features and can not avoid costly feature engineering. In recent years, deep learning, which is driven by artificial intelligence and cognitive computing, has been widely used in multiple NLP fields. Huang $et$ $al$. BIBREF3 proposed a model that combine the Bidirectional Long Short-Term Memory (BiLSTM) with the CRF. It can use both forward and backward input features to improve the performance of the NER task. Ma and Hovy BIBREF11 used a combination of the Convolutional Neural Networks (CNN) and the LSTM-CRF to recognize entities. Chiu and Nichols BIBREF12 improved the BiLSTM-CNN model and tested it on the CoNLL-2003 corpus.   \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ERNIE has the same model structure as BERT-base, which uses 12 Transformer encoder layers, 768 hidden units and 12 attention heads.   \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances. Then, similar evaluation techniques are applied to the baseline system and the results are presented in Table TABREF29 .   \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The baseline systems achieved better scores compared to CEN@AMRIT and SVNIT teams for HI-EN dataset; and AMRITA_CEN, CEN@Amrita and SVNIT teams for BN-EN dataset. IIIT-NBP team has achieved the maximum macro average f-score of 0.569 across all the sentiment classes for HI-EN dataset. IIIT-NBP also achieved the maximum macro average f-score of 0.526 for BN-EN dataset. Two way classification of HI-EN dataset achieved the maximum macro average f-score of 0.707, 0.666, and 0.663 for positive, negative, and neutral, respectively. Similarly, the two way classification of BN-EN dataset achieved the maximum average f-score of 0.641, 0.677, and 0.621 for positive, negative, and neutral, respectively. Again, the f-measure achieved using HI-EN dataset is better than BN-EN. The obvious reason for such result is that there are more instances in HI-EN than BN-EN dataset.   \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               This paper presents the details of shared task held during the ICON 2017. The competition presents the sentiment identification task from HI-EN and BN-EN code-mixed datasets. A random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively. The best performing team obtained maximum macro average f-score of 0.569 and 0.526 for HI-EN and BN-EN datasets, respectively. The team used word and character level n-grams as features and SVM for sentiment classification. We plan to enhance the current dataset and include more data pairs in the next version of the shared task. In future, more advanced task like aspect based sentiment analysis and stance detection can be performed on code-mixed dataset.   \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The paper is organized as following manner. Section SECREF2 describes the NLP in Indian languages mainly related to code-mixing and sentiment analysis. The detailed statistics of the dataset and evaluation are described in Section SECREF3 . The baseline systems and participant's system description are described in Section SECREF4 . Finally, conclusion and future research are drawn in Section SECREF5 .   \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      With the rise of social media and user-generated data, information extraction from user-generated text became an important research area. Social media has become the voice of many people over decades and it has special relations with real time events. The multilingual user have tendency to mix two or more languages while expressing their opinion in social media, this phenomenon leads to generate a new code-mixed language. So far, many studies have been conducted on why the code-mixing phenomena occurs and can be found in Kim kim2006reasons. Several experiments have been performed on social media texts including code-mixed data. The first step toward information gathering from these texts is to identify the languages present. Till date, several language identification experiments or tasks have been performed on several code-mixed language pairs such as Spanish-English BIBREF5 , BIBREF6 , French-English BIBREF7 , Hindi-English BIBREF0 , BIBREF1 , Hindi-English-Bengali BIBREF8 , Bengali-English BIBREF1 . Many shared tasks have also been organized for language identification of code-mixed texts. Language Identification in Code-Switched Data was one of the shared tasks which covered four language pairs such as Spanish-English, Modern Standard Arabic and Arabic dialects, Chinese-English, and Nepalese-English. In the case of Indian languages, Mixed Script Information Retrieval BIBREF9 shared task at FIRE-2015 was organized for eight code-mixed Indian languages such as Bangla, Gujarati, Hindi, Kannada, Malayalam, Marathi, Tamil, and Telugu mixed with English.   \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We use logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library BIBREF28 as the baseline classification models. As a baseline RNN, the LSTM network from the Keras library was applied BIBREF29. Both LSTM and MCD LSTM networks consist of an embedding layer, LSTM layer, and a fully connected layer within the Word2Vec and ELMo embeddings. The embedding layer was not used in TF-IDF and Universal Sentence encoding.   \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Hate speech represents written or oral communication that in any way discredits a person or a group based on characteristics such as race, color, ethnicity, gender, sexual orientation, nationality, or religion BIBREF0. Hate speech targets disadvantaged social groups and harms them both directly and indirectly BIBREF1. Social networks like Twitter and Facebook, where hate speech frequently occurs, receive many critics for not doing enough to deal with it. As the connection between hate speech and the actual hate crimes is high BIBREF2, the importance of detecting and managing hate speech is not questionable. Early identification of users who promote such kind of communication can prevent an escalation from speech to action. However, automatic hate speech detection is difficult, especially when the text does not contain explicit hate speech keywords. Lexical detection methods tend to have low precision because, during classification, they do not take into account the contextual information those messages carry BIBREF3. Recently, contextual word and sentence embedding methods capture semantic and syntactic relation among the words and improve prediction accuracy.   \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Recent works on combining probabilistic Bayesian inference and neural network methodology attracted much attention in the scientific community BIBREF4. The main reason is the ability of probabilistic neural networks to quantify trustworthiness of predicted results. This information can be important, especially in tasks were decision making plays an important role BIBREF5. The areas which can significantly benefit from prediction uncertainty estimation are text classification tasks which trigger specific actions. Hate speech detection is an example of a task where reliable results are needed to remove harmful contents and possibly ban malicious users without preventing the freedom of speech. In order to assess the uncertainty of the predicted values, the neural networks require a Bayesian framework. On the other hand, Srivastava et al. BIBREF6 proposed a regularization approach, called dropout, which has a considerable impact on the generalization ability of neural networks. The approach drops some randomly selected nodes from the neural network during the training process. Dropout increases the robustness of networks and prevents overfitting. Different variants of dropout improved classification results in various areas BIBREF7. Gal and Ghahramani BIBREF8 exploited the interpretation of dropout as a Bayesian approximation and proposed a Monte Carlo dropout (MCD) approach to estimate the prediction uncertainty. In this paper, we analyze the applicability of Monte Carlo dropout in assessing the predictive uncertainty.   \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Recurrent neural networks (RNNs) are a popular choice in text mining. The dropout technique was first introduced to RNNs in 2013 BIBREF14 but further research revealed negative impact of dropout in RNNs, especially within language modeling. For example, the dropout in RNNs employed on a handwriting recognition task, disrupted the ability of recurrent layers to effectively model sequences BIBREF15. The dropout was successfully applied to language modeling by BIBREF16 who applied it only on fully connected layers. The then state-of-the-art results were explained with the fact that by using the dropout, much deeper neural networks can be constructed without danger of overfitting. Gal and Ghahramani BIBREF17 implemented the variational inference based dropout which can also regularize recurrent layers. Additionally, they provide a solution for dropout within word embeddings. The method mimics Bayesian inference by combining probabilistic parameter interpretation and deep RNNs. Authors introduce the idea of augmenting probabilistic RNN models with the prediction uncertainty estimation. Recent works further investigate how to estimate prediction uncertainty within different data frameworks using RNNs BIBREF18. Some of the first investigation of probabilistic properties of SVM prediction is described in the work of Platt BIBREF19. Also, investigation how Bayes by Backprop (BBB) method can be applied to RNNs was done by BIBREF20.   \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Deep learning received significant attention in both NLP and other machine learning applications. However, standard deep neural networks do not provide information on reliability of predictions. Bayesian neural network (BNN) methodology can overcome this issue by probabilistic interpretation of model parameters. Apart from prediction uncertainty estimation, BNNs offer robustness to overfitting and can be efficiently trained on small data sets BIBREF26. However, neural networks that apply Bayesian inference can be computationally expensive, especially the ones with the complex, deep architectures. Our work is based on Monte Carlo Dropout (MCD) method proposed by BIBREF8. The idea of this approach is to capture prediction uncertainty using the dropout as a regularization technique.   \n",
       "\n",
       "                                                                                                                                          fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                           \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                                                        Weka baseline   \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                                                        Weka baseline   \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                                                        Weka baseline   \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                                                        Weka baseline   \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                                                        Weka baseline   \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                         Unanswerable   \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                         Unanswerable   \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                         Unanswerable   \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                         Unanswerable   \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                         Unanswerable   \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                         Unanswerable   \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                         Unanswerable   \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                         Unanswerable   \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                         Unanswerable   \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                         Unanswerable   \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                                Hybrid – the current state-of-the-art   \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                                Hybrid – the current state-of-the-art   \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                                Hybrid – the current state-of-the-art   \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                                Hybrid – the current state-of-the-art   \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                                Hybrid – the current state-of-the-art   \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                            BiGRU+CRF   \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                            BiGRU+CRF   \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                            BiGRU+CRF   \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                            BiGRU+CRF   \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                            BiGRU+CRF   \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a  random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively   \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a  random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively   \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a  random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively   \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a  random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively   \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a  random baseline system obtained macro average f-score of 0.331 and 0.339 for HI-EN and BN-EN datasets, respectively   \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9                             logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library   \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9                             logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library   \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9                             logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library   \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9                             logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library   \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9                             logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library   \n",
       "\n",
       "                                                                                                                                                                                       gold_answers  \n",
       "quids                                                                                                                                                                                                \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                               [Weka baseline BIBREF5, Weka baseline BIBREF5, Weka,  Weka baseline BIBREF5]  \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                               [Weka baseline BIBREF5, Weka baseline BIBREF5, Weka,  Weka baseline BIBREF5]  \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                               [Weka baseline BIBREF5, Weka baseline BIBREF5, Weka,  Weka baseline BIBREF5]  \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                               [Weka baseline BIBREF5, Weka baseline BIBREF5, Weka,  Weka baseline BIBREF5]  \n",
       "4137a82d7752be7a6c142ceb48ce784fd475fb06                                                                               [Weka baseline BIBREF5, Weka baseline BIBREF5, Weka,  Weka baseline BIBREF5]  \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                                 [Unanswerable, Unanswerable, Unanswerable]  \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                                 [Unanswerable, Unanswerable, Unanswerable]  \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                                 [Unanswerable, Unanswerable, Unanswerable]  \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                                 [Unanswerable, Unanswerable, Unanswerable]  \n",
       "7fba61426737394304e307cdc7537225f6253150                                                                                                                 [Unanswerable, Unanswerable, Unanswerable]  \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                                 [Unanswerable, Unanswerable, Unanswerable]  \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                                 [Unanswerable, Unanswerable, Unanswerable]  \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                                 [Unanswerable, Unanswerable, Unanswerable]  \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                                 [Unanswerable, Unanswerable, Unanswerable]  \n",
       "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521                                                                                                                 [Unanswerable, Unanswerable, Unanswerable]  \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                 [Dress, Dress-Ls, Pbmt-R, Hybrid,  Sbmt-Sari, Dress,  Dress-Ls, Pbmt-R, Hybrid, Sbmt-Sari]  \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                 [Dress, Dress-Ls, Pbmt-R, Hybrid,  Sbmt-Sari, Dress,  Dress-Ls, Pbmt-R, Hybrid, Sbmt-Sari]  \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                 [Dress, Dress-Ls, Pbmt-R, Hybrid,  Sbmt-Sari, Dress,  Dress-Ls, Pbmt-R, Hybrid, Sbmt-Sari]  \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                 [Dress, Dress-Ls, Pbmt-R, Hybrid,  Sbmt-Sari, Dress,  Dress-Ls, Pbmt-R, Hybrid, Sbmt-Sari]  \n",
       "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b                                                                 [Dress, Dress-Ls, Pbmt-R, Hybrid,  Sbmt-Sari, Dress,  Dress-Ls, Pbmt-R, Hybrid, Sbmt-Sari]  \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                                                     [BiGRU+CRF, BiGRU+CRF]  \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                                                     [BiGRU+CRF, BiGRU+CRF]  \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                                                     [BiGRU+CRF, BiGRU+CRF]  \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                                                     [BiGRU+CRF, BiGRU+CRF]  \n",
       "6bce04570d4745dcfaca5cba64075242308b65cf                                                                                                                                     [BiGRU+CRF, BiGRU+CRF]  \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a                                                           [Random labeling,  randomly assigning any of the sentiment values to each of the test instances]  \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a                                                           [Random labeling,  randomly assigning any of the sentiment values to each of the test instances]  \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a                                                           [Random labeling,  randomly assigning any of the sentiment values to each of the test instances]  \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a                                                           [Random labeling,  randomly assigning any of the sentiment values to each of the test instances]  \n",
       "e9a0a69eacd554141f56b60ab2d1912cc33f526a                                                           [Random labeling,  randomly assigning any of the sentiment values to each of the test instances]  \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9  [logistic regression (LR), Support Vector Machines (SVM), LSTM network from the Keras library ,  logistic regression (LR), Support Vector Machines (SVM)]  \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9  [logistic regression (LR), Support Vector Machines (SVM), LSTM network from the Keras library ,  logistic regression (LR), Support Vector Machines (SVM)]  \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9  [logistic regression (LR), Support Vector Machines (SVM), LSTM network from the Keras library ,  logistic regression (LR), Support Vector Machines (SVM)]  \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9  [logistic regression (LR), Support Vector Machines (SVM), LSTM network from the Keras library ,  logistic regression (LR), Support Vector Machines (SVM)]  \n",
       "ec54ae2f4811196fcaafa45e76130239e69995f9  [logistic regression (LR), Support Vector Machines (SVM), LSTM network from the Keras library ,  logistic regression (LR), Support Vector Machines (SVM)]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  what dataset was used?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fewshot_evidence</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0ac6fbd81e2dd95b800283dc7e59ce969d45fc02</th>\n",
       "      <td>To test our model, we experiment using the training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger. These were annotated using Best-Worst Scaling (BWS) to obtain very reliable scores BIBREF6 .</td>\n",
       "      <td>The training, validation and test datasets provided for the shared task BIBREF5</td>\n",
       "      <td>[ training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger, datasets provided for the shared task BIBREF5, Dataset of tweets provided for the shared task., Dataset from shared task BIBREF5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0ac6fbd81e2dd95b800283dc7e59ce969d45fc02</th>\n",
       "      <td>Finally, on the sadness datasets again we experimentally observed that using embeddings of 50 offered the best results, with a statistically significant average gain of 0.092 correlation points INLINEFORM0 over size 25. Results were statistically equivalent for size 100. We also observed that using 50 or 100 hidden units for the RNN offered statistically equivalent results, while both of these offered better performance than when using a hidden size of 200.</td>\n",
       "      <td>The training, validation and test datasets provided for the shared task BIBREF5</td>\n",
       "      <td>[ training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger, datasets provided for the shared task BIBREF5, Dataset of tweets provided for the shared task., Dataset from shared task BIBREF5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0ac6fbd81e2dd95b800283dc7e59ce969d45fc02</th>\n",
       "      <td>We experimented with GloVe BIBREF7 as pre-trained word embedding vectors, for sizes 25, 50 and 100. These are vectors trained on a dataset of 2B tweets, with a total vocabulary of 1.2 M. To pre-process the data, we used Twokenizer BIBREF8 , which basically provides a set of curated rules to split the tweets into tokens. We also use Tweeboparser BIBREF9 to get the POS-tags for each tweet.</td>\n",
       "      <td>The training, validation and test datasets provided for the shared task BIBREF5</td>\n",
       "      <td>[ training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger, datasets provided for the shared task BIBREF5, Dataset of tweets provided for the shared task., Dataset from shared task BIBREF5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0ac6fbd81e2dd95b800283dc7e59ce969d45fc02</th>\n",
       "      <td>On the fear dataset, again we observed that embeddings of size 50 provided the best results, offering average gains of 0.12 ( INLINEFORM0 ) and 0.11 ( INLINEFORM1 ) for sizes 25 and 100, respectively. When it comes to the size of the RNN hidden state, our experiments showed that using 100 hidden units offered the best results, with average absolute gains of 0.117 ( INLINEFORM2 ) and 0.108 ( INLINEFORM3 ) over sizes 50 and 200.</td>\n",
       "      <td>The training, validation and test datasets provided for the shared task BIBREF5</td>\n",
       "      <td>[ training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger, datasets provided for the shared task BIBREF5, Dataset of tweets provided for the shared task., Dataset from shared task BIBREF5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0ac6fbd81e2dd95b800283dc7e59ce969d45fc02</th>\n",
       "      <td>Table TABREF3 summarizes the average, maximum and minimum sentence lengths for each dataset after we processed them with Twokenizer. We can see the four corpora offer similar characteristics in terms of length, with a cross dataset maximum length of 41 tokens. We also see there is an important vocabulary gap between the dataset and GloVe, with an average coverage of only 64.3 %. To tackle this issue, we used a set of binary features derived from POS tags to capture some of the semantics of the words that are not covered by the GloVe embeddings. We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular expressions. Word elongation is very common in tweets, and is usually associated to strong sentiment. The following are the POS tag-derived rules we used to generate our binary features.</td>\n",
       "      <td>The training, validation and test datasets provided for the shared task BIBREF5</td>\n",
       "      <td>[ training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger, datasets provided for the shared task BIBREF5, Dataset of tweets provided for the shared task., Dataset from shared task BIBREF5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d36a6447bfe58204e0d29f9213d84be04d875624</th>\n",
       "      <td>For data set A, we used data from first 8 subjects for training the model, remaining two subjects data for validation and test set respectively.</td>\n",
       "      <td>Data set A and Data set B</td>\n",
       "      <td>[ two types of simultaneous speech EEG recording databases , The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment., Speech EEG recording collected from male and female subjects under different background noises, For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d36a6447bfe58204e0d29f9213d84be04d875624</th>\n",
       "      <td>For data set B, we used data from first 6 subjects for training the model, remaining two subjects data for validation and test set respectively.</td>\n",
       "      <td>Data set A and Data set B</td>\n",
       "      <td>[ two types of simultaneous speech EEG recording databases , The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment., Speech EEG recording collected from male and female subjects under different background noises, For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d36a6447bfe58204e0d29f9213d84be04d875624</th>\n",
       "      <td>For data set B, the 8 subjects were asked to repeat the same previous experiment but this time we used background music played from our lab computer to generate a background noise of 65 dB. Here we had 24 speech EEG recording examples for each sentence.</td>\n",
       "      <td>Data set A and Data set B</td>\n",
       "      <td>[ two types of simultaneous speech EEG recording databases , The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment., Speech EEG recording collected from male and female subjects under different background noises, For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d36a6447bfe58204e0d29f9213d84be04d875624</th>\n",
       "      <td>For data set A, the 10 subjects were asked to speak the first 30 sentences from the USC-TIMIT database BIBREF16 and their simultaneous speech and EEG signals were recorded. This data was recorded in presence of background noise of 40 dB (noise generated by room air conditioner fan). We then asked each subject to repeat the same experiment two more times, thus we had 30 speech EEG recording examples for each sentence.</td>\n",
       "      <td>Data set A and Data set B</td>\n",
       "      <td>[ two types of simultaneous speech EEG recording databases , The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment., Speech EEG recording collected from male and female subjects under different background noises, For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d36a6447bfe58204e0d29f9213d84be04d875624</th>\n",
       "      <td>We further plan to publish our speech EEG data base used in this work to help advancement of research in this area.</td>\n",
       "      <td>Data set A and Data set B</td>\n",
       "      <td>[ two types of simultaneous speech EEG recording databases , The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment., Speech EEG recording collected from male and female subjects under different background noises, For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237b6fcc64b43901415f3ded17cc210a54ab698</th>\n",
       "      <td>To optimize Reader, we take a surrogate approach to heuristically generate a set of salient sentences from a document collection, which constitute a training dataset for learning the probabilities of salient sentences INLINEFORM0 parametrized by INLINEFORM1 . More specifically, given a training set INLINEFORM2 of documents (e.g., body-text of research papers) and their associated summaries (e.g., abstracts) INLINEFORM3 , where INLINEFORM4 is a gold summary of document INLINEFORM5 , we employ a state-of-the-art sentence similarity model, DSSM BIBREF9 , BIBREF10 , to find the set of top- INLINEFORM6 sentences INLINEFORM8 in INLINEFORM9 , such that the similarity between INLINEFORM10 and any sentence in the gold summary INLINEFORM11 is above a pre-defined threshold. Note that here we assume each training document is associated with a gold summary composed of sentences that might not come from INLINEFORM12 . We make this assumption only for the sake of generating the set of salient sentences INLINEFORM13 which is usually not readily available.</td>\n",
       "      <td>113 GB</td>\n",
       "      <td>[669 academic papers published by IEEE, 850 academic papers, 669 academic papers published by IEEE, For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237b6fcc64b43901415f3ded17cc210a54ab698</th>\n",
       "      <td>Table TABREF15 presents P@10, MAP and MRR results of our KeyVec model and competing embedding methods in academic paper retrieval. word2vec averaging generates an embedding for a document by averaging the word2vec vectors of its constituent words. In the experiment, we used two different versions of word2vec: one from public release, and the other one trained specifically on our own academic corpus (113 GB). From Table TABREF15 , we observe that as a document-embedding model, Paragraph Vector gave better retrieval results than word2vec averagings did. In contrast, our KeyVec outperforms all the competitors given its unique capability of capturing and embedding the key information of documents.</td>\n",
       "      <td>113 GB</td>\n",
       "      <td>[669 academic papers published by IEEE, 850 academic papers, 669 academic papers published by IEEE, For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237b6fcc64b43901415f3ded17cc210a54ab698</th>\n",
       "      <td>In recent years, the use of word representations, such as word2vec BIBREF0 , BIBREF1 and GloVe BIBREF2 , has become a key “secret sauce” for the success of many natural language processing (NLP), information retrieval (IR) and machine learning (ML) tasks. The empirical success of word embeddings raises an interesting research question: Beyond words, can we learn fixed-length distributed representations for pieces of texts? The texts can be of variable-length, ranging from paragraphs to documents. Such document representations play a vital role in a large number of downstream NLP/IR/ML applications, such as text clustering, sentiment analysis, and document retrieval, which treat each piece of text as an instance. Learning a good representation that captures the semantics of each document is thus essential for the success of such applications.</td>\n",
       "      <td>113 GB</td>\n",
       "      <td>[669 academic papers published by IEEE, 850 academic papers, 669 academic papers published by IEEE, For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237b6fcc64b43901415f3ded17cc210a54ab698</th>\n",
       "      <td>In another recent work BIBREF5 , Djuric et al. introduced a Hierarchical Document Vector (HDV) model to learn representations from a document stream. Our KeyVec differs from HDV in that we do not assume the existence of a document stream and HDV does not model sentences.</td>\n",
       "      <td>113 GB</td>\n",
       "      <td>[669 academic papers published by IEEE, 850 academic papers, 669 academic papers published by IEEE, For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237b6fcc64b43901415f3ded17cc210a54ab698</th>\n",
       "      <td>Specifically, for the INLINEFORM0 -th sentence INLINEFORM1 with INLINEFORM2 words, Neural Reader maps each word INLINEFORM3 into a word embedding INLINEFORM4 . Pre-trained word embeddings like word2vec or GloVe may be used to initialize the embedding table. In our experiments, we use domain-specific word embeddings trained by word2vec on our corpus.</td>\n",
       "      <td>113 GB</td>\n",
       "      <td>[669 academic papers published by IEEE, 850 academic papers, 669 academic papers published by IEEE, For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748a50c96acb1aa03f7efd1b43376c193b2450a</th>\n",
       "      <td>We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.</td>\n",
       "      <td>Bing Liu's dataset</td>\n",
       "      <td>[SNAP Amazon Dataset , Bing Liu's dataset, Bing Liu's dataset, SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748a50c96acb1aa03f7efd1b43376c193b2450a</th>\n",
       "      <td>For the Rhetorical Parsing part of our experiment, we used a special library implemented for such purposes BIBREF18 . As a sentiment analysis model, we used the Bag of Word vectorization method with a Logistic Regression classifier trained on 1.2 million (1, 3 and 5-star rating only) of Electronic reviews from SNAP Amazon Dataset BIBREF19 . The BoW vectorization method built a vocabulary that considers the top 50,000 terms only ordered by their frequency across the corpus, similarly to supervised learning examples presented in our previous works in BIBREF8 . We used a noun and noun phrases extractor according to part-of-speech tagger from the Spacy Python library. In order to create an Aspect-Rhetorical Relation Graph we used breadth-first search (BFS) algorithm for each Discourse Tree.</td>\n",
       "      <td>Bing Liu's dataset</td>\n",
       "      <td>[SNAP Amazon Dataset , Bing Liu's dataset, Bing Liu's dataset, SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748a50c96acb1aa03f7efd1b43376c193b2450a</th>\n",
       "      <td>We implemented our framework in Python. The first computational step was to load the dataset and parse it into individual documents. Next, each document was processed through the Discourse Parser BIBREF18 and transformed into a Discourse Tree (DT). Then we extracted Elementary Discourse Units (EDUs) from the DT and each EDU was processed through the Logistic Regression sentiment algorithm. All neutral EDUs were taken off from consideration to ensure that the discovered aspects are correlated with authors' emotions. The remaining EDUs were processed through part-of-speech tagger to extract nouns and noun phrases which we decided to treat as potential aspects. The result of this step was a set of Aspect-based Discourse Trees (ADTs). Then, from each ADT relations between aspects were extracted using breadth-first search, and an Aspect-Rhetorical Relation Graph (ARRG) was created by using aspects and relations such as nodes and edges respectively. Next, we evaluated the importance of aspects using a PageRank algorithm. Our approach resulted in complete list of aspects sorted by PageRank score. We applied a user-selected importance threshold to filter trivial aspects.</td>\n",
       "      <td>Bing Liu's dataset</td>\n",
       "      <td>[SNAP Amazon Dataset , Bing Liu's dataset, Bing Liu's dataset, SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748a50c96acb1aa03f7efd1b43376c193b2450a</th>\n",
       "      <td>In Table TABREF18 there are presented some examples of the results of our approach compared with the annotated data from Bing Liu's dataset. In the first sentence, the results of the analysis differ because we decided to treat only nouns or noun phrases as aspects, while annotators also accepted verbs. In some cases, such as sentences 2 or 4, our approach generated more valuable aspects than the annotators found, but in some cases, like sentence 5, we found fewer. This is possibly the result of our method of filtering valuable aspects - if some aspects were not frequent enough in the dataset, we can treat them as void. In cases where there is neither aspect nor sentiment in the dataset, such as sentence 6, we measure sentiment as well, as one of our analysis steps.</td>\n",
       "      <td>Bing Liu's dataset</td>\n",
       "      <td>[SNAP Amazon Dataset , Bing Liu's dataset, Bing Liu's dataset, SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748a50c96acb1aa03f7efd1b43376c193b2450a</th>\n",
       "      <td>Figure FIGREF19 shows the agreement between our aspects and that of the dataset. We assumed two aspects as equal when they were textually the same. We made some experiments using text distance metrics, such as the Jaro-Winkler distance, but the results did not differ significantly from an exact matching. We fitted the importance factor value (on the X axis) so as to enrich final aspects set: a higher factor resulted in a larger aspects set and a higher value of precision metric, with slowly decreasing recall. First results (blue line on charts) were not satisfactory, so we removed a sentiment filtering step of analysis (orange line on chart), which doubled the precision value, with nearly the same value of recall. The level of precision for whole dataset (computer, router, and speaker) was most of the time at the same level. However, the recall of router was significantly worse than speaker and computer sets.</td>\n",
       "      <td>Bing Liu's dataset</td>\n",
       "      <td>[SNAP Amazon Dataset , Bing Liu's dataset, Bing Liu's dataset, SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5fd112980d0dd7f7ce30e6273fe6e7b230b13225</th>\n",
       "      <td>Concerning robustness to noisy user generated content, BIBREF0 stress differences with traditional domain adaptation problems, and propose a typology of errors, many of which we also detected in the Foursquare data. They also released a dataset (MTNT), whose sources were selected from a social media (Reddit) on the basis of being especially noisy (see Appendix for a comparison with Foursquare). These sources were then translated by humans to produce a parallel corpus that can be used to engineer more robust NMT systems and to evaluate them. This corpus was the basis of the WMT 2019 Robustness Task BIBREF1, in which BIBREF2 ranked first. We use the same set of robustness and domain adaptation techniques, which we study more in depth and apply to our review translation task.</td>\n",
       "      <td>UGC (User Generated Content), Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet</td>\n",
       "      <td>[WMT 2014,  UGC (User Generated Content), 11.5k French reviews from Foursquare, WMT 2014, UGC (User Generated Content)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5fd112980d0dd7f7ce30e6273fe6e7b230b13225</th>\n",
       "      <td>During our work, we used BLEU BIBREF28 on newstest[2012, 2013] to ensure that our models stayed good on a more general domain, and on Foursquare-valid to measure performance on the Foursquare domain.</td>\n",
       "      <td>UGC (User Generated Content), Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet</td>\n",
       "      <td>[WMT 2014,  UGC (User Generated Content), 11.5k French reviews from Foursquare, WMT 2014, UGC (User Generated Content)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5fd112980d0dd7f7ce30e6273fe6e7b230b13225</th>\n",
       "      <td>Foursquare-HT was translated from scratch by the same translators who post-edited Foursquare-PE. While we did not use it in this work, it can be used as extra training or development data. We also release a human translation of the French-language test set (668 sentences) of the Aspect-Based Sentiment Analysis task at SemEval 2016 BIBREF14.</td>\n",
       "      <td>UGC (User Generated Content), Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet</td>\n",
       "      <td>[WMT 2014,  UGC (User Generated Content), 11.5k French reviews from Foursquare, WMT 2014, UGC (User Generated Content)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5fd112980d0dd7f7ce30e6273fe6e7b230b13225</th>\n",
       "      <td>This in-domain data is concatenated to the out-of-domain parallel data and used for training.</td>\n",
       "      <td>UGC (User Generated Content), Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet</td>\n",
       "      <td>[WMT 2014,  UGC (User Generated Content), 11.5k French reviews from Foursquare, WMT 2014, UGC (User Generated Content)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5fd112980d0dd7f7ce30e6273fe6e7b230b13225</th>\n",
       "      <td>After some initial work with the WMT 2014 data, we built a new training corpus named UGC (User Generated Content), closer to our domain, by combining: Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet (See Table TABREF31). UGC does not include Common Crawl (which contains many misaligned sentences and caused hallucinations), but it includes OpenSubtitles BIBREF24 (spoken-language, possibly closer to Foursquare). We observed an improvement of more than 1 BLEU on newstest2014 when switching to UGC, and almost 6 BLEU on Foursquare-valid.</td>\n",
       "      <td>UGC (User Generated Content), Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet</td>\n",
       "      <td>[WMT 2014,  UGC (User Generated Content), 11.5k French reviews from Foursquare, WMT 2014, UGC (User Generated Content)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1f7a040545c9501215d3391e267c7874f9a6004</th>\n",
       "      <td>In this work, we evaluate our NER approach using two news corpora. One corpus is a set of 227 texts published on December 31, 2010 by the Lusa agency (portuguese agency of news) and will be referred to as `News'. The other corpus (named here `Sports news') is a set of 881 sports news. The texts were manually annotated according to the enamex designation and the type `miscellaneous'.</td>\n",
       "      <td>News corpus (227 texts published on December 31, 2010 by the Lusa agency) and Sports news corpus (881 sports news)</td>\n",
       "      <td>[CoNLL_X bosque data, News data by Lusa agency, Sports news data, News, Sports news, News, Sports news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1f7a040545c9501215d3391e267c7874f9a6004</th>\n",
       "      <td>In this work we propose a novel effective method to extract named entities from unstructured text. The proposed PAMPO method is implemented using free software, namely R and available packages. Two manually annotated Portuguese news corpora were used to empirically evaluate the algorithm using the measures of INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . These corpora did not influence the definition of the algorithm or the construction of its pattern bases. We have compared PAMPO with three other NER extractors: AlchemyAPI, Rembrandt and Zemanta. Experimental results clearly show that PAMPO obtains significantly higher INLINEFORM3 and INLINEFORM4 than existing tools. The values of INLINEFORM5 are identical. We may say also that PAMPO's performance in the HAREM corpus was at least as good as the best one of the systems reported over there when we consider all categories of entities. However, when we exclude dates and numeric expressions, it presents better results than the ones reported for other tools.</td>\n",
       "      <td>News corpus (227 texts published on December 31, 2010 by the Lusa agency) and Sports news corpus (881 sports news)</td>\n",
       "      <td>[CoNLL_X bosque data, News data by Lusa agency, Sports news data, News, Sports news, News, Sports news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1f7a040545c9501215d3391e267c7874f9a6004</th>\n",
       "      <td>The authors would like to thank SAPO Labs (http://labs.sapo.pt) for providing the data set of news from Lusa agency. The authors would also like to thank grant #2014/08996-0 and grant #2013/14757-6, São Paulo Research Foundation (FAPESP). This work is partially funded by FCT/MEC through PIDDAC and ERDF/ON2 within project NORTE-07-0124-FEDER-000059 and through the COMPETE Programme (operational programme for competitiveness) and by National Funds through the FCT - Fundação para a Ciência e a Tecnologia (Portuguese Foundation for Science and Technology) within project FCOMP-01-0124-FEDER-037281.</td>\n",
       "      <td>News corpus (227 texts published on December 31, 2010 by the Lusa agency) and Sports news corpus (881 sports news)</td>\n",
       "      <td>[CoNLL_X bosque data, News data by Lusa agency, Sports news data, News, Sports news, News, Sports news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1f7a040545c9501215d3391e267c7874f9a6004</th>\n",
       "      <td>In 1991, Lisa F. Rau presented a paper describing an algorithm, based on heuristics and handcrafted rules, to automatically extract company names from financial news BIBREF7 . This was one of the first research papers on the NER field BIBREF8 . NER was first introduced as an information extraction task but since then its use in natural language text has spread widely through several fields, namely Information Retrieval, Question Answering, Machine Translation, Text Translation, Text Clustering and Navigation Systems BIBREF9 . In an attempt to suit the needs of each application, nowadays, a NER extraction workflow comprises not only analysing some input content and detecting named entities, but also assigning them a type and a list of URIs for disambiguation BIBREF10 . New approaches have been developed with the application of Supervised machine Learning (SL) techniques BIBREF6 and NER evolved to NERC — Named Entity Recognition and Classification. The handicap of those techniques is the requirement of a training set, i.e., a data set manually labelled. Therefore, the NER task depends also on the data set used to train the NER extraction algorithm.</td>\n",
       "      <td>News corpus (227 texts published on December 31, 2010 by the Lusa agency) and Sports news corpus (881 sports news)</td>\n",
       "      <td>[CoNLL_X bosque data, News data by Lusa agency, Sports news data, News, Sports news, News, Sports news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1f7a040545c9501215d3391e267c7874f9a6004</th>\n",
       "      <td>Nowadays, a large amount of information is produced and shared in unstructured form, mostly unstructured text BIBREF0 , BIBREF1 . This information can be exploited in decision making processes but, to be useful, it should be transformed and presented in ways that make its intrinsic knowledge more readily intelligible. For that, we need efficient methods and tools that quickly extract useful information from unstructured text collections. Such demand can be observed, for instance, in Biology, where researchers, in order to be abreast of all developments, need to analyse new biomedical literature on a daily basis BIBREF2 . Another application is on fraud and corruption studies where the network information — the set of actors and their relationships — is implicitly stored in unstructured natural-language documents BIBREF3 . Hence, text mining and information extraction are required to pre-process the texts in order to extract the entities and the relations between them.</td>\n",
       "      <td>News corpus (227 texts published on December 31, 2010 by the Lusa agency) and Sports news corpus (881 sports news)</td>\n",
       "      <td>[CoNLL_X bosque data, News data by Lusa agency, Sports news data, News, Sports news, News, Sports news]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       fewshot_evidence  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   To test our model, we experiment using the training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger. These were annotated using Best-Worst Scaling (BWS) to obtain very reliable scores BIBREF6 .   \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Finally, on the sadness datasets again we experimentally observed that using embeddings of 50 offered the best results, with a statistically significant average gain of 0.092 correlation points INLINEFORM0 over size 25. Results were statistically equivalent for size 100. We also observed that using 50 or 100 hidden units for the RNN offered statistically equivalent results, while both of these offered better performance than when using a hidden size of 200.   \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         We experimented with GloVe BIBREF7 as pre-trained word embedding vectors, for sizes 25, 50 and 100. These are vectors trained on a dataset of 2B tweets, with a total vocabulary of 1.2 M. To pre-process the data, we used Twokenizer BIBREF8 , which basically provides a set of curated rules to split the tweets into tokens. We also use Tweeboparser BIBREF9 to get the POS-tags for each tweet.   \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 On the fear dataset, again we observed that embeddings of size 50 provided the best results, offering average gains of 0.12 ( INLINEFORM0 ) and 0.11 ( INLINEFORM1 ) for sizes 25 and 100, respectively. When it comes to the size of the RNN hidden state, our experiments showed that using 100 hidden units offered the best results, with average absolute gains of 0.117 ( INLINEFORM2 ) and 0.108 ( INLINEFORM3 ) over sizes 50 and 200.   \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                                                                                                                                                                                                                                                                                                                  Table TABREF3 summarizes the average, maximum and minimum sentence lengths for each dataset after we processed them with Twokenizer. We can see the four corpora offer similar characteristics in terms of length, with a cross dataset maximum length of 41 tokens. We also see there is an important vocabulary gap between the dataset and GloVe, with an average coverage of only 64.3 %. To tackle this issue, we used a set of binary features derived from POS tags to capture some of the semantics of the words that are not covered by the GloVe embeddings. We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular expressions. Word elongation is very common in tweets, and is usually associated to strong sentiment. The following are the POS tag-derived rules we used to generate our binary features.   \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               For data set A, we used data from first 8 subjects for training the model, remaining two subjects data for validation and test set respectively.   \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               For data set B, we used data from first 6 subjects for training the model, remaining two subjects data for validation and test set respectively.   \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  For data set B, the 8 subjects were asked to repeat the same previous experiment but this time we used background music played from our lab computer to generate a background noise of 65 dB. Here we had 24 speech EEG recording examples for each sentence.   \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           For data set A, the 10 subjects were asked to speak the first 30 sentences from the USC-TIMIT database BIBREF16 and their simultaneous speech and EEG signals were recorded. This data was recorded in presence of background noise of 40 dB (noise generated by room air conditioner fan). We then asked each subject to repeat the same experiment two more times, thus we had 30 speech EEG recording examples for each sentence.   \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We further plan to publish our speech EEG data base used in this work to help advancement of research in this area.   \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                                                To optimize Reader, we take a surrogate approach to heuristically generate a set of salient sentences from a document collection, which constitute a training dataset for learning the probabilities of salient sentences INLINEFORM0 parametrized by INLINEFORM1 . More specifically, given a training set INLINEFORM2 of documents (e.g., body-text of research papers) and their associated summaries (e.g., abstracts) INLINEFORM3 , where INLINEFORM4 is a gold summary of document INLINEFORM5 , we employ a state-of-the-art sentence similarity model, DSSM BIBREF9 , BIBREF10 , to find the set of top- INLINEFORM6 sentences INLINEFORM8 in INLINEFORM9 , such that the similarity between INLINEFORM10 and any sentence in the gold summary INLINEFORM11 is above a pre-defined threshold. Note that here we assume each training document is associated with a gold summary composed of sentences that might not come from INLINEFORM12 . We make this assumption only for the sake of generating the set of salient sentences INLINEFORM13 which is usually not readily available.   \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Table TABREF15 presents P@10, MAP and MRR results of our KeyVec model and competing embedding methods in academic paper retrieval. word2vec averaging generates an embedding for a document by averaging the word2vec vectors of its constituent words. In the experiment, we used two different versions of word2vec: one from public release, and the other one trained specifically on our own academic corpus (113 GB). From Table TABREF15 , we observe that as a document-embedding model, Paragraph Vector gave better retrieval results than word2vec averagings did. In contrast, our KeyVec outperforms all the competitors given its unique capability of capturing and embedding the key information of documents.   \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                                                                                                                                                                                                                                                          In recent years, the use of word representations, such as word2vec BIBREF0 , BIBREF1 and GloVe BIBREF2 , has become a key “secret sauce” for the success of many natural language processing (NLP), information retrieval (IR) and machine learning (ML) tasks. The empirical success of word embeddings raises an interesting research question: Beyond words, can we learn fixed-length distributed representations for pieces of texts? The texts can be of variable-length, ranging from paragraphs to documents. Such document representations play a vital role in a large number of downstream NLP/IR/ML applications, such as text clustering, sentiment analysis, and document retrieval, which treat each piece of text as an instance. Learning a good representation that captures the semantics of each document is thus essential for the success of such applications.   \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                In another recent work BIBREF5 , Djuric et al. introduced a Hierarchical Document Vector (HDV) model to learn representations from a document stream. Our KeyVec differs from HDV in that we do not assume the existence of a document stream and HDV does not model sentences.   \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Specifically, for the INLINEFORM0 -th sentence INLINEFORM1 with INLINEFORM2 words, Neural Reader maps each word INLINEFORM3 into a word embedding INLINEFORM4 . Pre-trained word embeddings like word2vec or GloVe may be used to initialize the embedding table. In our experiments, we use domain-specific word embeddings trained by word2vec on our corpus.   \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually.   \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                                                                                                                                                                                                                                                                                                                  For the Rhetorical Parsing part of our experiment, we used a special library implemented for such purposes BIBREF18 . As a sentiment analysis model, we used the Bag of Word vectorization method with a Logistic Regression classifier trained on 1.2 million (1, 3 and 5-star rating only) of Electronic reviews from SNAP Amazon Dataset BIBREF19 . The BoW vectorization method built a vocabulary that considers the top 50,000 terms only ordered by their frequency across the corpus, similarly to supervised learning examples presented in our previous works in BIBREF8 . We used a noun and noun phrases extractor according to part-of-speech tagger from the Spacy Python library. In order to create an Aspect-Rhetorical Relation Graph we used breadth-first search (BFS) algorithm for each Discourse Tree.   \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a  We implemented our framework in Python. The first computational step was to load the dataset and parse it into individual documents. Next, each document was processed through the Discourse Parser BIBREF18 and transformed into a Discourse Tree (DT). Then we extracted Elementary Discourse Units (EDUs) from the DT and each EDU was processed through the Logistic Regression sentiment algorithm. All neutral EDUs were taken off from consideration to ensure that the discovered aspects are correlated with authors' emotions. The remaining EDUs were processed through part-of-speech tagger to extract nouns and noun phrases which we decided to treat as potential aspects. The result of this step was a set of Aspect-based Discourse Trees (ADTs). Then, from each ADT relations between aspects were extracted using breadth-first search, and an Aspect-Rhetorical Relation Graph (ARRG) was created by using aspects and relations such as nodes and edges respectively. Next, we evaluated the importance of aspects using a PageRank algorithm. Our approach resulted in complete list of aspects sorted by PageRank score. We applied a user-selected importance threshold to filter trivial aspects.   \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                                                                                                                                                                                                                                                                                                                                        In Table TABREF18 there are presented some examples of the results of our approach compared with the annotated data from Bing Liu's dataset. In the first sentence, the results of the analysis differ because we decided to treat only nouns or noun phrases as aspects, while annotators also accepted verbs. In some cases, such as sentences 2 or 4, our approach generated more valuable aspects than the annotators found, but in some cases, like sentence 5, we found fewer. This is possibly the result of our method of filtering valuable aspects - if some aspects were not frequent enough in the dataset, we can treat them as void. In cases where there is neither aspect nor sentiment in the dataset, such as sentence 6, we measure sentiment as well, as one of our analysis steps.   \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                                                                                                                                                                                     Figure FIGREF19 shows the agreement between our aspects and that of the dataset. We assumed two aspects as equal when they were textually the same. We made some experiments using text distance metrics, such as the Jaro-Winkler distance, but the results did not differ significantly from an exact matching. We fitted the importance factor value (on the X axis) so as to enrich final aspects set: a higher factor resulted in a larger aspects set and a higher value of precision metric, with slowly decreasing recall. First results (blue line on charts) were not satisfactory, so we removed a sentiment filtering step of analysis (orange line on chart), which doubled the precision value, with nearly the same value of recall. The level of precision for whole dataset (computer, router, and speaker) was most of the time at the same level. However, the recall of router was significantly worse than speaker and computer sets.   \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225                                                                                                                                                                                                                                                                                                                                                                                                                Concerning robustness to noisy user generated content, BIBREF0 stress differences with traditional domain adaptation problems, and propose a typology of errors, many of which we also detected in the Foursquare data. They also released a dataset (MTNT), whose sources were selected from a social media (Reddit) on the basis of being especially noisy (see Appendix for a comparison with Foursquare). These sources were then translated by humans to produce a parallel corpus that can be used to engineer more robust NMT systems and to evaluate them. This corpus was the basis of the WMT 2019 Robustness Task BIBREF1, in which BIBREF2 ranked first. We use the same set of robustness and domain adaptation techniques, which we study more in depth and apply to our review translation task.   \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        During our work, we used BLEU BIBREF28 on newstest[2012, 2013] to ensure that our models stayed good on a more general domain, and on Foursquare-valid to measure performance on the Foursquare domain.   \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Foursquare-HT was translated from scratch by the same translators who post-edited Foursquare-PE. While we did not use it in this work, it can be used as extra training or development data. We also release a human translation of the French-language test set (668 sentences) of the Aspect-Based Sentiment Analysis task at SemEval 2016 BIBREF14.   \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This in-domain data is concatenated to the out-of-domain parallel data and used for training.   \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                After some initial work with the WMT 2014 data, we built a new training corpus named UGC (User Generated Content), closer to our domain, by combining: Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet (See Table TABREF31). UGC does not include Common Crawl (which contains many misaligned sentences and caused hallucinations), but it includes OpenSubtitles BIBREF24 (spoken-language, possibly closer to Foursquare). We observed an improvement of more than 1 BLEU on newstest2014 when switching to UGC, and almost 6 BLEU on Foursquare-valid.   \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In this work, we evaluate our NER approach using two news corpora. One corpus is a set of 227 texts published on December 31, 2010 by the Lusa agency (portuguese agency of news) and will be referred to as `News'. The other corpus (named here `Sports news') is a set of 881 sports news. The texts were manually annotated according to the enamex designation and the type `miscellaneous'.   \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004                                                                                                                                                                       In this work we propose a novel effective method to extract named entities from unstructured text. The proposed PAMPO method is implemented using free software, namely R and available packages. Two manually annotated Portuguese news corpora were used to empirically evaluate the algorithm using the measures of INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . These corpora did not influence the definition of the algorithm or the construction of its pattern bases. We have compared PAMPO with three other NER extractors: AlchemyAPI, Rembrandt and Zemanta. Experimental results clearly show that PAMPO obtains significantly higher INLINEFORM3 and INLINEFORM4 than existing tools. The values of INLINEFORM5 are identical. We may say also that PAMPO's performance in the HAREM corpus was at least as good as the best one of the systems reported over there when we consider all categories of entities. However, when we exclude dates and numeric expressions, it presents better results than the ones reported for other tools.   \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The authors would like to thank SAPO Labs (http://labs.sapo.pt) for providing the data set of news from Lusa agency. The authors would also like to thank grant #2014/08996-0 and grant #2013/14757-6, São Paulo Research Foundation (FAPESP). This work is partially funded by FCT/MEC through PIDDAC and ERDF/ON2 within project NORTE-07-0124-FEDER-000059 and through the COMPETE Programme (operational programme for competitiveness) and by National Funds through the FCT - Fundação para a Ciência e a Tecnologia (Portuguese Foundation for Science and Technology) within project FCOMP-01-0124-FEDER-037281.   \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004                   In 1991, Lisa F. Rau presented a paper describing an algorithm, based on heuristics and handcrafted rules, to automatically extract company names from financial news BIBREF7 . This was one of the first research papers on the NER field BIBREF8 . NER was first introduced as an information extraction task but since then its use in natural language text has spread widely through several fields, namely Information Retrieval, Question Answering, Machine Translation, Text Translation, Text Clustering and Navigation Systems BIBREF9 . In an attempt to suit the needs of each application, nowadays, a NER extraction workflow comprises not only analysing some input content and detecting named entities, but also assigning them a type and a list of URIs for disambiguation BIBREF10 . New approaches have been developed with the application of Supervised machine Learning (SL) techniques BIBREF6 and NER evolved to NERC — Named Entity Recognition and Classification. The handicap of those techniques is the requirement of a training set, i.e., a data set manually labelled. Therefore, the NER task depends also on the data set used to train the NER extraction algorithm.   \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004                                                                                                                                                                                                         Nowadays, a large amount of information is produced and shared in unstructured form, mostly unstructured text BIBREF0 , BIBREF1 . This information can be exploited in decision making processes but, to be useful, it should be transformed and presented in ways that make its intrinsic knowledge more readily intelligible. For that, we need efficient methods and tools that quickly extract useful information from unstructured text collections. Such demand can be observed, for instance, in Biology, where researchers, in order to be abreast of all developments, need to analyse new biomedical literature on a daily basis BIBREF2 . Another application is on fraud and corruption studies where the network information — the set of actors and their relationships — is implicitly stored in unstructured natural-language documents BIBREF3 . Hence, text mining and information extraction are required to pre-process the texts in order to extract the entities and the relations between them.   \n",
       "\n",
       "                                                                                                                                         fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                          \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                     The training, validation and test datasets provided for the shared task BIBREF5   \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                     The training, validation and test datasets provided for the shared task BIBREF5   \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                     The training, validation and test datasets provided for the shared task BIBREF5   \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                     The training, validation and test datasets provided for the shared task BIBREF5   \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                     The training, validation and test datasets provided for the shared task BIBREF5   \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624                                                                                           Data set A and Data set B   \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624                                                                                           Data set A and Data set B   \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624                                                                                           Data set A and Data set B   \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624                                                                                           Data set A and Data set B   \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624                                                                                           Data set A and Data set B   \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                              113 GB   \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                              113 GB   \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                              113 GB   \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                              113 GB   \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                              113 GB   \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                  Bing Liu's dataset   \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                  Bing Liu's dataset   \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                  Bing Liu's dataset   \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                  Bing Liu's dataset   \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                  Bing Liu's dataset   \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225  UGC (User Generated Content), Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet   \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225  UGC (User Generated Content), Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet   \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225  UGC (User Generated Content), Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet   \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225  UGC (User Generated Content), Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet   \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225  UGC (User Generated Content), Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet   \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004  News corpus (227 texts published on December 31, 2010 by the Lusa agency) and Sports news corpus (881 sports news)   \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004  News corpus (227 texts published on December 31, 2010 by the Lusa agency) and Sports news corpus (881 sports news)   \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004  News corpus (227 texts published on December 31, 2010 by the Lusa agency) and Sports news corpus (881 sports news)   \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004  News corpus (227 texts published on December 31, 2010 by the Lusa agency) and Sports news corpus (881 sports news)   \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004  News corpus (227 texts published on December 31, 2010 by the Lusa agency) and Sports news corpus (881 sports news)   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                                                                                                                                                                                                                                  [ training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger, datasets provided for the shared task BIBREF5, Dataset of tweets provided for the shared task., Dataset from shared task BIBREF5]  \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                                                                                                                                                                                                                                  [ training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger, datasets provided for the shared task BIBREF5, Dataset of tweets provided for the shared task., Dataset from shared task BIBREF5]  \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                                                                                                                                                                                                                                  [ training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger, datasets provided for the shared task BIBREF5, Dataset of tweets provided for the shared task., Dataset from shared task BIBREF5]  \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                                                                                                                                                                                                                                  [ training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger, datasets provided for the shared task BIBREF5, Dataset of tweets provided for the shared task., Dataset from shared task BIBREF5]  \n",
       "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02                                                                                                                                                                                                                                                  [ training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger, datasets provided for the shared task BIBREF5, Dataset of tweets provided for the shared task., Dataset from shared task BIBREF5]  \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624  [ two types of simultaneous speech EEG recording databases , The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment., Speech EEG recording collected from male and female subjects under different background noises, For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment.]  \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624  [ two types of simultaneous speech EEG recording databases , The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment., Speech EEG recording collected from male and female subjects under different background noises, For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment.]  \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624  [ two types of simultaneous speech EEG recording databases , The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment., Speech EEG recording collected from male and female subjects under different background noises, For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment.]  \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624  [ two types of simultaneous speech EEG recording databases , The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment., Speech EEG recording collected from male and female subjects under different background noises, For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment.]  \n",
       "d36a6447bfe58204e0d29f9213d84be04d875624  [ two types of simultaneous speech EEG recording databases , The two types of simultaneous speech EEG recording databases:  A- five female and five male subjects took part in the experiment, and B- five male and three female subjects took part in the experiment., Speech EEG recording collected from male and female subjects under different background noises, For database A five female and five male subjects took part in the experiment., For database B five male and three female subjects took part in the experiment.]  \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                                                                                 [669 academic papers published by IEEE, 850 academic papers, 669 academic papers published by IEEE, For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation.]  \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                                                                                 [669 academic papers published by IEEE, 850 academic papers, 669 academic papers published by IEEE, For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation.]  \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                                                                                 [669 academic papers published by IEEE, 850 academic papers, 669 academic papers published by IEEE, For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation.]  \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                                                                                 [669 academic papers published by IEEE, 850 academic papers, 669 academic papers published by IEEE, For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation.]  \n",
       "1237b6fcc64b43901415f3ded17cc210a54ab698                                                                                                                                                                 [669 academic papers published by IEEE, 850 academic papers, 669 academic papers published by IEEE, For the document retrieval task - the dataset of the document pool contained 669 academic papers published by IEEE. Fro the document clustering task -  the dataset of 850 academic papers, and 186 associated venues which are used as ground-truth for evaluation.]  \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                                                                                                                                                                                                                                                                                                                                 [SNAP Amazon Dataset , Bing Liu's dataset, Bing Liu's dataset, SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20]  \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                                                                                                                                                                                                                                                                                                                                 [SNAP Amazon Dataset , Bing Liu's dataset, Bing Liu's dataset, SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20]  \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                                                                                                                                                                                                                                                                                                                                 [SNAP Amazon Dataset , Bing Liu's dataset, Bing Liu's dataset, SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20]  \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                                                                                                                                                                                                                                                                                                                                 [SNAP Amazon Dataset , Bing Liu's dataset, Bing Liu's dataset, SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20]  \n",
       "4748a50c96acb1aa03f7efd1b43376c193b2450a                                                                                                                                                                                                                                                                                                                                                                                                                 [SNAP Amazon Dataset , Bing Liu's dataset, Bing Liu's dataset, SNAP Amazon Dataset BIBREF19, Bing Liu's dataset BIBREF20]  \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225                                                                                                                                                                                                                                                                                                                                                                                                                   [WMT 2014,  UGC (User Generated Content), 11.5k French reviews from Foursquare, WMT 2014, UGC (User Generated Content)]  \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225                                                                                                                                                                                                                                                                                                                                                                                                                   [WMT 2014,  UGC (User Generated Content), 11.5k French reviews from Foursquare, WMT 2014, UGC (User Generated Content)]  \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225                                                                                                                                                                                                                                                                                                                                                                                                                   [WMT 2014,  UGC (User Generated Content), 11.5k French reviews from Foursquare, WMT 2014, UGC (User Generated Content)]  \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225                                                                                                                                                                                                                                                                                                                                                                                                                   [WMT 2014,  UGC (User Generated Content), 11.5k French reviews from Foursquare, WMT 2014, UGC (User Generated Content)]  \n",
       "5fd112980d0dd7f7ce30e6273fe6e7b230b13225                                                                                                                                                                                                                                                                                                                                                                                                                   [WMT 2014,  UGC (User Generated Content), 11.5k French reviews from Foursquare, WMT 2014, UGC (User Generated Content)]  \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004                                                                                                                                                                                                                                                                                                                                                                                                                                   [CoNLL_X bosque data, News data by Lusa agency, Sports news data, News, Sports news, News, Sports news]  \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004                                                                                                                                                                                                                                                                                                                                                                                                                                   [CoNLL_X bosque data, News data by Lusa agency, Sports news data, News, Sports news, News, Sports news]  \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004                                                                                                                                                                                                                                                                                                                                                                                                                                   [CoNLL_X bosque data, News data by Lusa agency, Sports news data, News, Sports news, News, Sports news]  \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004                                                                                                                                                                                                                                                                                                                                                                                                                                   [CoNLL_X bosque data, News data by Lusa agency, Sports news data, News, Sports news, News, Sports news]  \n",
       "f1f7a040545c9501215d3391e267c7874f9a6004                                                                                                                                                                                                                                                                                                                                                                                                                                   [CoNLL_X bosque data, News data by Lusa agency, Sports news data, News, Sports news, News, Sports news]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What dataset do they use?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fewshot_evidence</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0fe49431db5ffaa24372919daf24d8f84117bfda</th>\n",
       "      <td>We evaluate our models both automatically (in terms of Rouge) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing hand-engineered features and sophisticated linguistic constraints.</td>\n",
       "      <td>DUC 2002 document summarization corpus and DailyMail news highlights corpus</td>\n",
       "      <td>[DUC 2002 document summarization corpus, our own DailyMail news highlights corpus, DUC 2002, our own Dailymail news highlights corpus, the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus, DailyMail news articles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0fe49431db5ffaa24372919daf24d8f84117bfda</th>\n",
       "      <td>Data-driven neural summarization models require a large training corpus of documents with labels indicating which sentences (or words) should be in the summary. Until now such corpora have been limited to hundreds of examples (e.g., the DUC 2002 single document summarization corpus) and thus used mostly for testing BIBREF7 . To overcome the paucity of annotated data for training, we adopt a methodology similar to hermann2015teaching and create two large-scale datasets, one for sentence extraction and another one for word extraction.</td>\n",
       "      <td>DUC 2002 document summarization corpus and DailyMail news highlights corpus</td>\n",
       "      <td>[DUC 2002 document summarization corpus, our own DailyMail news highlights corpus, DUC 2002, our own Dailymail news highlights corpus, the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus, DailyMail news articles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0fe49431db5ffaa24372919daf24d8f84117bfda</th>\n",
       "      <td>Table 1 (upper half) summarizes our results on the DUC 2002 test dataset using Rouge. nn-se represents our neural sentence extraction model, nn-we our word extraction model, and nn-abs the neural abstractive baseline. The table also includes results for the lead baseline, the logistic regression classifier (lreg), and three previously published systems (ilp, tgraph, and urank).</td>\n",
       "      <td>DUC 2002 document summarization corpus and DailyMail news highlights corpus</td>\n",
       "      <td>[DUC 2002 document summarization corpus, our own DailyMail news highlights corpus, DUC 2002, our own Dailymail news highlights corpus, the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus, DailyMail news articles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0fe49431db5ffaa24372919daf24d8f84117bfda</th>\n",
       "      <td>One stumbling block to applying neural network models to extractive summarization is the lack of training data, i.e., documents with sentences (and words) labeled as summary-worthy. Inspired by previous work on summarization BIBREF7 , BIBREF13 and reading comprehension BIBREF9 we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website. Highlights usually appear as bullet points giving a brief overview of the information contained in the article (see Figure 1 for an example). Using a number of transformation and scoring algorithms, we are able to match highlights to document content and construct two large scale training datasets, one for sentence extraction and the other for word extraction. Previous approaches have used small scale training data in the range of a few hundred examples.</td>\n",
       "      <td>DUC 2002 document summarization corpus and DailyMail news highlights corpus</td>\n",
       "      <td>[DUC 2002 document summarization corpus, our own DailyMail news highlights corpus, DUC 2002, our own Dailymail news highlights corpus, the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus, DailyMail news articles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0fe49431db5ffaa24372919daf24d8f84117bfda</th>\n",
       "      <td>In this section we present our experimental setup for assessing the performance of our summarization models. We discuss the datasets used for training and evaluation, give implementation details, briefly introduce comparison models, and explain how system output was evaluated.</td>\n",
       "      <td>DUC 2002 document summarization corpus and DailyMail news highlights corpus</td>\n",
       "      <td>[DUC 2002 document summarization corpus, our own DailyMail news highlights corpus, DUC 2002, our own Dailymail news highlights corpus, the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus, DailyMail news articles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f</th>\n",
       "      <td>We also leveraged an unlabeled massive dataset of conversation utterances to train our word embeddings with “virtual sentences.” The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).</td>\n",
       "      <td>The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).</td>\n",
       "      <td>[real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances, chatting corpus from DuMi and conversation data from Douban forum, chatting corpus from DuMi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f</th>\n",
       "      <td>We further conducted in-depth analysis of different strategies of training word-embeddings and matching heuristics in Table TABREF21 . For word embeddings, we trained them on the 3M-sentence dataset with three strategies: (1) virtual-sentence context proposed in our paper; (2) within-sentence context, where all words (except the current one) within a sentence (either a query or reply) are regarded as the context; (3) window-based context, which is the original form of BIBREF25 : the context is the words in a window (previous 2 words and future 2 words in the sentence). We observe that our virtual-sentence strategy consistently outperforms the other two in all three matching heuristics. The results suggest that combining a query and a reply does provide more information in learning dialogue-specific word embeddings.</td>\n",
       "      <td>The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).</td>\n",
       "      <td>[real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances, chatting corpus from DuMi and conversation data from Douban forum, chatting corpus from DuMi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f</th>\n",
       "      <td>MMD. We applied the MinMax-Dotplotting (MMD) approach proposed by Ye et al. BIBREF24 . We ran the executable program provided by the authors.</td>\n",
       "      <td>The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).</td>\n",
       "      <td>[real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances, chatting corpus from DuMi and conversation data from Douban forum, chatting corpus from DuMi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f</th>\n",
       "      <td>In this section, we evaluate our embedding-enhanced TextTiling method as well as the effect of session segmentation. In Subsection SECREF17 , we describe the datasets used in our experiments. Subsection SECREF22 presents the segmentation accuracy of our method and baselines. In Subsection SECREF27 , we show that, with our session segmentation, we can improve the performance of a retrieval-based conversation system.</td>\n",
       "      <td>The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).</td>\n",
       "      <td>[real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances, chatting corpus from DuMi and conversation data from Douban forum, chatting corpus from DuMi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f</th>\n",
       "      <td>We tuned the hyperparameter INLINEFORM0 in Equation ()on the validation set to make the number of segmentation close to that of manual annotation, and reported precision, recall, and the F-score on the test set in Table TABREF18 . As seen, our approach significantly outperforms baselines by a large margin in terms of both precision and recall. Besides, we can see that MMD obtains low performance, which is mainly because the approach cannot be easily adapted to other datasets like short sentences of conversation utterances. In summary, we achieve an INLINEFORM1 -score higher than baseline methods by more than 20%, showing the effectiveness of enhancing TextTiling with modern word embeddings.</td>\n",
       "      <td>The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).</td>\n",
       "      <td>[real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances, chatting corpus from DuMi and conversation data from Douban forum, chatting corpus from DuMi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9489b0ecb643c1fc95c001c65d4e9771315989aa</th>\n",
       "      <td>We run experiments on the English portion of CoNLL 2012 data BIBREF15 which consists of 3,492 documents in various domains and formats. The split provided in the CoNLL 2012 shared task is used. In all our resolvers, we use not the original features of P15-1137 but their slight modification described in N16-1114 (section 6.1).</td>\n",
       "      <td>The English portion of CoNLL 2012 data</td>\n",
       "      <td>[CoNLL 2012, English portion of CoNLL 2012 data BIBREF15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9489b0ecb643c1fc95c001c65d4e9771315989aa</th>\n",
       "      <td>Mention ranking and entity centricity are two main streams in the coreference resolution literature. Mention ranking BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 considers local and independent decisions when choosing a correct antecedent for a mention. This approach is computationally efficient and currently dominant with state-of-the-art performance BIBREF5 , BIBREF6 . P15-1137 propose to use simple neural networks to compute mention ranking scores and to use a heuristic loss to train the model. N16-1114 extend this by employing LSTMs to compute mention-chain representations which are then used to compute ranking scores. They call these representations global features. clark-manning:2016:EMNLP2016 build a similar resolver as in P15-1137 but much stronger thanks to deeper neural networks and “better mention detection, more effective, hyperparameters, and more epochs of training”. Furthermore, using reward rescaling they achieve the best performance in the literature on the English and Chinese portions of the CoNLL 2012 dataset. Our work is built upon mention ranking by turning a mention-ranking model into an entity-centric one. It is worth noting that although we use the model proposed by P15-1137, any mention-ranking models can be employed.</td>\n",
       "      <td>The English portion of CoNLL 2012 data</td>\n",
       "      <td>[CoNLL 2012, English portion of CoNLL 2012 data BIBREF15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9489b0ecb643c1fc95c001c65d4e9771315989aa</th>\n",
       "      <td>baseline: the resolver presented in Section SECREF2 . We use the identical configuration as in N16-1114: INLINEFORM0 , INLINEFORM1 , INLINEFORM2 (where INLINEFORM3 are respectively the numbers of mention features and pair-wise features). We also employ their pretraining methodology.</td>\n",
       "      <td>The English portion of CoNLL 2012 data</td>\n",
       "      <td>[CoNLL 2012, English portion of CoNLL 2012 data BIBREF15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9489b0ecb643c1fc95c001c65d4e9771315989aa</th>\n",
       "      <td>We use five most popular metrics,</td>\n",
       "      <td>The English portion of CoNLL 2012 data</td>\n",
       "      <td>[CoNLL 2012, English portion of CoNLL 2012 data BIBREF15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9489b0ecb643c1fc95c001c65d4e9771315989aa</th>\n",
       "      <td>INLINEFORM0 is used in the standard evaluation.</td>\n",
       "      <td>The English portion of CoNLL 2012 data</td>\n",
       "      <td>[CoNLL 2012, English portion of CoNLL 2012 data BIBREF15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763</th>\n",
       "      <td>For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively. In order to minimize the occurrence of duplicate commits in two of these splits (such as in both training and test), commits from no repository belong to more than one split. However, 808 commits may not be sufficient to train deep learning models. Hence, in order to answer RQ4, we augment the training split with commits mined using regular expression matching on the commit messages from the same set of open-source Java projects. This almost doubles the number of commits in the training split to 1493. We then repeat our experiments for the first three research questions on the augmented dataset, and evaluate our trained models on the same validation and test splits.</td>\n",
       "      <td>Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them</td>\n",
       "      <td>[manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763</th>\n",
       "      <td>This section details the methodology used in this study to build the training dataset, the models used for classification and the evaluation procedure. All of the experiments are conducted on Python 3.7 running on an Intel Core i7 6800K CPU and a Nvidia GTX 1080 GPU. All the deep learning models are implemented in PyTorch 0.4.1 BIBREF21, while Scikit-learn 0.19.2 BIBREF22 is used for computing the tf–idf vectors and performing logistic regression.</td>\n",
       "      <td>Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them</td>\n",
       "      <td>[manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763</th>\n",
       "      <td>Deep learning models are known for scaling well with more data. However, with less than 1,000 ground-truth training samples and around 1,800 augmented training samples, we are unable to exploit the full potential of deep learning. A reflection on the current state of labelled datasets in software engineering (or the lack thereof) throws light on limited practicality of deep learning models for certain software engineering tasks BIBREF29. As stated by BIBREF30, just as research in NLP changed focus from brittle rule-based expert systems to statistical methods, software engineering research should augment traditional methods that consider only the formal structure of programs with information about the statistical properties of code. Ongoing research on pre-trained code embeddings that don't require a labelled dataset for training is a step in the right direction. Drawing parallels with the recent history of NLP research, we are hoping that further study in the domain of code embeddings will considerably accelerate progress in tackling software problems with deep learning.</td>\n",
       "      <td>Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them</td>\n",
       "      <td>[manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763</th>\n",
       "      <td>We directly train code2vec on our dataset without pre-training it, in order to assess how well path-based representations perform for learning on code, as opposed to token-level representations on which H-CNN and HR-CNN are based. However, BIBREF16 pre-trained their model on 10M Java classes. It is possible that the performance of code2vec is considerably better than the results in Table TABREF22 after pre-training. Furthermore, our findings apply only to this particular technique to capturing path-based representations, not the approach in general. However, we leave both issues for future work.</td>\n",
       "      <td>Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them</td>\n",
       "      <td>[manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763</th>\n",
       "      <td>There is also a question of to what extent the 635 publicly disclosed vulnerabilities used for evaluation in this study represent the vulnerabilities found in real-world scenarios. While creating larger ground-truth datasets would always be helpful, it might not always be possible. To reduce the possibility of bias in our results, we ensure that we don't train commits from the same projects that we evaluate our models on. We also discard any commits belonging to the set of evaluation projects that are mined using regular expression matching.</td>\n",
       "      <td>Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them</td>\n",
       "      <td>[manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     fewshot_evidence  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We evaluate our models both automatically (in terms of Rouge) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing hand-engineered features and sophisticated linguistic constraints.   \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Data-driven neural summarization models require a large training corpus of documents with labels indicating which sentences (or words) should be in the summary. Until now such corpora have been limited to hundreds of examples (e.g., the DUC 2002 single document summarization corpus) and thus used mostly for testing BIBREF7 . To overcome the paucity of annotated data for training, we adopt a methodology similar to hermann2015teaching and create two large-scale datasets, one for sentence extraction and another one for word extraction.   \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Table 1 (upper half) summarizes our results on the DUC 2002 test dataset using Rouge. nn-se represents our neural sentence extraction model, nn-we our word extraction model, and nn-abs the neural abstractive baseline. The table also includes results for the lead baseline, the logistic regression classifier (lreg), and three previously published systems (ilp, tgraph, and urank).   \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda                                                                                                                                                                                                                                                                                                                                                                                                                                 One stumbling block to applying neural network models to extractive summarization is the lack of training data, i.e., documents with sentences (and words) labeled as summary-worthy. Inspired by previous work on summarization BIBREF7 , BIBREF13 and reading comprehension BIBREF9 we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website. Highlights usually appear as bullet points giving a brief overview of the information contained in the article (see Figure 1 for an example). Using a number of transformation and scoring algorithms, we are able to match highlights to document content and construct two large scale training datasets, one for sentence extraction and the other for word extraction. Previous approaches have used small scale training data in the range of a few hundred examples.   \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        In this section we present our experimental setup for assessing the performance of our summarization models. We discuss the datasets used for training and evaluation, give implementation details, briefly introduce comparison models, and explain how system output was evaluated.   \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We also leveraged an unlabeled massive dataset of conversation utterances to train our word embeddings with “virtual sentences.” The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).   \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f                                                                                                                                                                                                                                                                                                                                                                                                                                                   We further conducted in-depth analysis of different strategies of training word-embeddings and matching heuristics in Table TABREF21 . For word embeddings, we trained them on the 3M-sentence dataset with three strategies: (1) virtual-sentence context proposed in our paper; (2) within-sentence context, where all words (except the current one) within a sentence (either a query or reply) are regarded as the context; (3) window-based context, which is the original form of BIBREF25 : the context is the words in a window (previous 2 words and future 2 words in the sentence). We observe that our virtual-sentence strategy consistently outperforms the other two in all three matching heuristics. The results suggest that combining a query and a reply does provide more information in learning dialogue-specific word embeddings.   \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                MMD. We applied the MinMax-Dotplotting (MMD) approach proposed by Ye et al. BIBREF24 . We ran the executable program provided by the authors.   \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           In this section, we evaluate our embedding-enhanced TextTiling method as well as the effect of session segmentation. In Subsection SECREF17 , we describe the datasets used in our experiments. Subsection SECREF22 presents the segmentation accuracy of our method and baselines. In Subsection SECREF27 , we show that, with our session segmentation, we can improve the performance of a retrieval-based conversation system.   \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  We tuned the hyperparameter INLINEFORM0 in Equation ()on the validation set to make the number of segmentation close to that of manual annotation, and reported precision, recall, and the F-score on the test set in Table TABREF18 . As seen, our approach significantly outperforms baselines by a large margin in terms of both precision and recall. Besides, we can see that MMD obtains low performance, which is mainly because the approach cannot be easily adapted to other datasets like short sentences of conversation utterances. In summary, we achieve an INLINEFORM1 -score higher than baseline methods by more than 20%, showing the effectiveness of enhancing TextTiling with modern word embeddings.   \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We run experiments on the English portion of CoNLL 2012 data BIBREF15 which consists of 3,492 documents in various domains and formats. The split provided in the CoNLL 2012 shared task is used. In all our resolvers, we use not the original features of P15-1137 but their slight modification described in N16-1114 (section 6.1).   \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa  Mention ranking and entity centricity are two main streams in the coreference resolution literature. Mention ranking BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 considers local and independent decisions when choosing a correct antecedent for a mention. This approach is computationally efficient and currently dominant with state-of-the-art performance BIBREF5 , BIBREF6 . P15-1137 propose to use simple neural networks to compute mention ranking scores and to use a heuristic loss to train the model. N16-1114 extend this by employing LSTMs to compute mention-chain representations which are then used to compute ranking scores. They call these representations global features. clark-manning:2016:EMNLP2016 build a similar resolver as in P15-1137 but much stronger thanks to deeper neural networks and “better mention detection, more effective, hyperparameters, and more epochs of training”. Furthermore, using reward rescaling they achieve the best performance in the literature on the English and Chinese portions of the CoNLL 2012 dataset. Our work is built upon mention ranking by turning a mention-ranking model into an entity-centric one. It is worth noting that although we use the model proposed by P15-1137, any mention-ranking models can be employed.   \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  baseline: the resolver presented in Section SECREF2 . We use the identical configuration as in N16-1114: INLINEFORM0 , INLINEFORM1 , INLINEFORM2 (where INLINEFORM3 are respectively the numbers of mention features and pair-wise features). We also employ their pretraining methodology.   \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We use five most popular metrics,   \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              INLINEFORM0 is used in the standard evaluation.   \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763                                                                                                                                                                                                                                                                For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively. In order to minimize the occurrence of duplicate commits in two of these splits (such as in both training and test), commits from no repository belong to more than one split. However, 808 commits may not be sufficient to train deep learning models. Hence, in order to answer RQ4, we augment the training split with commits mined using regular expression matching on the commit messages from the same set of open-source Java projects. This almost doubles the number of commits in the training split to 1493. We then repeat our experiments for the first three research questions on the augmented dataset, and evaluate our trained models on the same validation and test splits.   \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          This section details the methodology used in this study to build the training dataset, the models used for classification and the evaluation procedure. All of the experiments are conducted on Python 3.7 running on an Intel Core i7 6800K CPU and a Nvidia GTX 1080 GPU. All the deep learning models are implemented in PyTorch 0.4.1 BIBREF21, while Scikit-learn 0.19.2 BIBREF22 is used for computing the tf–idf vectors and performing logistic regression.   \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763                                                                                                                                                                              Deep learning models are known for scaling well with more data. However, with less than 1,000 ground-truth training samples and around 1,800 augmented training samples, we are unable to exploit the full potential of deep learning. A reflection on the current state of labelled datasets in software engineering (or the lack thereof) throws light on limited practicality of deep learning models for certain software engineering tasks BIBREF29. As stated by BIBREF30, just as research in NLP changed focus from brittle rule-based expert systems to statistical methods, software engineering research should augment traditional methods that consider only the formal structure of programs with information about the statistical properties of code. Ongoing research on pre-trained code embeddings that don't require a labelled dataset for training is a step in the right direction. Drawing parallels with the recent history of NLP research, we are hoping that further study in the domain of code embeddings will considerably accelerate progress in tackling software problems with deep learning.   \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We directly train code2vec on our dataset without pre-training it, in order to assess how well path-based representations perform for learning on code, as opposed to token-level representations on which H-CNN and HR-CNN are based. However, BIBREF16 pre-trained their model on 10M Java classes. It is possible that the performance of code2vec is considerably better than the results in Table TABREF22 after pre-training. Furthermore, our findings apply only to this particular technique to capturing path-based representations, not the approach in general. However, we leave both issues for future work.   \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          There is also a question of to what extent the 635 publicly disclosed vulnerabilities used for evaluation in this study represent the vulnerabilities found in real-world scenarios. While creating larger ground-truth datasets would always be helpful, it might not always be possible. To reduce the possibility of bias in our results, we ensure that we don't train commits from the same projects that we evaluate our models on. We also discard any commits belonging to the set of evaluation projects that are mined using regular expression matching.   \n",
       "\n",
       "                                                                                                                                                             fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                                              \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda                                                             DUC 2002 document summarization corpus and DailyMail news highlights corpus   \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda                                                             DUC 2002 document summarization corpus and DailyMail news highlights corpus   \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda                                                             DUC 2002 document summarization corpus and DailyMail news highlights corpus   \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda                                                             DUC 2002 document summarization corpus and DailyMail news highlights corpus   \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda                                                             DUC 2002 document summarization corpus and DailyMail news highlights corpus   \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f  The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).   \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f  The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).   \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f  The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).   \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f  The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).   \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f  The dataset was crawled from the Douban forum, containing 3 million utterances and approximately 150,000 unique words (Chinese terms).   \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                  The English portion of CoNLL 2012 data   \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                  The English portion of CoNLL 2012 data   \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                  The English portion of CoNLL 2012 data   \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                  The English portion of CoNLL 2012 data   \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                  The English portion of CoNLL 2012 data   \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763  Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them   \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763  Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them   \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763  Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them   \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763  Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them   \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763  Manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                               gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                        \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda            [DUC 2002 document summarization corpus, our own DailyMail news highlights corpus, DUC 2002, our own Dailymail news highlights corpus, the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus, DailyMail news articles]  \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda            [DUC 2002 document summarization corpus, our own DailyMail news highlights corpus, DUC 2002, our own Dailymail news highlights corpus, the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus, DailyMail news articles]  \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda            [DUC 2002 document summarization corpus, our own DailyMail news highlights corpus, DUC 2002, our own Dailymail news highlights corpus, the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus, DailyMail news articles]  \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda            [DUC 2002 document summarization corpus, our own DailyMail news highlights corpus, DUC 2002, our own Dailymail news highlights corpus, the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus, DailyMail news articles]  \n",
       "0fe49431db5ffaa24372919daf24d8f84117bfda            [DUC 2002 document summarization corpus, our own DailyMail news highlights corpus, DUC 2002, our own Dailymail news highlights corpus, the benchmark DUC 2002 document summarization corpus, DailyMail news highlights corpus, DailyMail news articles]  \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f                                                                         [real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances, chatting corpus from DuMi and conversation data from Douban forum, chatting corpus from DuMi]  \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f                                                                         [real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances, chatting corpus from DuMi and conversation data from Douban forum, chatting corpus from DuMi]  \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f                                                                         [real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances, chatting corpus from DuMi and conversation data from Douban forum, chatting corpus from DuMi]  \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f                                                                         [real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances, chatting corpus from DuMi and conversation data from Douban forum, chatting corpus from DuMi]  \n",
       "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f                                                                         [real-world chatting corpus from DuMi, unlabeled massive dataset of conversation utterances, chatting corpus from DuMi and conversation data from Douban forum, chatting corpus from DuMi]  \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                                                                                                                          [CoNLL 2012, English portion of CoNLL 2012 data BIBREF15]  \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                                                                                                                          [CoNLL 2012, English portion of CoNLL 2012 data BIBREF15]  \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                                                                                                                          [CoNLL 2012, English portion of CoNLL 2012 data BIBREF15]  \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                                                                                                                          [CoNLL 2012, English portion of CoNLL 2012 data BIBREF15]  \n",
       "9489b0ecb643c1fc95c001c65d4e9771315989aa                                                                                                                                                                                                          [CoNLL 2012, English portion of CoNLL 2012 data BIBREF15]  \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763  [manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github]  \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763  [manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github]  \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763  [manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github]  \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763  [manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github]  \n",
       "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763  [manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, Dataset of publicly disclosed vulnerabilities from 205 Java projects from GitHub and 1000 Java repositories from Github]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Which dataset do they use?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fewshot_evidence</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1ecbbb60dc44a701e9c57c22167dd412711bb0be</th>\n",
       "      <td>We use the Clueweb09 derived dataset of BIBREF0 for baseline comparison. For cross-temporal, cross-topic and cross-domain training &amp; evaluation, we generate a new dataset based on Wikipedia crawl data. This dataset is gathered by using Wikipedia's `List of Contoversial articles' overview page of 2018 (time of writing) and 2009 (for comparison with baselines) . Using this as a `seed' set of controversial articles, we iteratively crawl the `See also', `References' and `External links' hyperlinks up to two hops from the seed list. The negative seed pages (i.e. non controversial) are gathered by using the random article endpoint. The snowball-sample approach includes general, non-Wikipedia, pages that are referred to from Wikipedia pages. The dataset thus extends beyond just the encyclopedia genre of texts. Labels are assumed to propagate: a page linked from a controversial issue is assumed to be controversial. The resulting dataset statistics are summarized in Table TABREF7 .</td>\n",
       "      <td>Clueweb09 derived dataset and Wikipedia crawl data</td>\n",
       "      <td>[Clueweb09, Clueweb09 derived dataset, new dataset based on Wikipedia crawl data, the Clueweb09 derived dataset , dataset based on Wikipedia crawl data, Clueweb09 derived dataset, Wikipedia crawl data]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1ecbbb60dc44a701e9c57c22167dd412711bb0be</th>\n",
       "      <td>We explore the potential of RNNs and CNNs for controversy detection using both the HAN BIBREF10 and the CNN BIBREF11 model. Similar to BIBREF10 , each bi-directional GRU cell is set to a dimension of 50, resulting in a word/sentence representation of size 100 after concatenation. The word/sentence attention vectors similarly contain 100 dimensions, all randomly initialized. The word windows defined in the CNN model are set to sizes: 2, 3 and 4 with 128 feature maps each. Each model is trained using mini batches of size 64 and uses both dropout (0.5) and INLINEFORM0 regularization (1e-3) at the dense prediction layer. Both networks use pre-trained embeddings, trained on 100 billion words of a Google News corpus, which are further fine-tuned during training on the controversy dataset. The optimization algorithm used is Adam BIBREF13 (learning rate: 1e-3).</td>\n",
       "      <td>Clueweb09 derived dataset and Wikipedia crawl data</td>\n",
       "      <td>[Clueweb09, Clueweb09 derived dataset, new dataset based on Wikipedia crawl data, the Clueweb09 derived dataset , dataset based on Wikipedia crawl data, Clueweb09 derived dataset, Wikipedia crawl data]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1ecbbb60dc44a701e9c57c22167dd412711bb0be</th>\n",
       "      <td>Currently, there is no open large-size controversy detection dataset that lends itself to test cross-temporal and cross-topic stability. Thus we generate a Wikipedia crawl-based dataset that includes general web pages and is sufficiently large to train and test high capacity models such as neural networks.</td>\n",
       "      <td>Clueweb09 derived dataset and Wikipedia crawl data</td>\n",
       "      <td>[Clueweb09, Clueweb09 derived dataset, new dataset based on Wikipedia crawl data, the Clueweb09 derived dataset , dataset based on Wikipedia crawl data, Clueweb09 derived dataset, Wikipedia crawl data]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1ecbbb60dc44a701e9c57c22167dd412711bb0be</th>\n",
       "      <td>Lastly, we examine model performance with respect to human annotation using the human annotated dataset of BIBREF6 . We assume that models that perform similarly to human annotators are preferable. In Table TABREF20 , we present three Spearman correlation metrics to express model congruence with human annotations. Mean annotation expresses the correlation of model error rates with the controversy values attributed to a web page by human annotators, with positive values expressing greater error rates on controversial, and negative expressing higher error rates on non-controversial pages. Here, the HAN shows most unbiased (closest to zero) performance.</td>\n",
       "      <td>Clueweb09 derived dataset and Wikipedia crawl data</td>\n",
       "      <td>[Clueweb09, Clueweb09 derived dataset, new dataset based on Wikipedia crawl data, the Clueweb09 derived dataset , dataset based on Wikipedia crawl data, Clueweb09 derived dataset, Wikipedia crawl data]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1ecbbb60dc44a701e9c57c22167dd412711bb0be</th>\n",
       "      <td>Table TABREF13 shows the relative performance of the neural models compared to previous controversy detection methods, evaluated on the Clueweb09 derived dataset of BIBREF6 and trained on the Wikipedia data from the same time frame. The TILE-Clique matching model outperforms all other models on Precision although this difference is not significant compared to the neural approaches. Similarly, the language model trained on the DBPedia dataset outperforms other models on Recall but shows no significant difference compared to the CNN model. Notably, the neural approaches show comparable results to the TILE-Clique model in terms of F1, demonstrating a balanced performance in terms of Precision and Recall. Furthermore, the CNN model shows a significant improvement compared to the other non neural baselines in terms of the AUC value (p &lt; 0.05).</td>\n",
       "      <td>Clueweb09 derived dataset and Wikipedia crawl data</td>\n",
       "      <td>[Clueweb09, Clueweb09 derived dataset, new dataset based on Wikipedia crawl data, the Clueweb09 derived dataset , dataset based on Wikipedia crawl data, Clueweb09 derived dataset, Wikipedia crawl data]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00e4c9aa87411dfc5455fc92f10e5c9266e7b95e</th>\n",
       "      <td>In BIBREF7 the authors proposes a method to detect and correct ASR output based on Microsoft N-Gram dataset. They use a context-sensitive error correction algorithm for selecting the best candidate for correction using the Microsoft N-Gram dataset which contains real-world data and word sequences extracted from the web which can mimic a comprehensive dictionary of words having a large and all-inclusive vocabulary.</td>\n",
       "      <td>U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013</td>\n",
       "      <td>[Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013,  survey data and hand crafted a total of 293 textual questions BIBREF13, U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013, Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00e4c9aa87411dfc5455fc92f10e5c9266e7b95e</th>\n",
       "      <td>Note that this is equivalent to, albiet loosely, learning the error model of a specific ASR. Since we have a small training set, we have used the Naive Bayes classifier that is known to perform well for small datasets with high bias and low variance. We have used the NLTK BIBREF11 Naive Bayes classifier in all our experiments.</td>\n",
       "      <td>U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013</td>\n",
       "      <td>[Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013,  survey data and hand crafted a total of 293 textual questions BIBREF13, U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013, Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00e4c9aa87411dfc5455fc92f10e5c9266e7b95e</th>\n",
       "      <td>We present the results of our experiments with both the Evo-Devo and the Machine Learning mechanisms described earlier using the U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12 .</td>\n",
       "      <td>U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013</td>\n",
       "      <td>[Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013,  survey data and hand crafted a total of 293 textual questions BIBREF13, U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013, Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00e4c9aa87411dfc5455fc92f10e5c9266e7b95e</th>\n",
       "      <td>In our experiment, we used a total of 570 misrecognition errors (for example, (dear, beer) and (have, has) derived from INLINEFORM0 or (than twenty, jewelry) derived from INLINEFORM1 ) in the 486 sentences. We performed 10-fold cross validation, each fold containing 513 INLINEFORM2 pairs for training and 57 pairs for testing, Note that we assume the erroneous words in the ASR output being marked by a human oracle, in the training as well as the testing set. Suppose the following example ( INLINEFORM3 ) occurs in the training set: INLINEFORM4 INLINEFORM5</td>\n",
       "      <td>U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013</td>\n",
       "      <td>[Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013,  survey data and hand crafted a total of 293 textual questions BIBREF13, U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013, Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00e4c9aa87411dfc5455fc92f10e5c9266e7b95e</th>\n",
       "      <td>In case of the other ASR engines, namely, Kaldi with US acoustic models (Ku), Kaldi with Indian Acoustic models (Ki) and PocketSphinx ASR (Ps) we first took the queries corresponding to the 250 utterances and built a statistical language model (SLM) and a lexicon using the scripts that are available with PocketSphinx BIBREF14 and Kaldi BIBREF15 . This language model and lexicon was used with the acoustic model that were readily available with Kaldi and Ps. In case of Ku we used the American English acoustic models, while in case of Ki we used the Indian English acoustic model. In case of Ps we used the Voxforge acoustic models BIBREF16 . Each utterance was passed through Kaldi ASR for two different acoustic models to get INLINEFORM0 corresponding to Ku and Ki. Similarly all the 250 audio utterance were passed through the Ps ASR to get the corresponding INLINEFORM1 for Ps. A sample utterance and the output of the four engines is shown in Figure FIGREF12 .</td>\n",
       "      <td>U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013</td>\n",
       "      <td>[Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013,  survey data and hand crafted a total of 293 textual questions BIBREF13, U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013, Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8c46a26f9b0b41c656b5b55142d491600663defa</th>\n",
       "      <td>We use the GlobalPhone corpus of speech read from news articles BIBREF20 . We chose 6 languages from different language families as zero-resource languages on which we evaluate the new feature representations. That means our models do not have any access to the transcriptions of the training data, although transcriptions still need to be available to run the evaluation. The selected languages and dataset sizes are shown in Table TABREF8 . Each GlobalPhone language has recordings from around 100 speakers, with 80% of these in the training sets and no speaker overlap between training, development, and test sets.</td>\n",
       "      <td>GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus</td>\n",
       "      <td>[GlobalPhone corpus, GlobalPhone\\nCroatian\\nHausa\\nMandarin\\nSpanish\\nSwedish\\nTurkish\\nZRSC\\nBuckeye\\nXitsonga, GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8c46a26f9b0b41c656b5b55142d491600663defa</th>\n",
       "      <td>In the previous experiments, we used data from GlobalPhone, which provides corpora collected and formatted similarly for a wide range of languages. However, GlobalPhone is not freely available and no previous zero-resource studies have used these corpora, so in this section we also provide results on the zrsc 2015 BIBREF0 data sets, which have been widely used in other work. The target languages are English (from the Buckeye corpus BIBREF38 ) and Xitsonga (NCHLT corpus BIBREF39 ). Table TABREF8 includes the corpus statistics. These corpora are not split into train/dev/test; since training is unsupervised, the system is simply trained directly on the unlabeled test set (which could also be done in deployment). Importantly, no hyperparameter tuning is done on the Buckeye or Xitsonga data, so these results still provide a useful test of generalization. Notably, the Buckeye English corpus contains conversational speech and is therefore different in style from the rest of our data.</td>\n",
       "      <td>GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus</td>\n",
       "      <td>[GlobalPhone corpus, GlobalPhone\\nCroatian\\nHausa\\nMandarin\\nSpanish\\nSwedish\\nTurkish\\nZRSC\\nBuckeye\\nXitsonga, GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8c46a26f9b0b41c656b5b55142d491600663defa</th>\n",
       "      <td>We picked another 10 languages (different from the target languages described in Section SECREF7 ) with a combined 198.3 hours of speech from the GlobalPhone corpus. We consider these as high-resource languages, for which transcriptions are available to train a supervised asr system. The languages and dataset sizes are listed in Table TABREF16 . We also use the English wsj corpus BIBREF35 which is comparable to the GlobalPhone corpus. It contains a total of 81 hours of speech, which we either use in its entirety or from which we use a 15 hour subset; this allows us to compare the effect of increasing the amount of data for one language with training on similar amounts of data but from different languages.</td>\n",
       "      <td>GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus</td>\n",
       "      <td>[GlobalPhone corpus, GlobalPhone\\nCroatian\\nHausa\\nMandarin\\nSpanish\\nSwedish\\nTurkish\\nZRSC\\nBuckeye\\nXitsonga, GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8c46a26f9b0b41c656b5b55142d491600663defa</th>\n",
       "      <td>In this work we investigated different representations obtained using data from the target language alone (i.e., fully unsupervised) and from multilingual supervised systems trained on labeled data from non-target languages. We found that the cae, a recent neural approach to unsupervised subword modeling, learns complementary information to the more traditional approach of vtln. This suggests that vtln should also be considered by other researchers using neural approaches. On the other hand, our best results were achieved using multilingual bnfs. These results are competitive with state-of-the-art features learned from target language data only BIBREF17 , BIBREF18 , but have the advantage of a much smaller dimensionality. In addition, it is easy to control the dimensionality of the bnfs, unlike in the nonparametric models of BIBREF17 , BIBREF18 , and this allowed us to use them in the downstream task of word segmentation and clustering. We observed consistent improvements from bnfs across all metrics in this downstream task, and other work demonstrates that these features are also useful for downstream keyword spotting in settings with very small amounts of labeled data BIBREF45 . We also showed that it is theoretically possible to further improve bnfs with language-specific fine-tuning, and we hope to explore models that can do this more reliably than the cae in the future.</td>\n",
       "      <td>GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus</td>\n",
       "      <td>[GlobalPhone corpus, GlobalPhone\\nCroatian\\nHausa\\nMandarin\\nSpanish\\nSwedish\\nTurkish\\nZRSC\\nBuckeye\\nXitsonga, GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8c46a26f9b0b41c656b5b55142d491600663defa</th>\n",
       "      <td>We start by investigating how unlabeled data from the target language alone can be used for unsupervised subword modeling. Below we first review related work and provide a brief introduction to the cae and vtln methods. We then describe our experiments directly comparing these methods, both alone and in combination.</td>\n",
       "      <td>GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus</td>\n",
       "      <td>[GlobalPhone corpus, GlobalPhone\\nCroatian\\nHausa\\nMandarin\\nSpanish\\nSwedish\\nTurkish\\nZRSC\\nBuckeye\\nXitsonga, GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a398c9b061f28543bc77c2951d0dfc5d1bee9e87</th>\n",
       "      <td>In Table 1, we evaluate our model against the existing state-of-the-art for the dataset used and other models which have employed similar techniques to accomplish the task. It is clear that our proposed model outperforms the previous feature engineering benchmark and other work done in the field both in terms of F1 score and accuracy of detection. Feature engineering models rely on a selection of handcrafted attributes which may not be able to consider all the factors involved in making a post clickbait. The approach proposed in BIBREF8 takes into account each of the textual features available in an individual fashion, considering them to be independent of each other, which is not the case since, by definition of clickbait, the content of the article title and text are not mutually exclusive. BIBREF21 proposed the integration of multimodal embeddings. BIBREF6 utilise word and character embeddings which do not capture morpheme-level information that may incorporate a surprise element.</td>\n",
       "      <td>BIBREF4 crowdsourced the annotation of 19538 tweets they had curated</td>\n",
       "      <td>[A crowdsourced twitter dataset containing 19358 tweets, BIBREF4, 19538 tweets  from BIBREF4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a398c9b061f28543bc77c2951d0dfc5d1bee9e87</th>\n",
       "      <td>The importance of detecting clickbait headlines has increased exponentially in recent years. Initial work in this domain can be traced back to BIBREF2 , relying on heavy feature engineering on a specific news dataset. These works define the various types of clickbait and focus on the presence of linguistic peculiarities in the headline text, including various informality metrics and the use of forward references. Applying such techniques over a social media stream was first attempted by BIBREF3 as the authors crowdsourced a dataset of tweets BIBREF4 and performed feature engineering to accomplish the task. BIBREF5 have tried to expand the work done for news headlines they collected from trusted sources.</td>\n",
       "      <td>BIBREF4 crowdsourced the annotation of 19538 tweets they had curated</td>\n",
       "      <td>[A crowdsourced twitter dataset containing 19358 tweets, BIBREF4, 19538 tweets  from BIBREF4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a398c9b061f28543bc77c2951d0dfc5d1bee9e87</th>\n",
       "      <td>BIBREF6 used the same collection of headlines as BIBREF5 and proposed the first neural network based approach in the field. They employed various recurrent neural network architectures to model sequential data and its dependencies, taking as its inputs a concatenation of the word and character-level embeddings of the headline. Their experiments yielded that bidirectional LSTMs BIBREF7 were best suited for the same. BIBREF8 built BiLSTMs to model each textual attribute of the post (post-text, target-title, target-paragraphs, target-description, target-keywords, post-time) available in the corpus BIBREF4 , concatenating their outputs and feeding it to a fully connected layer to classify the post. Attention mechanisms BIBREF1 have grown popular for various text classification tasks, like aspect based sentiment analysis. Utilising this technique, BIBREF9 deployed a self-attentive bidirectional GRU to infer the importance of each tweet token and model the annotation distribution of headlines in the corpus.</td>\n",
       "      <td>BIBREF4 crowdsourced the annotation of 19538 tweets they had curated</td>\n",
       "      <td>[A crowdsourced twitter dataset containing 19358 tweets, BIBREF4, 19538 tweets  from BIBREF4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a398c9b061f28543bc77c2951d0dfc5d1bee9e87</th>\n",
       "      <td>BIBREF4 crowdsourced the annotation of 19538 tweets they had curated, into various levels of their clickbait-y nature. These tweets contained the title and text of the article and also included supplementary information such as target description, target keywords and linked images. We trained our model over 17000 records in the described dataset and test it over 2538 disjoint instances from the same. We performed our experiments with the aim of increasing the accuracy and F1 score of the model. Other metrics like mean squared error (MSE) were also considered.</td>\n",
       "      <td>BIBREF4 crowdsourced the annotation of 19538 tweets they had curated</td>\n",
       "      <td>[A crowdsourced twitter dataset containing 19358 tweets, BIBREF4, 19538 tweets  from BIBREF4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a398c9b061f28543bc77c2951d0dfc5d1bee9e87</th>\n",
       "      <td>Each record in the dataset has a target description attached with it. This is the entire text of the article whose title has been given. By definition, clickbait articles differ from the content described in their headline. We generate document embeddings for both the title and the article text and perform element wise multiplication over the two. This allows us to capture the interaction between the two, something which has not been used before. Since the title is supposed to mislead the reader with respect to the content, modeling this interaction in terms of their similarity gives an added dimenstion to our approach. It augments the output obtained from the first component.</td>\n",
       "      <td>BIBREF4 crowdsourced the annotation of 19538 tweets they had curated</td>\n",
       "      <td>[A crowdsourced twitter dataset containing 19358 tweets, BIBREF4, 19538 tweets  from BIBREF4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8de0e1fdcca81b49615a6839076f8d42226bf1fe</th>\n",
       "      <td>To evaluate the effectiveness of the proposed algorithm, we have sampled a dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain. The correctness of the rescoring was judged by two annotators, who labeled 250 examples each. The annotators read the whole conversation transcript and listened to the recording to establish whether the rescoring is meaningful. In cases when a rescored word was technically incorrect (e.g., mistaken tense of a verb), but the rescoring led to the recognition of the correct intent, we labeled the intent annotation as correct. The results are shown in Table TABREF22. Please note that every result above 50% indicates an improvement over the ASR best path recognition, since we correct more ASR errors than we introduce new mistakes.</td>\n",
       "      <td>500 rescored intent annotations found in the lattices in cancellations and refunds domain</td>\n",
       "      <td>[500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8de0e1fdcca81b49615a6839076f8d42226bf1fe</th>\n",
       "      <td>The power of some of the best conversational assistants lies in domain-dependent human knowledge. Amazon's Alexa is improving with the user generated data it gathers BIBREF10. Some of the most common human knowledge base structures used in NLP are word lists such as dictionaries for ASR BIBREF11, sentiment lexicons BIBREF12 knowledge graphs such as WordNet BIBREF13, BIBREF14 and ConceptNet BIBREF15. Conceptually, our work is similar to BIBREF16, however, they do not allow for fuzzy search through the lattice.</td>\n",
       "      <td>500 rescored intent annotations found in the lattices in cancellations and refunds domain</td>\n",
       "      <td>[500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8de0e1fdcca81b49615a6839076f8d42226bf1fe</th>\n",
       "      <td>Finite state transducers have been widely used in speech recognition BIBREF36, BIBREF37, BIBREF38, named entity recognition BIBREF39, BIBREF40, morpho-syntactic tagging BIBREF41, BIBREF42, BIBREF43 or language generation BIBREF44.</td>\n",
       "      <td>500 rescored intent annotations found in the lattices in cancellations and refunds domain</td>\n",
       "      <td>[500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8de0e1fdcca81b49615a6839076f8d42226bf1fe</th>\n",
       "      <td>In this section, we present a quantitative analysis of the proposed algorithm. The baseline algorithm annotates only the best ASR hypothesis. We perform the experiments with an intent library comprised of 313 intents in total, each of which is expressed using 169 examples on average. The annotations are performed on more than 70 000 US English phone conversations with an average duration of 11 minutes, but some of them take even over one hour. The topics of these conversations span across several domains, such as inquiry for account information or instructions, refund requests or service cancellations. Each domain uses a relevant subset of the intent library (typically 100-150 intents are active).</td>\n",
       "      <td>500 rescored intent annotations found in the lattices in cancellations and refunds domain</td>\n",
       "      <td>[500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8de0e1fdcca81b49615a6839076f8d42226bf1fe</th>\n",
       "      <td>Various language understanding tasks have been improved in recent years using WCNs: language model learning BIBREF20, ASR improvements BIBREF21, BIBREF22, BIBREF23, classification BIBREF24, BIBREF25, word spotting BIBREF26, BIBREF27, voice search BIBREF28, dialog state tracking BIBREF29 and named entity extraction BIBREF22, BIBREF30, BIBREF31. BIBREF32 modified the WCN approach to include part-of-speech information in order to achieve an improvement in semantic quality of recognized speech.</td>\n",
       "      <td>500 rescored intent annotations found in the lattices in cancellations and refunds domain</td>\n",
       "      <td>[500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9d963d385bd495a7e193f8a498d64c1612e6c20c</th>\n",
       "      <td>We also use the spmrl datasets, a collection of parallel dependency and constituency treebanks for morphologically rich languages BIBREF42 . In this case, we use the predicted PoS tags provided by the organizers. We observed some differences between the constituency and dependency predicted input features provided with the corpora. For experiments where dependency parsing is the main task, we use the input from the dependency file, and the converse for constituency, for comparability with other work. d-mtl models were trained twice (one for each input), and dependency and constituent scores are reported on the model trained on the corresponding input.</td>\n",
       "      <td>spmrl datasets, English Penn Treebank, Stanford dependencies</td>\n",
       "      <td>[English Penn Treebank, spmrl datasets,  English Penn Treebank, spmrl datasets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9d963d385bd495a7e193f8a498d64c1612e6c20c</th>\n",
       "      <td>In the following experiments we use two parallel datasets that provide syntactic analyses for both dependency and constituency parsing.</td>\n",
       "      <td>spmrl datasets, English Penn Treebank, Stanford dependencies</td>\n",
       "      <td>[English Penn Treebank, spmrl datasets,  English Penn Treebank, spmrl datasets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9d963d385bd495a7e193f8a498d64c1612e6c20c</th>\n",
       "      <td>mtl models that use auxiliary tasks (d-mtl-aux) consistently outperform the single-task models (s-s) in all datasets, both for constituency parsing and for dependency parsing in terms of uas. However, this does not extend to las. This different behavior between uas and las seems to be originated by the fact that 2-task dependency parsing models, which are the basis for the corresponding auxiliary task and mtl models, improve uas but not las with respect to single-task dependency parsing models. The reason might be that the single-task setup excludes unlikely combinations of dependency labels with PoS tags or dependency directions that are not found in the training set, while in the 2-task setup, both components are treated separately, which may be having a negative influence on dependency labeling accuracy.</td>\n",
       "      <td>spmrl datasets, English Penn Treebank, Stanford dependencies</td>\n",
       "      <td>[English Penn Treebank, spmrl datasets,  English Penn Treebank, spmrl datasets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9d963d385bd495a7e193f8a498d64c1612e6c20c</th>\n",
       "      <td>For the evaluation on English language we use the English Penn Treebank BIBREF40 , transformed into Stanford dependencies BIBREF41 with the predicted PoS tags as in BIBREF32 .</td>\n",
       "      <td>spmrl datasets, English Penn Treebank, Stanford dependencies</td>\n",
       "      <td>[English Penn Treebank, spmrl datasets,  English Penn Treebank, spmrl datasets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9d963d385bd495a7e193f8a498d64c1612e6c20c</th>\n",
       "      <td>In addition, Table TABREF19 provides a comparison of the d-mtl-aux models for dependency and constituency parsing against existing models on the PTB test set. Tables TABREF20 and TABREF21 shows the results for various existing models on the SPMRL test sets.</td>\n",
       "      <td>spmrl datasets, English Penn Treebank, Stanford dependencies</td>\n",
       "      <td>[English Penn Treebank, spmrl datasets,  English Penn Treebank, spmrl datasets]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               fewshot_evidence  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                                                                                                                                                                                                                                                                                                                                                                            We use the Clueweb09 derived dataset of BIBREF0 for baseline comparison. For cross-temporal, cross-topic and cross-domain training & evaluation, we generate a new dataset based on Wikipedia crawl data. This dataset is gathered by using Wikipedia's `List of Contoversial articles' overview page of 2018 (time of writing) and 2009 (for comparison with baselines) . Using this as a `seed' set of controversial articles, we iteratively crawl the `See also', `References' and `External links' hyperlinks up to two hops from the seed list. The negative seed pages (i.e. non controversial) are gathered by using the random article endpoint. The snowball-sample approach includes general, non-Wikipedia, pages that are referred to from Wikipedia pages. The dataset thus extends beyond just the encyclopedia genre of texts. Labels are assumed to propagate: a page linked from a controversial issue is assumed to be controversial. The resulting dataset statistics are summarized in Table TABREF7 .   \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We explore the potential of RNNs and CNNs for controversy detection using both the HAN BIBREF10 and the CNN BIBREF11 model. Similar to BIBREF10 , each bi-directional GRU cell is set to a dimension of 50, resulting in a word/sentence representation of size 100 after concatenation. The word/sentence attention vectors similarly contain 100 dimensions, all randomly initialized. The word windows defined in the CNN model are set to sizes: 2, 3 and 4 with 128 feature maps each. Each model is trained using mini batches of size 64 and uses both dropout (0.5) and INLINEFORM0 regularization (1e-3) at the dense prediction layer. Both networks use pre-trained embeddings, trained on 100 billion words of a Google News corpus, which are further fine-tuned during training on the controversy dataset. The optimization algorithm used is Adam BIBREF13 (learning rate: 1e-3).   \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Currently, there is no open large-size controversy detection dataset that lends itself to test cross-temporal and cross-topic stability. Thus we generate a Wikipedia crawl-based dataset that includes general web pages and is sufficiently large to train and test high capacity models such as neural networks.   \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Lastly, we examine model performance with respect to human annotation using the human annotated dataset of BIBREF6 . We assume that models that perform similarly to human annotators are preferable. In Table TABREF20 , we present three Spearman correlation metrics to express model congruence with human annotations. Mean annotation expresses the correlation of model error rates with the controversy values attributed to a web page by human annotators, with positive values expressing greater error rates on controversial, and negative expressing higher error rates on non-controversial pages. Here, the HAN shows most unbiased (closest to zero) performance.   \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Table TABREF13 shows the relative performance of the neural models compared to previous controversy detection methods, evaluated on the Clueweb09 derived dataset of BIBREF6 and trained on the Wikipedia data from the same time frame. The TILE-Clique matching model outperforms all other models on Precision although this difference is not significant compared to the neural approaches. Similarly, the language model trained on the DBPedia dataset outperforms other models on Recall but shows no significant difference compared to the CNN model. Notably, the neural approaches show comparable results to the TILE-Clique model in terms of F1, demonstrating a balanced performance in terms of Precision and Recall. Furthermore, the CNN model shows a significant improvement compared to the other non neural baselines in terms of the AUC value (p < 0.05).   \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In BIBREF7 the authors proposes a method to detect and correct ASR output based on Microsoft N-Gram dataset. They use a context-sensitive error correction algorithm for selecting the best candidate for correction using the Microsoft N-Gram dataset which contains real-world data and word sequences extracted from the web which can mimic a comprehensive dictionary of words having a large and all-inclusive vocabulary.   \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Note that this is equivalent to, albiet loosely, learning the error model of a specific ASR. Since we have a small training set, we have used the Naive Bayes classifier that is known to perform well for small datasets with high bias and low variance. We have used the NLTK BIBREF11 Naive Bayes classifier in all our experiments.   \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              We present the results of our experiments with both the Evo-Devo and the Machine Learning mechanisms described earlier using the U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12 .   \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In our experiment, we used a total of 570 misrecognition errors (for example, (dear, beer) and (have, has) derived from INLINEFORM0 or (than twenty, jewelry) derived from INLINEFORM1 ) in the 486 sentences. We performed 10-fold cross validation, each fold containing 513 INLINEFORM2 pairs for training and 57 pairs for testing, Note that we assume the erroneous words in the ASR output being marked by a human oracle, in the training as well as the testing set. Suppose the following example ( INLINEFORM3 ) occurs in the training set: INLINEFORM4 INLINEFORM5    \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e                                                                                                                                                                                                                                                                                                                                                                                                                                               In case of the other ASR engines, namely, Kaldi with US acoustic models (Ku), Kaldi with Indian Acoustic models (Ki) and PocketSphinx ASR (Ps) we first took the queries corresponding to the 250 utterances and built a statistical language model (SLM) and a lexicon using the scripts that are available with PocketSphinx BIBREF14 and Kaldi BIBREF15 . This language model and lexicon was used with the acoustic model that were readily available with Kaldi and Ps. In case of Ku we used the American English acoustic models, while in case of Ki we used the Indian English acoustic model. In case of Ps we used the Voxforge acoustic models BIBREF16 . Each utterance was passed through Kaldi ASR for two different acoustic models to get INLINEFORM0 corresponding to Ku and Ki. Similarly all the 250 audio utterance were passed through the Ps ASR to get the corresponding INLINEFORM1 for Ps. A sample utterance and the output of the four engines is shown in Figure FIGREF12 .   \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              We use the GlobalPhone corpus of speech read from news articles BIBREF20 . We chose 6 languages from different language families as zero-resource languages on which we evaluate the new feature representations. That means our models do not have any access to the transcriptions of the training data, although transcriptions still need to be available to run the evaluation. The selected languages and dataset sizes are shown in Table TABREF8 . Each GlobalPhone language has recordings from around 100 speakers, with 80% of these in the training sets and no speaker overlap between training, development, and test sets.   \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                                                                                                                                                                                                                                                                                                                                                                                        In the previous experiments, we used data from GlobalPhone, which provides corpora collected and formatted similarly for a wide range of languages. However, GlobalPhone is not freely available and no previous zero-resource studies have used these corpora, so in this section we also provide results on the zrsc 2015 BIBREF0 data sets, which have been widely used in other work. The target languages are English (from the Buckeye corpus BIBREF38 ) and Xitsonga (NCHLT corpus BIBREF39 ). Table TABREF8 includes the corpus statistics. These corpora are not split into train/dev/test; since training is unsupervised, the system is simply trained directly on the unlabeled test set (which could also be done in deployment). Importantly, no hyperparameter tuning is done on the Buckeye or Xitsonga data, so these results still provide a useful test of generalization. Notably, the Buckeye English corpus contains conversational speech and is therefore different in style from the rest of our data.   \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             We picked another 10 languages (different from the target languages described in Section SECREF7 ) with a combined 198.3 hours of speech from the GlobalPhone corpus. We consider these as high-resource languages, for which transcriptions are available to train a supervised asr system. The languages and dataset sizes are listed in Table TABREF16 . We also use the English wsj corpus BIBREF35 which is comparable to the GlobalPhone corpus. It contains a total of 81 hours of speech, which we either use in its entirety or from which we use a 15 hour subset; this allows us to compare the effect of increasing the amount of data for one language with training on similar amounts of data but from different languages.   \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa  In this work we investigated different representations obtained using data from the target language alone (i.e., fully unsupervised) and from multilingual supervised systems trained on labeled data from non-target languages. We found that the cae, a recent neural approach to unsupervised subword modeling, learns complementary information to the more traditional approach of vtln. This suggests that vtln should also be considered by other researchers using neural approaches. On the other hand, our best results were achieved using multilingual bnfs. These results are competitive with state-of-the-art features learned from target language data only BIBREF17 , BIBREF18 , but have the advantage of a much smaller dimensionality. In addition, it is easy to control the dimensionality of the bnfs, unlike in the nonparametric models of BIBREF17 , BIBREF18 , and this allowed us to use them in the downstream task of word segmentation and clustering. We observed consistent improvements from bnfs across all metrics in this downstream task, and other work demonstrates that these features are also useful for downstream keyword spotting in settings with very small amounts of labeled data BIBREF45 . We also showed that it is theoretically possible to further improve bnfs with language-specific fine-tuning, and we hope to explore models that can do this more reliably than the cae in the future.   \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          We start by investigating how unlabeled data from the target language alone can be used for unsupervised subword modeling. Below we first review related work and provide a brief introduction to the cae and vtln methods. We then describe our experiments directly comparing these methods, both alone and in combination.   \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                                                                                                                                                                                                                                                                                                                                                                                 In Table 1, we evaluate our model against the existing state-of-the-art for the dataset used and other models which have employed similar techniques to accomplish the task. It is clear that our proposed model outperforms the previous feature engineering benchmark and other work done in the field both in terms of F1 score and accuracy of detection. Feature engineering models rely on a selection of handcrafted attributes which may not be able to consider all the factors involved in making a post clickbait. The approach proposed in BIBREF8 takes into account each of the textual features available in an individual fashion, considering them to be independent of each other, which is not the case since, by definition of clickbait, the content of the article title and text are not mutually exclusive. BIBREF21 proposed the integration of multimodal embeddings. BIBREF6 utilise word and character embeddings which do not capture morpheme-level information that may incorporate a surprise element.   \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The importance of detecting clickbait headlines has increased exponentially in recent years. Initial work in this domain can be traced back to BIBREF2 , relying on heavy feature engineering on a specific news dataset. These works define the various types of clickbait and focus on the presence of linguistic peculiarities in the headline text, including various informality metrics and the use of forward references. Applying such techniques over a social media stream was first attempted by BIBREF3 as the authors crowdsourced a dataset of tweets BIBREF4 and performed feature engineering to accomplish the task. BIBREF5 have tried to expand the work done for news headlines they collected from trusted sources.   \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                                                                                                                                                                                                                                                                                                                                                               BIBREF6 used the same collection of headlines as BIBREF5 and proposed the first neural network based approach in the field. They employed various recurrent neural network architectures to model sequential data and its dependencies, taking as its inputs a concatenation of the word and character-level embeddings of the headline. Their experiments yielded that bidirectional LSTMs BIBREF7 were best suited for the same. BIBREF8 built BiLSTMs to model each textual attribute of the post (post-text, target-title, target-paragraphs, target-description, target-keywords, post-time) available in the corpus BIBREF4 , concatenating their outputs and feeding it to a fully connected layer to classify the post. Attention mechanisms BIBREF1 have grown popular for various text classification tasks, like aspect based sentiment analysis. Utilising this technique, BIBREF9 deployed a self-attentive bidirectional GRU to infer the importance of each tweet token and model the annotation distribution of headlines in the corpus.   \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  BIBREF4 crowdsourced the annotation of 19538 tweets they had curated, into various levels of their clickbait-y nature. These tweets contained the title and text of the article and also included supplementary information such as target description, target keywords and linked images. We trained our model over 17000 records in the described dataset and test it over 2538 disjoint instances from the same. We performed our experiments with the aim of increasing the accuracy and F1 score of the model. Other metrics like mean squared error (MSE) were also considered.   \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Each record in the dataset has a target description attached with it. This is the entire text of the article whose title has been given. By definition, clickbait articles differ from the content described in their headline. We generate document embeddings for both the title and the article text and perform element wise multiplication over the two. This allows us to capture the interaction between the two, something which has not been used before. Since the title is supposed to mislead the reader with respect to the content, modeling this interaction in terms of their similarity gives an added dimenstion to our approach. It augments the output obtained from the first component.   \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             To evaluate the effectiveness of the proposed algorithm, we have sampled a dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain. The correctness of the rescoring was judged by two annotators, who labeled 250 examples each. The annotators read the whole conversation transcript and listened to the recording to establish whether the rescoring is meaningful. In cases when a rescored word was technically incorrect (e.g., mistaken tense of a verb), but the rescoring led to the recognition of the correct intent, we labeled the intent annotation as correct. The results are shown in Table TABREF22. Please note that every result above 50% indicates an improvement over the ASR best path recognition, since we correct more ASR errors than we introduce new mistakes.   \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The power of some of the best conversational assistants lies in domain-dependent human knowledge. Amazon's Alexa is improving with the user generated data it gathers BIBREF10. Some of the most common human knowledge base structures used in NLP are word lists such as dictionaries for ASR BIBREF11, sentiment lexicons BIBREF12 knowledge graphs such as WordNet BIBREF13, BIBREF14 and ConceptNet BIBREF15. Conceptually, our work is similar to BIBREF16, however, they do not allow for fuzzy search through the lattice.   \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Finite state transducers have been widely used in speech recognition BIBREF36, BIBREF37, BIBREF38, named entity recognition BIBREF39, BIBREF40, morpho-syntactic tagging BIBREF41, BIBREF42, BIBREF43 or language generation BIBREF44.   \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In this section, we present a quantitative analysis of the proposed algorithm. The baseline algorithm annotates only the best ASR hypothesis. We perform the experiments with an intent library comprised of 313 intents in total, each of which is expressed using 169 examples on average. The annotations are performed on more than 70 000 US English phone conversations with an average duration of 11 minutes, but some of them take even over one hour. The topics of these conversations span across several domains, such as inquiry for account information or instructions, refund requests or service cancellations. Each domain uses a relevant subset of the intent library (typically 100-150 intents are active).   \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Various language understanding tasks have been improved in recent years using WCNs: language model learning BIBREF20, ASR improvements BIBREF21, BIBREF22, BIBREF23, classification BIBREF24, BIBREF25, word spotting BIBREF26, BIBREF27, voice search BIBREF28, dialog state tracking BIBREF29 and named entity extraction BIBREF22, BIBREF30, BIBREF31. BIBREF32 modified the WCN approach to include part-of-speech information in order to achieve an improvement in semantic quality of recognized speech.   \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    We also use the spmrl datasets, a collection of parallel dependency and constituency treebanks for morphologically rich languages BIBREF42 . In this case, we use the predicted PoS tags provided by the organizers. We observed some differences between the constituency and dependency predicted input features provided with the corpora. For experiments where dependency parsing is the main task, we use the input from the dependency file, and the converse for constituency, for comparability with other work. d-mtl models were trained twice (one for each input), and dependency and constituent scores are reported on the model trained on the corresponding input.   \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                In the following experiments we use two parallel datasets that provide syntactic analyses for both dependency and constituency parsing.   \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     mtl models that use auxiliary tasks (d-mtl-aux) consistently outperform the single-task models (s-s) in all datasets, both for constituency parsing and for dependency parsing in terms of uas. However, this does not extend to las. This different behavior between uas and las seems to be originated by the fact that 2-task dependency parsing models, which are the basis for the corresponding auxiliary task and mtl models, improve uas but not las with respect to single-task dependency parsing models. The reason might be that the single-task setup excludes unlikely combinations of dependency labels with PoS tags or dependency directions that are not found in the training set, while in the 2-task setup, both components are treated separately, which may be having a negative influence on dependency labeling accuracy.   \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        For the evaluation on English language we use the English Penn Treebank BIBREF40 , transformed into Stanford dependencies BIBREF41 with the predicted PoS tags as in BIBREF32 .   \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In addition, Table TABREF19 provides a comparison of the d-mtl-aux models for dependency and constituency parsing against existing models on the PTB test set. Tables TABREF20 and TABREF21 shows the results for various existing models on the SPMRL test sets.   \n",
       "\n",
       "                                                                                                                                                    fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                                     \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                             Clueweb09 derived dataset and Wikipedia crawl data   \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                             Clueweb09 derived dataset and Wikipedia crawl data   \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                             Clueweb09 derived dataset and Wikipedia crawl data   \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                             Clueweb09 derived dataset and Wikipedia crawl data   \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                             Clueweb09 derived dataset and Wikipedia crawl data   \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e  U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013   \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e  U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013   \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e  U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013   \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e  U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013   \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e  U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013   \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus   \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus   \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus   \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus   \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                GlobalPhone corpus, zrsc 2015, Buckeye corpus, NCHLT corpus, English wsj corpus   \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                           BIBREF4 crowdsourced the annotation of 19538 tweets they had curated   \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                           BIBREF4 crowdsourced the annotation of 19538 tweets they had curated   \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                           BIBREF4 crowdsourced the annotation of 19538 tweets they had curated   \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                           BIBREF4 crowdsourced the annotation of 19538 tweets they had curated   \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                           BIBREF4 crowdsourced the annotation of 19538 tweets they had curated   \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                      500 rescored intent annotations found in the lattices in cancellations and refunds domain   \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                      500 rescored intent annotations found in the lattices in cancellations and refunds domain   \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                      500 rescored intent annotations found in the lattices in cancellations and refunds domain   \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                      500 rescored intent annotations found in the lattices in cancellations and refunds domain   \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                      500 rescored intent annotations found in the lattices in cancellations and refunds domain   \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                   spmrl datasets, English Penn Treebank, Stanford dependencies   \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                   spmrl datasets, English Penn Treebank, Stanford dependencies   \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                   spmrl datasets, English Penn Treebank, Stanford dependencies   \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                   spmrl datasets, English Penn Treebank, Stanford dependencies   \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                   spmrl datasets, English Penn Treebank, Stanford dependencies   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                   gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                                                                                                                                                              [Clueweb09, Clueweb09 derived dataset, new dataset based on Wikipedia crawl data, the Clueweb09 derived dataset , dataset based on Wikipedia crawl data, Clueweb09 derived dataset, Wikipedia crawl data]  \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                                                                                                                                                              [Clueweb09, Clueweb09 derived dataset, new dataset based on Wikipedia crawl data, the Clueweb09 derived dataset , dataset based on Wikipedia crawl data, Clueweb09 derived dataset, Wikipedia crawl data]  \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                                                                                                                                                              [Clueweb09, Clueweb09 derived dataset, new dataset based on Wikipedia crawl data, the Clueweb09 derived dataset , dataset based on Wikipedia crawl data, Clueweb09 derived dataset, Wikipedia crawl data]  \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                                                                                                                                                              [Clueweb09, Clueweb09 derived dataset, new dataset based on Wikipedia crawl data, the Clueweb09 derived dataset , dataset based on Wikipedia crawl data, Clueweb09 derived dataset, Wikipedia crawl data]  \n",
       "1ecbbb60dc44a701e9c57c22167dd412711bb0be                                                                                                                                                                                                              [Clueweb09, Clueweb09 derived dataset, new dataset based on Wikipedia crawl data, the Clueweb09 derived dataset , dataset based on Wikipedia crawl data, Clueweb09 derived dataset, Wikipedia crawl data]  \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e  [Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013,  survey data and hand crafted a total of 293 textual questions BIBREF13, U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013, Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12]  \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e  [Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013,  survey data and hand crafted a total of 293 textual questions BIBREF13, U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013, Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12]  \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e  [Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013,  survey data and hand crafted a total of 293 textual questions BIBREF13, U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013, Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12]  \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e  [Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013,  survey data and hand crafted a total of 293 textual questions BIBREF13, U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013, Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12]  \n",
       "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e  [Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013,  survey data and hand crafted a total of 293 textual questions BIBREF13, U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013, Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12]  \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                                                                                                                                                                                                 [GlobalPhone corpus, GlobalPhone\\nCroatian\\nHausa\\nMandarin\\nSpanish\\nSwedish\\nTurkish\\nZRSC\\nBuckeye\\nXitsonga, GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus]  \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                                                                                                                                                                                                 [GlobalPhone corpus, GlobalPhone\\nCroatian\\nHausa\\nMandarin\\nSpanish\\nSwedish\\nTurkish\\nZRSC\\nBuckeye\\nXitsonga, GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus]  \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                                                                                                                                                                                                 [GlobalPhone corpus, GlobalPhone\\nCroatian\\nHausa\\nMandarin\\nSpanish\\nSwedish\\nTurkish\\nZRSC\\nBuckeye\\nXitsonga, GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus]  \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                                                                                                                                                                                                 [GlobalPhone corpus, GlobalPhone\\nCroatian\\nHausa\\nMandarin\\nSpanish\\nSwedish\\nTurkish\\nZRSC\\nBuckeye\\nXitsonga, GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus]  \n",
       "8c46a26f9b0b41c656b5b55142d491600663defa                                                                                                                                                                                                                                 [GlobalPhone corpus, GlobalPhone\\nCroatian\\nHausa\\nMandarin\\nSpanish\\nSwedish\\nTurkish\\nZRSC\\nBuckeye\\nXitsonga, GlobalPhone corpus, English wsj corpus, Buckeye corpus, NCHLT corpus]  \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                                                                                                                                                                                                                                                                                          [A crowdsourced twitter dataset containing 19358 tweets, BIBREF4, 19538 tweets  from BIBREF4]  \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                                                                                                                                                                                                                                                                                          [A crowdsourced twitter dataset containing 19358 tweets, BIBREF4, 19538 tweets  from BIBREF4]  \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                                                                                                                                                                                                                                                                                          [A crowdsourced twitter dataset containing 19358 tweets, BIBREF4, 19538 tweets  from BIBREF4]  \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                                                                                                                                                                                                                                                                                          [A crowdsourced twitter dataset containing 19358 tweets, BIBREF4, 19538 tweets  from BIBREF4]  \n",
       "a398c9b061f28543bc77c2951d0dfc5d1bee9e87                                                                                                                                                                                                                                                                                                                          [A crowdsourced twitter dataset containing 19358 tweets, BIBREF4, 19538 tweets  from BIBREF4]  \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                                                                                                [500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain]  \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                                                                                                [500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain]  \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                                                                                                [500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain]  \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                                                                                                [500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain]  \n",
       "8de0e1fdcca81b49615a6839076f8d42226bf1fe                                                                                                                [500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain, dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain]  \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                                                                                                                                                                                                                                                                                        [English Penn Treebank, spmrl datasets,  English Penn Treebank, spmrl datasets]  \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                                                                                                                                                                                                                                                                                        [English Penn Treebank, spmrl datasets,  English Penn Treebank, spmrl datasets]  \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                                                                                                                                                                                                                                                                                        [English Penn Treebank, spmrl datasets,  English Penn Treebank, spmrl datasets]  \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                                                                                                                                                                                                                                                                                        [English Penn Treebank, spmrl datasets,  English Penn Treebank, spmrl datasets]  \n",
       "9d963d385bd495a7e193f8a498d64c1612e6c20c                                                                                                                                                                                                                                                                                                                                        [English Penn Treebank, spmrl datasets,  English Penn Treebank, spmrl datasets]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  what are the baselines?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fewshot_evidence</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ebe1084a06abdabefffc66f029eeb0b69f114fd9</th>\n",
       "      <td>baseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus. It operates on the sentence level and does not see any additional context. The input and output embeddings of the decoder are tied, encoder embeddings are not.</td>\n",
       "      <td>a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline</td>\n",
       "      <td>[bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21,  standard bidirectional RNN model with attention, A standard context-agnostic Transformer, standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ebe1084a06abdabefffc66f029eeb0b69f114fd9</th>\n",
       "      <td>baseline A standard context-agnostic Transformer. All model parameters are identical to a Transformer-base in BIBREF2 .</td>\n",
       "      <td>a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline</td>\n",
       "      <td>[bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21,  standard bidirectional RNN model with attention, A standard context-agnostic Transformer, standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ebe1084a06abdabefffc66f029eeb0b69f114fd9</th>\n",
       "      <td>We consider the following recurrent baselines:</td>\n",
       "      <td>a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline</td>\n",
       "      <td>[bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21,  standard bidirectional RNN model with attention, A standard context-agnostic Transformer, standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ebe1084a06abdabefffc66f029eeb0b69f114fd9</th>\n",
       "      <td>It is unfortunate, then, that current NMT systems generally operate on the sentence level BIBREF2 , BIBREF3 , BIBREF4 . Documents are translated sentence-by-sentence for practical reasons, such as line-based processing in a pipeline and reduced computational complexity. Furthermore, improvements of larger-context models over baselines in terms of document-level metrics such as BLEU or RIBES have been moderate, so that their computational overhead does not seem justified, and so that it is hard to develop more effective context-aware architectures and empirically validate them.</td>\n",
       "      <td>a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline</td>\n",
       "      <td>[bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21,  standard bidirectional RNN model with attention, A standard context-agnostic Transformer, standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ebe1084a06abdabefffc66f029eeb0b69f114fd9</th>\n",
       "      <td>Table TABREF32 shows that context-aware models perform better than the baseline when the antecedent is outside the current sentence. In our experiments, all context-aware models consider one preceding sentence as context. The evaluation according to the distance of the antecedent in Table TABREF35 confirms that the subset of sentences with antecedent distance 1 benefits most from the tested context-aware models (up to +20 percentage points accuracy). However, we note two surprising patterns:</td>\n",
       "      <td>a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline</td>\n",
       "      <td>[bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21,  standard bidirectional RNN model with attention, A standard context-agnostic Transformer, standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eb653a5c59851eda313ece0bcd8c589b6155d73e</th>\n",
       "      <td>The baseline we use for comparison is a one-stage RNN system, the RNN structure is the same as the last stage containing 2-layer BLSTM and directly trained to recognize dialect category. In the process of evaluation, we compute the accuracy of the two sub-tasks and the whole test set to evaluate the performance of each system.</td>\n",
       "      <td>a one-stage RNN system</td>\n",
       "      <td>[one-stage RNN system containing 2-layer BLSTM, one-stage RNN system, a one-stage RNN system]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eb653a5c59851eda313ece0bcd8c589b6155d73e</th>\n",
       "      <td>These two multi-stage systems both much outperform the baseline system. They learn acoustic and language knowledge successively, indicating that language and phoneme are features of different levels, so we have to train step by step to avoid the networks “forget\" some knowledge. Through the process, we can find the rules of multi-task and multi-stage training, if the labels are in different levels then multi-stage training should be used such as the situation in our paper, otherwise multi-task training should be used for parallel learning a wide range of knowledge.</td>\n",
       "      <td>a one-stage RNN system</td>\n",
       "      <td>[one-stage RNN system containing 2-layer BLSTM, one-stage RNN system, a one-stage RNN system]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eb653a5c59851eda313ece0bcd8c589b6155d73e</th>\n",
       "      <td>First of all, we compare the two-stage system and the three-stage system trained with phonetic sequence annotation and dialect category label with the baseline trained only with dialect category label. The two multi-stage system have the same ResNet14 architecture and use 2-layer BLSTM as the RNN part with 256 nodes. From the results in the Table TABREF20 , we can see that the relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline and the two-stage system performs best. We also observe that both two multi-stage systems perform excellently in long duration ( INLINEFORM0 3s) task and the two-stage system illustrates its advantageous and robustness in short duration ( INLINEFORM1 3s) task.</td>\n",
       "      <td>a one-stage RNN system</td>\n",
       "      <td>[one-stage RNN system containing 2-layer BLSTM, one-stage RNN system, a one-stage RNN system]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eb653a5c59851eda313ece0bcd8c589b6155d73e</th>\n",
       "      <td>where INLINEFORM0 is the ground truth label and INLINEFORM1 is the output probability distribution.</td>\n",
       "      <td>a one-stage RNN system</td>\n",
       "      <td>[one-stage RNN system containing 2-layer BLSTM, one-stage RNN system, a one-stage RNN system]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eb653a5c59851eda313ece0bcd8c589b6155d73e</th>\n",
       "      <td>Recently, the use of deep neural network (DNN) has been explored in LID tasks. The DNN is trained to discriminate individual physical states of a tied-state triphone and then extract the bottleneck features to a back-end system for classification BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . End-to-end frameworks based on DNN later are trained for LID BIBREF4 . Other network architectures are successfully applied to LID task, example for convolutional neural network (CNN) BIBREF5 , BIBREF6 , time delay neural network (TDNN) BIBREF7 , RNN BIBREF8 , BIBREF9 , BIBREF10 , and BIBREF11 has a CNN followed by an RNN structure, which is similar to ours. They predict the final category of an utterance directly by the last fully connected layer, or derive the results by averaging the the frame-level posteriors. These frameworks just trained end-to-end to recognize languages, but they do not consider the phonetic information concretely.</td>\n",
       "      <td>a one-stage RNN system</td>\n",
       "      <td>[one-stage RNN system containing 2-layer BLSTM, one-stage RNN system, a one-stage RNN system]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634a071b13eb7139e77872ecfdc135a2eb2f89da</th>\n",
       "      <td>Naive Bayes (NB) Estimator: A Naive Bayes model was learned, with a bag-of-words feature set, using the word counts in the sentences of our training data – the contexts of the controversial and non-controversial concepts. The controversiality score of a concept $c$ for its occurrence in a sentence $s$, is taken as the posterior probability (according to the NB model) of $s$ to contain a controversial concept, given the words of $s$ excluding $c$, and taking a prior of $0.5$ for controversiality (as is the case in the datasets). The controversiality score of $c$ is then defined as the average score over all sentences referencing $c$.</td>\n",
       "      <td>The methodology suggested by BIBREF1, BIBREF4</td>\n",
       "      <td>[Nearest neighbors (NN) Estimator, Naive Bayes (NB) Estimator, Recurrent neural network (RNN), Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016).]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634a071b13eb7139e77872ecfdc135a2eb2f89da</th>\n",
       "      <td>Focusing here on Wikipedia concepts, we adopt as an initial “ground truth” the titles listed on the Wikipedia list of controversial issues, which is curated based on so-called “edit wars”. We then manually annotate a set of Wikipedia titles which are locked for editing, and evaluate our system on this much larger and more challenging dataset.</td>\n",
       "      <td>The methodology suggested by BIBREF1, BIBREF4</td>\n",
       "      <td>[Nearest neighbors (NN) Estimator, Naive Bayes (NB) Estimator, Recurrent neural network (RNN), Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016).]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634a071b13eb7139e77872ecfdc135a2eb2f89da</th>\n",
       "      <td>Analysis of controversy in Wikipedia, online news and social media has attracted considerable attention in recent years. Exploiting the collaborative structure of Wikipedia, estimators of the level of controversy in a Wikipedia article were developed based on the edit-history of the article BIBREF0, BIBREF3. Along these lines, BIBREF4 detect controversy based on mutual reverts, bi-polarity in the collaboration network, and mutually-reinforced scores for editors and articles. Similarly, BIBREF1 classify whether a Wikipedia page is controversial through the combined evaluation of the topically neighboring set of pages.</td>\n",
       "      <td>The methodology suggested by BIBREF1, BIBREF4</td>\n",
       "      <td>[Nearest neighbors (NN) Estimator, Naive Bayes (NB) Estimator, Recurrent neural network (RNN), Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016).]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634a071b13eb7139e77872ecfdc135a2eb2f89da</th>\n",
       "      <td>Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives. Over this dataset, we compare the methodology suggested here to those reported by BIBREF1, BIBREF4. As the latter report overall accuracy of their binary prediction, we convert our controversiality estimates to a binary classification by classifying the higher-scored half as controversial, and the lower half as non-controversial.</td>\n",
       "      <td>The methodology suggested by BIBREF1, BIBREF4</td>\n",
       "      <td>[Nearest neighbors (NN) Estimator, Naive Bayes (NB) Estimator, Recurrent neural network (RNN), Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016).]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634a071b13eb7139e77872ecfdc135a2eb2f89da</th>\n",
       "      <td>We consider three datasets, two of which are a contribution of this work.</td>\n",
       "      <td>The methodology suggested by BIBREF1, BIBREF4</td>\n",
       "      <td>[Nearest neighbors (NN) Estimator, Naive Bayes (NB) Estimator, Recurrent neural network (RNN), Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016).]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8cf52ba480d372fc15024b3db704952f10fdca27</th>\n",
       "      <td>On Douban conversation corpus, FastText BIBREF7 pre-trained Chinese embedding vectors are used in ESIM + enhanced word vector whereas word2vec generated on training set is used in baseline model (ESIM). It can be seen from table TABREF23 that character embedding enhances the performance of original ESIM. Enhanced Word representation in algorithm SECREF12 improves the performance further and has shown that the proposed method is effective. Most models (RNN, CNN, LSTM, BiLSTM, Dual-Encoder) which encode the whole context (or response) into compact vectors before matching do not perform well. INLINEFORM0 directly models sequential structure of multi utterances in context and achieves good performance whereas ESIM implicitly makes use of end-of-utterance(__eou__) and end-of-turn (__eot__) token tags as shown in subsection SECREF41 .</td>\n",
       "      <td>RNN, CNN, LSTM, BiLSTM, Dual-Encoder</td>\n",
       "      <td>[ESIM, ESIM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8cf52ba480d372fc15024b3db704952f10fdca27</th>\n",
       "      <td>The rest paper is organized as follows. In Section SECREF2 , we review the related work. In Section SECREF3 we provide an overview of ESIM (baseline) model and describe our methods to address out-of-vocabulary issues. In Section SECREF4 , we conduct extensive experiments to show the effectiveness of the proposed method. Finally we conclude with remarks and summarize our findings and outline future research directions.</td>\n",
       "      <td>RNN, CNN, LSTM, BiLSTM, Dual-Encoder</td>\n",
       "      <td>[ESIM, ESIM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8cf52ba480d372fc15024b3db704952f10fdca27</th>\n",
       "      <td>In this paper, we generate word embedding vectors on the training corpus based on word2vec BIBREF9 . Then we propose an algorithm to combine the generated one with the pre-trained word embedding vectors on a large general text corpus based on vector concatenation. The new word representation maintains information learned from both general text corpus and task-domain. The nice property of the algorithm is simplicity and little extra computational cost will be added. It can address word out-of-vocabulary issue effectively. This method can be applied to most NLP deep neural network models and is language-independent. We integrated our methods with ESIM(baseline model) BIBREF10 . The experimental results have shown that the proposed method has significantly improved the performance of original ESIM model and obtained state-of-the-art results on both Ubuntu Dialogue Corpus and Douban Conversation Corpus BIBREF11 . On Ubuntu Dialogue Corpus (V2), the improvement to the previous best baseline model (single) on INLINEFORM0 is 3.8% and our ensemble model on INLINEFORM1 is 75.9%. On Douban Conversation Corpus, the improvement to the previous best model (single) on INLINEFORM2 is 3.6%.</td>\n",
       "      <td>RNN, CNN, LSTM, BiLSTM, Dual-Encoder</td>\n",
       "      <td>[ESIM, ESIM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8cf52ba480d372fc15024b3db704952f10fdca27</th>\n",
       "      <td>Character-level representation has been widely used in information retrieval, tagging, language modeling and question answering. BIBREF12 represented a word based on character trigram in convolution neural network for web-search ranking. BIBREF7 represented a word by the sum of the vector representation of character n-gram. Santos et al BIBREF13 , BIBREF14 and BIBREF8 used convolution neural network to generate character-level representation (embedding) of a word. The former combined both word-level and character-level representation for part-of-speech and name entity tagging tasks while the latter used only character-level representation for language modeling. BIBREF15 employed a deep bidirectional GRU network to learn character-level representation and then concatenated word-level and character-level representation vectors together. BIBREF16 used a fine-grained gating mechanism to combine the word-level and character-level representation for reading comprehension. Character-level representation can help address out-of-vocabulary issue to some extent for western languages, which is mainly used to capture character ngram similarity.</td>\n",
       "      <td>RNN, CNN, LSTM, BiLSTM, Dual-Encoder</td>\n",
       "      <td>[ESIM, ESIM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8cf52ba480d372fc15024b3db704952f10fdca27</th>\n",
       "      <td>Ubuntu dialogue corpus BIBREF5 is the public largest unstructured multi-turns dialogue corpus which consists of about one-million two-person conversations. The size of the corpus makes it attractive for the exploration of deep neural network modeling in the context of dialogue systems. Most deep neural networks use word embedding as the first layer. They either use fixed pre-trained word embedding vectors generated on a large text corpus or learn word embedding for the specific task. The former is lack of flexibility of domain adaptation. The latter requires a very large training corpus and significantly increases model training time. Word out-of-vocabulary issue occurs for both cases. Ubuntu dialogue corpus also contains many technical words (e.g. “ctrl+alt+f1\", “/dev/sdb1\"). The ubuntu corpus (V2) contains 823057 unique tokens whereas only 22% tokens occur in the pre-built GloVe word vectors. Although character-level representation which models sub-word morphologies can alleviate this problem to some extent BIBREF6 , BIBREF7 , BIBREF8 , character-level representation still have limitations: learn only morphological and orthographic similarity, other than semantic similarity (e.g. `car' and `bmw') and it cannot be applied to Asian languages (e.g. Chinese characters).</td>\n",
       "      <td>RNN, CNN, LSTM, BiLSTM, Dual-Encoder</td>\n",
       "      <td>[ESIM, ESIM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726c5c1b6951287f4bae22978f9a91d22d9bef61</th>\n",
       "      <td>In future work, the groundwork laid here may be applied to larger data sets, and may help motivate the development of such data. Larger noisy data sets would enable the differentiable constraints and weighted aggregation to be included during the optimization, and tuned with respect to data. In addition, we find the incorporation of graphical model inference into neural architectures to be a powerful new tool, and potentially an important step towards incorporating higher-level reasoning and prior knowledge into neural models of NLP.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Reschke CRF, Reschke Noisy-OR, Reschke Best, Reschke CRF, Reschke Noisy-OR, Reschke Best]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726c5c1b6951287f4bae22978f9a91d22d9bef61</th>\n",
       "      <td>For each mention INLINEFORM0 we construct a representation INLINEFORM1 of the mention in its context. This representation functions as a general “reading” or encoding of the mention, irrespective of the type of slots for which it will later be considered. This differs from some previous machine reading research where the model provides a query-specific reading of the document, or reads the document multiple times when answering a single query BIBREF0 . As in previous work, an embedding of a mention's context serves as its representation. We construct an embedding matrix INLINEFORM2 , using pre-trained word embeddings, where INLINEFORM3 is the dimensionality of the embeddings and INLINEFORM4 the number of words in the cluster. These are held fixed during training. All mentions are masked and receive the same one-hot vector in place of a pretrained embedding. From this matrix we embed the context using a two-layer convolutional neural network (CNN), with a detailed discussion of the architecture parameters provided in Section SECREF4 . CNNs have been used in a similar manner for a number of information extraction and classification tasks BIBREF6 , BIBREF7 and are capable of producing rich sentence representations BIBREF8 .</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Reschke CRF, Reschke Noisy-OR, Reschke Best, Reschke CRF, Reschke Noisy-OR, Reschke Best]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726c5c1b6951287f4bae22978f9a91d22d9bef61</th>\n",
       "      <td>INLINEFORM0</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Reschke CRF, Reschke Noisy-OR, Reschke Best, Reschke CRF, Reschke Noisy-OR, Reschke Best]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726c5c1b6951287f4bae22978f9a91d22d9bef61</th>\n",
       "      <td>Each of these models uses features drawn from dependency trees, local context (unigram/part-of-speech features for up to 5 neighboring words), sentence context (bag-of-word/part-of-speech), words/part-of-speech of words occurring within the value, as well as the entity type of the mention itself.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Reschke CRF, Reschke Noisy-OR, Reschke Best, Reschke CRF, Reschke Noisy-OR, Reschke Best]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726c5c1b6951287f4bae22978f9a91d22d9bef61</th>\n",
       "      <td>We hypothesize that the earliest articles in each cluster are the most likely to contain misinformation, which we explore via a measure of information content. We define the information content of an article as the number of correct values which it mentions. Using this measure, we fit a skewed Gaussian distribution over the ordered news articles, assigning INLINEFORM0 , where INLINEFORM1 is the smoothed information content of INLINEFORM2 as drawn from the Gaussian.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Reschke CRF, Reschke Noisy-OR, Reschke Best, Reschke CRF, Reschke Noisy-OR, Reschke Best]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  fewshot_evidence  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   baseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus. It operates on the sentence level and does not see any additional context. The input and output embeddings of the decoder are tied, encoder embeddings are not.   \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   baseline A standard context-agnostic Transformer. All model parameters are identical to a Transformer-base in BIBREF2 .   \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We consider the following recurrent baselines:   \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   It is unfortunate, then, that current NMT systems generally operate on the sentence level BIBREF2 , BIBREF3 , BIBREF4 . Documents are translated sentence-by-sentence for practical reasons, such as line-based processing in a pipeline and reduced computational complexity. Furthermore, improvements of larger-context models over baselines in terms of document-level metrics such as BLEU or RIBES have been moderate, so that their computational overhead does not seem justified, and so that it is hard to develop more effective context-aware architectures and empirically validate them.   \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Table TABREF32 shows that context-aware models perform better than the baseline when the antecedent is outside the current sentence. In our experiments, all context-aware models consider one preceding sentence as context. The evaluation according to the distance of the antecedent in Table TABREF35 confirms that the subset of sentences with antecedent distance 1 benefits most from the tested context-aware models (up to +20 percentage points accuracy). However, we note two surprising patterns:   \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The baseline we use for comparison is a one-stage RNN system, the RNN structure is the same as the last stage containing 2-layer BLSTM and directly trained to recognize dialect category. In the process of evaluation, we compute the accuracy of the two sub-tasks and the whole test set to evaluate the performance of each system.   \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               These two multi-stage systems both much outperform the baseline system. They learn acoustic and language knowledge successively, indicating that language and phoneme are features of different levels, so we have to train step by step to avoid the networks “forget\" some knowledge. Through the process, we can find the rules of multi-task and multi-stage training, if the labels are in different levels then multi-stage training should be used such as the situation in our paper, otherwise multi-task training should be used for parallel learning a wide range of knowledge.   \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           First of all, we compare the two-stage system and the three-stage system trained with phonetic sequence annotation and dialect category label with the baseline trained only with dialect category label. The two multi-stage system have the same ResNet14 architecture and use 2-layer BLSTM as the RNN part with 256 nodes. From the results in the Table TABREF20 , we can see that the relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline and the two-stage system performs best. We also observe that both two multi-stage systems perform excellently in long duration ( INLINEFORM0 3s) task and the two-stage system illustrates its advantageous and robustness in short duration ( INLINEFORM1 3s) task.   \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       where INLINEFORM0 is the ground truth label and INLINEFORM1 is the output probability distribution.   \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                                                                                                                                                                                                                                                                                     Recently, the use of deep neural network (DNN) has been explored in LID tasks. The DNN is trained to discriminate individual physical states of a tied-state triphone and then extract the bottleneck features to a back-end system for classification BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . End-to-end frameworks based on DNN later are trained for LID BIBREF4 . Other network architectures are successfully applied to LID task, example for convolutional neural network (CNN) BIBREF5 , BIBREF6 , time delay neural network (TDNN) BIBREF7 , RNN BIBREF8 , BIBREF9 , BIBREF10 , and BIBREF11 has a CNN followed by an RNN structure, which is similar to ours. They predict the final category of an utterance directly by the last fully connected layer, or derive the results by averaging the the frame-level posteriors. These frameworks just trained end-to-end to recognize languages, but they do not consider the phonetic information concretely.   \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Naive Bayes (NB) Estimator: A Naive Bayes model was learned, with a bag-of-words feature set, using the word counts in the sentences of our training data – the contexts of the controversial and non-controversial concepts. The controversiality score of a concept $c$ for its occurrence in a sentence $s$, is taken as the posterior probability (according to the NB model) of $s$ to contain a controversial concept, given the words of $s$ excluding $c$, and taking a prior of $0.5$ for controversiality (as is the case in the datasets). The controversiality score of $c$ is then defined as the average score over all sentences referencing $c$.   \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Focusing here on Wikipedia concepts, we adopt as an initial “ground truth” the titles listed on the Wikipedia list of controversial issues, which is curated based on so-called “edit wars”. We then manually annotate a set of Wikipedia titles which are locked for editing, and evaluate our system on this much larger and more challenging dataset.   \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Analysis of controversy in Wikipedia, online news and social media has attracted considerable attention in recent years. Exploiting the collaborative structure of Wikipedia, estimators of the level of controversy in a Wikipedia article were developed based on the edit-history of the article BIBREF0, BIBREF3. Along these lines, BIBREF4 detect controversy based on mutual reverts, bi-polarity in the collaboration network, and mutually-reinforced scores for editors and articles. Similarly, BIBREF1 classify whether a Wikipedia page is controversial through the combined evaluation of the topically neighboring set of pages.   \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives. Over this dataset, we compare the methodology suggested here to those reported by BIBREF1, BIBREF4. As the latter report overall accuracy of their binary prediction, we convert our controversiality estimates to a binary classification by classifying the higher-scored half as controversial, and the lower half as non-controversial.   \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We consider three datasets, two of which are a contribution of this work.   \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                                                                                                                                                                                                                                                                                                                                                                                                  On Douban conversation corpus, FastText BIBREF7 pre-trained Chinese embedding vectors are used in ESIM + enhanced word vector whereas word2vec generated on training set is used in baseline model (ESIM). It can be seen from table TABREF23 that character embedding enhances the performance of original ESIM. Enhanced Word representation in algorithm SECREF12 improves the performance further and has shown that the proposed method is effective. Most models (RNN, CNN, LSTM, BiLSTM, Dual-Encoder) which encode the whole context (or response) into compact vectors before matching do not perform well. INLINEFORM0 directly models sequential structure of multi utterances in context and achieves good performance whereas ESIM implicitly makes use of end-of-utterance(__eou__) and end-of-turn (__eot__) token tags as shown in subsection SECREF41 .   \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The rest paper is organized as follows. In Section SECREF2 , we review the related work. In Section SECREF3 we provide an overview of ESIM (baseline) model and describe our methods to address out-of-vocabulary issues. In Section SECREF4 , we conduct extensive experiments to show the effectiveness of the proposed method. Finally we conclude with remarks and summarize our findings and outline future research directions.   \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                                 In this paper, we generate word embedding vectors on the training corpus based on word2vec BIBREF9 . Then we propose an algorithm to combine the generated one with the pre-trained word embedding vectors on a large general text corpus based on vector concatenation. The new word representation maintains information learned from both general text corpus and task-domain. The nice property of the algorithm is simplicity and little extra computational cost will be added. It can address word out-of-vocabulary issue effectively. This method can be applied to most NLP deep neural network models and is language-independent. We integrated our methods with ESIM(baseline model) BIBREF10 . The experimental results have shown that the proposed method has significantly improved the performance of original ESIM model and obtained state-of-the-art results on both Ubuntu Dialogue Corpus and Douban Conversation Corpus BIBREF11 . On Ubuntu Dialogue Corpus (V2), the improvement to the previous best baseline model (single) on INLINEFORM0 is 3.8% and our ensemble model on INLINEFORM1 is 75.9%. On Douban Conversation Corpus, the improvement to the previous best model (single) on INLINEFORM2 is 3.6%.   \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                                                                            Character-level representation has been widely used in information retrieval, tagging, language modeling and question answering. BIBREF12 represented a word based on character trigram in convolution neural network for web-search ranking. BIBREF7 represented a word by the sum of the vector representation of character n-gram. Santos et al BIBREF13 , BIBREF14 and BIBREF8 used convolution neural network to generate character-level representation (embedding) of a word. The former combined both word-level and character-level representation for part-of-speech and name entity tagging tasks while the latter used only character-level representation for language modeling. BIBREF15 employed a deep bidirectional GRU network to learn character-level representation and then concatenated word-level and character-level representation vectors together. BIBREF16 used a fine-grained gating mechanism to combine the word-level and character-level representation for reading comprehension. Character-level representation can help address out-of-vocabulary issue to some extent for western languages, which is mainly used to capture character ngram similarity.   \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27  Ubuntu dialogue corpus BIBREF5 is the public largest unstructured multi-turns dialogue corpus which consists of about one-million two-person conversations. The size of the corpus makes it attractive for the exploration of deep neural network modeling in the context of dialogue systems. Most deep neural networks use word embedding as the first layer. They either use fixed pre-trained word embedding vectors generated on a large text corpus or learn word embedding for the specific task. The former is lack of flexibility of domain adaptation. The latter requires a very large training corpus and significantly increases model training time. Word out-of-vocabulary issue occurs for both cases. Ubuntu dialogue corpus also contains many technical words (e.g. “ctrl+alt+f1\", “/dev/sdb1\"). The ubuntu corpus (V2) contains 823057 unique tokens whereas only 22% tokens occur in the pre-built GloVe word vectors. Although character-level representation which models sub-word morphologies can alleviate this problem to some extent BIBREF6 , BIBREF7 , BIBREF8 , character-level representation still have limitations: learn only morphological and orthographic similarity, other than semantic similarity (e.g. `car' and `bmw') and it cannot be applied to Asian languages (e.g. Chinese characters).   \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               In future work, the groundwork laid here may be applied to larger data sets, and may help motivate the development of such data. Larger noisy data sets would enable the differentiable constraints and weighted aggregation to be included during the optimization, and tuned with respect to data. In addition, we find the incorporation of graphical model inference into neural architectures to be a powerful new tool, and potentially an important step towards incorporating higher-level reasoning and prior knowledge into neural models of NLP.   \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                  For each mention INLINEFORM0 we construct a representation INLINEFORM1 of the mention in its context. This representation functions as a general “reading” or encoding of the mention, irrespective of the type of slots for which it will later be considered. This differs from some previous machine reading research where the model provides a query-specific reading of the document, or reads the document multiple times when answering a single query BIBREF0 . As in previous work, an embedding of a mention's context serves as its representation. We construct an embedding matrix INLINEFORM2 , using pre-trained word embeddings, where INLINEFORM3 is the dimensionality of the embeddings and INLINEFORM4 the number of words in the cluster. These are held fixed during training. All mentions are masked and receive the same one-hot vector in place of a pretrained embedding. From this matrix we embed the context using a two-layer convolutional neural network (CNN), with a detailed discussion of the architecture parameters provided in Section SECREF4 . CNNs have been used in a similar manner for a number of information extraction and classification tasks BIBREF6 , BIBREF7 and are capable of producing rich sentence representations BIBREF8 .   \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              INLINEFORM0    \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Each of these models uses features drawn from dependency trees, local context (unigram/part-of-speech features for up to 5 neighboring words), sentence context (bag-of-word/part-of-speech), words/part-of-speech of words occurring within the value, as well as the entity type of the mention itself.   \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We hypothesize that the earliest articles in each cluster are the most likely to contain misinformation, which we explore via a measure of information content. We define the information content of an article as the number of correct values which it mentions. Using this measure, we fit a skewed Gaussian distribution over the ordered news articles, assigning INLINEFORM0 , where INLINEFORM1 is the smoothed information content of INLINEFORM2 as drawn from the Gaussian.   \n",
       "\n",
       "                                                                                                                                           fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                            \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline   \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline   \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline   \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline   \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  a standard bidirectional RNN model with attention, a standard context-agnostic Transformer, and a recurrent baseline   \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                a one-stage RNN system   \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                a one-stage RNN system   \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                a one-stage RNN system   \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                a one-stage RNN system   \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                a one-stage RNN system   \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                         The methodology suggested by BIBREF1, BIBREF4   \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                         The methodology suggested by BIBREF1, BIBREF4   \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                         The methodology suggested by BIBREF1, BIBREF4   \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                         The methodology suggested by BIBREF1, BIBREF4   \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                         The methodology suggested by BIBREF1, BIBREF4   \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                  RNN, CNN, LSTM, BiLSTM, Dual-Encoder   \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                  RNN, CNN, LSTM, BiLSTM, Dual-Encoder   \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                  RNN, CNN, LSTM, BiLSTM, Dual-Encoder   \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                  RNN, CNN, LSTM, BiLSTM, Dual-Encoder   \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                  RNN, CNN, LSTM, BiLSTM, Dual-Encoder   \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                          Unanswerable   \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                          Unanswerable   \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                          Unanswerable   \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                          Unanswerable   \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                          Unanswerable   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                            gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  [bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21,  standard bidirectional RNN model with attention, A standard context-agnostic Transformer, standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8]  \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  [bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21,  standard bidirectional RNN model with attention, A standard context-agnostic Transformer, standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8]  \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  [bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21,  standard bidirectional RNN model with attention, A standard context-agnostic Transformer, standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8]  \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  [bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21,  standard bidirectional RNN model with attention, A standard context-agnostic Transformer, standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8]  \n",
       "ebe1084a06abdabefffc66f029eeb0b69f114fd9  [bidirectional RNN model with attention, concat22, s-hier, s-t-hier, s-hier-to-2, Transformer-base, concat22, concat21,  standard bidirectional RNN model with attention, A standard context-agnostic Transformer, standard bidirectional RNN model with attention, concat22, s-hier A multi-encoder architecture with hierarchical attention, s-t-hier , s-hier-to-2 , A standard context-agnostic Transformer., concat22, concat21, BIBREF8]  \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                                                                                                                                                                                                                                                                   [one-stage RNN system containing 2-layer BLSTM, one-stage RNN system, a one-stage RNN system]  \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                                                                                                                                                                                                                                                                   [one-stage RNN system containing 2-layer BLSTM, one-stage RNN system, a one-stage RNN system]  \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                                                                                                                                                                                                                                                                   [one-stage RNN system containing 2-layer BLSTM, one-stage RNN system, a one-stage RNN system]  \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                                                                                                                                                                                                                                                                   [one-stage RNN system containing 2-layer BLSTM, one-stage RNN system, a one-stage RNN system]  \n",
       "eb653a5c59851eda313ece0bcd8c589b6155d73e                                                                                                                                                                                                                                                                                                                                                   [one-stage RNN system containing 2-layer BLSTM, one-stage RNN system, a one-stage RNN system]  \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                                                                                                                                                                                                                        [Nearest neighbors (NN) Estimator, Naive Bayes (NB) Estimator, Recurrent neural network (RNN), Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016).]  \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                                                                                                                                                                                                                        [Nearest neighbors (NN) Estimator, Naive Bayes (NB) Estimator, Recurrent neural network (RNN), Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016).]  \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                                                                                                                                                                                                                        [Nearest neighbors (NN) Estimator, Naive Bayes (NB) Estimator, Recurrent neural network (RNN), Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016).]  \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                                                                                                                                                                                                                        [Nearest neighbors (NN) Estimator, Naive Bayes (NB) Estimator, Recurrent neural network (RNN), Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016).]  \n",
       "634a071b13eb7139e77872ecfdc135a2eb2f89da                                                                                                                                                                                                                                                                        [Nearest neighbors (NN) Estimator, Naive Bayes (NB) Estimator, Recurrent neural network (RNN), Classifiers by Rad and Barbosa (2012) and by Dori-Hacohen et al. (2016).]  \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                                                                                                                                                                                                                                                                                                                                                                    [ESIM, ESIM]  \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                                                                                                                                                                                                                                                                                                                                                                    [ESIM, ESIM]  \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                                                                                                                                                                                                                                                                                                                                                                    [ESIM, ESIM]  \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                                                                                                                                                                                                                                                                                                                                                                    [ESIM, ESIM]  \n",
       "8cf52ba480d372fc15024b3db704952f10fdca27                                                                                                                                                                                                                                                                                                                                                                                                                                    [ESIM, ESIM]  \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                                                                                                                                                                                                                                                                      [Reschke CRF, Reschke Noisy-OR, Reschke Best, Reschke CRF, Reschke Noisy-OR, Reschke Best]  \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                                                                                                                                                                                                                                                                      [Reschke CRF, Reschke Noisy-OR, Reschke Best, Reschke CRF, Reschke Noisy-OR, Reschke Best]  \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                                                                                                                                                                                                                                                                      [Reschke CRF, Reschke Noisy-OR, Reschke Best, Reschke CRF, Reschke Noisy-OR, Reschke Best]  \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                                                                                                                                                                                                                                                                      [Reschke CRF, Reschke Noisy-OR, Reschke Best, Reschke CRF, Reschke Noisy-OR, Reschke Best]  \n",
       "726c5c1b6951287f4bae22978f9a91d22d9bef61                                                                                                                                                                                                                                                                                                                                                      [Reschke CRF, Reschke Noisy-OR, Reschke Best, Reschke CRF, Reschke Noisy-OR, Reschke Best]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Which datasets do they evaluate on?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fewshot_evidence</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3f717e6eceab0a066af65ddf782c1ebc502c28c0</th>\n",
       "      <td>We conduct experiments on two commonly used FET datasets. Experimental results show that introducing information obtained through entity linking and having a deep neural model both helps to improve FET performance. Our model achieves more than 5% absolute strict accuracy improvement over the state of the art on both datasets.</td>\n",
       "      <td>FIGER (GOLD) and BBN</td>\n",
       "      <td>[FIGER (GOLD) BIBREF0, BBN BIBREF5, FIGER (GOLD) , BBN, FIGER (GOLD), BBN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3f717e6eceab0a066af65ddf782c1ebc502c28c0</th>\n",
       "      <td>We demonstrate the effectiveness of our approach with experimental results on commonly used FET datasets.</td>\n",
       "      <td>FIGER (GOLD) and BBN</td>\n",
       "      <td>[FIGER (GOLD) BIBREF0, BBN BIBREF5, FIGER (GOLD) , BBN, FIGER (GOLD), BBN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3f717e6eceab0a066af65ddf782c1ebc502c28c0</th>\n",
       "      <td>The experimental results are listed in Table TABREF16. As we can see, our approach performs much better than existing approaches on both datasets.</td>\n",
       "      <td>FIGER (GOLD) and BBN</td>\n",
       "      <td>[FIGER (GOLD) BIBREF0, BBN BIBREF5, FIGER (GOLD) , BBN, FIGER (GOLD), BBN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3f717e6eceab0a066af65ddf782c1ebc502c28c0</th>\n",
       "      <td>Ours (LocAttEL), which uses a more advanced EL system, does not achieve better performance than Ours (Full), which uses our own EL approach. After manually checking the results of the two EL approaches and the predictions of our model on FIGER (GOLD), we think this is mainly because: 1) Our model also uses the context while making predictions. Sometimes, if it “thinks” that the type information provided by EL is incorrect, it may not use it. 2) The performances of different EL approaches also depends on the dataset and the types of entities used for evaluation. We find that on FIGER (GOLD), the approach in BIBREF19 is better at distinguishing locations and sports teams, but it may also make some mistakes that our simple EL method does not. For example, it may incorrectly link “March,” the month, to an entity whose Wikipedia description fits the context better. 3) For some mentions, although the EL system links it to an incorrect entity, the type of this entity is the same with the correct entity.</td>\n",
       "      <td>FIGER (GOLD) and BBN</td>\n",
       "      <td>[FIGER (GOLD) BIBREF0, BBN BIBREF5, FIGER (GOLD) , BBN, FIGER (GOLD), BBN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3f717e6eceab0a066af65ddf782c1ebc502c28c0</th>\n",
       "      <td>We use two datasets: FIGER (GOLD) BIBREF0 and BBN BIBREF5. The sizes of their tag sets are 113 and 47, respectively. FIGER (GOLD) allows mentions to have multiple type paths, but BBN does not. Another commonly used dataset, OntoNotes BIBREF1, is not used since it contains many pronoun and common noun phrase mentions such as “it,” “he,” “a thrift institution,” which are not suitable to directly apply entity linking on.</td>\n",
       "      <td>FIGER (GOLD) and BBN</td>\n",
       "      <td>[FIGER (GOLD) BIBREF0, BBN BIBREF5, FIGER (GOLD) , BBN, FIGER (GOLD), BBN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420862798054f736128a6f0c4393c7f9cc648b40</th>\n",
       "      <td>We selected two related sentence relation modeling tasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 ( very related); and textual entailment task, which determines whether the truth of a text entails the truth of another text called hypothesis. We use standard SICK (Sentences Involving Compositional Knowledge) dataset for evaluation. It consists of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment.</td>\n",
       "      <td>SICK (Sentences Involving Compositional Knowledge) dataset</td>\n",
       "      <td>[SICK (Sentences Involving Compositional Knowledge) dataset , SICK (Sentences Involving Compositional Knowledge) dataset, SICK (Sentences Involving Compositional Knowledge) dataset]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420862798054f736128a6f0c4393c7f9cc648b40</th>\n",
       "      <td>Our model shows that when trained on small size datasets, combining pre-trained word embeddings with auxiliary character-level embedding can improve the sentence representation. Word embeddings can help capturing general word semantic meanings, whereas char-level embedding can help modeling task specific word meanings. Note that auxiliary character-level embedding based sentence representation do not require the knowledge of words or even syntactic structure of a language. The enhanced sentence representation generated by multi-layer bidirectional LSTM will encapsulate the character and word levels informations. Furthermore, it may enhance matching features that generated by computing similarity measures on sentence pairs. Quantitative evaluations on standard dataset demonstrate the effectiveness and advantages of our method.</td>\n",
       "      <td>SICK (Sentences Involving Compositional Knowledge) dataset</td>\n",
       "      <td>[SICK (Sentences Involving Compositional Knowledge) dataset , SICK (Sentences Involving Compositional Knowledge) dataset, SICK (Sentences Involving Compositional Knowledge) dataset]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420862798054f736128a6f0c4393c7f9cc648b40</th>\n",
       "      <td>In this paper, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and character embedding to learn sentence meanings. Our new approach first generates two kinds of word sequence representations as inputs into bidirectional LSTM to learn sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Our model shows that combining pre-trained word embeddings with auxiliary character-level embedding can improve the sentence representation. The enhanced sentence representation generated by multi-layer bidirectional LSTM will encapsulate the character and word levels informations. Furthermore, it may enhance matching features that generated by computing similarity measures on sentence pairs. Experimental results on benchmark datasets demonstrate that our new framework achieved the state-of-the-art performance compared with other deep neural networks based approaches.</td>\n",
       "      <td>SICK (Sentences Involving Compositional Knowledge) dataset</td>\n",
       "      <td>[SICK (Sentences Involving Compositional Knowledge) dataset , SICK (Sentences Involving Compositional Knowledge) dataset, SICK (Sentences Involving Compositional Knowledge) dataset]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420862798054f736128a6f0c4393c7f9cc648b40</th>\n",
       "      <td>Table TABREF31 and TABREF32 show the Pearson correlation and accuracy comparison results of semantic relatedness and text entailment tasks. We can see that combining CharCNN with multi-layer bidirectional LSTM yields better performance compared with other traditional machine learning methods such as SVM and MaxEnt approach BIBREF17 , BIBREF0 that served with many handcraft features. Note that our method doesn't need extra handcrafted feature extraction procedure. Also our method doesn't leverage external linguistic resources such as wordnet or parsing which get best results in BIBREF10 . More importantly, both task prediction results close to the state-of-the-art results. It proved that our approaches successfully simultaneously predict heterogeneous tasks. Note that for semantic relatedness task, the latest research BIBREF10 proposed a tree-structure based LSTM, the Pearson correlation score of their system can reach 0.863. Compared with their approach, our method didn't use dependency parsing and can be used to predict tasks contains multiple languages.</td>\n",
       "      <td>SICK (Sentences Involving Compositional Knowledge) dataset</td>\n",
       "      <td>[SICK (Sentences Involving Compositional Knowledge) dataset , SICK (Sentences Involving Compositional Knowledge) dataset, SICK (Sentences Involving Compositional Knowledge) dataset]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420862798054f736128a6f0c4393c7f9cc648b40</th>\n",
       "      <td>A key component of deep neural network is word embedding which serve as an lookup table to get word representations. From low level NLP tasks such as language modeling, POS tagging, name entity recognition, and semantic role labeling BIBREF6 , BIBREF7 , to high level tasks such as machine translation, information retrieval and semantic analysis BIBREF8 , BIBREF9 , BIBREF10 . Deep word representation learning has demonstrated its importance for these tasks. All the tasks get performance improvement via further learning either word level representations or sentence level representations. On the other hand, some researchers have found character-level convolutional networks BIBREF11 , BIBREF12 are useful in extracting information from raw signals for the task such as language modeling or text classification.</td>\n",
       "      <td>SICK (Sentences Involving Compositional Knowledge) dataset</td>\n",
       "      <td>[SICK (Sentences Involving Compositional Knowledge) dataset , SICK (Sentences Involving Compositional Knowledge) dataset, SICK (Sentences Involving Compositional Knowledge) dataset]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bc6ad5964f444cf414b661a4b942dafb7640c564</th>\n",
       "      <td>In this paper we address the following problem. Given a set of narrative texts, each of which describes a stereotypical event sequence drawn from a fixed but unknown distribution, learn the structure and parameters of a Left-to-Right HMM model that best captures the distribution of the event sequences. We evaluate the algorithm on natural datasets by how well the learned HMM can predict observations removed from the test sequences.</td>\n",
       "      <td>The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types.</td>\n",
       "      <td>[The Open Minds Indoor Common Sense (OMICS) corpus , Open Minds Indoor Common Sense (OMICS) corpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bc6ad5964f444cf414b661a4b942dafb7640c564</th>\n",
       "      <td>We now present our experimental results on SEM-HMM and SEM-HMM-Approx. The evaluation task is to predict missing events from an observed sequence of events. For comparison, four baselines were also evaluated. The “Frequency” baseline predicts the most frequent event in the training set that is not found in the observed test sequence. The “Conditional” baseline predicts the next event based on what most frequently follows the prior event. A third baseline, referred to as “BMM,” is a version of our algorithm that does not use EM for parameter estimation and instead only incrementally updates the parameters starting from the raw document counts. Further, it learns a standard HMM, that is, with no INLINEFORM0 transitions. This is very similar to the Bayesian Model Merging approach for HMMs BIBREF9 . The fourth baseline is the same as above, but uses our EM algorithm for parameter estimation without INLINEFORM1 transitions. It is referred to as “BMM + EM.”</td>\n",
       "      <td>The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types.</td>\n",
       "      <td>[The Open Minds Indoor Common Sense (OMICS) corpus , Open Minds Indoor Common Sense (OMICS) corpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bc6ad5964f444cf414b661a4b942dafb7640c564</th>\n",
       "      <td>The 84 domains with at least 50 narratives and 3 event types were used for evaluation. For each domain, forty percent of the narratives were withheld for testing, each with one randomly-chosen event omitted. The model was evaluated on the proportion of correctly predicted events given the remaining sequence. On average each domain has 21.7 event types with a standard deviation of 4.6. Further, the average narrative length across domains is 3.8 with standard deviation of 1.7. This implies that only a frcation of the event types are present in any given narrative. There is a high degree of omission of events and many different ways of accomplishing each task. Hence, the prediction task is reasonably difficult, as evidenced by the simple baselines. Neither the frequency of events nor simple temporal structure is enough to accurately fill in the gaps which indicates that most sophisticated modeling such as SEM-HMM is needed.</td>\n",
       "      <td>The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types.</td>\n",
       "      <td>[The Open Minds Indoor Common Sense (OMICS) corpus , Open Minds Indoor Common Sense (OMICS) corpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bc6ad5964f444cf414b661a4b942dafb7640c564</th>\n",
       "      <td>We now describe our structure learning algorithm, SEM-HMM. Our algorithm is inspired by Bayesian Model Merging (BMM) BIBREF9 and Structural EM (SEM) BIBREF10 and adapts them to learning HMMs with missing observations. SEM-HMM performs a greedy hill climbing search through the space of acyclic HMM structures. It iteratively proposes changes to the structure either by merging states or by deleting edges. It evaluates each change and makes the one with the best score. An exact implementation of this method is expensive, because, each time a structure change is considered, the MAP parameters of the structure given the data must be re-estimated. One of the key insights of both SEM and BMM is that this expensive re-estimation can be avoided in factored models by incrementally computing the changes to various expected counts using only local information. While this calculation is only approximate, it is highly efficient.</td>\n",
       "      <td>The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types.</td>\n",
       "      <td>[The Open Minds Indoor Common Sense (OMICS) corpus , Open Minds Indoor Common Sense (OMICS) corpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bc6ad5964f444cf414b661a4b942dafb7640c564</th>\n",
       "      <td>The semantic constraints are learned from the event sequences for use in the model prior. The constraints take the simple form “ INLINEFORM0 never follows INLINEFORM1 .” They are learned by generating all possible such rules using pairwise permutations of event types, and evaluating them on the training data. In particular, the number of times each rule is violated is counted and a INLINEFORM2 -test is performed to determine if the violation rate is lower than a predetermined error rate. Those rules that pass the hypothesis test with a threshold of INLINEFORM3 are included. When evaluating a model, these contraints are considered violated if the model could generate a sequence of observations that violates the constraint.</td>\n",
       "      <td>The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types.</td>\n",
       "      <td>[The Open Minds Indoor Common Sense (OMICS) corpus , Open Minds Indoor Common Sense (OMICS) corpus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f3b4e52ba962a0004064132d123fd9b78d9e12e2</th>\n",
       "      <td>To our knowledge, almost all of the previous related work on simultaneous translation evaluate their models upon the clean testing data without ASR errors and with explicit sentence boundaries annotated by human translators. Certainly, testing data with real ASR errors and without explicit sentence boundaries is beneficial to evaluate the robustness of translation models. To this end, we perform experiments on our proposed BSTC dataset.</td>\n",
       "      <td>NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus</td>\n",
       "      <td>[NIST02, NIST03, NIST04, NIST05, NIST08, 2008 (NIST08) datasets, Baidu Speech Translation Corpus (BSTC)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f3b4e52ba962a0004064132d123fd9b78d9e12e2</th>\n",
       "      <td>We use a subset of the data available for NIST OpenMT08 task . The parallel training corpus contains approximate 2 million sentence pairs. We choose NIST 2006 (NIST06) dataset as our development set, and the NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets as our test sets. We will use this dataset to evaluate the performance of our partial decoding and context-aware decoding strategy from the perspective of translation quality and latency.</td>\n",
       "      <td>NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus</td>\n",
       "      <td>[NIST02, NIST03, NIST04, NIST05, NIST08, 2008 (NIST08) datasets, Baidu Speech Translation Corpus (BSTC)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f3b4e52ba962a0004064132d123fd9b78d9e12e2</th>\n",
       "      <td>We firstly run the standard Transformer model on the NIST dataset. Then we evaluate the quality of the pre-trained model on our proposed speech translation dataset, and propose effective methods to improve the performance of the baseline. In that the testing data in this dataset contains ASR errors and speech irregularities, it can be used to evaluate the robustness of novel methods.</td>\n",
       "      <td>NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus</td>\n",
       "      <td>[NIST02, NIST03, NIST04, NIST05, NIST08, 2008 (NIST08) datasets, Baidu Speech Translation Corpus (BSTC)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f3b4e52ba962a0004064132d123fd9b78d9e12e2</th>\n",
       "      <td>The testing data in BSTC corpus consists of six talks. We firstly employ our ASR model to recognize the acoustic waves into Chinese text, which will be further segmented into small pieces of sub-sentences by our IU detector. To evaluate the contribution of our proposed BSTC dataset, we firstly train all models on the NIST dataset, and then check whether the performance can be further improved by fine-tuning them on the BSTC dataset.</td>\n",
       "      <td>NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus</td>\n",
       "      <td>[NIST02, NIST03, NIST04, NIST05, NIST08, 2008 (NIST08) datasets, Baidu Speech Translation Corpus (BSTC)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f3b4e52ba962a0004064132d123fd9b78d9e12e2</th>\n",
       "      <td>As shown in Table TABREF70 , the performance of our model is comparable to the interpreting. It is worth mentioning that both automatic and human evaluation criteria are designed for evaluating written translation and have a special emphasis on adequacy and faithfulness. But in simultaneous interpreting, human interpreters routinely omit less-important information to overcome their limitations in working memory. As the last column in Table 6 shows, human interpreters' oral translations have more omissions than machine's and receive lower acceptability. The evaluation results do not mean that machines have exceeded human interpreters in simultaneous interpreting. Instead, it means we need machine translation criteria that suit simultaneous interpreting. We also find that the BSTC dataset is extremely difficult as the best human interpreter obtains a lower Acceptability 73.04%. Although the NMT model obtains impressive translation quality, we do not compare the latency of machine translation and human interpreting in this paper, and leave it to the future work.</td>\n",
       "      <td>NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus</td>\n",
       "      <td>[NIST02, NIST03, NIST04, NIST05, NIST08, 2008 (NIST08) datasets, Baidu Speech Translation Corpus (BSTC)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             fewshot_evidence  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              We conduct experiments on two commonly used FET datasets. Experimental results show that introducing information obtained through entity linking and having a deep neural model both helps to improve FET performance. Our model achieves more than 5% absolute strict accuracy improvement over the state of the art on both datasets.   \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We demonstrate the effectiveness of our approach with experimental results on commonly used FET datasets.   \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The experimental results are listed in Table TABREF16. As we can see, our approach performs much better than existing approaches on both datasets.   \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                  Ours (LocAttEL), which uses a more advanced EL system, does not achieve better performance than Ours (Full), which uses our own EL approach. After manually checking the results of the two EL approaches and the predictions of our model on FIGER (GOLD), we think this is mainly because: 1) Our model also uses the context while making predictions. Sometimes, if it “thinks” that the type information provided by EL is incorrect, it may not use it. 2) The performances of different EL approaches also depends on the dataset and the types of entities used for evaluation. We find that on FIGER (GOLD), the approach in BIBREF19 is better at distinguishing locations and sports teams, but it may also make some mistakes that our simple EL method does not. For example, it may incorrectly link “March,” the month, to an entity whose Wikipedia description fits the context better. 3) For some mentions, although the EL system links it to an incorrect entity, the type of this entity is the same with the correct entity.   \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                We use two datasets: FIGER (GOLD) BIBREF0 and BBN BIBREF5. The sizes of their tag sets are 113 and 47, respectively. FIGER (GOLD) allows mentions to have multiple type paths, but BBN does not. Another commonly used dataset, OntoNotes BIBREF1, is not used since it contains many pronoun and common noun phrase mentions such as “it,” “he,” “a thrift institution,” which are not suitable to directly apply entity linking on.   \n",
       "420862798054f736128a6f0c4393c7f9cc648b40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             We selected two related sentence relation modeling tasks: semantic relatedness task, which measures the degree of semantic relatedness of a sentence pair by assigning a relatedness score ranging from 1 (completely unrelated) to 5 ( very related); and textual entailment task, which determines whether the truth of a text entails the truth of another text called hypothesis. We use standard SICK (Sentences Involving Compositional Knowledge) dataset for evaluation. It consists of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment.   \n",
       "420862798054f736128a6f0c4393c7f9cc648b40                                                                                                                                                                                                                                                Our model shows that when trained on small size datasets, combining pre-trained word embeddings with auxiliary character-level embedding can improve the sentence representation. Word embeddings can help capturing general word semantic meanings, whereas char-level embedding can help modeling task specific word meanings. Note that auxiliary character-level embedding based sentence representation do not require the knowledge of words or even syntactic structure of a language. The enhanced sentence representation generated by multi-layer bidirectional LSTM will encapsulate the character and word levels informations. Furthermore, it may enhance matching features that generated by computing similarity measures on sentence pairs. Quantitative evaluations on standard dataset demonstrate the effectiveness and advantages of our method.   \n",
       "420862798054f736128a6f0c4393c7f9cc648b40                                                        In this paper, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and character embedding to learn sentence meanings. Our new approach first generates two kinds of word sequence representations as inputs into bidirectional LSTM to learn sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Our model shows that combining pre-trained word embeddings with auxiliary character-level embedding can improve the sentence representation. The enhanced sentence representation generated by multi-layer bidirectional LSTM will encapsulate the character and word levels informations. Furthermore, it may enhance matching features that generated by computing similarity measures on sentence pairs. Experimental results on benchmark datasets demonstrate that our new framework achieved the state-of-the-art performance compared with other deep neural networks based approaches.   \n",
       "420862798054f736128a6f0c4393c7f9cc648b40      Table TABREF31 and TABREF32 show the Pearson correlation and accuracy comparison results of semantic relatedness and text entailment tasks. We can see that combining CharCNN with multi-layer bidirectional LSTM yields better performance compared with other traditional machine learning methods such as SVM and MaxEnt approach BIBREF17 , BIBREF0 that served with many handcraft features. Note that our method doesn't need extra handcrafted feature extraction procedure. Also our method doesn't leverage external linguistic resources such as wordnet or parsing which get best results in BIBREF10 . More importantly, both task prediction results close to the state-of-the-art results. It proved that our approaches successfully simultaneously predict heterogeneous tasks. Note that for semantic relatedness task, the latest research BIBREF10 proposed a tree-structure based LSTM, the Pearson correlation score of their system can reach 0.863. Compared with their approach, our method didn't use dependency parsing and can be used to predict tasks contains multiple languages.   \n",
       "420862798054f736128a6f0c4393c7f9cc648b40                                                                                                                                                                                                                                                                      A key component of deep neural network is word embedding which serve as an lookup table to get word representations. From low level NLP tasks such as language modeling, POS tagging, name entity recognition, and semantic role labeling BIBREF6 , BIBREF7 , to high level tasks such as machine translation, information retrieval and semantic analysis BIBREF8 , BIBREF9 , BIBREF10 . Deep word representation learning has demonstrated its importance for these tasks. All the tasks get performance improvement via further learning either word level representations or sentence level representations. On the other hand, some researchers have found character-level convolutional networks BIBREF11 , BIBREF12 are useful in extracting information from raw signals for the task such as language modeling or text classification.   \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In this paper we address the following problem. Given a set of narrative texts, each of which describes a stereotypical event sequence drawn from a fixed but unknown distribution, learn the structure and parameters of a Left-to-Right HMM model that best captures the distribution of the event sequences. We evaluate the algorithm on natural datasets by how well the learned HMM can predict observations removed from the test sequences.   \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                                                                                                                We now present our experimental results on SEM-HMM and SEM-HMM-Approx. The evaluation task is to predict missing events from an observed sequence of events. For comparison, four baselines were also evaluated. The “Frequency” baseline predicts the most frequent event in the training set that is not found in the observed test sequence. The “Conditional” baseline predicts the next event based on what most frequently follows the prior event. A third baseline, referred to as “BMM,” is a version of our algorithm that does not use EM for parameter estimation and instead only incrementally updates the parameters starting from the raw document counts. Further, it learns a standard HMM, that is, with no INLINEFORM0 transitions. This is very similar to the Bayesian Model Merging approach for HMMs BIBREF9 . The fourth baseline is the same as above, but uses our EM algorithm for parameter estimation without INLINEFORM1 transitions. It is referred to as “BMM + EM.”   \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                                                                                                                                               The 84 domains with at least 50 narratives and 3 event types were used for evaluation. For each domain, forty percent of the narratives were withheld for testing, each with one randomly-chosen event omitted. The model was evaluated on the proportion of correctly predicted events given the remaining sequence. On average each domain has 21.7 event types with a standard deviation of 4.6. Further, the average narrative length across domains is 3.8 with standard deviation of 1.7. This implies that only a frcation of the event types are present in any given narrative. There is a high degree of omission of events and many different ways of accomplishing each task. Hence, the prediction task is reasonably difficult, as evidenced by the simple baselines. Neither the frequency of events nor simple temporal structure is enough to accurately fill in the gaps which indicates that most sophisticated modeling such as SEM-HMM is needed.   \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                                                                                                                                                      We now describe our structure learning algorithm, SEM-HMM. Our algorithm is inspired by Bayesian Model Merging (BMM) BIBREF9 and Structural EM (SEM) BIBREF10 and adapts them to learning HMMs with missing observations. SEM-HMM performs a greedy hill climbing search through the space of acyclic HMM structures. It iteratively proposes changes to the structure either by merging states or by deleting edges. It evaluates each change and makes the one with the best score. An exact implementation of this method is expensive, because, each time a structure change is considered, the MAP parameters of the structure given the data must be re-estimated. One of the key insights of both SEM and BMM is that this expensive re-estimation can be avoided in factored models by incrementally computing the changes to various expected counts using only local information. While this calculation is only approximate, it is highly efficient.   \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                                                                                                                                                                                                                                                                                                                                                          The semantic constraints are learned from the event sequences for use in the model prior. The constraints take the simple form “ INLINEFORM0 never follows INLINEFORM1 .” They are learned by generating all possible such rules using pairwise permutations of event types, and evaluating them on the training data. In particular, the number of times each rule is violated is counted and a INLINEFORM2 -test is performed to determine if the violation rate is lower than a predetermined error rate. Those rules that pass the hypothesis test with a threshold of INLINEFORM3 are included. When evaluating a model, these contraints are considered violated if the model could generate a sequence of observations that violates the constraint.   \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             To our knowledge, almost all of the previous related work on simultaneous translation evaluate their models upon the clean testing data without ASR errors and with explicit sentence boundaries annotated by human translators. Certainly, testing data with real ASR errors and without explicit sentence boundaries is beneficial to evaluate the robustness of translation models. To this end, we perform experiments on our proposed BSTC dataset.   \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We use a subset of the data available for NIST OpenMT08 task . The parallel training corpus contains approximate 2 million sentence pairs. We choose NIST 2006 (NIST06) dataset as our development set, and the NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets as our test sets. We will use this dataset to evaluate the performance of our partial decoding and context-aware decoding strategy from the perspective of translation quality and latency.   \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We firstly run the standard Transformer model on the NIST dataset. Then we evaluate the quality of the pre-trained model on our proposed speech translation dataset, and propose effective methods to improve the performance of the baseline. In that the testing data in this dataset contains ASR errors and speech irregularities, it can be used to evaluate the robustness of novel methods.   \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The testing data in BSTC corpus consists of six talks. We firstly employ our ASR model to recognize the acoustic waves into Chinese text, which will be further segmented into small pieces of sub-sentences by our IU detector. To evaluate the contribution of our proposed BSTC dataset, we firstly train all models on the NIST dataset, and then check whether the performance can be further improved by fine-tuning them on the BSTC dataset.   \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2  As shown in Table TABREF70 , the performance of our model is comparable to the interpreting. It is worth mentioning that both automatic and human evaluation criteria are designed for evaluating written translation and have a special emphasis on adequacy and faithfulness. But in simultaneous interpreting, human interpreters routinely omit less-important information to overcome their limitations in working memory. As the last column in Table 6 shows, human interpreters' oral translations have more omissions than machine's and receive lower acceptability. The evaluation results do not mean that machines have exceeded human interpreters in simultaneous interpreting. Instead, it means we need machine translation criteria that suit simultaneous interpreting. We also find that the BSTC dataset is extremely difficult as the best human interpreter obtains a lower Acceptability 73.04%. Although the NMT model obtains impressive translation quality, we do not compare the latency of machine translation and human interpreting in this paper, and leave it to the future work.   \n",
       "\n",
       "                                                                                                                                                                              fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                                                               \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                                                     FIGER (GOLD) and BBN   \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                                                     FIGER (GOLD) and BBN   \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                                                     FIGER (GOLD) and BBN   \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                                                     FIGER (GOLD) and BBN   \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                                                     FIGER (GOLD) and BBN   \n",
       "420862798054f736128a6f0c4393c7f9cc648b40                                                                                               SICK (Sentences Involving Compositional Knowledge) dataset   \n",
       "420862798054f736128a6f0c4393c7f9cc648b40                                                                                               SICK (Sentences Involving Compositional Knowledge) dataset   \n",
       "420862798054f736128a6f0c4393c7f9cc648b40                                                                                               SICK (Sentences Involving Compositional Knowledge) dataset   \n",
       "420862798054f736128a6f0c4393c7f9cc648b40                                                                                               SICK (Sentences Involving Compositional Knowledge) dataset   \n",
       "420862798054f736128a6f0c4393c7f9cc648b40                                                                                               SICK (Sentences Involving Compositional Knowledge) dataset   \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                          The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types.   \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                          The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types.   \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                          The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types.   \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                          The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types.   \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                          The SEM-HMM and SEM-HMM-Approx are evaluated on natural datasets with 84 domains with at least 50 narratives and 3 event types.   \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2  NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus   \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2  NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus   \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2  NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus   \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2  NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus   \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2  NIST OpenMT08 task, NIST 2006 (NIST06) dataset, NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets, BSTC corpus   \n",
       "\n",
       "                                                                                                                                                                                                                   gold_answers  \n",
       "quids                                                                                                                                                                                                                            \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                             [FIGER (GOLD) BIBREF0, BBN BIBREF5, FIGER (GOLD) , BBN, FIGER (GOLD), BBN]  \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                             [FIGER (GOLD) BIBREF0, BBN BIBREF5, FIGER (GOLD) , BBN, FIGER (GOLD), BBN]  \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                             [FIGER (GOLD) BIBREF0, BBN BIBREF5, FIGER (GOLD) , BBN, FIGER (GOLD), BBN]  \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                             [FIGER (GOLD) BIBREF0, BBN BIBREF5, FIGER (GOLD) , BBN, FIGER (GOLD), BBN]  \n",
       "3f717e6eceab0a066af65ddf782c1ebc502c28c0                                                                                                             [FIGER (GOLD) BIBREF0, BBN BIBREF5, FIGER (GOLD) , BBN, FIGER (GOLD), BBN]  \n",
       "420862798054f736128a6f0c4393c7f9cc648b40  [SICK (Sentences Involving Compositional Knowledge) dataset , SICK (Sentences Involving Compositional Knowledge) dataset, SICK (Sentences Involving Compositional Knowledge) dataset]  \n",
       "420862798054f736128a6f0c4393c7f9cc648b40  [SICK (Sentences Involving Compositional Knowledge) dataset , SICK (Sentences Involving Compositional Knowledge) dataset, SICK (Sentences Involving Compositional Knowledge) dataset]  \n",
       "420862798054f736128a6f0c4393c7f9cc648b40  [SICK (Sentences Involving Compositional Knowledge) dataset , SICK (Sentences Involving Compositional Knowledge) dataset, SICK (Sentences Involving Compositional Knowledge) dataset]  \n",
       "420862798054f736128a6f0c4393c7f9cc648b40  [SICK (Sentences Involving Compositional Knowledge) dataset , SICK (Sentences Involving Compositional Knowledge) dataset, SICK (Sentences Involving Compositional Knowledge) dataset]  \n",
       "420862798054f736128a6f0c4393c7f9cc648b40  [SICK (Sentences Involving Compositional Knowledge) dataset , SICK (Sentences Involving Compositional Knowledge) dataset, SICK (Sentences Involving Compositional Knowledge) dataset]  \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                                                                                    [The Open Minds Indoor Common Sense (OMICS) corpus , Open Minds Indoor Common Sense (OMICS) corpus]  \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                                                                                    [The Open Minds Indoor Common Sense (OMICS) corpus , Open Minds Indoor Common Sense (OMICS) corpus]  \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                                                                                    [The Open Minds Indoor Common Sense (OMICS) corpus , Open Minds Indoor Common Sense (OMICS) corpus]  \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                                                                                    [The Open Minds Indoor Common Sense (OMICS) corpus , Open Minds Indoor Common Sense (OMICS) corpus]  \n",
       "bc6ad5964f444cf414b661a4b942dafb7640c564                                                                                    [The Open Minds Indoor Common Sense (OMICS) corpus , Open Minds Indoor Common Sense (OMICS) corpus]  \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2                                                                               [NIST02, NIST03, NIST04, NIST05, NIST08, 2008 (NIST08) datasets, Baidu Speech Translation Corpus (BSTC)]  \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2                                                                               [NIST02, NIST03, NIST04, NIST05, NIST08, 2008 (NIST08) datasets, Baidu Speech Translation Corpus (BSTC)]  \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2                                                                               [NIST02, NIST03, NIST04, NIST05, NIST08, 2008 (NIST08) datasets, Baidu Speech Translation Corpus (BSTC)]  \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2                                                                               [NIST02, NIST03, NIST04, NIST05, NIST08, 2008 (NIST08) datasets, Baidu Speech Translation Corpus (BSTC)]  \n",
       "f3b4e52ba962a0004064132d123fd9b78d9e12e2                                                                               [NIST02, NIST03, NIST04, NIST05, NIST08, 2008 (NIST08) datasets, Baidu Speech Translation Corpus (BSTC)]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  what were the evaluation metrics?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fewshot_evidence</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50bda708293532f07a3193aaea0519d433fcc040</th>\n",
       "      <td>In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0</td>\n",
       "      <td>cost measure that uses the confusion matrix of the prediction and prior knowledge</td>\n",
       "      <td>[Accuracy metric, accuracy, Accuracy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50bda708293532f07a3193aaea0519d433fcc040</th>\n",
       "      <td>In this work I presented the feature extraction, feature engineering and model evaluation steps I followed while developing balikasg for CAp 2018 that was ranked first among 14 other systems. I evaluated the efficiency of the different feature groups and found that readbility and complexity scores as well as topic models to be effective predictors. Further, I evaluated the the effectiveness of different classification algorithms and found that Gradient Boosted Trees outperform the rest of the models in this problem.</td>\n",
       "      <td>cost measure that uses the confusion matrix of the prediction and prior knowledge</td>\n",
       "      <td>[Accuracy metric, accuracy, Accuracy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50bda708293532f07a3193aaea0519d433fcc040</th>\n",
       "      <td>In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics.</td>\n",
       "      <td>cost measure that uses the confusion matrix of the prediction and prior knowledge</td>\n",
       "      <td>[Accuracy metric, accuracy, Accuracy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50bda708293532f07a3193aaea0519d433fcc040</th>\n",
       "      <td>While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.</td>\n",
       "      <td>cost measure that uses the confusion matrix of the prediction and prior knowledge</td>\n",
       "      <td>[Accuracy metric, accuracy, Accuracy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50bda708293532f07a3193aaea0519d433fcc040</th>\n",
       "      <td>Automatically predicting the level of English of non-native speakers from their written text is an interesting text mining task. Systems that perform well in the task can be useful components for online, second-language learning platforms as well as for organisations that tutor students for this purpose. In this paper I present the system balikasg that achieved the state-of-the-art performance in the CAp 2018 data science challenge among 14 systems. In order to achieve the best performance in the challenge, I decided to use a variety of features that describe an essay's readability and syntactic complexity as well as its content. For the prediction step, I found Gradient Boosted Trees, whose efficiency is proven in several data science challenges, to be the most efficient across a variety of classifiers.</td>\n",
       "      <td>cost measure that uses the confusion matrix of the prediction and prior knowledge</td>\n",
       "      <td>[Accuracy metric, accuracy, Accuracy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8908d1b865137bc309dde10a93735ec76037e5f9</th>\n",
       "      <td>For training the network, we used about 30000 English tweets provided by SemEval organisers and the test set of 2016 which contains 12000 tweets as development set. The test set of 2017 is used to evaluate the system in SemEval-2017 competition. For implementing our system we used python and Keras.</td>\n",
       "      <td>macro-average recall</td>\n",
       "      <td>[Unanswerable, macro-average recall]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8908d1b865137bc309dde10a93735ec76037e5f9</th>\n",
       "      <td>Official ranking: Our system is ranked fourth over 38 systems in terms of macro-average recall. Table 4 shows the results of our system on the test set of 2016 and 2017.</td>\n",
       "      <td>macro-average recall</td>\n",
       "      <td>[Unanswerable, macro-average recall]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8908d1b865137bc309dde10a93735ec76037e5f9</th>\n",
       "      <td>We presented our deep learning approach to Twitter sentiment analysis. We used ten convolutional neural network voters to get the polarity of a tweet, each voter has been trained on the same training data using the same word embeddings but different initial weights. The results demonstrate that our system is competitive as it is ranked forth in SemEval-2017 task 4-A.</td>\n",
       "      <td>macro-average recall</td>\n",
       "      <td>[Unanswerable, macro-average recall]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8908d1b865137bc309dde10a93735ec76037e5f9</th>\n",
       "      <td>We create ten instances of this network, we randomly initialize them using the uniform distribution, we repeat the random initialization for each instance 100 times, then we pick the networks which gives the highest average recall score as it is considered the official measure for system ranking. If the top network of each instance gives more than 95% of its results identical to another chosen network, we choose the next top networks to make sure that the ten networks are enough different.</td>\n",
       "      <td>macro-average recall</td>\n",
       "      <td>[Unanswerable, macro-average recall]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8908d1b865137bc309dde10a93735ec76037e5f9</th>\n",
       "      <td>We set the network parameters as follows: SSG embbeding size d is chosen to be 200, the tweet max legnth maxl is 99. For convolutional layers, we set the number of feature maps f to 50 and used 8 filter sizes (1,2,3,4,5,2,3,4). The p value of Dropout layer is set to 0.3. We used Nadam optimizer BIBREF8 to update the weights of the network and back-propogation algorithm to compute the gradients. The batch size is set to be 50 and the training data is shuffled after each iteration.</td>\n",
       "      <td>macro-average recall</td>\n",
       "      <td>[Unanswerable, macro-average recall]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301a453abaa3bc15976817fefce7a41f3b779907</th>\n",
       "      <td>The official evaluation metric was the F-score for class 1 (ADR): INLINEFORM0</td>\n",
       "      <td>F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0</td>\n",
       "      <td>[micro-averaged F-score of the class 1 (intake) and class 2 (possible intake), F-score for class 1 (ADR), micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301a453abaa3bc15976817fefce7a41f3b779907</th>\n",
       "      <td>The official evaluation metric for this task was micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1</td>\n",
       "      <td>F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0</td>\n",
       "      <td>[micro-averaged F-score of the class 1 (intake) and class 2 (possible intake), F-score for class 1 (ADR), micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301a453abaa3bc15976817fefce7a41f3b779907</th>\n",
       "      <td>the total score = INLINEFORM0 ;</td>\n",
       "      <td>F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0</td>\n",
       "      <td>[micro-averaged F-score of the class 1 (intake) and class 2 (possible intake), F-score for class 1 (ADR), micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301a453abaa3bc15976817fefce7a41f3b779907</th>\n",
       "      <td>16. weight</td>\n",
       "      <td>F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0</td>\n",
       "      <td>[micro-averaged F-score of the class 1 (intake) and class 2 (possible intake), F-score for class 1 (ADR), micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301a453abaa3bc15976817fefce7a41f3b779907</th>\n",
       "      <td>Information on how the data was collected and annotated was not available until after the evaluation.</td>\n",
       "      <td>F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0</td>\n",
       "      <td>[micro-averaged F-score of the class 1 (intake) and class 2 (possible intake), F-score for class 1 (ADR), micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74a17eb3bf1d4f36e2db1459a342c529b9785f6e</th>\n",
       "      <td>Besides the BLEU score, we also conduct a subjective evaluation to validate the benefit of incorporating a phrase table in NMT. The subjective evaluation is conducted on CH-EN translation. As our method tries to solve the problem that NMT system cannot reflect the true meaning of the source sentence, the criterion of the subjective evaluation is the faithfulness of translation results. Specifically, five human evaluators, who are native Chinese and expert in English, are asked to evaluate the translations of 500 source sentences randomly sampled from the test sets without knowing which system a translation is selected from. The score ranges from 0 to 5. For a translation result, the higher its score is, the more faithful it is. Table 2 shows the average results of five subjective evaluations on CH-EN translation. As shown in Table 2，the faithfulness of translation results produced by our method is better than Arthur and baseline NMT system.</td>\n",
       "      <td>case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)</td>\n",
       "      <td>[BLEU , BLEU]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74a17eb3bf1d4f36e2db1459a342c529b9785f6e</th>\n",
       "      <td>We use the Zoph_RNN toolkit to implement all our described methods. In all experiments, the encoder and decoder include two stacked LSTM layers. The word embedding dimension and the size of hidden layers are both set to 1,000. The minibatch size is set to 128. We limit the vocabulary to 30K most frequent words for both the source and target languages. Other words are replaced by a special symbol “UNK”. At test time, we employ beam search and beam size is set to 12. We use case-insensitive 4-gram BLEU score as the automatic metric BIBREF21 for translation quality evaluation.</td>\n",
       "      <td>case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)</td>\n",
       "      <td>[BLEU , BLEU]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74a17eb3bf1d4f36e2db1459a342c529b9785f6e</th>\n",
       "      <td>In this section, we describe the experiments to evaluate our proposed methods.</td>\n",
       "      <td>case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)</td>\n",
       "      <td>[BLEU , BLEU]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74a17eb3bf1d4f36e2db1459a342c529b9785f6e</th>\n",
       "      <td>We compare our method with other relevant methods as follows:</td>\n",
       "      <td>case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)</td>\n",
       "      <td>[BLEU , BLEU]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74a17eb3bf1d4f36e2db1459a342c529b9785f6e</th>\n",
       "      <td>2) Baseline: It is the baseline attention-based NMT system BIBREF23 , BIBREF24 .</td>\n",
       "      <td>case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)</td>\n",
       "      <td>[BLEU , BLEU]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           fewshot_evidence  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0    \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In this work I presented the feature extraction, feature engineering and model evaluation steps I followed while developing balikasg for CAp 2018 that was ranked first among 14 other systems. I evaluated the efficiency of the different feature groups and found that readbility and complexity scores as well as topic models to be effective predictors. Further, I evaluated the the effectiveness of different classification algorithms and found that Gradient Boosted Trees outperform the rest of the models in this problem.   \n",
       "50bda708293532f07a3193aaea0519d433fcc040  In order to approach the language-level prediction task as a supervised classification problem, I frame it as an ordinal classification problem. In particular, given a written essay INLINEFORM0 from a candidate, the goal is to associate the essay with the level INLINEFORM1 of English according to the Common European Framework of Reference for languages (CEFR) system. Under CEFR there are six language levels INLINEFORM2 , such that INLINEFORM3 . In this notation, INLINEFORM4 is the beginner level while INLINEFORM5 is the most advanced level. Notice that the levels of INLINEFORM6 are ordered, thus defining an ordered classification problem. In this sense, care must be taken both during the phase of model selection and during the phase of evaluation. In the latter, predicting a class far from the true should incur a higher penalty. In other words, given a INLINEFORM7 essay, predicting INLINEFORM8 is worse than predicting INLINEFORM9 , and this difference must be captured by the evaluation metrics.   \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                                                                                                                       While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.   \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                                                                                                                                    Automatically predicting the level of English of non-native speakers from their written text is an interesting text mining task. Systems that perform well in the task can be useful components for online, second-language learning platforms as well as for organisations that tutor students for this purpose. In this paper I present the system balikasg that achieved the state-of-the-art performance in the CAp 2018 data science challenge among 14 systems. In order to achieve the best performance in the challenge, I decided to use a variety of features that describe an essay's readability and syntactic complexity as well as its content. For the prediction step, I found Gradient Boosted Trees, whose efficiency is proven in several data science challenges, to be the most efficient across a variety of classifiers.   \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        For training the network, we used about 30000 English tweets provided by SemEval organisers and the test set of 2016 which contains 12000 tweets as development set. The test set of 2017 is used to evaluate the system in SemEval-2017 competition. For implementing our system we used python and Keras.   \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Official ranking: Our system is ranked fourth over 38 systems in terms of macro-average recall. Table 4 shows the results of our system on the test set of 2016 and 2017.   \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We presented our deep learning approach to Twitter sentiment analysis. We used ten convolutional neural network voters to get the polarity of a tweet, each voter has been trained on the same training data using the same word embeddings but different initial weights. The results demonstrate that our system is competitive as it is ranked forth in SemEval-2017 task 4-A.    \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We create ten instances of this network, we randomly initialize them using the uniform distribution, we repeat the random initialization for each instance 100 times, then we pick the networks which gives the highest average recall score as it is considered the official measure for system ranking. If the top network of each instance gives more than 95% of its results identical to another chosen network, we choose the next top networks to make sure that the ten networks are enough different.   \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We set the network parameters as follows: SSG embbeding size d is chosen to be 200, the tweet max legnth maxl is 99. For convolutional layers, we set the number of feature maps f to 50 and used 8 filter sizes (1,2,3,4,5,2,3,4). The p value of Dropout layer is set to 0.3. We used Nadam optimizer BIBREF8 to update the weights of the network and back-propogation algorithm to compute the gradients. The batch size is set to be 50 and the training data is shuffled after each iteration.   \n",
       "301a453abaa3bc15976817fefce7a41f3b779907                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The official evaluation metric was the F-score for class 1 (ADR): INLINEFORM0    \n",
       "301a453abaa3bc15976817fefce7a41f3b779907                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The official evaluation metric for this task was micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1    \n",
       "301a453abaa3bc15976817fefce7a41f3b779907                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    the total score = INLINEFORM0 ;   \n",
       "301a453abaa3bc15976817fefce7a41f3b779907                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         16. weight   \n",
       "301a453abaa3bc15976817fefce7a41f3b779907                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Information on how the data was collected and annotated was not available until after the evaluation.   \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                         Besides the BLEU score, we also conduct a subjective evaluation to validate the benefit of incorporating a phrase table in NMT. The subjective evaluation is conducted on CH-EN translation. As our method tries to solve the problem that NMT system cannot reflect the true meaning of the source sentence, the criterion of the subjective evaluation is the faithfulness of translation results. Specifically, five human evaluators, who are native Chinese and expert in English, are asked to evaluate the translations of 500 source sentences randomly sampled from the test sets without knowing which system a translation is selected from. The score ranges from 0 to 5. For a translation result, the higher its score is, the more faithful it is. Table 2 shows the average results of five subjective evaluations on CH-EN translation. As shown in Table 2，the faithfulness of translation results produced by our method is better than Arthur and baseline NMT system.   \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                                                                                                                                                                                                                                                                                                                                                                               We use the Zoph_RNN toolkit to implement all our described methods. In all experiments, the encoder and decoder include two stacked LSTM layers. The word embedding dimension and the size of hidden layers are both set to 1,000. The minibatch size is set to 128. We limit the vocabulary to 30K most frequent words for both the source and target languages. Other words are replaced by a special symbol “UNK”. At test time, we employ beam search and beam size is set to 12. We use case-insensitive 4-gram BLEU score as the automatic metric BIBREF21 for translation quality evaluation.   \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In this section, we describe the experiments to evaluate our proposed methods.   \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We compare our method with other relevant methods as follows:   \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   2) Baseline: It is the baseline attention-based NMT system BIBREF23 , BIBREF24 .   \n",
       "\n",
       "                                                                                                                                                                                               fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                                                                                \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                         cost measure that uses the confusion matrix of the prediction and prior knowledge   \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                         cost measure that uses the confusion matrix of the prediction and prior knowledge   \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                         cost measure that uses the confusion matrix of the prediction and prior knowledge   \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                         cost measure that uses the confusion matrix of the prediction and prior knowledge   \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                         cost measure that uses the confusion matrix of the prediction and prior knowledge   \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                      macro-average recall   \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                      macro-average recall   \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                      macro-average recall   \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                      macro-average recall   \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                      macro-average recall   \n",
       "301a453abaa3bc15976817fefce7a41f3b779907  F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0   \n",
       "301a453abaa3bc15976817fefce7a41f3b779907  F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0   \n",
       "301a453abaa3bc15976817fefce7a41f3b779907  F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0   \n",
       "301a453abaa3bc15976817fefce7a41f3b779907  F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0   \n",
       "301a453abaa3bc15976817fefce7a41f3b779907  F-score for class 1 (ADR): INLINEFORM0, micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1, total score = INLINEFORM0   \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                   case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)   \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                   case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)   \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                   case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)   \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                   case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)   \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                   case-insensitive 4-gram BLEU score and faithfulness of translation results (scored 0-5)   \n",
       "\n",
       "                                                                                                                                                                                                                     gold_answers  \n",
       "quids                                                                                                                                                                                                                              \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                                                                                    [Accuracy metric, accuracy, Accuracy]  \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                                                                                    [Accuracy metric, accuracy, Accuracy]  \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                                                                                    [Accuracy metric, accuracy, Accuracy]  \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                                                                                    [Accuracy metric, accuracy, Accuracy]  \n",
       "50bda708293532f07a3193aaea0519d433fcc040                                                                                                                                                    [Accuracy metric, accuracy, Accuracy]  \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                     [Unanswerable, macro-average recall]  \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                     [Unanswerable, macro-average recall]  \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                     [Unanswerable, macro-average recall]  \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                     [Unanswerable, macro-average recall]  \n",
       "8908d1b865137bc309dde10a93735ec76037e5f9                                                                                                                                                     [Unanswerable, macro-average recall]  \n",
       "301a453abaa3bc15976817fefce7a41f3b779907  [micro-averaged F-score of the class 1 (intake) and class 2 (possible intake), F-score for class 1 (ADR), micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)]  \n",
       "301a453abaa3bc15976817fefce7a41f3b779907  [micro-averaged F-score of the class 1 (intake) and class 2 (possible intake), F-score for class 1 (ADR), micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)]  \n",
       "301a453abaa3bc15976817fefce7a41f3b779907  [micro-averaged F-score of the class 1 (intake) and class 2 (possible intake), F-score for class 1 (ADR), micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)]  \n",
       "301a453abaa3bc15976817fefce7a41f3b779907  [micro-averaged F-score of the class 1 (intake) and class 2 (possible intake), F-score for class 1 (ADR), micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)]  \n",
       "301a453abaa3bc15976817fefce7a41f3b779907  [micro-averaged F-score of the class 1 (intake) and class 2 (possible intake), F-score for class 1 (ADR), micro-averaged F-score of the class 1 (intake) and class 2 (possible intake)]  \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                                                                                                            [BLEU , BLEU]  \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                                                                                                            [BLEU , BLEU]  \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                                                                                                            [BLEU , BLEU]  \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                                                                                                            [BLEU , BLEU]  \n",
       "74a17eb3bf1d4f36e2db1459a342c529b9785f6e                                                                                                                                                                            [BLEU , BLEU]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  How long is their dataset?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fewshot_evidence</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4a91432abe3f54fcbdd00bb85dc0df95b16edf42</th>\n",
       "      <td>We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets.</td>\n",
       "      <td>85.4 million tweets</td>\n",
       "      <td>[85400000, 24,802 , 24,802 labeled tweets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4a91432abe3f54fcbdd00bb85dc0df95b16edf42</th>\n",
       "      <td>We first use a logistic regression with L1 regularization to reduce the dimensionality of the data. We then test a variety of models that have been used in prior work: logistic regression, naïve Bayes, decision trees, random forests, and linear SVMs. We tested each model using 5-fold cross validation, holding out 10% of the sample for evaluation to help prevent over-fitting. After using a grid-search to iterate over the models and parameters we find that the Logistic Regression and Linear SVM tended to perform significantly better than other models. We decided to use a logistic regression with L2 regularization for the final model as it more readily allows us to examine the predicted probabilities of class membership and has performed well in previous papers BIBREF5 , BIBREF8 . We trained the final model using the entire dataset and used it to predict the label for each tweet. We use a one-versus-rest framework where a separate classifier is trained for each class and the class label with the highest predicted probability across all classifiers is assigned to each tweet. All modeling was performing using scikit-learn BIBREF12 .</td>\n",
       "      <td>85.4 million tweets</td>\n",
       "      <td>[85400000, 24,802 , 24,802 labeled tweets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4a91432abe3f54fcbdd00bb85dc0df95b16edf42</th>\n",
       "      <td>We lowercased each tweet and stemmed it using the Porter stemmer, then create bigram, unigram, and trigram features, each weighted by its TF-IDF. To capture information about the syntactic structure we use NLTK BIBREF10 to construct Penn Part-of-Speech (POS) tag unigrams, bigrams, and trigrams. To capture the quality of each tweet we use modified Flesch-Kincaid Grade Level and Flesch Reading Ease scores, where the number of sentences is fixed at one. We also use a sentiment lexicon designed for social media to assign sentiment scores to each tweet BIBREF11 . We also include binary and count indicators for hashtags, mentions, retweets, and URLs, as well as features for the number of characters, words, and syllables in each tweet.</td>\n",
       "      <td>85.4 million tweets</td>\n",
       "      <td>[85400000, 24,802 , 24,802 labeled tweets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4a91432abe3f54fcbdd00bb85dc0df95b16edf42</th>\n",
       "      <td>Only 5% of tweets were coded as hate speech by the majority of coders and only 1.3% were coded unanimously, demonstrating the imprecision of the Hatebase lexicon. This is much lower than a comparable study using Twitter, where 11.6% of tweets were flagged as hate speech BIBREF5 , likely because we use a stricter criteria for hate speech. The majority of the tweets were considered to be offensive language (76% at 2/3, 53% at 3/3) and the remainder were considered to be non-offensive (16.6% at 2/3, 11.8% at 3/3). We then constructed features from these tweets and used them to train a classifier.</td>\n",
       "      <td>85.4 million tweets</td>\n",
       "      <td>[85400000, 24,802 , 24,802 labeled tweets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4a91432abe3f54fcbdd00bb85dc0df95b16edf42</th>\n",
       "      <td>Other supervised approaches to hate speech classification have unfortunately conflated hate speech with offensive language, making it difficult to ascertain the extent to which they are really identifying hate speech BIBREF5 , BIBREF8 . Neural language models show promise in the task but existing work has used training data has a similarly broad definition of hate speech BIBREF9 . Non-linguistic features like the gender or ethnicity of the author can help improve hate speech classification but this information is often unavailable or unreliable on social media BIBREF8 .</td>\n",
       "      <td>85.4 million tweets</td>\n",
       "      <td>[85400000, 24,802 , 24,802 labeled tweets]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1d255f181b18f7cf8eb3dd2369a082a2a398b7b</th>\n",
       "      <td>In this effort, we develop our supervised WSD model that leverages a Bidirectional Long Short-Term Memory (BLSTM) network. This network works with neural sense vectors (i.e. sense embeddings), which are learned during model training, and employs neural word vectors (i.e. word embeddings), which are learned through an unsupervised deep learning approach called GloVe (Global Vectors for word representation) BIBREF2 for the context words. By evaluating our one-model-fits-all WSD network over the public gold standard dataset of SensEval-3 BIBREF3 , we demonstrate that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1d255f181b18f7cf8eb3dd2369a082a2a398b7b</th>\n",
       "      <td>The hyper-parameters that were determined during the validation is presented in Table TABREF17 . The preprocessing of the data was conducted by lower-casing all the words in the documents and removing numbers. This results in a vocabulary size of INLINEFORM0 = 29044. Words not present in the training set are considered unknown during testing. Also, in order to have fixed-size contexts around the ambiguous words, the padding and truncating are applied to them whenever needed.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1d255f181b18f7cf8eb3dd2369a082a2a398b7b</th>\n",
       "      <td>For future work, besides following the discussed direction in order to resolve the inadequacy of the network regarding having two non-overlapping vector spaces of the embeddings, we plan to examine the network on technical domains such as biomedicine as well. In this case, our model will be evaluated on MSH WSD dataset prepared by National Library of Medicine (NLM). Also, construction of sense embeddings using (extended) definitions of senses BIBREF25 BIBREF26 can be tested. Moreover, considering that for many senses we have at least one (lexically) unambiguous word representing that sense, we also aim to experiment with unsupervised (pre-)training of our network which benefits form quarry management by which more training data will be automatically collected from the web.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1d255f181b18f7cf8eb3dd2369a082a2a398b7b</th>\n",
       "      <td>Long Short-Term Memory (LSTM), introduced by Hochreiter and Schmidhuber (1997) BIBREF13 , is a gated recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. A Bidirectional LSTM is made up of two reversed unidirectional LSTMs BIBREF14 . For WSD this means we are able to encode information of both preceding and succeeding words within context of an ambiguous word, which is necessary to correctly classify its sense.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1d255f181b18f7cf8eb3dd2369a082a2a398b7b</th>\n",
       "      <td>In SensEval-3 data (lexical sample task), the sense inventory used for nouns and adjectives is WordNet 1.7.1 BIBREF5 whereas verbs are annotated with senses from Wordsmyth. Table TABREF15 presents the number of words under each part of speech, and the average number of senses for each class.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0b5c599195973c563c4b1a0fe5d8fc77204d71a0</th>\n",
       "      <td>The version of the database which contains both GL and non-GL studies consists of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays. Specifically, each entry in the database describes one study, and studies are linked to publications using PubMed reference numbers (PMIDs). Each study is assigned seven 0/1 labels – one for each of the minimum criteria and one for the overall GL/non-GL label. The database also contains more detailed subcategories for each label (for example “species” label for MC 1) which were not used in this study. The publication PDFs were provided to us by the database creators. We have used the Grobid library to convert the PDF files into structured text. After removing documents with missing PDF files and documents which were not converted successfully, we were left with 624 full text documents.</td>\n",
       "      <td>624 full text documents</td>\n",
       "      <td>[670, 670 publications]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0b5c599195973c563c4b1a0fe5d8fc77204d71a0</th>\n",
       "      <td>Significant efforts in toxicology research are being devoted towards developing new in vitro methods for testing chemicals due to the large number of untested chemicals in use ( INLINEFORM0 75,000-80,000 BIBREF8 , BIBREF1 ) and the cost and time required by existing in vivo methods (2-3 years and millions of dollars per chemical BIBREF8 ). To facilitate the development of novel in vitro methods and assess the adherence to existing study guidelines, a curated database of high-quality in vivo rodent uterotrophic bioassay data extracted from research publications has recently been developed and published BIBREF1 .</td>\n",
       "      <td>624 full text documents</td>\n",
       "      <td>[670, 670 publications]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0b5c599195973c563c4b1a0fe5d8fc77204d71a0</th>\n",
       "      <td>Each publication contains on average 3.7 studies (separate bioassays), 194 publications contain a single study, while the rest contain two or more studies (with 82 being the most bioassays per publication). The following excerpt shows an example sentence mentioning multiple bioassays (with different study protocols):</td>\n",
       "      <td>624 full text documents</td>\n",
       "      <td>[670, 670 publications]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0b5c599195973c563c4b1a0fe5d8fc77204d71a0</th>\n",
       "      <td>With the exception of the first study (experiment 1), which had group sizes of 12, all other studies had group sizes of 8.</td>\n",
       "      <td>624 full text documents</td>\n",
       "      <td>[670, 670 publications]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0b5c599195973c563c4b1a0fe5d8fc77204d71a0</th>\n",
       "      <td>Extracting the data elements needed in these tasks is a time-consuming and at present a largely manual process which requires domain expertise. For example, in systematic review preparation, information extraction generally constitutes the most time consuming task BIBREF4 . This situation is made worse by the rapidly expanding body of potentially relevant literature with more than one million papers added into PubMed each year BIBREF5 . Therefore, data annotation and extraction presents an important challenge for automation.</td>\n",
       "      <td>624 full text documents</td>\n",
       "      <td>[670, 670 publications]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80bb07e553449bde9ac0ff35fcc718d7c161f2d4</th>\n",
       "      <td>The dataset used for the supervised was obtained from the JW300 large-scale, parallel corpus for Machine Translation (MT) by BIBREF8. The train set contained 20214 sentence pairs, while the validation contained 1000 sentence pairs. Both the supervised and unsupervised models were evaluated on a test set of 2101 sentences preprocessed by the Masakhane group. The model with the highest test BLEU score is selected as the best.</td>\n",
       "      <td>20214 sentence pairs</td>\n",
       "      <td>[21214, Data used has total of 23315 sentences.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80bb07e553449bde9ac0ff35fcc718d7c161f2d4</th>\n",
       "      <td>Supervised model training was performed with the open-source machine translation toolkit JoeyNMT by BIBREF9. For the byte pair encoding, embedding dimension was set to 256, while the embedding dimension was set to 300 for the word-level tokenization. The Transformer used for the byte pair encoding model had 6 encoder and 6 decoder layers, with 4 attention heads. For word-level, the encoder and decoder each had 4 layers with 10 attention heads for fair comparison to the unsupervised model. The models were each trained for 200 epochs on an Amazon EC2 p3.2xlarge instance.</td>\n",
       "      <td>20214 sentence pairs</td>\n",
       "      <td>[21214, Data used has total of 23315 sentences.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80bb07e553449bde9ac0ff35fcc718d7c161f2d4</th>\n",
       "      <td>Unsupervised model training followed BIBREF6 which used a Transformer of 4 encoder and 4 decoder layers with 10 attention heads. Embedding dimension was set to 300.</td>\n",
       "      <td>20214 sentence pairs</td>\n",
       "      <td>[21214, Data used has total of 23315 sentences.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80bb07e553449bde9ac0ff35fcc718d7c161f2d4</th>\n",
       "      <td>The supervised translation models seem to perform better at longer example translations than the unsupervised example.</td>\n",
       "      <td>20214 sentence pairs</td>\n",
       "      <td>[21214, Data used has total of 23315 sentences.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80bb07e553449bde9ac0ff35fcc718d7c161f2d4</th>\n",
       "      <td>Special thanks to the Masakhane group for catalysing this work.</td>\n",
       "      <td>20214 sentence pairs</td>\n",
       "      <td>[21214, Data used has total of 23315 sentences.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         fewshot_evidence  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42  We begin with a hate speech lexicon containing words and phrases identified by internet users as hate speech, compiled by Hatebase.org. Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users. We extracted the time-line for each user, resulting in a set of 85.4 million tweets. From this corpus we then took a random sample of 25k tweets containing terms from the lexicon and had them manually coded by CrowdFlower (CF) workers. Workers were asked to label each tweet as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. They were provided with our definition along with a paragraph explaining it in further detail. Users were asked to think not just about the words appearing in a given tweet but about the context in which they were used. They were instructed that the presence of a particular word, however offensive, did not necessarily indicate a tweet is hate speech. Each tweet was coded by three or more people. The intercoder-agreement score provided by CF is 92%. We use the majority decision for each tweet to assign a label. Some tweets were not assigned labels as there was no majority class. This results in a sample of 24,802 labeled tweets.   \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42                                                                                                                                                        We first use a logistic regression with L1 regularization to reduce the dimensionality of the data. We then test a variety of models that have been used in prior work: logistic regression, naïve Bayes, decision trees, random forests, and linear SVMs. We tested each model using 5-fold cross validation, holding out 10% of the sample for evaluation to help prevent over-fitting. After using a grid-search to iterate over the models and parameters we find that the Logistic Regression and Linear SVM tended to perform significantly better than other models. We decided to use a logistic regression with L2 regularization for the final model as it more readily allows us to examine the predicted probabilities of class membership and has performed well in previous papers BIBREF5 , BIBREF8 . We trained the final model using the entire dataset and used it to predict the label for each tweet. We use a one-versus-rest framework where a separate classifier is trained for each class and the class label with the highest predicted probability across all classifiers is assigned to each tweet. All modeling was performing using scikit-learn BIBREF12 .   \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We lowercased each tweet and stemmed it using the Porter stemmer, then create bigram, unigram, and trigram features, each weighted by its TF-IDF. To capture information about the syntactic structure we use NLTK BIBREF10 to construct Penn Part-of-Speech (POS) tag unigrams, bigrams, and trigrams. To capture the quality of each tweet we use modified Flesch-Kincaid Grade Level and Flesch Reading Ease scores, where the number of sentences is fixed at one. We also use a sentiment lexicon designed for social media to assign sentiment scores to each tweet BIBREF11 . We also include binary and count indicators for hashtags, mentions, retweets, and URLs, as well as features for the number of characters, words, and syllables in each tweet.   \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Only 5% of tweets were coded as hate speech by the majority of coders and only 1.3% were coded unanimously, demonstrating the imprecision of the Hatebase lexicon. This is much lower than a comparable study using Twitter, where 11.6% of tweets were flagged as hate speech BIBREF5 , likely because we use a stricter criteria for hate speech. The majority of the tweets were considered to be offensive language (76% at 2/3, 53% at 3/3) and the remainder were considered to be non-offensive (16.6% at 2/3, 11.8% at 3/3). We then constructed features from these tweets and used them to train a classifier.   \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Other supervised approaches to hate speech classification have unfortunately conflated hate speech with offensive language, making it difficult to ascertain the extent to which they are really identifying hate speech BIBREF5 , BIBREF8 . Neural language models show promise in the task but existing work has used training data has a similarly broad definition of hate speech BIBREF9 . Non-linguistic features like the gender or ethnicity of the author can help improve hate speech classification but this information is often unavailable or unreliable on social media BIBREF8 .   \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In this effort, we develop our supervised WSD model that leverages a Bidirectional Long Short-Term Memory (BLSTM) network. This network works with neural sense vectors (i.e. sense embeddings), which are learned during model training, and employs neural word vectors (i.e. word embeddings), which are learned through an unsupervised deep learning approach called GloVe (Global Vectors for word representation) BIBREF2 for the context words. By evaluating our one-model-fits-all WSD network over the public gold standard dataset of SensEval-3 BIBREF3 , we demonstrate that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'.   \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The hyper-parameters that were determined during the validation is presented in Table TABREF17 . The preprocessing of the data was conducted by lower-casing all the words in the documents and removing numbers. This results in a vocabulary size of INLINEFORM0 = 29044. Words not present in the training set are considered unknown during testing. Also, in order to have fixed-size contexts around the ambiguous words, the padding and truncating are applied to them whenever needed.   \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  For future work, besides following the discussed direction in order to resolve the inadequacy of the network regarding having two non-overlapping vector spaces of the embeddings, we plan to examine the network on technical domains such as biomedicine as well. In this case, our model will be evaluated on MSH WSD dataset prepared by National Library of Medicine (NLM). Also, construction of sense embeddings using (extended) definitions of senses BIBREF25 BIBREF26 can be tested. Moreover, considering that for many senses we have at least one (lexically) unambiguous word representing that sense, we also aim to experiment with unsupervised (pre-)training of our network which benefits form quarry management by which more training data will be automatically collected from the web.   \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Long Short-Term Memory (LSTM), introduced by Hochreiter and Schmidhuber (1997) BIBREF13 , is a gated recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. A Bidirectional LSTM is made up of two reversed unidirectional LSTMs BIBREF14 . For WSD this means we are able to encode information of both preceding and succeeding words within context of an ambiguous word, which is necessary to correctly classify its sense.   \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             In SensEval-3 data (lexical sample task), the sense inventory used for nouns and adjectives is WordNet 1.7.1 BIBREF5 whereas verbs are annotated with senses from Wordsmyth. Table TABREF15 presents the number of words under each part of speech, and the average number of senses for each class.   \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0                                                                                                                                                                                                                                                                                                                                                                                                                                 The version of the database which contains both GL and non-GL studies consists of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays. Specifically, each entry in the database describes one study, and studies are linked to publications using PubMed reference numbers (PMIDs). Each study is assigned seven 0/1 labels – one for each of the minimum criteria and one for the overall GL/non-GL label. The database also contains more detailed subcategories for each label (for example “species” label for MC 1) which were not used in this study. The publication PDFs were provided to us by the database creators. We have used the Grobid library to convert the PDF files into structured text. After removing documents with missing PDF files and documents which were not converted successfully, we were left with 624 full text documents.   \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Significant efforts in toxicology research are being devoted towards developing new in vitro methods for testing chemicals due to the large number of untested chemicals in use ( INLINEFORM0 75,000-80,000 BIBREF8 , BIBREF1 ) and the cost and time required by existing in vivo methods (2-3 years and millions of dollars per chemical BIBREF8 ). To facilitate the development of novel in vitro methods and assess the adherence to existing study guidelines, a curated database of high-quality in vivo rodent uterotrophic bioassay data extracted from research publications has recently been developed and published BIBREF1 .   \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Each publication contains on average 3.7 studies (separate bioassays), 194 publications contain a single study, while the rest contain two or more studies (with 82 being the most bioassays per publication). The following excerpt shows an example sentence mentioning multiple bioassays (with different study protocols):   \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       With the exception of the first study (experiment 1), which had group sizes of 12, all other studies had group sizes of 8.   \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Extracting the data elements needed in these tasks is a time-consuming and at present a largely manual process which requires domain expertise. For example, in systematic review preparation, information extraction generally constitutes the most time consuming task BIBREF4 . This situation is made worse by the rapidly expanding body of potentially relevant literature with more than one million papers added into PubMed each year BIBREF5 . Therefore, data annotation and extraction presents an important challenge for automation.   \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The dataset used for the supervised was obtained from the JW300 large-scale, parallel corpus for Machine Translation (MT) by BIBREF8. The train set contained 20214 sentence pairs, while the validation contained 1000 sentence pairs. Both the supervised and unsupervised models were evaluated on a test set of 2101 sentences preprocessed by the Masakhane group. The model with the highest test BLEU score is selected as the best.   \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Supervised model training was performed with the open-source machine translation toolkit JoeyNMT by BIBREF9. For the byte pair encoding, embedding dimension was set to 256, while the embedding dimension was set to 300 for the word-level tokenization. The Transformer used for the byte pair encoding model had 6 encoder and 6 decoder layers, with 4 attention heads. For word-level, the encoder and decoder each had 4 layers with 10 attention heads for fair comparison to the unsupervised model. The models were each trained for 200 epochs on an Amazon EC2 p3.2xlarge instance.   \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Unsupervised model training followed BIBREF6 which used a Transformer of 4 encoder and 4 decoder layers with 10 attention heads. Embedding dimension was set to 300.   \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The supervised translation models seem to perform better at longer example translations than the unsupervised example.   \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Special thanks to the Masakhane group for catalysing this work.   \n",
       "\n",
       "                                              fewshot_pred_answer  \\\n",
       "quids                                                               \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42      85.4 million tweets   \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42      85.4 million tweets   \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42      85.4 million tweets   \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42      85.4 million tweets   \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42      85.4 million tweets   \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b             Unanswerable   \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b             Unanswerable   \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b             Unanswerable   \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b             Unanswerable   \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b             Unanswerable   \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0  624 full text documents   \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0  624 full text documents   \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0  624 full text documents   \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0  624 full text documents   \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0  624 full text documents   \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4     20214 sentence pairs   \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4     20214 sentence pairs   \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4     20214 sentence pairs   \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4     20214 sentence pairs   \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4     20214 sentence pairs   \n",
       "\n",
       "                                                                              gold_answers  \n",
       "quids                                                                                       \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42        [85400000, 24,802 , 24,802 labeled tweets]  \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42        [85400000, 24,802 , 24,802 labeled tweets]  \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42        [85400000, 24,802 , 24,802 labeled tweets]  \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42        [85400000, 24,802 , 24,802 labeled tweets]  \n",
       "4a91432abe3f54fcbdd00bb85dc0df95b16edf42        [85400000, 24,802 , 24,802 labeled tweets]  \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b        [Unanswerable, Unanswerable, Unanswerable]  \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b        [Unanswerable, Unanswerable, Unanswerable]  \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b        [Unanswerable, Unanswerable, Unanswerable]  \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b        [Unanswerable, Unanswerable, Unanswerable]  \n",
       "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b        [Unanswerable, Unanswerable, Unanswerable]  \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0                           [670, 670 publications]  \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0                           [670, 670 publications]  \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0                           [670, 670 publications]  \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0                           [670, 670 publications]  \n",
       "0b5c599195973c563c4b1a0fe5d8fc77204d71a0                           [670, 670 publications]  \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4  [21214, Data used has total of 23315 sentences.]  \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4  [21214, Data used has total of 23315 sentences.]  \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4  [21214, Data used has total of 23315 sentences.]  \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4  [21214, Data used has total of 23315 sentences.]  \n",
       "80bb07e553449bde9ac0ff35fcc718d7c161f2d4  [21214, Data used has total of 23315 sentences.]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Do they report results only on English data?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fewshot_evidence</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bc7081aaa207de2362e0bea7bc8108d338aee36f</th>\n",
       "      <td>Model performances for each dataset are reported in Table . Extractive baselines show the best results for KPCrowd and DUC-2001 which is not surprising given that these datasets exhibit the lowest ratio of absent keyphrases. Neural-based models obtain the greatest performance, but only for the dataset on which they were trained. We therefore see that these models do not generalize well across domains, confirming previous preliminary findings BIBREF2 and exacerbating the need for further research on this topic. Interestingly, CopyNews outperforms the other models on JPTimes and achieves very low scores for KPCrowd and DUC-2001, although all these datasets are from the same domain. This emphasizes the differences that exist between the reader- and editor-assigned gold standard. The score difference may be explained by the ratio of absent keyphrases that differs greatly between the reader-annotated datasets and JPTimes (see Table ), and thus question the use of these rather extractive datasets for evaluating keyphrase generation.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, No, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bc7081aaa207de2362e0bea7bc8108d338aee36f</th>\n",
       "      <td>Frequently used datasets for keyphrase generation have a common characteristic that they are, by and large, made from scholarly documents (abstracts or full texts) paired with non-expert (mostly from authors) annotations. Notable examples of such datasets are SemEval-2010 BIBREF8 and KP20k BIBREF2, which respectively comprises scientific articles and paper abstracts, both about computer science and information technology. Detailed statistics are listed in Table . Only two publicly available datasets, that we are aware of, contain news documents: DUC-2001 BIBREF9 and KPCrowd BIBREF10. Originally created for the DUC evaluation campaign on text summarization BIBREF11, the former is composed of 308 news annotated by graduate students. The latter includes 500 news annotated by crowdsourcing. Both datasets are very small and contain newswire articles from various online sources labelled by non-expert annotators, in this case readers, which is not without issues.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, No, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bc7081aaa207de2362e0bea7bc8108d338aee36f</th>\n",
       "      <td>Thus, unlike author annotations, those produced by readers exhibit significantly lower missing keyphrases, that is, gold keyphrases that do not occur in the content of the document. In the DUC-2001 dataset for example, more than 96% of the gold keyphrases actually appear in the documents. This confirms previous observations that readers tend to assign keyphrases in an extractive fashion BIBREF12, which makes these datasets less suitable for the task at hand (keyphrase generation) but rather relevant for a purely extractive task (keyphrase extraction). Yet, author-assigned keyphrases commonly found in scientific paper datasets are not perfect either, as they are less constrained BIBREF13 and include seldom-used variants or misspellings that negatively impact performance. One can see there is an apparent lack of sizeable expert-annotated data that enables the development of neural keyphrase generation models in a domain other than scholarly texts. Here, we fill this gap and propose a large-scale dataset that includes news texts paired with manually curated gold standard annotations.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, No, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bc7081aaa207de2362e0bea7bc8108d338aee36f</th>\n",
       "      <td>In this paper we presented KPTimes, a large-scale dataset of newswire articles to train and test deep learning models for keyphrase generation. The dataset and the code are available at https://github.com/ygorg/KPTimes. Large datasets have driven rapid improvement in other natural language generation tasks, such as machine translation or summarization. We hope that KPTimes will play this role and help the community in devising more robust and generalizable neural keyphrase generation models.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, No, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bc7081aaa207de2362e0bea7bc8108d338aee36f</th>\n",
       "      <td>We follow the common practice and evaluate the performance of each model in terms of f-measure (F$_1$) at the top $N=10$ keyphrases, and apply stemming to reduce the number of mismatches. We also report the Mean Average Precision (MAP) scores of the ranked lists of keyphrases.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, No, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693cdb9978749db04ba34d9c168e71534f00a226</th>\n",
       "      <td>[2] Devlin, J., Chang, M. W., Lee, K. &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Yes, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693cdb9978749db04ba34d9c168e71534f00a226</th>\n",
       "      <td>[4] Baddeley, A., Gathercole, S. &amp; Papagno, C. (1998). The phonological loop as a language learning device. Psychological review, 105(1), 158.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Yes, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693cdb9978749db04ba34d9c168e71534f00a226</th>\n",
       "      <td>[17] Zhou, T., Brown, M., Snavely, N. &amp; Lowe, D. G. (2017). Unsupervised learning of depth and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1851-1858).</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Yes, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693cdb9978749db04ba34d9c168e71534f00a226</th>\n",
       "      <td>The first syntaxes that LGI has learned are the ‘move left’ and ‘move right’ random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word ‘move’ given the first letter ‘m’ (till now, LGI has only learned syntaxes of ‘move left or right’). LGI tried to predict the second word ‘right’ with initial letter ‘r’, however, after knowing the command text is ‘l’, it turned to complete the following symbols with ‘eft’. It doesn’t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position. Even if the command asked to move out of screen, LGI still could reconstruct the partially occluded image with high fidelity.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Yes, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693cdb9978749db04ba34d9c168e71534f00a226</th>\n",
       "      <td>[1] Wei, M., He, Y., Zhang, Q. &amp; Si, L. (2019). Multi-Instance Learning for End-to-End Knowledge Base Question Answering. arXiv preprint arXiv:1903.02652.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Yes, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a17fc7b96753f85aee1d2036e2627570f4b50c30</th>\n",
       "      <td>Results: The comparison between BERT embeddings and other models is presented in Table TABREF5. Overall, in-domain fine-tuned BERT delivers the best performance. We report new state-of-the-art results on WikiPassageQA ($33\\%$ improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a17fc7b96753f85aee1d2036e2627570f4b50c30</th>\n",
       "      <td>Pre-trained vs. Fine-tuned BERT: All the models we considered in this paper benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment. Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a17fc7b96753f85aee1d2036e2627570f4b50c30</th>\n",
       "      <td>As we can see from the table, embeddings from pre-trained BERT are good at capturing sentence-level syntactic information and semantic information, but poor at semantic similarity tasks and surface information tasks. Our findings are consistent with BIBREF12 work on assessing BERT's syntactic abilities. Fine-tuning on natural language inference datasets improves the quality of sentence embedding, especially on semantic similarity tasks and entailment tasks. Combining embeddings from two layers can further boost the performance on sentence surface and syntactic information probing tasks. Experiments were also conducted by combining embeddings from multiple layers. However, there is no significant and consistent improvement over pooling just from two layers. Adding multi-layer perceptron (MLP) instead of logistic regression layer on top of the embeddings also provides no significant changes in performance, which suggests that most linguistic properties can be extracted with just a linear readout of the embeddings. Our best model is the combination of embeddings from the top and bottom layer of the BERT fine-tuned on SNLI dataset.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a17fc7b96753f85aee1d2036e2627570f4b50c30</th>\n",
       "      <td>Datasets: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16. They cover both factoid and non-factoid QA and different average passage length. The statistics of the four datasets are provided in the Appendix. To generate passage-level question-answering data from Quasart-t and SearchQA, we used the retrieved passages for each question from OpenQA, and generated question-passage relevance label based on whether the ground truth answer is contained in the passage.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a17fc7b96753f85aee1d2036e2627570f4b50c30</th>\n",
       "      <td>In this paper, we conducted an empirical study of layer-wise activations of BERT as general-purpose text embeddings. We want to understand to what extent does the BERT representation capture syntactic and semantic information. The sentence-level embeddings are evaluated on downstream and probing tasks using the SentEval toolkit BIBREF8, while the passage-level encodings are evaluated on four passage-level QA datasets (both factoid and non-factoid) under a learning-to-rank setting. Different methods of combining query embeddings with passage-level answer embeddings are examined.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2f142cd11731d29d0c3fa426e26ef80d997862e0</th>\n",
       "      <td>We perform experiments on RumourEval and PHEME datasets to evaluate the performance of our method and the baselines. The experimental results are shown in Table TABREF27. We gain the following observations:</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2f142cd11731d29d0c3fa426e26ef80d997862e0</th>\n",
       "      <td>Table TABREF30 provides the experimental results of these methods on RumourEval and PHEME datasets. We have the following observations:</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2f142cd11731d29d0c3fa426e26ef80d997862e0</th>\n",
       "      <td>We use two public datasets for fake news detection and stance detection, i.e., RumourEval BIBREF36 and PHEME BIBREF12. We introduce both the datasets in details from three aspects: content, labels, and distribution.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2f142cd11731d29d0c3fa426e26ef80d997862e0</th>\n",
       "      <td>Experiments on two public, widely used fake news datasets demonstrate that our method significantly outperforms previous state-of-the-art methods.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2f142cd11731d29d0c3fa426e26ef80d997862e0</th>\n",
       "      <td>Although our method shows relatively low performance in terms of precision (P) and recall (R) compared with some specific models, our method achieves the state-of-the-art performance in terms of accuracy (A) and F1-score (F1) on both datasets. Taking into account the tradeoff among different performance measures, this reveals the effectiveness of our method in the task of fake news detection.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ec5e84a1d1b12f7185183d165cbb5eae66d9833e</th>\n",
       "      <td>Automated Essay Scoring (AES) and Automated Short Answer Scoring (ASAS) has become more prevalent among testing agencies BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . These systems are often designed to address one task and one task alone; to determine whether a written piece of text addresses a question or not. These engines were originally based on either hand-crafted features or term frequency–inverse document frequency (TF-IDF) approaches BIBREF4 . More recently, these techniques have been superseded by the combination of word-embeddings and neural networks BIBREF5 , BIBREF6 , BIBREF7 . For semantically simple responses, the accuracy of these approaches can often be greater than accuracy of human raters, however, these systems are not trained to appropriately deal with the anomalous cases in which a student writes something that elicits concern for the writer or those around them, which we simply call an `alert'. Typically essay scoring systems do not handle alerts, but rather, separate systems must be designed to process these types of responses before they are sent to the essay scoring system. Our goal is not to produce a classification, but rather to use the same methods developed in AES, ASAS and sentiment analysis BIBREF8 , BIBREF9 to identify some percentage of responses that fit patterns seen in known alerts and send them to be assessed by a team of reviewers.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ec5e84a1d1b12f7185183d165cbb5eae66d9833e</th>\n",
       "      <td>Since the programs inception, we have greatly expanded our collection of training data, which is summarized below in Table TABREF3 . While we have accumulated over 1.11 million essay responses, which include many types of essays over a range of essay topics, student age ranges, styles of writing as well as a multitude of types of alerts, we find that many of them are mapped to the same set of words after applying our preprocessing steps. When we disregard duplicate responses after preprocessing, our training sample consists of only 866,137 unique responses.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ec5e84a1d1b12f7185183d165cbb5eae66d9833e</th>\n",
       "      <td>The American Institutes for Research tests up to 1.8 million students a day during peak testing periods. Over the 2016–2017 period AIR delivered 48 million online tests across America. Each test could involve a number of comments, notes and long answer free-form text responses that are considered to be a possible alerts as well as equations or other interactive items that are not considered to be possible alerts. In a single year we evaluate approximately 90 million free-form text responses which range anywhere from a single word or number to ten thousand word essays. These responses are recorded in html and embedded within an xml file along with additional information that allows our clients to identify which student wrote the response. The first step in processing such a response is to remove tags, html code and any non-text using regular expressions.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ec5e84a1d1b12f7185183d165cbb5eae66d9833e</th>\n",
       "      <td>The American Institutes for Research has a hand-scoring team specifically devoted to verifying whether a given response satisfies the requirements of being an alert. At the beginning of this program, we had very few examples of student responses that satisfied the above requirements, moreover, given the diverse nature of what constitutes an alert, the alerts we did have did not span all the types of responses we considered to be worthy of attention. As part of the initial data collection, we accumulated synthetic responses from the sites Reddit and Teen Line that were likely to be of interest. These were sent to the hand-scoring team and assessed as if they were student responses. The responses pulled consisted of posts from forums that we suspected of containing alerts as well as generic forums so that the engine produced did not simply classify forum posts from student responses. We observed that the manner in which the students engaged with the our essay platform in cases of alerts mimicked the way in which students used online forums in a sufficiently similar manner for the data to faithfully represent real alerts. This additional data also provided crucial examples of classes of alerts found too infrequently in student data for a valid classification. This initial data allowed us to build preliminary models and hence build better engines.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ec5e84a1d1b12f7185183d165cbb5eae66d9833e</th>\n",
       "      <td>We should also mention that the above results do not represent the state-of-the-art, since we were able to take simple aggregated results from the models to produce better statistics at each threshold level than our best model. This can be done in a similar manner to the work of BIBREF23 , however, this is a topic we leave for a future paper. It is also unclear as to whether traditional sentiment analysis provides additional information from which better estimates may be possible.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74cc0300e22f60232812019011a09df92bbec803</th>\n",
       "      <td>Other types of language data also typically contains a mixture of subjective and objective sentences, e.g. Wiebe et al. wiebeetal2001a,wiebeetalcl04 found that 44% of sentences in a news corpus were subjective. Our work is also related to research on distinguishing subjective and objective text BIBREF33 , BIBREF34 , BIBREF14 , including bootstrapped pattern learning for subjective/objective sentence classification BIBREF15 . However, prior work has primarily focused on news texts, not argumentation, and the notion of objective language is not exactly the same as factual. Our work also aims to recognize emotional language specifically, rather than all forms of subjective language. There has been substantial work on sentiment and opinion analysis (e.g., BIBREF35 , BIBREF36 , BIBREF37 , BIBREF38 , BIBREF39 , BIBREF40 ) and recognition of specific emotions in text BIBREF41 , BIBREF42 , BIBREF43 , BIBREF44 , which could be incorporated in future extensions of our work. We also hope to examine more closely the relationship of this work to previous work aimed at the identification of nasty vs. nice arguments in the IAC BIBREF45 , BIBREF8 .</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74cc0300e22f60232812019011a09df92bbec803</th>\n",
       "      <td>The upper section of Table TABREF11 shows the Precision and Recall results for the patterns learned during bootstrapping. The Iter 0 row shows the performance of the patterns learned only from the original, annotated training data. The remaining rows show the results for the patterns learned from the unannotated texts during bootstrapping, added cumulatively. We show the results after each iteration of bootstrapping.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74cc0300e22f60232812019011a09df92bbec803</th>\n",
       "      <td>We also evaluated the performance of a Naive Bayes (NB) classifier to assess the difficulty of this task with a traditional supervised learning algorithm. We trained a Naive Bayes classifier with unigram features and binary values on the training data, and identified the best Laplace smoothing parameter using the development data. The bottom row of Table TABREF11 shows the results for the NB classifier on the test data. These results show that the NB classifier yields substantially higher recall for both categories, undoubtedly due to the fact that the classifier uses all unigram information available in the text. Our pattern learner, however, was restricted to learning linguistic expressions in specific syntactic constructions, usually requiring more than one word, because our goal was to study specific expressions associated with factual and feeling argument styles. Table TABREF11 shows that the lexico-syntactic patterns did obtain higher precision than the NB classifier, but with lower recall.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74cc0300e22f60232812019011a09df92bbec803</th>\n",
       "      <td>The goal of our research is to gain insights into the types of linguistic expressions and properties that are distinctive and common in factual and feeling based argumentation. We also explore whether it is possible to develop a high-precision fact vs. feeling classifier that can be applied to unannotated data to find new linguistic expressions that did not occur in our original labeled corpus.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74cc0300e22f60232812019011a09df92bbec803</th>\n",
       "      <td>Initially, we give the labeled training data to AutoSlog-TS, which generates patterns and associated statistics. The next step identifies high-precision patterns that can be used to label some of the unannotated texts as factual or feeling. We define two thresholds: INLINEFORM0 to represent a minimum frequency value, and INLINEFORM1 to represent a minimum probability value. We found that using only a small set of patterns (when INLINEFORM2 is set to a high value) achieves extremely high precision, yet results in a very low recall. Instead, we adopt a strategy of setting a moderate probability threshold to identify reasonably reliable patterns, but labeling a text as factual or feeling only if it contains at least a certain number different patterns for that category, INLINEFORM3 . In order to calibrate the thresholds, we experimented with a range of threshold values on the development (tuning) data and identified INLINEFORM4 =3, INLINEFORM5 =.70, and INLINEFORM6 =3 for the factual class, and INLINEFORM7 =3, INLINEFORM8 =.55, and INLINEFORM9 =3 for the feeling class as having the highest classification precision (with non-trivial recall).</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37b0ee4a9d0df3ae3493e3b9114c3f385746da5c</th>\n",
       "      <td>We address the data bias issue as much as possible by carefully analyzing the relationships of different variables in the data generating process. We use a Causal Diagram BIBREF5 , BIBREF6 to analyze and remove the effects of the data bias (e.g., the speakers' reputations, popularity gained by publicity, etc.) in our prediction model. In order to make the prediction model less biased to the speakers' race and gender, we confine our analysis to the transcripts only. Besides, we normalize the ratings to remove the effects of the unwanted variables such as the speakers' reputations, publicity, contemporary hot topics, etc.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37b0ee4a9d0df3ae3493e3b9114c3f385746da5c</th>\n",
       "      <td>BIBREF30 analyzed the TED Talks for humor detection. BIBREF31 analyzed the transcripts of the TED talks to predict audience engagement in the form of applause. BIBREF32 predicted user interest (engaging vs. non-engaging) from high-level visual features (e.g., camera angles) and audience applause. BIBREF33 proposed a sentiment-aware nearest neighbor model for a multimedia recommendation over the TED talks. BIBREF34 predicted the TED talk ratings from the linguistic features of the transcripts. This work is most similar to ours. However, we are proposing a new prediction framework using the Neural Networks.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37b0ee4a9d0df3ae3493e3b9114c3f385746da5c</th>\n",
       "      <td>We use two neural network architectures in the prediction task. In the first architecture, we use LSTM BIBREF7 for a sequential input of the words within the sentences of the transcripts. In the second architecture, we use TreeLSTM BIBREF8 to represent the input sentences in the form of a dependency tree. Our experiments show that the dependency tree-based model can predict the TED talk ratings with slightly higher performance (average F-score 0.77) than the word sequence model (average F-score 0.76). To the best of our knowledge, this is the best performance in the literature on predicting the TED talk ratings. We compare the performances of these two models with a baseline of classical machine learning techniques using hand-engineered features. We find that the neural networks largely outperform the classical methods. We believe this gain in performance is achieved by the networks' ability to capture better the natural relationship of the words (as compared to the hand engineered feature selection approach in the baseline methods) and the correlations among different rating labels.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37b0ee4a9d0df3ae3493e3b9114c3f385746da5c</th>\n",
       "      <td>For our analysis, we curate an observational dataset of public speech transcripts and other meta-data collected from the ted.com website. This website contains a large collection of high-quality public speeches that are freely available to watch, share, rate, and comment on. Every day, numerous people watch and annotate their perceptions about the talks. Our dataset contains 2231 public speech transcripts and over 5 million ratings from the spontaneous viewers of the talks. The viewers annotate each talk by 14 different labels—Beautiful, Confusing, Courageous, Fascinating, Funny, Informative, Ingenious, Inspiring, Jaw-Dropping, Long-winded, Obnoxious, OK, Persuasive, and Unconvincing.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37b0ee4a9d0df3ae3493e3b9114c3f385746da5c</th>\n",
       "      <td>The data for this study was gathered from the ted.com website on November 15, 2017. We removed the talks published six months before the crawling date to make sure each talk has enough ratings for a robust analysis. More specifically, we filtered any talk that—</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd</th>\n",
       "      <td>We collected a total of 18 million, geo-tagged, English-language tweets over three years, from January 1st, 2012 to January 1st, 2015, evenly divided across all 36 months, using Historical PowerTrack for Twitter provided by GNIP. We created geolocation bounding boxes for each of the 50 states which were used to collect our dataset. All 18 million tweets originated from one of the 50 states and are tagged as such. Moreover, all tweets contained one of the six emoticons in Table TABREF7 and were labelled as either positive or negative based on the emoticon. Out of the 18 million tweets, INLINEFORM0 million ( INLINEFORM1 ) were labelled as positive and INLINEFORM2 million ( INLINEFORM3 ) were labelled as negative. The 18 million tweets came from INLINEFORM4 distinct users.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd</th>\n",
       "      <td>There are different methods of obtaining labelled data using distant supervision BIBREF1 , BIBREF6 , BIBREF19 , BIBREF12 . We used emoticons to label tweets as positive or negative, an approach that was introduced by Read BIBREF1 and used in multiple works BIBREF6 , BIBREF12 . We collected millions of English-language tweets from different times, dates, authors and US states. We used a total of six emoticons, three mapping to positive and three mapping to negative sentiment (table TABREF7 ). We identified more than 120 positive and negative ASCII emoticons and unicode emojis, but we decided to only use the six most common emoticons in order to avoid possible selection biases. For example, people who use obscure emoticons and emojis might have a different base sentiment from those who do not. Using the six most commonly used emoticons limits this bias. Since there are no \"neutral\" emoticons, our dataset is limited to tweets with positive or negative sentiments. Accordingly, in this work we are only concerned with analysing and classifying the polarity of tweets (negative vs. positive) and not their subjectivity (neutral vs. non-neutral). Below we will explain our data collection and corpus in greater detail.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd</th>\n",
       "      <td>This is our purely linguistic baseline model.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd</th>\n",
       "      <td>There has also been previous work on measuring the happiness of people in different contexts (location, time, etc). This has been done mostly through traditional land-line polling BIBREF5 , BIBREF4 , with Gallup's annual happiness index being a prime example BIBREF4 . More recently, some have utilized Twitter to measure people's mood and happiness and have found Twitter to be a generally good measure of the public's overall happiness, well-being and mood. For example, Bollen et al. BIBREF15 used Twitter to measure the daily mood of the public and compare that to the record of social, political, cultural and economic events in the real world. They found that these events have a significant effect on the public mood as measured through Twitter. Another example would be the work of Mitchell et al. BIBREF16 , in which they estimated the happiness levels of different states and cities in the USA using Twitter and found statistically significant correlations between happiness level and the demographic characteristics (such as obesity rates and education levels) of those regions. Finally, improving natural language processing by incorporating contextual information has been successfully attempted before BIBREF17 , BIBREF18 ; but as far as we are aware, this has not been attempted for sentiment classification.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd</th>\n",
       "      <td>Even though our contextual classifier was able to outperform the previous state-of-the-art, distant supervised sentiment classifier, it should be noted that our contextual classifier's performance is boosted significantly by spatial information extracted through geo-tags. However, only about one to two percent of tweets in the wild are geo-tagged. Therefore, we trained and evaluated our contextual model using all the variables except for state. The accuracy of this model was INLINEFORM0 , which is still significantly better than the performance of the purely linguistic classifier. Fortunately, all tweets are tagged with timestamps and author information, so all the other four contextual variables used in our model can be used for classifying the sentiment of any tweet.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Yes, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d662fb442d5fc332194770aac835f401c2148d9</th>\n",
       "      <td>The English-German translation models are trained on WMT datasets, including News Commentary 13, Europarl v7, and Common Crawl, and evaluated on newstest2013 for early stopping. On the newstest2013 dev set, the En$\\rightarrow $De model reaches a BLEU-4 score of 19.6, and the De$\\rightarrow $En model reaches a BLEU-4 score of 24.6.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[No, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d662fb442d5fc332194770aac835f401c2148d9</th>\n",
       "      <td>The English-French models are trained on Common Crawl 13, Europarl v7, News Commentary v9, Giga release 2, and UN doc 2000. On the newstest2013 dev set, the En$\\rightarrow $Fr model reaches a BLEU-4 score of 25.6, and the Fr$\\rightarrow $En model reaches a BLEU-4 score of 26.1.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[No, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d662fb442d5fc332194770aac835f401c2148d9</th>\n",
       "      <td>We use 303 sub areas from Stack Exchange data dumps. The full list of area names is in the appendix. We do not include Stack Overflow because it is too specific to programming related questions. We also exclude all questions under the following language sub areas: Chinese, German, Spanish, Russian, Japanese, Korean, Latin, Ukrainian. This ensures that the questions in MQR are mostly English sentences. Having questions from 303 Stack Exchange sites makes the MQR dataset cover a broad range of domains.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[No, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d662fb442d5fc332194770aac835f401c2148d9</th>\n",
       "      <td>We also train a paraphrase generation model on a subset of the ParaNMT dataset BIBREF24, which was created automatically by using neural machine translation to translate the Czech side of a large Czech-English parallel corpus. We use the filtered subset of 5M pairs provided by the authors. For each pair of paraphrases (S1 and S2) in the dataset, we train the model to rewrite from S1 to S2 and also rewrite from S2 to S1. We use the MQR DEV set for early stopping during training.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[No, Yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d662fb442d5fc332194770aac835f401c2148d9</th>\n",
       "      <td>To ensure there are no sentences written in non-English languages, we keep questions that contain 80% or more of valid English characters, including punctuation.</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[No, Yes]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  fewshot_evidence  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Model performances for each dataset are reported in Table . Extractive baselines show the best results for KPCrowd and DUC-2001 which is not surprising given that these datasets exhibit the lowest ratio of absent keyphrases. Neural-based models obtain the greatest performance, but only for the dataset on which they were trained. We therefore see that these models do not generalize well across domains, confirming previous preliminary findings BIBREF2 and exacerbating the need for further research on this topic. Interestingly, CopyNews outperforms the other models on JPTimes and achieves very low scores for KPCrowd and DUC-2001, although all these datasets are from the same domain. This emphasizes the differences that exist between the reader- and editor-assigned gold standard. The score difference may be explained by the ratio of absent keyphrases that differs greatly between the reader-annotated datasets and JPTimes (see Table ), and thus question the use of these rather extractive datasets for evaluating keyphrase generation.   \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Frequently used datasets for keyphrase generation have a common characteristic that they are, by and large, made from scholarly documents (abstracts or full texts) paired with non-expert (mostly from authors) annotations. Notable examples of such datasets are SemEval-2010 BIBREF8 and KP20k BIBREF2, which respectively comprises scientific articles and paper abstracts, both about computer science and information technology. Detailed statistics are listed in Table . Only two publicly available datasets, that we are aware of, contain news documents: DUC-2001 BIBREF9 and KPCrowd BIBREF10. Originally created for the DUC evaluation campaign on text summarization BIBREF11, the former is composed of 308 news annotated by graduate students. The latter includes 500 news annotated by crowdsourcing. Both datasets are very small and contain newswire articles from various online sources labelled by non-expert annotators, in this case readers, which is not without issues.   \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Thus, unlike author annotations, those produced by readers exhibit significantly lower missing keyphrases, that is, gold keyphrases that do not occur in the content of the document. In the DUC-2001 dataset for example, more than 96% of the gold keyphrases actually appear in the documents. This confirms previous observations that readers tend to assign keyphrases in an extractive fashion BIBREF12, which makes these datasets less suitable for the task at hand (keyphrase generation) but rather relevant for a purely extractive task (keyphrase extraction). Yet, author-assigned keyphrases commonly found in scientific paper datasets are not perfect either, as they are less constrained BIBREF13 and include seldom-used variants or misspellings that negatively impact performance. One can see there is an apparent lack of sizeable expert-annotated data that enables the development of neural keyphrase generation models in a domain other than scholarly texts. Here, we fill this gap and propose a large-scale dataset that includes news texts paired with manually curated gold standard annotations.   \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In this paper we presented KPTimes, a large-scale dataset of newswire articles to train and test deep learning models for keyphrase generation. The dataset and the code are available at https://github.com/ygorg/KPTimes. Large datasets have driven rapid improvement in other natural language generation tasks, such as machine translation or summarization. We hope that KPTimes will play this role and help the community in devising more robust and generalizable neural keyphrase generation models.   \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We follow the common practice and evaluate the performance of each model in terms of f-measure (F$_1$) at the top $N=10$ keyphrases, and apply stemming to reduce the number of mismatches. We also report the Mean Average Precision (MAP) scores of the ranked lists of keyphrases.   \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [2] Devlin, J., Chang, M. W., Lee, K. & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.   \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            [4] Baddeley, A., Gathercole, S. & Papagno, C. (1998). The phonological loop as a language learning device. Psychological review, 105(1), 158.   \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [17] Zhou, T., Brown, M., Snavely, N. & Lowe, D. G. (2017). Unsupervised learning of depth and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1851-1858).   \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The first syntaxes that LGI has learned are the ‘move left’ and ‘move right’ random pixels, with the corresponding results shown in Figure 3. After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed. The predicted text can complete the word ‘move’ given the first letter ‘m’ (till now, LGI has only learned syntaxes of ‘move left or right’). LGI tried to predict the second word ‘right’ with initial letter ‘r’, however, after knowing the command text is ‘l’, it turned to complete the following symbols with ‘eft’. It doesn’t care if the sentence length is 12 or 11, the predicted image and text just came at proper time and position. Even if the command asked to move out of screen, LGI still could reconstruct the partially occluded image with high fidelity.   \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                [1] Wei, M., He, Y., Zhang, Q. & Si, L. (2019). Multi-Instance Learning for End-to-End Knowledge Base Question Answering. arXiv preprint arXiv:1903.02652.   \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30  Results: The comparison between BERT embeddings and other models is presented in Table TABREF5. Overall, in-domain fine-tuned BERT delivers the best performance. We report new state-of-the-art results on WikiPassageQA ($33\\%$ improvement in MAP) and InsuranceQA (version 1.0) ($3.6\\%$ improvement in P@1) by supervised fine-tuning BERT using pairwise rank hinge loss. When evaluated on non-factoid QA datasets, there is a big gap between BERT embeddings and the fully fine-tuned BERT, which suggests that deep interactions between questions and answers are critical to the task. However, the gap is much smaller for factoid QA datasets. Since non-factoid QA depends more on content matching rather than vocabulary matching, the results are kind of expected. Similar to BERT for sentence embeddings, mean-pooling and combining the top and bottom layer embeddings lead to better performance, and $(u, v, u * v, |u - v|)$ shows the strongest results among other interaction schemes. Different from sentence-level embeddings, fine-tuning BERT on SNLI doesn't lead to significant improvement, which suggests possible domain mismatch between SNLI and the QA datasets. MLP layer usually provided a 1-2 percent boost in performance compared to the logistic regression layer. For WikiPassageQA, BERT embeddings perform comparably as BM25 baseline. For InsuranceQA, BERT embeddings outperform a strong representation-based matching model DSSM BIBREF18, but still far behind the state-of-the-art interaction-based model SUBMULT+NN BIBREF17 and fully fine-tuned BERT. On factoid datasets (Quasar-t and SearchQA), BERT embeddings outperform BM25 baseline significantly.   \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Pre-trained vs. Fine-tuned BERT: All the models we considered in this paper benefit from supervised training on natural language inference datasets. In this section, we compare the performance of embeddings from pre-trained BERT and fine-tuned BERT. Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment. Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix.   \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 As we can see from the table, embeddings from pre-trained BERT are good at capturing sentence-level syntactic information and semantic information, but poor at semantic similarity tasks and surface information tasks. Our findings are consistent with BIBREF12 work on assessing BERT's syntactic abilities. Fine-tuning on natural language inference datasets improves the quality of sentence embedding, especially on semantic similarity tasks and entailment tasks. Combining embeddings from two layers can further boost the performance on sentence surface and syntactic information probing tasks. Experiments were also conducted by combining embeddings from multiple layers. However, there is no significant and consistent improvement over pooling just from two layers. Adding multi-layer perceptron (MLP) instead of logistic regression layer on top of the embeddings also provides no significant changes in performance, which suggests that most linguistic properties can be extracted with just a linear readout of the embeddings. Our best model is the combination of embeddings from the top and bottom layer of the BERT fine-tuned on SNLI dataset.   \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Datasets: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16. They cover both factoid and non-factoid QA and different average passage length. The statistics of the four datasets are provided in the Appendix. To generate passage-level question-answering data from Quasart-t and SearchQA, we used the retrieved passages for each question from OpenQA, and generated question-passage relevance label based on whether the ground truth answer is contained in the passage.   \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In this paper, we conducted an empirical study of layer-wise activations of BERT as general-purpose text embeddings. We want to understand to what extent does the BERT representation capture syntactic and semantic information. The sentence-level embeddings are evaluated on downstream and probing tasks using the SentEval toolkit BIBREF8, while the passage-level encodings are evaluated on four passage-level QA datasets (both factoid and non-factoid) under a learning-to-rank setting. Different methods of combining query embeddings with passage-level answer embeddings are examined.   \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We perform experiments on RumourEval and PHEME datasets to evaluate the performance of our method and the baselines. The experimental results are shown in Table TABREF27. We gain the following observations:   \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Table TABREF30 provides the experimental results of these methods on RumourEval and PHEME datasets. We have the following observations:   \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We use two public datasets for fake news detection and stance detection, i.e., RumourEval BIBREF36 and PHEME BIBREF12. We introduce both the datasets in details from three aspects: content, labels, and distribution.   \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Experiments on two public, widely used fake news datasets demonstrate that our method significantly outperforms previous state-of-the-art methods.   \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Although our method shows relatively low performance in terms of precision (P) and recall (R) compared with some specific models, our method achieves the state-of-the-art performance in terms of accuracy (A) and F1-score (F1) on both datasets. Taking into account the tradeoff among different performance measures, this reveals the effectiveness of our method in the task of fake news detection.   \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                                                                                                                                                                                                                                                                               Automated Essay Scoring (AES) and Automated Short Answer Scoring (ASAS) has become more prevalent among testing agencies BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . These systems are often designed to address one task and one task alone; to determine whether a written piece of text addresses a question or not. These engines were originally based on either hand-crafted features or term frequency–inverse document frequency (TF-IDF) approaches BIBREF4 . More recently, these techniques have been superseded by the combination of word-embeddings and neural networks BIBREF5 , BIBREF6 , BIBREF7 . For semantically simple responses, the accuracy of these approaches can often be greater than accuracy of human raters, however, these systems are not trained to appropriately deal with the anomalous cases in which a student writes something that elicits concern for the writer or those around them, which we simply call an `alert'. Typically essay scoring systems do not handle alerts, but rather, separate systems must be designed to process these types of responses before they are sent to the essay scoring system. Our goal is not to produce a classification, but rather to use the same methods developed in AES, ASAS and sentiment analysis BIBREF8 , BIBREF9 to identify some percentage of responses that fit patterns seen in known alerts and send them to be assessed by a team of reviewers.   \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Since the programs inception, we have greatly expanded our collection of training data, which is summarized below in Table TABREF3 . While we have accumulated over 1.11 million essay responses, which include many types of essays over a range of essay topics, student age ranges, styles of writing as well as a multitude of types of alerts, we find that many of them are mapped to the same set of words after applying our preprocessing steps. When we disregard duplicate responses after preprocessing, our training sample consists of only 866,137 unique responses.   \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The American Institutes for Research tests up to 1.8 million students a day during peak testing periods. Over the 2016–2017 period AIR delivered 48 million online tests across America. Each test could involve a number of comments, notes and long answer free-form text responses that are considered to be a possible alerts as well as equations or other interactive items that are not considered to be possible alerts. In a single year we evaluate approximately 90 million free-form text responses which range anywhere from a single word or number to ten thousand word essays. These responses are recorded in html and embedded within an xml file along with additional information that allows our clients to identify which student wrote the response. The first step in processing such a response is to remove tags, html code and any non-text using regular expressions.   \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                                                                                                                                                                                                                                                                                                     The American Institutes for Research has a hand-scoring team specifically devoted to verifying whether a given response satisfies the requirements of being an alert. At the beginning of this program, we had very few examples of student responses that satisfied the above requirements, moreover, given the diverse nature of what constitutes an alert, the alerts we did have did not span all the types of responses we considered to be worthy of attention. As part of the initial data collection, we accumulated synthetic responses from the sites Reddit and Teen Line that were likely to be of interest. These were sent to the hand-scoring team and assessed as if they were student responses. The responses pulled consisted of posts from forums that we suspected of containing alerts as well as generic forums so that the engine produced did not simply classify forum posts from student responses. We observed that the manner in which the students engaged with the our essay platform in cases of alerts mimicked the way in which students used online forums in a sufficiently similar manner for the data to faithfully represent real alerts. This additional data also provided crucial examples of classes of alerts found too infrequently in student data for a valid classification. This initial data allowed us to build preliminary models and hence build better engines.   \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We should also mention that the above results do not represent the state-of-the-art, since we were able to take simple aggregated results from the models to produce better statistics at each threshold level than our best model. This can be done in a similar manner to the work of BIBREF23 , however, this is a topic we leave for a future paper. It is also unclear as to whether traditional sentiment analysis provides additional information from which better estimates may be possible.   \n",
       "74cc0300e22f60232812019011a09df92bbec803                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Other types of language data also typically contains a mixture of subjective and objective sentences, e.g. Wiebe et al. wiebeetal2001a,wiebeetalcl04 found that 44% of sentences in a news corpus were subjective. Our work is also related to research on distinguishing subjective and objective text BIBREF33 , BIBREF34 , BIBREF14 , including bootstrapped pattern learning for subjective/objective sentence classification BIBREF15 . However, prior work has primarily focused on news texts, not argumentation, and the notion of objective language is not exactly the same as factual. Our work also aims to recognize emotional language specifically, rather than all forms of subjective language. There has been substantial work on sentiment and opinion analysis (e.g., BIBREF35 , BIBREF36 , BIBREF37 , BIBREF38 , BIBREF39 , BIBREF40 ) and recognition of specific emotions in text BIBREF41 , BIBREF42 , BIBREF43 , BIBREF44 , which could be incorporated in future extensions of our work. We also hope to examine more closely the relationship of this work to previous work aimed at the identification of nasty vs. nice arguments in the IAC BIBREF45 , BIBREF8 .   \n",
       "74cc0300e22f60232812019011a09df92bbec803                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The upper section of Table TABREF11 shows the Precision and Recall results for the patterns learned during bootstrapping. The Iter 0 row shows the performance of the patterns learned only from the original, annotated training data. The remaining rows show the results for the patterns learned from the unannotated texts during bootstrapping, added cumulatively. We show the results after each iteration of bootstrapping.   \n",
       "74cc0300e22f60232812019011a09df92bbec803                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We also evaluated the performance of a Naive Bayes (NB) classifier to assess the difficulty of this task with a traditional supervised learning algorithm. We trained a Naive Bayes classifier with unigram features and binary values on the training data, and identified the best Laplace smoothing parameter using the development data. The bottom row of Table TABREF11 shows the results for the NB classifier on the test data. These results show that the NB classifier yields substantially higher recall for both categories, undoubtedly due to the fact that the classifier uses all unigram information available in the text. Our pattern learner, however, was restricted to learning linguistic expressions in specific syntactic constructions, usually requiring more than one word, because our goal was to study specific expressions associated with factual and feeling argument styles. Table TABREF11 shows that the lexico-syntactic patterns did obtain higher precision than the NB classifier, but with lower recall.   \n",
       "74cc0300e22f60232812019011a09df92bbec803                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The goal of our research is to gain insights into the types of linguistic expressions and properties that are distinctive and common in factual and feeling based argumentation. We also explore whether it is possible to develop a high-precision fact vs. feeling classifier that can be applied to unannotated data to find new linguistic expressions that did not occur in our original labeled corpus.   \n",
       "74cc0300e22f60232812019011a09df92bbec803                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Initially, we give the labeled training data to AutoSlog-TS, which generates patterns and associated statistics. The next step identifies high-precision patterns that can be used to label some of the unannotated texts as factual or feeling. We define two thresholds: INLINEFORM0 to represent a minimum frequency value, and INLINEFORM1 to represent a minimum probability value. We found that using only a small set of patterns (when INLINEFORM2 is set to a high value) achieves extremely high precision, yet results in a very low recall. Instead, we adopt a strategy of setting a moderate probability threshold to identify reasonably reliable patterns, but labeling a text as factual or feeling only if it contains at least a certain number different patterns for that category, INLINEFORM3 . In order to calibrate the thresholds, we experimented with a range of threshold values on the development (tuning) data and identified INLINEFORM4 =3, INLINEFORM5 =.70, and INLINEFORM6 =3 for the factual class, and INLINEFORM7 =3, INLINEFORM8 =.55, and INLINEFORM9 =3 for the feeling class as having the highest classification precision (with non-trivial recall).   \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We address the data bias issue as much as possible by carefully analyzing the relationships of different variables in the data generating process. We use a Causal Diagram BIBREF5 , BIBREF6 to analyze and remove the effects of the data bias (e.g., the speakers' reputations, popularity gained by publicity, etc.) in our prediction model. In order to make the prediction model less biased to the speakers' race and gender, we confine our analysis to the transcripts only. Besides, we normalize the ratings to remove the effects of the unwanted variables such as the speakers' reputations, publicity, contemporary hot topics, etc.   \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      BIBREF30 analyzed the TED Talks for humor detection. BIBREF31 analyzed the transcripts of the TED talks to predict audience engagement in the form of applause. BIBREF32 predicted user interest (engaging vs. non-engaging) from high-level visual features (e.g., camera angles) and audience applause. BIBREF33 proposed a sentiment-aware nearest neighbor model for a multimedia recommendation over the TED talks. BIBREF34 predicted the TED talk ratings from the linguistic features of the transcripts. This work is most similar to ours. However, we are proposing a new prediction framework using the Neural Networks.   \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              We use two neural network architectures in the prediction task. In the first architecture, we use LSTM BIBREF7 for a sequential input of the words within the sentences of the transcripts. In the second architecture, we use TreeLSTM BIBREF8 to represent the input sentences in the form of a dependency tree. Our experiments show that the dependency tree-based model can predict the TED talk ratings with slightly higher performance (average F-score 0.77) than the word sequence model (average F-score 0.76). To the best of our knowledge, this is the best performance in the literature on predicting the TED talk ratings. We compare the performances of these two models with a baseline of classical machine learning techniques using hand-engineered features. We find that the neural networks largely outperform the classical methods. We believe this gain in performance is achieved by the networks' ability to capture better the natural relationship of the words (as compared to the hand engineered feature selection approach in the baseline methods) and the correlations among different rating labels.   \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     For our analysis, we curate an observational dataset of public speech transcripts and other meta-data collected from the ted.com website. This website contains a large collection of high-quality public speeches that are freely available to watch, share, rate, and comment on. Every day, numerous people watch and annotate their perceptions about the talks. Our dataset contains 2231 public speech transcripts and over 5 million ratings from the spontaneous viewers of the talks. The viewers annotate each talk by 14 different labels—Beautiful, Confusing, Courageous, Fascinating, Funny, Informative, Ingenious, Inspiring, Jaw-Dropping, Long-winded, Obnoxious, OK, Persuasive, and Unconvincing.   \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The data for this study was gathered from the ted.com website on November 15, 2017. We removed the talks published six months before the crawling date to make sure each talk has enough ratings for a robust analysis. More specifically, we filtered any talk that—   \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              We collected a total of 18 million, geo-tagged, English-language tweets over three years, from January 1st, 2012 to January 1st, 2015, evenly divided across all 36 months, using Historical PowerTrack for Twitter provided by GNIP. We created geolocation bounding boxes for each of the 50 states which were used to collect our dataset. All 18 million tweets originated from one of the 50 states and are tagged as such. Moreover, all tweets contained one of the six emoticons in Table TABREF7 and were labelled as either positive or negative based on the emoticon. Out of the 18 million tweets, INLINEFORM0 million ( INLINEFORM1 ) were labelled as positive and INLINEFORM2 million ( INLINEFORM3 ) were labelled as negative. The 18 million tweets came from INLINEFORM4 distinct users.   \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                                                                                                                                                                                                                                                                                                                                                                                                                                                There are different methods of obtaining labelled data using distant supervision BIBREF1 , BIBREF6 , BIBREF19 , BIBREF12 . We used emoticons to label tweets as positive or negative, an approach that was introduced by Read BIBREF1 and used in multiple works BIBREF6 , BIBREF12 . We collected millions of English-language tweets from different times, dates, authors and US states. We used a total of six emoticons, three mapping to positive and three mapping to negative sentiment (table TABREF7 ). We identified more than 120 positive and negative ASCII emoticons and unicode emojis, but we decided to only use the six most common emoticons in order to avoid possible selection biases. For example, people who use obscure emoticons and emojis might have a different base sentiment from those who do not. Using the six most commonly used emoticons limits this bias. Since there are no \"neutral\" emoticons, our dataset is limited to tweets with positive or negative sentiments. Accordingly, in this work we are only concerned with analysing and classifying the polarity of tweets (negative vs. positive) and not their subjectivity (neutral vs. non-neutral). Below we will explain our data collection and corpus in greater detail.   \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             This is our purely linguistic baseline model.   \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                                                                                                                                                                                                                                                                                                                                               There has also been previous work on measuring the happiness of people in different contexts (location, time, etc). This has been done mostly through traditional land-line polling BIBREF5 , BIBREF4 , with Gallup's annual happiness index being a prime example BIBREF4 . More recently, some have utilized Twitter to measure people's mood and happiness and have found Twitter to be a generally good measure of the public's overall happiness, well-being and mood. For example, Bollen et al. BIBREF15 used Twitter to measure the daily mood of the public and compare that to the record of social, political, cultural and economic events in the real world. They found that these events have a significant effect on the public mood as measured through Twitter. Another example would be the work of Mitchell et al. BIBREF16 , in which they estimated the happiness levels of different states and cities in the USA using Twitter and found statistically significant correlations between happiness level and the demographic characteristics (such as obesity rates and education levels) of those regions. Finally, improving natural language processing by incorporating contextual information has been successfully attempted before BIBREF17 , BIBREF18 ; but as far as we are aware, this has not been attempted for sentiment classification.   \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Even though our contextual classifier was able to outperform the previous state-of-the-art, distant supervised sentiment classifier, it should be noted that our contextual classifier's performance is boosted significantly by spatial information extracted through geo-tags. However, only about one to two percent of tweets in the wild are geo-tagged. Therefore, we trained and evaluated our contextual model using all the variables except for state. The accuracy of this model was INLINEFORM0 , which is still significantly better than the performance of the purely linguistic classifier. Fortunately, all tweets are tagged with timestamps and author information, so all the other four contextual variables used in our model can be used for classifying the sentiment of any tweet.   \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The English-German translation models are trained on WMT datasets, including News Commentary 13, Europarl v7, and Common Crawl, and evaluated on newstest2013 for early stopping. On the newstest2013 dev set, the En$\\rightarrow $De model reaches a BLEU-4 score of 19.6, and the De$\\rightarrow $En model reaches a BLEU-4 score of 24.6.   \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The English-French models are trained on Common Crawl 13, Europarl v7, News Commentary v9, Giga release 2, and UN doc 2000. On the newstest2013 dev set, the En$\\rightarrow $Fr model reaches a BLEU-4 score of 25.6, and the Fr$\\rightarrow $En model reaches a BLEU-4 score of 26.1.   \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We use 303 sub areas from Stack Exchange data dumps. The full list of area names is in the appendix. We do not include Stack Overflow because it is too specific to programming related questions. We also exclude all questions under the following language sub areas: Chinese, German, Spanish, Russian, Japanese, Korean, Latin, Ukrainian. This ensures that the questions in MQR are mostly English sentences. Having questions from 303 Stack Exchange sites makes the MQR dataset cover a broad range of domains.   \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        We also train a paraphrase generation model on a subset of the ParaNMT dataset BIBREF24, which was created automatically by using neural machine translation to translate the Czech side of a large Czech-English parallel corpus. We use the filtered subset of 5M pairs provided by the authors. For each pair of paraphrases (S1 and S2) in the dataset, we train the model to rewrite from S1 to S2 and also rewrite from S2 to S1. We use the MQR DEV set for early stopping during training.   \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         To ensure there are no sentences written in non-English languages, we keep questions that contain 80% or more of valid English characters, including punctuation.   \n",
       "\n",
       "                                         fewshot_pred_answer  \\\n",
       "quids                                                          \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                 Yes   \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                 Yes   \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                 Yes   \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                 Yes   \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                 Yes   \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                 Yes   \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                 Yes   \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                 Yes   \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                 Yes   \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                 Yes   \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30                 Yes   \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30                 Yes   \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30                 Yes   \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30                 Yes   \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30                 Yes   \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0                 Yes   \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0                 Yes   \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0                 Yes   \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0                 Yes   \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0                 Yes   \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                 Yes   \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                 Yes   \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                 Yes   \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                 Yes   \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                 Yes   \n",
       "74cc0300e22f60232812019011a09df92bbec803                 Yes   \n",
       "74cc0300e22f60232812019011a09df92bbec803                 Yes   \n",
       "74cc0300e22f60232812019011a09df92bbec803                 Yes   \n",
       "74cc0300e22f60232812019011a09df92bbec803                 Yes   \n",
       "74cc0300e22f60232812019011a09df92bbec803                 Yes   \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                 Yes   \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                 Yes   \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                 Yes   \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                 Yes   \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                 Yes   \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                 Yes   \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                 Yes   \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                 Yes   \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                 Yes   \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                 Yes   \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                 Yes   \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                 Yes   \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                 Yes   \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                 Yes   \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                 Yes   \n",
       "\n",
       "                                                                        gold_answers  \n",
       "quids                                                                                 \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                     [Unanswerable, No, Yes]  \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                     [Unanswerable, No, Yes]  \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                     [Unanswerable, No, Yes]  \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                     [Unanswerable, No, Yes]  \n",
       "bc7081aaa207de2362e0bea7bc8108d338aee36f                     [Unanswerable, No, Yes]  \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                    [Unanswerable, Yes, Yes]  \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                    [Unanswerable, Yes, Yes]  \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                    [Unanswerable, Yes, Yes]  \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                    [Unanswerable, Yes, Yes]  \n",
       "693cdb9978749db04ba34d9c168e71534f00a226                    [Unanswerable, Yes, Yes]  \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30  [Unanswerable, Unanswerable, Unanswerable]  \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30  [Unanswerable, Unanswerable, Unanswerable]  \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30  [Unanswerable, Unanswerable, Unanswerable]  \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30  [Unanswerable, Unanswerable, Unanswerable]  \n",
       "a17fc7b96753f85aee1d2036e2627570f4b50c30  [Unanswerable, Unanswerable, Unanswerable]  \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0           [Yes, Unanswerable, Unanswerable]  \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0           [Yes, Unanswerable, Unanswerable]  \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0           [Yes, Unanswerable, Unanswerable]  \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0           [Yes, Unanswerable, Unanswerable]  \n",
       "2f142cd11731d29d0c3fa426e26ef80d997862e0           [Yes, Unanswerable, Unanswerable]  \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                         [Yes, Unanswerable]  \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                         [Yes, Unanswerable]  \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                         [Yes, Unanswerable]  \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                         [Yes, Unanswerable]  \n",
       "ec5e84a1d1b12f7185183d165cbb5eae66d9833e                         [Yes, Unanswerable]  \n",
       "74cc0300e22f60232812019011a09df92bbec803                         [Unanswerable, Yes]  \n",
       "74cc0300e22f60232812019011a09df92bbec803                         [Unanswerable, Yes]  \n",
       "74cc0300e22f60232812019011a09df92bbec803                         [Unanswerable, Yes]  \n",
       "74cc0300e22f60232812019011a09df92bbec803                         [Unanswerable, Yes]  \n",
       "74cc0300e22f60232812019011a09df92bbec803                         [Unanswerable, Yes]  \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                [Unanswerable, Unanswerable]  \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                [Unanswerable, Unanswerable]  \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                [Unanswerable, Unanswerable]  \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                [Unanswerable, Unanswerable]  \n",
       "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c                [Unanswerable, Unanswerable]  \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                                  [Yes, Yes]  \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                                  [Yes, Yes]  \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                                  [Yes, Yes]  \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                                  [Yes, Yes]  \n",
       "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd                                  [Yes, Yes]  \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                                   [No, Yes]  \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                                   [No, Yes]  \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                                   [No, Yes]  \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                                   [No, Yes]  \n",
       "3d662fb442d5fc332194770aac835f401c2148d9                                   [No, Yes]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What datasets are used?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fewshot_evidence</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90aba75508aa145475d7cc9a501bbe987c0e8413</th>\n",
       "      <td>Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by BIBREF4. Instead of focusing on the previously studied game environments BIBREF5, BIBREF6, the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist. The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table TABREF1). In total, 1402 product ad postings were scraped from Craigslist, belonging to six categories: phones, bikes, housing, furniture, car and electronics. Each ad posting contains details such as Product Title, Category Type and a Listing Price. Moreover, a secret target price is also pre-decided for the buyer. The final price after the agreement is called the Agreed Price, which we aim to predict.</td>\n",
       "      <td>Craigslist Bargaining dataset (CB)</td>\n",
       "      <td>[Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB) ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90aba75508aa145475d7cc9a501bbe987c0e8413</th>\n",
       "      <td>We study human-human negotiations in the buyer-seller bargaining scenario, which has been a key research area in the literature BIBREF0. In this section, we first describe our problem setup and key terminologies by discussing the dataset used. Later, we formalize our problem definition.</td>\n",
       "      <td>Craigslist Bargaining dataset (CB)</td>\n",
       "      <td>[Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB) ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90aba75508aa145475d7cc9a501bbe987c0e8413</th>\n",
       "      <td>We perform experiments on the CB dataset to primarily answer two questions: 1) Is it feasible to predict negotiation outcomes without observing the complete conversation between the buyer and seller? 2) To what extent does the natural language incorporation help in the prediction? In order to answer these questions, we compare our model empirically with a number of baseline methods. This section presents the methods we compare to, the training setup and the evaluation metrics.</td>\n",
       "      <td>Craigslist Bargaining dataset (CB)</td>\n",
       "      <td>[Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB) ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90aba75508aa145475d7cc9a501bbe987c0e8413</th>\n",
       "      <td>Training Details: Given the multiple segments in our model input and small data size, we use BERT-base BIBREF8, having output dimension of 768. To tackle the variance in product prices across different categories, all prices in the inputs and outputs were normalized by the listing price. The predictions were unnormalized before final evaluations. Further, we only considered the negotiations where an agreement was reached. These were the instances for which ground truth was available ($\\sim 75\\%$ of the data). We use a two-layer GRU with a dropout of $0.1$ and 50 hidden units. The models were trained for a maximum of 5000 iterations, with AdamW optimizer BIBREF13, a learning rate of 2x$10^{^-5}$ and a batch size of 4. We used a linear warmup schedule for the first $0.1$ fraction of the steps. All the hyper-parameters were optimized on the provided development set.</td>\n",
       "      <td>Craigslist Bargaining dataset (CB)</td>\n",
       "      <td>[Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB) ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90aba75508aa145475d7cc9a501bbe987c0e8413</th>\n",
       "      <td>Defining the problem: Say we are provided with a product scenario $S$, a tuple: (Category, Title, Listing Price, Target Price). Define the interactions between a buyer and seller using a sequence of $n$ events $E_n:&lt;e_{1}, e_{2}, ..., e_{n}&gt;$, where $e_{i}$ occurs before $e_{j}$ iff $i&lt;j$. Event $e_{i}$ is also a tuple: (Initiator, Type, Data). Initiator is either the Buyer or Seller, Type can be one of (message, offer, accept, reject or quit) and Data consists of either the corresponding natural language dialogue, offer price or can be empty. Nearly $80\\%$ of events in CB dataset are of type `message', each consisting a textual message as Data. An offer is usually made and accepted at the end of each negotiation. Since the offers directly contain the agreed price (which we want to predict), we only consider `message' events in our models. Given the scenario $S$ and first $n$ events $E_n$, our problem is then to learn the function $f_{n}$: $A = f_{n}(S, E_n)$ where $A$ refers to the final agreed price between the two negotiating parties.</td>\n",
       "      <td>Craigslist Bargaining dataset (CB)</td>\n",
       "      <td>[Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB) ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47ffc9811b037613c9c4d1ec1e4f13c08396ed1c</th>\n",
       "      <td>We used the largest hand-annotated discourse corpus PDTB 2.0 BIBREF12 (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work BIBREF6 , BIBREF13 , BIBREF14 , BIBREF9 , we used sections 2-20 as our training set, sections 21-22 as the test set. Sections 0-1 were used as the development set for hyperparameter optimization.</td>\n",
       "      <td>PDTB 2.0 and Chatbot NLU Corpus with STT error</td>\n",
       "      <td>[PDTB 2.0, PDTB 2.0 , PDTB 2.0 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47ffc9811b037613c9c4d1ec1e4f13c08396ed1c</th>\n",
       "      <td>We tokenized all datasets using Stanford NLP Toolkit. For optimization, we employed the Adam algorithm BIBREF15 to update parameters. With respect to the hyperparameters $M,L,A$ and the dimensionality of all vector representations, we set them according to previous work BIBREF10 , BIBREF11 and preliminary experiments on the development set. Finally, we set $M=16,A=1000,L=1,d_z=20,d_{x_1}=d_{x_2}=10001,d_{h_1}=d_{h_2}=d_{h_1^\\prime }=d_{h_2^\\prime }=d_m=d_{h_y}=400,d_y=2$ for all experiments.. All parameters of VarNDRR are initialized by a Gaussian distribution ( $\\mu =0, \\sigma =0.01$ ). For Adam, we set $\\beta _1=0.9$ , $\\beta _2=0.999$ with a learning rate $0.001$ . Additionally, we tied the following parameters in practice: $W_{h_1}$ and $W_{h_2}$ , $M=16,A=1000,L=1,d_z=20,d_{x_1}=d_{x_2}=10001,d_{h_1}=d_{h_2}=d_{h_1^\\prime }=d_{h_2^\\prime }=d_m=d_{h_y}=400,d_y=2$0 and $M=16,A=1000,L=1,d_z=20,d_{x_1}=d_{x_2}=10001,d_{h_1}=d_{h_2}=d_{h_1^\\prime }=d_{h_2^\\prime }=d_m=d_{h_y}=400,d_y=2$1 .</td>\n",
       "      <td>PDTB 2.0 and Chatbot NLU Corpus with STT error</td>\n",
       "      <td>[PDTB 2.0, PDTB 2.0 , PDTB 2.0 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47ffc9811b037613c9c4d1ec1e4f13c08396ed1c</th>\n",
       "      <td>Before we describe these neural networks, it is necessary to briefly introduce how discourse relations are annotated in our training data. The PDTB corpus, used as our training data, annotates implicit discourse relations between two neighboring arguments, namely Arg1 and Arg2. In VarNDRR, we represent the two arguments with bag-of-word representations, and denote them as $\\mathbf {x_1}$ and $\\mathbf {x_2}$ .</td>\n",
       "      <td>PDTB 2.0 and Chatbot NLU Corpus with STT error</td>\n",
       "      <td>[PDTB 2.0, PDTB 2.0 , PDTB 2.0 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47ffc9811b037613c9c4d1ec1e4f13c08396ed1c</th>\n",
       "      <td>Features used in SVM are taken from the state-of-the-art implicit discourse relation recognition model, including Bag of Words, Cross-Argument Word Pairs, Polarity, First-Last, First3, Production Rules, Dependency Rules and Brown cluster pair BIBREF16 . In order to collect bag of words, production rules, dependency rules, and cross-argument word pairs, we used a frequency cutoff of 5 to remove rare features, following Lin et al. lin2009recognizing.</td>\n",
       "      <td>PDTB 2.0 and Chatbot NLU Corpus with STT error</td>\n",
       "      <td>[PDTB 2.0, PDTB 2.0 , PDTB 2.0 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47ffc9811b037613c9c4d1ec1e4f13c08396ed1c</th>\n",
       "      <td>We also provide results from two state-of-the-art systems:</td>\n",
       "      <td>PDTB 2.0 and Chatbot NLU Corpus with STT error</td>\n",
       "      <td>[PDTB 2.0, PDTB 2.0 , PDTB 2.0 ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3e23cc3c5e4d5cec51d158130d6aeae120e94fc8</th>\n",
       "      <td>For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs. For each query $q$ , we searched for a candidate reply $r^*$ by the retrieval component and obtained a tuple $\\langle q, r^*, r\\rangle $ . As a friendly reminder, $q$ and $r^*$ are the input of biseq2seq, whose output should approximate $r$ . We randomly selected 100k triples for validation and another 6,741 for testing. The train-val-test split remains the same for all competing models.</td>\n",
       "      <td>A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs</td>\n",
       "      <td>[They create their own datasets from online text., To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, the database contains 7 million query-reply pairs for retrieval, For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3e23cc3c5e4d5cec51d158130d6aeae120e94fc8</th>\n",
       "      <td>For biseq2seq, we use human-human utterance pairs $\\langle q, r\\rangle $ as data samples. A retrieved candidate $r^*$ is also provided as the input when we train the neural network. Standard cross-entropy loss of all words in the reply is applied as the training objective. For a particular training sample whose reply is of length $T$ , the cost is</td>\n",
       "      <td>A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs</td>\n",
       "      <td>[They create their own datasets from online text., To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, the database contains 7 million query-reply pairs for retrieval, For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3e23cc3c5e4d5cec51d158130d6aeae120e94fc8</th>\n",
       "      <td>Typically, a very large database of query-reply pairs is a premise for a successful retrieval-based conversation system, because the reply must appear in the database. For RNN-based sequence generators, however, it is time-consuming to train with such a large dataset; RNN's performance may also saturate when we have several million samples.</td>\n",
       "      <td>A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs</td>\n",
       "      <td>[They create their own datasets from online text., To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, the database contains 7 million query-reply pairs for retrieval, For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3e23cc3c5e4d5cec51d158130d6aeae120e94fc8</th>\n",
       "      <td>To the best of our knowledge, we are the first to combine retrieval-based and generation-based dialog systems. The use of biseq2seq and post-reranking is also a new insight of this paper.</td>\n",
       "      <td>A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs</td>\n",
       "      <td>[They create their own datasets from online text., To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, the database contains 7 million query-reply pairs for retrieval, For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3e23cc3c5e4d5cec51d158130d6aeae120e94fc8</th>\n",
       "      <td>We adopted BLEU-1, BLEU-2, BLEU-3 and BLEU-4 as automatic evaluation. While nbcitehowNOTto further aggressively argues that no existing automatic metric is appropriate for open-domain dialogs, they show a slight positive correlation between BLEU-2 and human evaluation in non-technical Twitter domain, which is similar to our scenario. We nonetheless include BLEU scores as expedient objective evaluation, serving as supporting evidence. BLEUs are also used in nbcitenaacl for model comparison and in nbciteseq2BF for model selection.</td>\n",
       "      <td>A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs</td>\n",
       "      <td>[They create their own datasets from online text., To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, the database contains 7 million query-reply pairs for retrieval, For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e</th>\n",
       "      <td>Speech recordings of patients in three different languages are considered: Spanish, German, and Czech. All of the recordings were captured in noise controlled conditions. The speech signals were down-sampled to 16 kHz. The patients in the three datasets were evaluated by a neurologist expert according to the third section of the movement disorder society, unified Parkinson's disease rating scale (MDS-UPDRS-III) BIBREF16. Table TABREF5 summarizes the information about the patients and healthy speakers.</td>\n",
       "      <td>Spanish, German, and Czech</td>\n",
       "      <td>[PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, 100 native Czech speakers (50 PD, 50 HC) , the PC-GITA corpus BIBREF5, BIBREF17, BIBREF18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e</th>\n",
       "      <td>Most of the studies in the literature to classify PD from speech are based on computing hand-crafted features and using classifiers such as support vector machines (SVMs) or K-nearest neighbors (KNN). For instance, in BIBREF3, the authors computed features related to perturbations of the fundamental frequency and amplitude of the speech signal to classify utterances from 20 PD patients and 20 HC subjects, Turkish speakers. Classifiers based on KNN and SVMs were considered, and accuracies of up to 75% were reported. Later, in BIBREF4 the authors proposed a phonation analysis based on several time frequency representations to assess tremor in the speech of PD patients. The extracted features were based on energy and entropy computed from time frequency representations. Several classifiers were used, including Gaussian mixture models (GMMs) and SVMs. Accuracies of up to 77% were reported in utterances of the PC-GITA database BIBREF5, formed with utterances from 50 PD patients and 50 HC subjects, Colombian Spanish native speakers. The authors from BIBREF6 computed features to model different articulation deficits in PD such as vowel quality, coordination of laryngeal and supra-laryngeal activity, precision of consonant articulation, tongue movement, occlusion weakening, and speech timing. The authors studied the rapid repetition of the syllables /pa-ta-ka/ pronounced by 24 Czech native speakers, and reported an accuracy of 88% discriminating between PD patients and HC speakers, using an SVM classifier. Additional articulation features were proposed in BIBREF7, where the authors modeled the difficulty of PD patients to start/stop the vocal fold vibration in continuous speech. The model was based on the energy content in the transitions between unvoiced and voiced segments. The authors classified PD patients and HC speakers with speech recordings in three different languages (Spanish, German, and Czech), and reported accuracies ranging from 80% to 94% depending on the language; however, the results were optimistic, since the hyper-parameters of the classifier were optimized based on the accuracy on the test set. Another articulation model was proposed in BIBREF8. The authors considered a forced alignment strategy to segment the different phonetic units in the speech utterances. The phonemes were segmented and grouped to train different GMMs. The classification was performed based on a threshold of the difference between the posterior probabilities from the models created for HC subjects and PD patients. The model was tested with Colombian Spanish utterances from the PC-GITA database BIBREF5 and with the Czech data from BIBREF9. The authors reported accuracies of up to 81% for the Spanish data, and of up to 94% for the Czech data.</td>\n",
       "      <td>Spanish, German, and Czech</td>\n",
       "      <td>[PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, 100 native Czech speakers (50 PD, 50 HC) , the PC-GITA corpus BIBREF5, BIBREF17, BIBREF18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e</th>\n",
       "      <td>Time frequency representations based on the short-time Fourier transform (STFT) are used as input to a CNN, which extract the most suitable features to discriminate between PD patients and HC subjects. The STFT with 256 frequency bins is computed for each segmented transition, for a window length of 16 ms and a step-size of 4 ms, forming 41 time frames per transition. The obtained spectrogram is transformed into the Mel-scale using 80 filters, forming an spectrogram with a size of 80$\\times $41, which is used to train the CNNs. The architecture of the implemented CNN is summarized in Table TABREF9. It consists of four convolutional and max-pooling layers, dropout to regularize the weights, and two fully connected layers followed by the output layer to make the final decision using a softmax activation function. The number of feature maps on each convolutional layer is twice the previous one in order to get more detailed representations of the input space in the deeper layers. The CNN is trained using the the cross-entropy as the loss function, using an Adam optimizer BIBREF19.</td>\n",
       "      <td>Spanish, German, and Czech</td>\n",
       "      <td>[PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, 100 native Czech speakers (50 PD, 50 HC) , the PC-GITA corpus BIBREF5, BIBREF17, BIBREF18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e</th>\n",
       "      <td>This study proposed the use of a transfer learning strategy based on fine-tuning to classify PD from speech in three different languages: Spanish, German, and Czech. The transfer learning among languages aimed to improve the accuracy when the models are initialized with utterances from a different language than the one used for the test set. Mel-scale spectrograms extracted from the transitions between voiced and unvoiced segments are used to train a CNN for each language. Then, the trained models are used to fine-tune a model to classify utterances in the remaining two languages.</td>\n",
       "      <td>Spanish, German, and Czech</td>\n",
       "      <td>[PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, 100 native Czech speakers (50 PD, 50 HC) , the PC-GITA corpus BIBREF5, BIBREF17, BIBREF18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e</th>\n",
       "      <td>The Spanish data consider the PC-GITA corpus BIBREF5, which contains utterances from 50 PD patients and 50 HC, Colombian Spanish native speakers. The participants were asked to pronounce a total of 10 sentences, the rapid repetition of /pa-ta-ka/, /pe-ta-ka/, /pa-ka-ta/, /pa/, /ta/, and /ka/, one text with 36 words, and a monologue. All patients were in ON state at the time of the recording, i.e., under the effect of their daily medication.</td>\n",
       "      <td>Spanish, German, and Czech</td>\n",
       "      <td>[PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, 100 native Czech speakers (50 PD, 50 HC) , the PC-GITA corpus BIBREF5, BIBREF17, BIBREF18]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               fewshot_evidence  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by BIBREF4. Instead of focusing on the previously studied game environments BIBREF5, BIBREF6, the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist. The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table TABREF1). In total, 1402 product ad postings were scraped from Craigslist, belonging to six categories: phones, bikes, housing, furniture, car and electronics. Each ad posting contains details such as Product Title, Category Type and a Listing Price. Moreover, a secret target price is also pre-decided for the buyer. The final price after the agreement is called the Agreed Price, which we aim to predict.   \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        We study human-human negotiations in the buyer-seller bargaining scenario, which has been a key research area in the literature BIBREF0. In this section, we first describe our problem setup and key terminologies by discussing the dataset used. Later, we formalize our problem definition.   \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We perform experiments on the CB dataset to primarily answer two questions: 1) Is it feasible to predict negotiation outcomes without observing the complete conversation between the buyer and seller? 2) To what extent does the natural language incorporation help in the prediction? In order to answer these questions, we compare our model empirically with a number of baseline methods. This section presents the methods we compare to, the training setup and the evaluation metrics.   \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Training Details: Given the multiple segments in our model input and small data size, we use BERT-base BIBREF8, having output dimension of 768. To tackle the variance in product prices across different categories, all prices in the inputs and outputs were normalized by the listing price. The predictions were unnormalized before final evaluations. Further, we only considered the negotiations where an agreement was reached. These were the instances for which ground truth was available ($\\sim 75\\%$ of the data). We use a two-layer GRU with a dropout of $0.1$ and 50 hidden units. The models were trained for a maximum of 5000 iterations, with AdamW optimizer BIBREF13, a learning rate of 2x$10^{^-5}$ and a batch size of 4. We used a linear warmup schedule for the first $0.1$ fraction of the steps. All the hyper-parameters were optimized on the provided development set.   \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Defining the problem: Say we are provided with a product scenario $S$, a tuple: (Category, Title, Listing Price, Target Price). Define the interactions between a buyer and seller using a sequence of $n$ events $E_n:<e_{1}, e_{2}, ..., e_{n}>$, where $e_{i}$ occurs before $e_{j}$ iff $i<j$. Event $e_{i}$ is also a tuple: (Initiator, Type, Data). Initiator is either the Buyer or Seller, Type can be one of (message, offer, accept, reject or quit) and Data consists of either the corresponding natural language dialogue, offer price or can be empty. Nearly $80\\%$ of events in CB dataset are of type `message', each consisting a textual message as Data. An offer is usually made and accepted at the end of each negotiation. Since the offers directly contain the agreed price (which we want to predict), we only consider `message' events in our models. Given the scenario $S$ and first $n$ events $E_n$, our problem is then to learn the function $f_{n}$: $A = f_{n}(S, E_n)$ where $A$ refers to the final agreed price between the two negotiating parties.   \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        We used the largest hand-annotated discourse corpus PDTB 2.0 BIBREF12 (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work BIBREF6 , BIBREF13 , BIBREF14 , BIBREF9 , we used sections 2-20 as our training set, sections 21-22 as the test set. Sections 0-1 were used as the development set for hyperparameter optimization.   \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We tokenized all datasets using Stanford NLP Toolkit. For optimization, we employed the Adam algorithm BIBREF15 to update parameters. With respect to the hyperparameters $M,L,A$ and the dimensionality of all vector representations, we set them according to previous work BIBREF10 , BIBREF11 and preliminary experiments on the development set. Finally, we set $M=16,A=1000,L=1,d_z=20,d_{x_1}=d_{x_2}=10001,d_{h_1}=d_{h_2}=d_{h_1^\\prime }=d_{h_2^\\prime }=d_m=d_{h_y}=400,d_y=2$ for all experiments.. All parameters of VarNDRR are initialized by a Gaussian distribution ( $\\mu =0, \\sigma =0.01$ ). For Adam, we set $\\beta _1=0.9$ , $\\beta _2=0.999$ with a learning rate $0.001$ . Additionally, we tied the following parameters in practice: $W_{h_1}$ and $W_{h_2}$ , $M=16,A=1000,L=1,d_z=20,d_{x_1}=d_{x_2}=10001,d_{h_1}=d_{h_2}=d_{h_1^\\prime }=d_{h_2^\\prime }=d_m=d_{h_y}=400,d_y=2$0 and $M=16,A=1000,L=1,d_z=20,d_{x_1}=d_{x_2}=10001,d_{h_1}=d_{h_2}=d_{h_1^\\prime }=d_{h_2^\\prime }=d_m=d_{h_y}=400,d_y=2$1 .   \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Before we describe these neural networks, it is necessary to briefly introduce how discourse relations are annotated in our training data. The PDTB corpus, used as our training data, annotates implicit discourse relations between two neighboring arguments, namely Arg1 and Arg2. In VarNDRR, we represent the two arguments with bag-of-word representations, and denote them as $\\mathbf {x_1}$ and $\\mathbf {x_2}$ .   \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Features used in SVM are taken from the state-of-the-art implicit discourse relation recognition model, including Bag of Words, Cross-Argument Word Pairs, Polarity, First-Last, First3, Production Rules, Dependency Rules and Brown cluster pair BIBREF16 . In order to collect bag of words, production rules, dependency rules, and cross-argument word pairs, we used a frequency cutoff of 5 to remove rare features, following Lin et al. lin2009recognizing.   \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             We also provide results from two state-of-the-art systems:   \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs. For each query $q$ , we searched for a candidate reply $r^*$ by the retrieval component and obtained a tuple $\\langle q, r^*, r\\rangle $ . As a friendly reminder, $q$ and $r^*$ are the input of biseq2seq, whose output should approximate $r$ . We randomly selected 100k triples for validation and another 6,741 for testing. The train-val-test split remains the same for all competing models.   \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         For biseq2seq, we use human-human utterance pairs $\\langle q, r\\rangle $ as data samples. A retrieved candidate $r^*$ is also provided as the input when we train the neural network. Standard cross-entropy loss of all words in the reply is applied as the training objective. For a particular training sample whose reply is of length $T$ , the cost is    \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Typically, a very large database of query-reply pairs is a premise for a successful retrieval-based conversation system, because the reply must appear in the database. For RNN-based sequence generators, however, it is time-consuming to train with such a large dataset; RNN's performance may also saturate when we have several million samples.   \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            To the best of our knowledge, we are the first to combine retrieval-based and generation-based dialog systems. The use of biseq2seq and post-reranking is also a new insight of this paper.   \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We adopted BLEU-1, BLEU-2, BLEU-3 and BLEU-4 as automatic evaluation. While nbcitehowNOTto further aggressively argues that no existing automatic metric is appropriate for open-domain dialogs, they show a slight positive correlation between BLEU-2 and human evaluation in non-technical Twitter domain, which is similar to our scenario. We nonetheless include BLEU scores as expedient objective evaluation, serving as supporting evidence. BLEUs are also used in nbcitenaacl for model comparison and in nbciteseq2BF for model selection.   \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Speech recordings of patients in three different languages are considered: Spanish, German, and Czech. All of the recordings were captured in noise controlled conditions. The speech signals were down-sampled to 16 kHz. The patients in the three datasets were evaluated by a neurologist expert according to the third section of the movement disorder society, unified Parkinson's disease rating scale (MDS-UPDRS-III) BIBREF16. Table TABREF5 summarizes the information about the patients and healthy speakers.   \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e  Most of the studies in the literature to classify PD from speech are based on computing hand-crafted features and using classifiers such as support vector machines (SVMs) or K-nearest neighbors (KNN). For instance, in BIBREF3, the authors computed features related to perturbations of the fundamental frequency and amplitude of the speech signal to classify utterances from 20 PD patients and 20 HC subjects, Turkish speakers. Classifiers based on KNN and SVMs were considered, and accuracies of up to 75% were reported. Later, in BIBREF4 the authors proposed a phonation analysis based on several time frequency representations to assess tremor in the speech of PD patients. The extracted features were based on energy and entropy computed from time frequency representations. Several classifiers were used, including Gaussian mixture models (GMMs) and SVMs. Accuracies of up to 77% were reported in utterances of the PC-GITA database BIBREF5, formed with utterances from 50 PD patients and 50 HC subjects, Colombian Spanish native speakers. The authors from BIBREF6 computed features to model different articulation deficits in PD such as vowel quality, coordination of laryngeal and supra-laryngeal activity, precision of consonant articulation, tongue movement, occlusion weakening, and speech timing. The authors studied the rapid repetition of the syllables /pa-ta-ka/ pronounced by 24 Czech native speakers, and reported an accuracy of 88% discriminating between PD patients and HC speakers, using an SVM classifier. Additional articulation features were proposed in BIBREF7, where the authors modeled the difficulty of PD patients to start/stop the vocal fold vibration in continuous speech. The model was based on the energy content in the transitions between unvoiced and voiced segments. The authors classified PD patients and HC speakers with speech recordings in three different languages (Spanish, German, and Czech), and reported accuracies ranging from 80% to 94% depending on the language; however, the results were optimistic, since the hyper-parameters of the classifier were optimized based on the accuracy on the test set. Another articulation model was proposed in BIBREF8. The authors considered a forced alignment strategy to segment the different phonetic units in the speech utterances. The phonemes were segmented and grouped to train different GMMs. The classification was performed based on a threshold of the difference between the posterior probabilities from the models created for HC subjects and PD patients. The model was tested with Colombian Spanish utterances from the PC-GITA database BIBREF5 and with the Czech data from BIBREF9. The authors reported accuracies of up to 81% for the Spanish data, and of up to 94% for the Czech data.   \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Time frequency representations based on the short-time Fourier transform (STFT) are used as input to a CNN, which extract the most suitable features to discriminate between PD patients and HC subjects. The STFT with 256 frequency bins is computed for each segmented transition, for a window length of 16 ms and a step-size of 4 ms, forming 41 time frames per transition. The obtained spectrogram is transformed into the Mel-scale using 80 filters, forming an spectrogram with a size of 80$\\times $41, which is used to train the CNNs. The architecture of the implemented CNN is summarized in Table TABREF9. It consists of four convolutional and max-pooling layers, dropout to regularize the weights, and two fully connected layers followed by the output layer to make the final decision using a softmax activation function. The number of feature maps on each convolutional layer is twice the previous one in order to get more detailed representations of the input space in the deeper layers. The CNN is trained using the the cross-entropy as the loss function, using an Adam optimizer BIBREF19.   \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            This study proposed the use of a transfer learning strategy based on fine-tuning to classify PD from speech in three different languages: Spanish, German, and Czech. The transfer learning among languages aimed to improve the accuracy when the models are initialized with utterances from a different language than the one used for the test set. Mel-scale spectrograms extracted from the transitions between voiced and unvoiced segments are used to train a CNN for each language. Then, the trained models are used to fine-tune a model to classify utterances in the remaining two languages.   \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The Spanish data consider the PC-GITA corpus BIBREF5, which contains utterances from 50 PD patients and 50 HC, Colombian Spanish native speakers. The participants were asked to pronounce a total of 10 sentences, the rapid repetition of /pa-ta-ka/, /pe-ta-ka/, /pa-ka-ta/, /pa/, /ta/, and /ka/, one text with 36 words, and a monologue. All patients were in ON state at the time of the recording, i.e., under the effect of their daily medication.   \n",
       "\n",
       "                                                                                                            fewshot_pred_answer  \\\n",
       "quids                                                                                                                             \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                     Craigslist Bargaining dataset (CB)   \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                     Craigslist Bargaining dataset (CB)   \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                     Craigslist Bargaining dataset (CB)   \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                     Craigslist Bargaining dataset (CB)   \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                     Craigslist Bargaining dataset (CB)   \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                         PDTB 2.0 and Chatbot NLU Corpus with STT error   \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                         PDTB 2.0 and Chatbot NLU Corpus with STT error   \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                         PDTB 2.0 and Chatbot NLU Corpus with STT error   \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                         PDTB 2.0 and Chatbot NLU Corpus with STT error   \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                         PDTB 2.0 and Chatbot NLU Corpus with STT error   \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8  A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs   \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8  A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs   \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8  A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs   \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8  A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs   \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8  A dataset of 1,606,741 query-reply pairs and a dataset of human-human utterance pairs   \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                             Spanish, German, and Czech   \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                             Spanish, German, and Czech   \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                             Spanish, German, and Czech   \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                             Spanish, German, and Czech   \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                             Spanish, German, and Czech   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                              gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                                                                                                                                                                                                                                                                                     [Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB) ]  \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                                                                                                                                                                                                                                                                                     [Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB) ]  \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                                                                                                                                                                                                                                                                                     [Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB) ]  \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                                                                                                                                                                                                                                                                                     [Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB) ]  \n",
       "90aba75508aa145475d7cc9a501bbe987c0e8413                                                                                                                                                                                                                                                                                                                     [Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB), Craigslist Bargaining dataset (CB) ]  \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                                                                                                                                                                                                                                                                                                                                                                                  [PDTB 2.0, PDTB 2.0 , PDTB 2.0 ]  \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                                                                                                                                                                                                                                                                                                                                                                                  [PDTB 2.0, PDTB 2.0 , PDTB 2.0 ]  \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                                                                                                                                                                                                                                                                                                                                                                                  [PDTB 2.0, PDTB 2.0 , PDTB 2.0 ]  \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                                                                                                                                                                                                                                                                                                                                                                                  [PDTB 2.0, PDTB 2.0 , PDTB 2.0 ]  \n",
       "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c                                                                                                                                                                                                                                                                                                                                                                                                  [PDTB 2.0, PDTB 2.0 , PDTB 2.0 ]  \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8  [They create their own datasets from online text., To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, the database contains 7 million query-reply pairs for retrieval, For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs]  \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8  [They create their own datasets from online text., To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, the database contains 7 million query-reply pairs for retrieval, For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs]  \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8  [They create their own datasets from online text., To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, the database contains 7 million query-reply pairs for retrieval, For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs]  \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8  [They create their own datasets from online text., To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, the database contains 7 million query-reply pairs for retrieval, For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs]  \n",
       "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8  [They create their own datasets from online text., To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, the database contains 7 million query-reply pairs for retrieval, For the generation part, we constructed another dataset from various resources in public websites comprising 1,606,741 query-reply pairs]  \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                                                                                                                                                                                                                                       [PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, 100 native Czech speakers (50 PD, 50 HC) , the PC-GITA corpus BIBREF5, BIBREF17, BIBREF18]  \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                                                                                                                                                                                                                                       [PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, 100 native Czech speakers (50 PD, 50 HC) , the PC-GITA corpus BIBREF5, BIBREF17, BIBREF18]  \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                                                                                                                                                                                                                                       [PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, 100 native Czech speakers (50 PD, 50 HC) , the PC-GITA corpus BIBREF5, BIBREF17, BIBREF18]  \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                                                                                                                                                                                                                                       [PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, 100 native Czech speakers (50 PD, 50 HC) , the PC-GITA corpus BIBREF5, BIBREF17, BIBREF18]  \n",
       "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e                                                                                                                                                                                                                                                                       [PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, 100 native Czech speakers (50 PD, 50 HC) , the PC-GITA corpus BIBREF5, BIBREF17, BIBREF18]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What are the baseline models?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fewshot_evidence</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77bbe1698e001c5889217be3164982ea36e85752</th>\n",
       "      <td>Lacking the ability to model cross-context patterns, Baseline inadvertently learned to retract to predict single-token entities (0.13 vs. -0.63, -0.41, -0.38) when an easy hint from a familiar surface form is not available. This indicates a major flaw in BiLSTM-CNNs prevalently used for real-world NER today.</td>\n",
       "      <td>BiLSTM-CNN proposed by BIBREF1</td>\n",
       "      <td>[BiLSTM-CNN, BiLSTM-CNN proposed by BIBREF1, Baseline-BiLSTM-CNN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77bbe1698e001c5889217be3164982ea36e85752</th>\n",
       "      <td>This paper explores two types of cross-structures to help cope with the problem: Cross-BiLSTM-CNN and Att-BiLSTM-CNN. Previous studies have tried to stack multiple LSTMs for sequence-labeling NER BIBREF1. As they follow the trend of stacking forward and backward LSTMs independently, the Baseline-BiLSTM-CNN is only able to learn higher-level representations of past or future per se. Instead, Cross-BiLSTM-CNN, which interleaves every layer of the two directions, models cross-context in an additive manner by learning higher-level representations of the whole context of each token. On the other hand, Att-BiLSTM-CNN models cross-context in a multiplicative manner by capturing the interaction between past and future with a dot-product self-attentive mechanism BIBREF5, BIBREF6.</td>\n",
       "      <td>BiLSTM-CNN proposed by BIBREF1</td>\n",
       "      <td>[BiLSTM-CNN, BiLSTM-CNN proposed by BIBREF1, Baseline-BiLSTM-CNN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77bbe1698e001c5889217be3164982ea36e85752</th>\n",
       "      <td>Section SECREF3 formulates the three Baseline, Cross, and Att-BiLSTM-CNN models. The section gives a concrete proof that patterns forming an XOR cannot be modeled by Baseline-BiLSTM-CNN used in all previous work. Cross-BiLSTM-CNN and Att-BiLSTM-CNN are shown to have additive and multiplicative cross-structures respectively to deal with the problem. Section SECREF4 evaluates the approaches on two challenging NER datasets spanning a wide range of domains with complex, noisy, and emerging entities. The cross-structures bring consistent improvements over the prevalently used Baseline-BiLSTM-CNN without additional gazetteers, POS taggers, language-modeling, or multi-task supervision. The improved core module surpasses comparable previous models on OntoNotes 5.0 and WNUT 2017 by 1.4% and 4.6% respectively. Experiments reveal that emerging, complex, confusing, and multi-token entity mentions benefitted much from the cross-structures, and the in-depth entity-chunking analysis finds that the prevalently used Baseline-BiLSTM-CNN is flawed for real-world NER.</td>\n",
       "      <td>BiLSTM-CNN proposed by BIBREF1</td>\n",
       "      <td>[BiLSTM-CNN, BiLSTM-CNN proposed by BIBREF1, Baseline-BiLSTM-CNN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77bbe1698e001c5889217be3164982ea36e85752</th>\n",
       "      <td>Many have attempted tackling the NER task with LSTM-based sequence encoders BIBREF7, BIBREF0, BIBREF1, BIBREF8. Among these, the most sophisticated, state-of-the-art is the BiLSTM-CNN proposed by BIBREF1. They stack multiple layers of LSTM cells per direction and also use a CNN to compute character-level word vectors alongside pre-trained word vectors. This paper largely follows their work in constructing the Baseline-BiLSTM-CNN, including the selection of raw features, the CNN, and the multi-layer BiLSTM. A subtle difference is that they send the output of each direction through separate affine-softmax classifiers and then sum their probabilities, while this paper sum the scores from affine layers before computing softmax once. While not changing the modeling capacity regarded in this paper, the baseline model does perform better than their formulation.</td>\n",
       "      <td>BiLSTM-CNN proposed by BIBREF1</td>\n",
       "      <td>[BiLSTM-CNN, BiLSTM-CNN proposed by BIBREF1, Baseline-BiLSTM-CNN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77bbe1698e001c5889217be3164982ea36e85752</th>\n",
       "      <td>Besides comparing to the Baseline implemented in this paper, results also compared against previously reported results of BiLSTM-CNN BIBREF1, CRF-BiLSTM(-BiLSTM) BIBREF10, BIBREF25, and CRF-IDCNN BIBREF10 on the two datasets. Among them, IDCNN was a CNN-based sentence encoder, which should not have the XOR limitation raised in this paper. Only fair comparisons against models without using additional resources were made. However, the models that used those additional resources (Secion SECREF2) actually all used a BiLSTM sentence encoder with the XOR limitation, so they could indeed integrate with and benefit from the cross-structures.</td>\n",
       "      <td>BiLSTM-CNN proposed by BIBREF1</td>\n",
       "      <td>[BiLSTM-CNN, BiLSTM-CNN proposed by BIBREF1, Baseline-BiLSTM-CNN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fe6181ab0aecf5bc8c3def843f82e530347d918b</th>\n",
       "      <td>We first train an MLE model as our baseline, trained on the Conceptual Captions training split alone. We referred to this model as Baseline. For a baseline approach that utilizes (some of) the Caption-Quality data, we merge positively-rated captions from the Caption-Quality training split with the Conceptual Captions examples and finetune the baseline model. We call this model Baseline$+(t)$, where $t \\in [0,1]$ is the rating threshold for the included positive captions. We train models for two variants, $t\\in \\lbrace 0.5, 0.7\\rbrace $, which results in $\\sim $72K and $\\sim $51K additional (pseudo-)ground-truth captions, respectively. Note that the Baseline$+(t)$ approaches attempt to make use of the same additional dataset as our two reinforced models, OnPG and OffPG, but they need to exclude below-threshold captions due to the constraints in MLE.</td>\n",
       "      <td>MLE model and Baseline$+(t)$ with $t \\in [0,1]$</td>\n",
       "      <td>[ MLE model, Baseline$+(t)$, MLE model]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fe6181ab0aecf5bc8c3def843f82e530347d918b</th>\n",
       "      <td>In addition to the baselines, we train two reinforced models: one based on the on-policy policy gradient method with a rating estimator (OnPG), and the other based on the off-policy policy gradient method with the true ratings (OffPG). The differences between the methods are shown in Figure FIGREF27.</td>\n",
       "      <td>MLE model and Baseline$+(t)$ with $t \\in [0,1]$</td>\n",
       "      <td>[ MLE model, Baseline$+(t)$, MLE model]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fe6181ab0aecf5bc8c3def843f82e530347d918b</th>\n",
       "      <td>We train Baseline using the Adam optimizer BIBREF34 on the training split of the Conceptual dataset for 3M iterations with the batch size of 4,096 and the learning rate of $3.2\\times 10^{-5}$. The learning rate is warmed up for 20 epochs and exponentially decayed by a factor of 0.95 every 25 epochs. Baseline$+(t)$ are obtained by fine-tuning Baseline on the merged dataset for 1M iterations, with the learning rate of $3.2\\times 10^{-7}$ and the same decaying factor. For OnPG, because its memory footprint is increased significantly due to the additional parameters for the rating estimator, we reduce the batch size for training this model by a 0.25 factor; the value of $b$ in Eq. (DISPLAY_FORM12) is set to the moving average of the rating estimates. During OffPG training, for each batch, we sample half of the examples from the Conceptual dataset and the other half from Caption-Quality dataset; $b$ is set to the average of the ratings in the dataset.</td>\n",
       "      <td>MLE model and Baseline$+(t)$ with $t \\in [0,1]$</td>\n",
       "      <td>[ MLE model, Baseline$+(t)$, MLE model]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fe6181ab0aecf5bc8c3def843f82e530347d918b</th>\n",
       "      <td>Each model is evaluated by the average rating scores from 3 distinct raters. As a result, we obtain 3 values for each model in the range $[-1, 1]$, where a negative score means a performance degradation in the given dimension with respect to Baseline. For every human evaluation, we report confidence intervals based on bootstrap resampling BIBREF35.</td>\n",
       "      <td>MLE model and Baseline$+(t)$ with $t \\in [0,1]$</td>\n",
       "      <td>[ MLE model, Baseline$+(t)$, MLE model]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fe6181ab0aecf5bc8c3def843f82e530347d918b</th>\n",
       "      <td>In the other type of evaluation, we measure the relative improvement of a model against the Baseline model; Three professional raters are shown the input image and two captions (anonymized and randomly shuffled with respect to their left/right position) side-by-side. One of the captions is from a candidate model and the other always from Baseline. We ask for relative judgments on three dimensions – Informativeness, Correctness and Fluency, using their corresponding questions shown in Table TABREF32. Each of these dimensions allows a 5-way choice, shown below together with their corresponding scores:</td>\n",
       "      <td>MLE model and Baseline$+(t)$ with $t \\in [0,1]$</td>\n",
       "      <td>[ MLE model, Baseline$+(t)$, MLE model]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8e113fd9661bc8af97e30c75a20712f01fc4520a</th>\n",
       "      <td>The results are summarized in the tables TABREF14-TABREF17; each table refers to the respective comparison study. All tables present the performance results of our proposed method (“Proposed”) and contrast them to eight state-of-the-art baseline methodologies along with published results using the same dataset. Specifically, Table TABREF14 presents the results obtained using the ironic dataset used in SemEval-2018 Task 3.A, compared with recently published studies and two high performing teams from the respective SemEval shared task BIBREF98, BIBREF99. Tables TABREF15,TABREF16 summarize results obtained using Sarcastic datasets (Reddit SARC politics BIBREF97 and Riloff Twitter BIBREF96). Finally, Table TABREF17 compares the results from baseline models, from top two ranked task participants BIBREF68, BIBREF67, from our previous study with the DESC methodology BIBREF0 with the proposed RCNN-RoBERTa framework on a Sentiment Analysis task with figurative language, using the SemEval 2015 Task 11 dataset.</td>\n",
       "      <td>ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model</td>\n",
       "      <td>[ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased), BERT base uncased (BERT-Uncased), RoBERTa base model, ELMo, USE , NBSVM , FastText , XLnet base cased model (XLnet, BERT base cased (BERT-Cased) , BERT base uncased (BERT-Uncased), RoBERTa ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8e113fd9661bc8af97e30c75a20712f01fc4520a</th>\n",
       "      <td>To assess the performance of the proposed method we performed an exhaustive comparison with several advanced state-of-the-art methodologies along with published results. The used methodologies were appropriately implemented using the available codes and guidelines, and include: ELMo BIBREF64, USE BIBREF79, NBSVM BIBREF93, FastText BIBREF94, XLnet base cased model (XLnet) BIBREF88, BERT BIBREF18 in two setups: BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model. The published results were acquired from the respective original publication (the reference publication is indicated in the respective tables). For the comparison we utilized benchmark datasets that include ironic, sarcastic and metaphoric expressions. Namely, we used the dataset provided in “Semantic Evaluation Workshop Task 3” (SemEval-2018) that contains ironic tweets BIBREF95; Riloff’s high quality sarcastic unbalanced dataset BIBREF96; a large dataset containing political comments from Reddit BIBREF97; and a SA dataset that contains tweets with various FL forms from “SemEval-2015 Task 11” BIBREF66. All datasets are used in a binary classification manner (i.e., irony/sarcasm vs. literal), except from the “SemEval-2015 Task 11” dataset where the task is to predict a sentiment integer score (from -5 to 5) for each tweet (refer to BIBREF0 for more details). The evaluation was made across standard five metrics namely, Accuracy (Acc), Precision (Pre), Recall (Rec), F1-score (F1), and Area Under the Receiver Operating Characteristics Curve (AUC). For the SA task the cosine similarity metric (Cos) and mean squared error (MSE) metrics are used, as proposed in the original study BIBREF66.</td>\n",
       "      <td>ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model</td>\n",
       "      <td>[ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased), BERT base uncased (BERT-Uncased), RoBERTa base model, ELMo, USE , NBSVM , FastText , XLnet base cased model (XLnet, BERT base cased (BERT-Cased) , BERT base uncased (BERT-Uncased), RoBERTa ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8e113fd9661bc8af97e30c75a20712f01fc4520a</th>\n",
       "      <td>Sequence-to-sequence (seq2seq) methods using encoder-decoder schemes are a popular choice for several tasks such as Machine Translation, Text Summarization, Question Answering etc. BIBREF82. However, encoder’s contextual representations are uncertain when dealing with long-range dependencies. To address these drawbacks, Vaswani et al. in BIBREF80 introduced a novel network architecture, called Transformer, relying entirely on self-attention units to map input sequences to output sequences without the use of RNNs. The Transformer’s decoder unit architecture contains a masked multi-head attention layer followed by a multi-head attention unit and a feed forward network whereas the decoder unit is almost identical without the masked attention unit. Multi-head self-attention layers are calculated in parallel facing the computational costs of regular attention layers used by previous seq2seq network architectures. In BIBREF18 the authors presented a model that is founded on findings from various previous studies (e.g., BIBREF83, BIBREF84, BIBREF64, BIBREF49, BIBREF80), which achieved state-of-the-art results on eleven NLP tasks, called BERT - Bidirectional Encoder Representations from Transformers. The BERT training process is split in two phases, the unsupervised pre-training phase and the fine-tuning phase using labelled data for down-streaming tasks. In contrast with previous proposed models (e.g., BIBREF64, BIBREF49), BERT uses masked language models (MLMs) to enable pre-trained deep bidirectional representations. In the pre-training phase the model is trained with a large amount of unlabeled data from Wikipedia, BookCorpus BIBREF85 and WordPiece BIBREF86 embeddings. In this training part, the model was tested on two tasks; on the first task, the model randomly masks 15% of the input tokens aiming to capture conceptual representations of word sequences by predicting masked words inside the corpus, whereas in the second task the model is given two sentences and tries to predict whether the second sentence is the next sentence of the first. In the second phase, BERT is extended with a task-related classifier model that is trained on a supervised manner. During this supervised phase, the pre-trained BERT model receives minimal changes, with the classifier’s parameters trained in order to minimize the loss function. Two models presented in BIBREF18, a “Base Bert” model with 12 encoder layers (i.e. transformer blocks), feed-forward networks with 768 hidden units and 12 attention heads, and a “Large Bert” model with 24 encoder layers 1024 feed-the pre-trained Bert model, an architecture almost identical with the aforementioned Transformer network. A [CLS] token is supplied in the input as the first token, the final hidden state of which is aggregated for classification tasks. Despite the achieved breakthroughs, the BERT model suffers from several drawbacks. Firstly, BERT, as all language models using Transformers, assumes (and pre-supposes) independence between the masked words from the input sequence, and neglects all the positional and dependency information between words. In other words, for the prediction of a masked token both word and position embeddings are masked out, even if positional information is a key-aspect of NLP BIBREF87. In addition, the [MASK] token which, is substituted with masked words, is mostly absent in fine-tuning phase for down-streaming tasks, leading to a pre-training fine-turning discrepancy. To address the cons of BERT, a permutation language model was introduced, so-called XLnet, trained to predict masked tokens in a non-sequential random order, factorizing likelihood in an autoregressive manner without the independence assumption and without relying on any input corruption BIBREF88. In particular, a query stream is used that extends embedding representations to incorporate positional information about the masked words. The original representation set (content stream), including both token and positional embeddings, is then used as input to the query stream following a scheme called “Two-Stream SelfAttention”. To overcome the problem of slow convergence the authors propose the prediction of the last token in the permutation phase, instead of predicting the entire sequence. Finally, XLnet uses also a special token for the classification and separation of the input sequence, [CLS] and [SEP] respectively, however it also learns an embedding that denotes whether the two words are from the same segment. This is similar to relative positional encodings introduced in TrasformerXL BIBREF87, and extents the ability of XLnet to cope with tasks that encompass arbitrary input segments. Recently, a replication study, BIBREF18, suggested several modifications in the training procedure of BERT which, outperforms the original XLNet architecture on several NLP tasks. The optimized model, called Robustly Optimized BERT Approach (RoBERTa), used 10 times more data (160GB compared with the 16GB originally exploited), and is trained with far more epochs than the BERT model (500K vs 100K), using also 8-times larger batch sizes, and a byte-level BPE vocabulary instead of the character-level vocabulary that was previously utilized. Another significant modification, was the dynamic masking technique instead of the single static mask used in BERT. In addition, RoBERTa model removes the next sentence prediction objective used in BERT, following advises by several other studies that question the NSP loss term BIBREF89, BIBREF90, BIBREF91.</td>\n",
       "      <td>ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model</td>\n",
       "      <td>[ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased), BERT base uncased (BERT-Uncased), RoBERTa base model, ELMo, USE , NBSVM , FastText , XLnet base cased model (XLnet, BERT base cased (BERT-Cased) , BERT base uncased (BERT-Uncased), RoBERTa ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8e113fd9661bc8af97e30c75a20712f01fc4520a</th>\n",
       "      <td>In our experiments we compared our model with several seven different classifiers under different settings. For the ELMo system we used the mean-pooling of all contextualized word representations, i.e. character-based embedding representations and the output of the two layer LSTM resulting with a 1024 dimensional vector, and passed it through two deep dense ReLu activated layers with 256 and 64 units. Similarly, USE embeddings are trained with a Transformer encoder and output 512 dimensional vector for each sample, which is also passed through through two deep dense ReLu activated layers with 256 and 64 units. Both ELMo and USE embeddings retrieved from TensorFlow Hub. NBSVM system was modified according to BIBREF93 and trained with a ${10^{-3}}$ leaning rate for 5 epochs with Adam optimizer BIBREF100. FastText system was implemented by utilizing pre-trained embeddings BIBREF94 passed through a global max-pooling and a 64 unit fully connected layer. System was trained with Adam optimizer with learning rate ${0.1}$ for 3 epochs. XLnet model implemented using the base-cased model with 12 layers, 768 hidden units and 12 attention heads. Model trained with learning rate ${4 \\times 10^{-5}}$ using ${10^{-5}}$ weight decay for 3 epochs. We exploited both cased and uncased BERT-base models containing 12 layers, 768 hidden units and 12 attention heads. We trained models for 3 epochs with learning rate ${2 \\times 10^{-5}}$ using ${10^{-5}}$ weight decay. We trained RoBERTa model following the setting of BERT model. RoBERTa, XLnet and BERT models implemented using pytorch-transformers library .</td>\n",
       "      <td>ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model</td>\n",
       "      <td>[ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased), BERT base uncased (BERT-Uncased), RoBERTa base model, ELMo, USE , NBSVM , FastText , XLnet base cased model (XLnet, BERT base cased (BERT-Cased) , BERT base uncased (BERT-Uncased), RoBERTa ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8e113fd9661bc8af97e30c75a20712f01fc4520a</th>\n",
       "      <td>Pre-trained word embeddings proved to increase classification performances in many NLP tasks. In particular, Global Vectors (GloVe) BIBREF74 and Word2Vec BIBREF75 became popular in various tasks due to their ability to capture representative semantic representations of words, trained on large amount of data. However, in various studies (e.g., BIBREF76, BIBREF64, BIBREF77) it is argued that the actual meaning of words along with their semantics representations varies according to their context. Following this assumption, researchers in BIBREF64 present an approach that is based on the creation of pre-trained word embeddings through building a bidirectional Language model, i.e. predicting next word within a sequence. The ELMo model was exhaustingly trained on 30 million sentences corpus BIBREF78, with a two layered bidirectional LSTM architecture, aiming to predict both next and previous words, introducing the concept of contextual embeddings. The final embeddings vector is produced by a task specific weighted sum of the two directional hidden layers of LSTM models. Another contextual approach for creating embedding vector representations is proposed in BIBREF79 where, complete sentences, instead of words, are mapped to a latent vector space. The approach provides two variations of Universal Sentence Encoder (USE) with some trade-offs in computation and accuracy. The first approach consists of a computationally intensive transformer that resembles a transformer network BIBREF80, proved to achieve higher performance figures. In contrast, the second approach provides a light-weight model that averages input embedding weights for words and bi-grams by utilizing of a Deep Average Network (DAN) BIBREF81. The output of the DAN is passed through a feedforward neural network in order to produce the sentence embeddings. Both approaches take as input lowercased PTB tokenized strings, and output a 512-dimensional sentence embedding vectors.</td>\n",
       "      <td>ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model</td>\n",
       "      <td>[ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased), BERT base uncased (BERT-Uncased), RoBERTa base model, ELMo, USE , NBSVM , FastText , XLnet base cased model (XLnet, BERT base cased (BERT-Cased) , BERT base uncased (BERT-Uncased), RoBERTa ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23cbf6ab365c1eb760b565d8ba51fb3f06257d62</th>\n",
       "      <td>Our models are trained and evaluated on the WikiLarge dataset BIBREF10 which contains 296,402/2,000/359 samples (train/validation/test). WikiLarge is a set of automatically aligned complex-simple sentence pairs from English Wikipedia (EW) and Simple English Wikipedia (SEW). It is compiled from previous extractions of EW-SEW BIBREF11, BIBREF28, BIBREF29. Its validation and test sets are taken from Turkcorpus BIBREF9, where each complex sentence has 8 human simplifications created by Amazon Mechanical Turk workers. Human annotators were instructed to only paraphrase the source sentences while keeping as much meaning as possible. Hence, no sentence splitting, minimal structural simplification and little content reduction occurs in this test set BIBREF9.</td>\n",
       "      <td>Seq2Seq MT models</td>\n",
       "      <td>[PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS, BIBREF12, BIBREF33, BIBREF9, BIBREF10, BIBREF17, BIBREF15, BIBREF35, BIBREF16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23cbf6ab365c1eb760b565d8ba51fb3f06257d62</th>\n",
       "      <td>ACCESS scores best on SARI (41.87), a significant improvement over previous state of the art (40.45), and third to best FKGL (7.22). The second and third models in terms of SARI, DMASS+DCSS (40.45) and SBMT+PPDB+SARI (39.96), both use the external resource Simple PPDB BIBREF36 that was extracted from 1000 times more data than what we used for training. Our FKGL is also better (lower) than these methods. The Hybrid model scores best on FKGL (4.56) i.e. they generated the simplest (and shortest) sentences, but it was done at the expense of SARI (31.40).</td>\n",
       "      <td>Seq2Seq MT models</td>\n",
       "      <td>[PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS, BIBREF12, BIBREF33, BIBREF9, BIBREF10, BIBREF17, BIBREF15, BIBREF35, BIBREF16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23cbf6ab365c1eb760b565d8ba51fb3f06257d62</th>\n",
       "      <td>Lately, SS has mostly been tackled using Seq2Seq MT models BIBREF14. Seq2Seq models were either used as-is BIBREF15 or combined with reinforcement learning thanks to a specific simplification reward BIBREF10, augmented with an external simplification database as a dynamic memory BIBREF16 or trained with multi-tasking on entailment and paraphrase generation BIBREF17.</td>\n",
       "      <td>Seq2Seq MT models</td>\n",
       "      <td>[PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS, BIBREF12, BIBREF33, BIBREF9, BIBREF10, BIBREF17, BIBREF15, BIBREF35, BIBREF16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23cbf6ab365c1eb760b565d8ba51fb3f06257d62</th>\n",
       "      <td>Table TABREF24 compares our best model to state-of-the-art methods:</td>\n",
       "      <td>Seq2Seq MT models</td>\n",
       "      <td>[PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS, BIBREF12, BIBREF33, BIBREF9, BIBREF10, BIBREF17, BIBREF15, BIBREF35, BIBREF16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23cbf6ab365c1eb760b565d8ba51fb3f06257d62</th>\n",
       "      <td>Conditional training with Seq2Seq models was applied to multiple natural language processing tasks such as summarization BIBREF5, BIBREF6, dialog BIBREF18, sentence compression BIBREF19, BIBREF20 or poetry generation BIBREF21.</td>\n",
       "      <td>Seq2Seq MT models</td>\n",
       "      <td>[PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS, BIBREF12, BIBREF33, BIBREF9, BIBREF10, BIBREF17, BIBREF15, BIBREF35, BIBREF16]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           fewshot_evidence  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Lacking the ability to model cross-context patterns, Baseline inadvertently learned to retract to predict single-token entities (0.13 vs. -0.63, -0.41, -0.38) when an easy hint from a familiar surface form is not available. This indicates a major flaw in BiLSTM-CNNs prevalently used for real-world NER today.   \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      This paper explores two types of cross-structures to help cope with the problem: Cross-BiLSTM-CNN and Att-BiLSTM-CNN. Previous studies have tried to stack multiple LSTMs for sequence-labeling NER BIBREF1. As they follow the trend of stacking forward and backward LSTMs independently, the Baseline-BiLSTM-CNN is only able to learn higher-level representations of past or future per se. Instead, Cross-BiLSTM-CNN, which interleaves every layer of the two directions, models cross-context in an additive manner by learning higher-level representations of the whole context of each token. On the other hand, Att-BiLSTM-CNN models cross-context in a multiplicative manner by capturing the interaction between past and future with a dot-product self-attentive mechanism BIBREF5, BIBREF6.   \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Section SECREF3 formulates the three Baseline, Cross, and Att-BiLSTM-CNN models. The section gives a concrete proof that patterns forming an XOR cannot be modeled by Baseline-BiLSTM-CNN used in all previous work. Cross-BiLSTM-CNN and Att-BiLSTM-CNN are shown to have additive and multiplicative cross-structures respectively to deal with the problem. Section SECREF4 evaluates the approaches on two challenging NER datasets spanning a wide range of domains with complex, noisy, and emerging entities. The cross-structures bring consistent improvements over the prevalently used Baseline-BiLSTM-CNN without additional gazetteers, POS taggers, language-modeling, or multi-task supervision. The improved core module surpasses comparable previous models on OntoNotes 5.0 and WNUT 2017 by 1.4% and 4.6% respectively. Experiments reveal that emerging, complex, confusing, and multi-token entity mentions benefitted much from the cross-structures, and the in-depth entity-chunking analysis finds that the prevalently used Baseline-BiLSTM-CNN is flawed for real-world NER.   \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Many have attempted tackling the NER task with LSTM-based sequence encoders BIBREF7, BIBREF0, BIBREF1, BIBREF8. Among these, the most sophisticated, state-of-the-art is the BiLSTM-CNN proposed by BIBREF1. They stack multiple layers of LSTM cells per direction and also use a CNN to compute character-level word vectors alongside pre-trained word vectors. This paper largely follows their work in constructing the Baseline-BiLSTM-CNN, including the selection of raw features, the CNN, and the multi-layer BiLSTM. A subtle difference is that they send the output of each direction through separate affine-softmax classifiers and then sum their probabilities, while this paper sum the scores from affine layers before computing softmax once. While not changing the modeling capacity regarded in this paper, the baseline model does perform better than their formulation.   \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Besides comparing to the Baseline implemented in this paper, results also compared against previously reported results of BiLSTM-CNN BIBREF1, CRF-BiLSTM(-BiLSTM) BIBREF10, BIBREF25, and CRF-IDCNN BIBREF10 on the two datasets. Among them, IDCNN was a CNN-based sentence encoder, which should not have the XOR limitation raised in this paper. Only fair comparisons against models without using additional resources were made. However, the models that used those additional resources (Secion SECREF2) actually all used a BiLSTM sentence encoder with the XOR limitation, so they could indeed integrate with and benefit from the cross-structures.   \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We first train an MLE model as our baseline, trained on the Conceptual Captions training split alone. We referred to this model as Baseline. For a baseline approach that utilizes (some of) the Caption-Quality data, we merge positively-rated captions from the Caption-Quality training split with the Conceptual Captions examples and finetune the baseline model. We call this model Baseline$+(t)$, where $t \\in [0,1]$ is the rating threshold for the included positive captions. We train models for two variants, $t\\in \\lbrace 0.5, 0.7\\rbrace $, which results in $\\sim $72K and $\\sim $51K additional (pseudo-)ground-truth captions, respectively. Note that the Baseline$+(t)$ approaches attempt to make use of the same additional dataset as our two reinforced models, OnPG and OffPG, but they need to exclude below-threshold captions due to the constraints in MLE.   \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In addition to the baselines, we train two reinforced models: one based on the on-policy policy gradient method with a rating estimator (OnPG), and the other based on the off-policy policy gradient method with the true ratings (OffPG). The differences between the methods are shown in Figure FIGREF27.   \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We train Baseline using the Adam optimizer BIBREF34 on the training split of the Conceptual dataset for 3M iterations with the batch size of 4,096 and the learning rate of $3.2\\times 10^{-5}$. The learning rate is warmed up for 20 epochs and exponentially decayed by a factor of 0.95 every 25 epochs. Baseline$+(t)$ are obtained by fine-tuning Baseline on the merged dataset for 1M iterations, with the learning rate of $3.2\\times 10^{-7}$ and the same decaying factor. For OnPG, because its memory footprint is increased significantly due to the additional parameters for the rating estimator, we reduce the batch size for training this model by a 0.25 factor; the value of $b$ in Eq. (DISPLAY_FORM12) is set to the moving average of the rating estimates. During OffPG training, for each batch, we sample half of the examples from the Conceptual dataset and the other half from Caption-Quality dataset; $b$ is set to the average of the ratings in the dataset.   \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Each model is evaluated by the average rating scores from 3 distinct raters. As a result, we obtain 3 values for each model in the range $[-1, 1]$, where a negative score means a performance degradation in the given dimension with respect to Baseline. For every human evaluation, we report confidence intervals based on bootstrap resampling BIBREF35.   \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In the other type of evaluation, we measure the relative improvement of a model against the Baseline model; Three professional raters are shown the input image and two captions (anonymized and randomly shuffled with respect to their left/right position) side-by-side. One of the captions is from a candidate model and the other always from Baseline. We ask for relative judgments on three dimensions – Informativeness, Correctness and Fluency, using their corresponding questions shown in Table TABREF32. Each of these dimensions allows a 5-way choice, shown below together with their corresponding scores:   \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The results are summarized in the tables TABREF14-TABREF17; each table refers to the respective comparison study. All tables present the performance results of our proposed method (“Proposed”) and contrast them to eight state-of-the-art baseline methodologies along with published results using the same dataset. Specifically, Table TABREF14 presents the results obtained using the ironic dataset used in SemEval-2018 Task 3.A, compared with recently published studies and two high performing teams from the respective SemEval shared task BIBREF98, BIBREF99. Tables TABREF15,TABREF16 summarize results obtained using Sarcastic datasets (Reddit SARC politics BIBREF97 and Riloff Twitter BIBREF96). Finally, Table TABREF17 compares the results from baseline models, from top two ranked task participants BIBREF68, BIBREF67, from our previous study with the DESC methodology BIBREF0 with the proposed RCNN-RoBERTa framework on a Sentiment Analysis task with figurative language, using the SemEval 2015 Task 11 dataset.   \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  To assess the performance of the proposed method we performed an exhaustive comparison with several advanced state-of-the-art methodologies along with published results. The used methodologies were appropriately implemented using the available codes and guidelines, and include: ELMo BIBREF64, USE BIBREF79, NBSVM BIBREF93, FastText BIBREF94, XLnet base cased model (XLnet) BIBREF88, BERT BIBREF18 in two setups: BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model. The published results were acquired from the respective original publication (the reference publication is indicated in the respective tables). For the comparison we utilized benchmark datasets that include ironic, sarcastic and metaphoric expressions. Namely, we used the dataset provided in “Semantic Evaluation Workshop Task 3” (SemEval-2018) that contains ironic tweets BIBREF95; Riloff’s high quality sarcastic unbalanced dataset BIBREF96; a large dataset containing political comments from Reddit BIBREF97; and a SA dataset that contains tweets with various FL forms from “SemEval-2015 Task 11” BIBREF66. All datasets are used in a binary classification manner (i.e., irony/sarcasm vs. literal), except from the “SemEval-2015 Task 11” dataset where the task is to predict a sentiment integer score (from -5 to 5) for each tweet (refer to BIBREF0 for more details). The evaluation was made across standard five metrics namely, Accuracy (Acc), Precision (Pre), Recall (Rec), F1-score (F1), and Area Under the Receiver Operating Characteristics Curve (AUC). For the SA task the cosine similarity metric (Cos) and mean squared error (MSE) metrics are used, as proposed in the original study BIBREF66.   \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a  Sequence-to-sequence (seq2seq) methods using encoder-decoder schemes are a popular choice for several tasks such as Machine Translation, Text Summarization, Question Answering etc. BIBREF82. However, encoder’s contextual representations are uncertain when dealing with long-range dependencies. To address these drawbacks, Vaswani et al. in BIBREF80 introduced a novel network architecture, called Transformer, relying entirely on self-attention units to map input sequences to output sequences without the use of RNNs. The Transformer’s decoder unit architecture contains a masked multi-head attention layer followed by a multi-head attention unit and a feed forward network whereas the decoder unit is almost identical without the masked attention unit. Multi-head self-attention layers are calculated in parallel facing the computational costs of regular attention layers used by previous seq2seq network architectures. In BIBREF18 the authors presented a model that is founded on findings from various previous studies (e.g., BIBREF83, BIBREF84, BIBREF64, BIBREF49, BIBREF80), which achieved state-of-the-art results on eleven NLP tasks, called BERT - Bidirectional Encoder Representations from Transformers. The BERT training process is split in two phases, the unsupervised pre-training phase and the fine-tuning phase using labelled data for down-streaming tasks. In contrast with previous proposed models (e.g., BIBREF64, BIBREF49), BERT uses masked language models (MLMs) to enable pre-trained deep bidirectional representations. In the pre-training phase the model is trained with a large amount of unlabeled data from Wikipedia, BookCorpus BIBREF85 and WordPiece BIBREF86 embeddings. In this training part, the model was tested on two tasks; on the first task, the model randomly masks 15% of the input tokens aiming to capture conceptual representations of word sequences by predicting masked words inside the corpus, whereas in the second task the model is given two sentences and tries to predict whether the second sentence is the next sentence of the first. In the second phase, BERT is extended with a task-related classifier model that is trained on a supervised manner. During this supervised phase, the pre-trained BERT model receives minimal changes, with the classifier’s parameters trained in order to minimize the loss function. Two models presented in BIBREF18, a “Base Bert” model with 12 encoder layers (i.e. transformer blocks), feed-forward networks with 768 hidden units and 12 attention heads, and a “Large Bert” model with 24 encoder layers 1024 feed-the pre-trained Bert model, an architecture almost identical with the aforementioned Transformer network. A [CLS] token is supplied in the input as the first token, the final hidden state of which is aggregated for classification tasks. Despite the achieved breakthroughs, the BERT model suffers from several drawbacks. Firstly, BERT, as all language models using Transformers, assumes (and pre-supposes) independence between the masked words from the input sequence, and neglects all the positional and dependency information between words. In other words, for the prediction of a masked token both word and position embeddings are masked out, even if positional information is a key-aspect of NLP BIBREF87. In addition, the [MASK] token which, is substituted with masked words, is mostly absent in fine-tuning phase for down-streaming tasks, leading to a pre-training fine-turning discrepancy. To address the cons of BERT, a permutation language model was introduced, so-called XLnet, trained to predict masked tokens in a non-sequential random order, factorizing likelihood in an autoregressive manner without the independence assumption and without relying on any input corruption BIBREF88. In particular, a query stream is used that extends embedding representations to incorporate positional information about the masked words. The original representation set (content stream), including both token and positional embeddings, is then used as input to the query stream following a scheme called “Two-Stream SelfAttention”. To overcome the problem of slow convergence the authors propose the prediction of the last token in the permutation phase, instead of predicting the entire sequence. Finally, XLnet uses also a special token for the classification and separation of the input sequence, [CLS] and [SEP] respectively, however it also learns an embedding that denotes whether the two words are from the same segment. This is similar to relative positional encodings introduced in TrasformerXL BIBREF87, and extents the ability of XLnet to cope with tasks that encompass arbitrary input segments. Recently, a replication study, BIBREF18, suggested several modifications in the training procedure of BERT which, outperforms the original XLNet architecture on several NLP tasks. The optimized model, called Robustly Optimized BERT Approach (RoBERTa), used 10 times more data (160GB compared with the 16GB originally exploited), and is trained with far more epochs than the BERT model (500K vs 100K), using also 8-times larger batch sizes, and a byte-level BPE vocabulary instead of the character-level vocabulary that was previously utilized. Another significant modification, was the dynamic masking technique instead of the single static mask used in BERT. In addition, RoBERTa model removes the next sentence prediction objective used in BERT, following advises by several other studies that question the NSP loss term BIBREF89, BIBREF90, BIBREF91.   \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        In our experiments we compared our model with several seven different classifiers under different settings. For the ELMo system we used the mean-pooling of all contextualized word representations, i.e. character-based embedding representations and the output of the two layer LSTM resulting with a 1024 dimensional vector, and passed it through two deep dense ReLu activated layers with 256 and 64 units. Similarly, USE embeddings are trained with a Transformer encoder and output 512 dimensional vector for each sample, which is also passed through through two deep dense ReLu activated layers with 256 and 64 units. Both ELMo and USE embeddings retrieved from TensorFlow Hub. NBSVM system was modified according to BIBREF93 and trained with a ${10^{-3}}$ leaning rate for 5 epochs with Adam optimizer BIBREF100. FastText system was implemented by utilizing pre-trained embeddings BIBREF94 passed through a global max-pooling and a 64 unit fully connected layer. System was trained with Adam optimizer with learning rate ${0.1}$ for 3 epochs. XLnet model implemented using the base-cased model with 12 layers, 768 hidden units and 12 attention heads. Model trained with learning rate ${4 \\times 10^{-5}}$ using ${10^{-5}}$ weight decay for 3 epochs. We exploited both cased and uncased BERT-base models containing 12 layers, 768 hidden units and 12 attention heads. We trained models for 3 epochs with learning rate ${2 \\times 10^{-5}}$ using ${10^{-5}}$ weight decay. We trained RoBERTa model following the setting of BERT model. RoBERTa, XLnet and BERT models implemented using pytorch-transformers library .   \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Pre-trained word embeddings proved to increase classification performances in many NLP tasks. In particular, Global Vectors (GloVe) BIBREF74 and Word2Vec BIBREF75 became popular in various tasks due to their ability to capture representative semantic representations of words, trained on large amount of data. However, in various studies (e.g., BIBREF76, BIBREF64, BIBREF77) it is argued that the actual meaning of words along with their semantics representations varies according to their context. Following this assumption, researchers in BIBREF64 present an approach that is based on the creation of pre-trained word embeddings through building a bidirectional Language model, i.e. predicting next word within a sequence. The ELMo model was exhaustingly trained on 30 million sentences corpus BIBREF78, with a two layered bidirectional LSTM architecture, aiming to predict both next and previous words, introducing the concept of contextual embeddings. The final embeddings vector is produced by a task specific weighted sum of the two directional hidden layers of LSTM models. Another contextual approach for creating embedding vector representations is proposed in BIBREF79 where, complete sentences, instead of words, are mapped to a latent vector space. The approach provides two variations of Universal Sentence Encoder (USE) with some trade-offs in computation and accuracy. The first approach consists of a computationally intensive transformer that resembles a transformer network BIBREF80, proved to achieve higher performance figures. In contrast, the second approach provides a light-weight model that averages input embedding weights for words and bi-grams by utilizing of a Deep Average Network (DAN) BIBREF81. The output of the DAN is passed through a feedforward neural network in order to produce the sentence embeddings. Both approaches take as input lowercased PTB tokenized strings, and output a 512-dimensional sentence embedding vectors.   \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Our models are trained and evaluated on the WikiLarge dataset BIBREF10 which contains 296,402/2,000/359 samples (train/validation/test). WikiLarge is a set of automatically aligned complex-simple sentence pairs from English Wikipedia (EW) and Simple English Wikipedia (SEW). It is compiled from previous extractions of EW-SEW BIBREF11, BIBREF28, BIBREF29. Its validation and test sets are taken from Turkcorpus BIBREF9, where each complex sentence has 8 human simplifications created by Amazon Mechanical Turk workers. Human annotators were instructed to only paraphrase the source sentences while keeping as much meaning as possible. Hence, no sentence splitting, minimal structural simplification and little content reduction occurs in this test set BIBREF9.   \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ACCESS scores best on SARI (41.87), a significant improvement over previous state of the art (40.45), and third to best FKGL (7.22). The second and third models in terms of SARI, DMASS+DCSS (40.45) and SBMT+PPDB+SARI (39.96), both use the external resource Simple PPDB BIBREF36 that was extracted from 1000 times more data than what we used for training. Our FKGL is also better (lower) than these methods. The Hybrid model scores best on FKGL (4.56) i.e. they generated the simplest (and shortest) sentences, but it was done at the expense of SARI (31.40).   \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Lately, SS has mostly been tackled using Seq2Seq MT models BIBREF14. Seq2Seq models were either used as-is BIBREF15 or combined with reinforcement learning thanks to a specific simplification reward BIBREF10, augmented with an external simplification database as a dynamic memory BIBREF16 or trained with multi-tasking on entailment and paraphrase generation BIBREF17.   \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Table TABREF24 compares our best model to state-of-the-art methods:   \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Conditional training with Seq2Seq models was applied to multiple natural language processing tasks such as summarization BIBREF5, BIBREF6, dialog BIBREF18, sentence compression BIBREF19, BIBREF20 or poetry generation BIBREF21.   \n",
       "\n",
       "                                                                                                                                                                                   fewshot_pred_answer  \\\n",
       "quids                                                                                                                                                                                                    \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                BiLSTM-CNN proposed by BIBREF1   \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                BiLSTM-CNN proposed by BIBREF1   \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                BiLSTM-CNN proposed by BIBREF1   \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                BiLSTM-CNN proposed by BIBREF1   \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                BiLSTM-CNN proposed by BIBREF1   \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                               MLE model and Baseline$+(t)$ with $t \\in [0,1]$   \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                               MLE model and Baseline$+(t)$ with $t \\in [0,1]$   \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                               MLE model and Baseline$+(t)$ with $t \\in [0,1]$   \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                               MLE model and Baseline$+(t)$ with $t \\in [0,1]$   \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                               MLE model and Baseline$+(t)$ with $t \\in [0,1]$   \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a  ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model   \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a  ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model   \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a  ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model   \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a  ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model   \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a  ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model   \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                                                             Seq2Seq MT models   \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                                                             Seq2Seq MT models   \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                                                             Seq2Seq MT models   \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                                                             Seq2Seq MT models   \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                                                             Seq2Seq MT models   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                       gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                                                \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                                                                                                          [BiLSTM-CNN, BiLSTM-CNN proposed by BIBREF1, Baseline-BiLSTM-CNN]  \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                                                                                                          [BiLSTM-CNN, BiLSTM-CNN proposed by BIBREF1, Baseline-BiLSTM-CNN]  \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                                                                                                          [BiLSTM-CNN, BiLSTM-CNN proposed by BIBREF1, Baseline-BiLSTM-CNN]  \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                                                                                                          [BiLSTM-CNN, BiLSTM-CNN proposed by BIBREF1, Baseline-BiLSTM-CNN]  \n",
       "77bbe1698e001c5889217be3164982ea36e85752                                                                                                                                                                                                                          [BiLSTM-CNN, BiLSTM-CNN proposed by BIBREF1, Baseline-BiLSTM-CNN]  \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                                                                                                                                                                    [ MLE model, Baseline$+(t)$, MLE model]  \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                                                                                                                                                                    [ MLE model, Baseline$+(t)$, MLE model]  \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                                                                                                                                                                    [ MLE model, Baseline$+(t)$, MLE model]  \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                                                                                                                                                                    [ MLE model, Baseline$+(t)$, MLE model]  \n",
       "fe6181ab0aecf5bc8c3def843f82e530347d918b                                                                                                                                                                                                                                                    [ MLE model, Baseline$+(t)$, MLE model]  \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a  [ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased), BERT base uncased (BERT-Uncased), RoBERTa base model, ELMo, USE , NBSVM , FastText , XLnet base cased model (XLnet, BERT base cased (BERT-Cased) , BERT base uncased (BERT-Uncased), RoBERTa ]  \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a  [ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased), BERT base uncased (BERT-Uncased), RoBERTa base model, ELMo, USE , NBSVM , FastText , XLnet base cased model (XLnet, BERT base cased (BERT-Cased) , BERT base uncased (BERT-Uncased), RoBERTa ]  \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a  [ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased), BERT base uncased (BERT-Uncased), RoBERTa base model, ELMo, USE , NBSVM , FastText , XLnet base cased model (XLnet, BERT base cased (BERT-Cased) , BERT base uncased (BERT-Uncased), RoBERTa ]  \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a  [ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased), BERT base uncased (BERT-Uncased), RoBERTa base model, ELMo, USE , NBSVM , FastText , XLnet base cased model (XLnet, BERT base cased (BERT-Cased) , BERT base uncased (BERT-Uncased), RoBERTa ]  \n",
       "8e113fd9661bc8af97e30c75a20712f01fc4520a  [ELMo, USE, NBSVM, FastText, XLnet base cased model (XLnet), BERT base cased (BERT-Cased), BERT base uncased (BERT-Uncased), RoBERTa base model, ELMo, USE , NBSVM , FastText , XLnet base cased model (XLnet, BERT base cased (BERT-Cased) , BERT base uncased (BERT-Uncased), RoBERTa ]  \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                             [PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS, BIBREF12, BIBREF33, BIBREF9, BIBREF10, BIBREF17, BIBREF15, BIBREF35, BIBREF16]  \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                             [PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS, BIBREF12, BIBREF33, BIBREF9, BIBREF10, BIBREF17, BIBREF15, BIBREF35, BIBREF16]  \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                             [PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS, BIBREF12, BIBREF33, BIBREF9, BIBREF10, BIBREF17, BIBREF15, BIBREF35, BIBREF16]  \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                             [PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS, BIBREF12, BIBREF33, BIBREF9, BIBREF10, BIBREF17, BIBREF15, BIBREF35, BIBREF16]  \n",
       "23cbf6ab365c1eb760b565d8ba51fb3f06257d62                                                                                                             [PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS, BIBREF12, BIBREF33, BIBREF9, BIBREF10, BIBREF17, BIBREF15, BIBREF35, BIBREF16]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  How long is the dataset?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fewshot_evidence</th>\n",
       "      <th>fewshot_pred_answer</th>\n",
       "      <th>gold_answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quids</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e801b6a6048175d3b1f3440852386adb220bcb36</th>\n",
       "      <td>We used two pre-trained embedding models in En-SiTAKA. The first one is word2vec which is provided by Google. It is trained on part of the Google News dataset (about 100 billion words) and it contains 300-dimensional vectors for 3M words and phrases BIBREF11 . The second one is SSWEu, which has been trained to capture the sentiment information of sentences as well as the syntactic contexts of words BIBREF12 . The SSWEu model contains 50-dimensional vectors for 100K words.</td>\n",
       "      <td>12,284 English-language tweets and 6100 Arabic-language tweets</td>\n",
       "      <td>[Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e801b6a6048175d3b1f3440852386adb220bcb36</th>\n",
       "      <td>We used two set of clusters in En-SiTAKA to represent the English-language tweets by mapping each tweet to a set of clusters. The first one is the well known set of clusters provided by the Ark Tweet NLP tool which contains 1000 clusters produced with the Brown clustering algorithm from 56M English-language tweets. These 1000 clusters are used to represent each tweet by mapping each word in the tweet to its cluster. The second one is Word2vec cluster ngrams, which is provided by BIBREF21 . They used the word2vec tool to learn 40-dimensional word embeddings of 255,657 words from a Twitter dataset and the K-means algorithm to cluster them into 4960 clusters. We were not able to find publicly available semantic clusters to be used in Ar-SiTAKA.</td>\n",
       "      <td>12,284 English-language tweets and 6100 Arabic-language tweets</td>\n",
       "      <td>[Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e801b6a6048175d3b1f3440852386adb220bcb36</th>\n",
       "      <td>Up to now, support vector machines (SVM) BIBREF24 have been used widely and reported as the best classifier in the sentiment analysis problem. Thus, we trained a SVM classifier on the training sets provided by the organizers. For the English-language we combined the training sets of SemEval13-16 and testing sets of SemEval13-15, and used them as a training set. Table TABREF20 shows the numerical description of the datasets used in this work. We used the linear kernel with the value 0.5 for the cost parameter C. All the parameters and the set of features have been experimentally chosen based on the development sets.</td>\n",
       "      <td>12,284 English-language tweets and 6100 Arabic-language tweets</td>\n",
       "      <td>[Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e801b6a6048175d3b1f3440852386adb220bcb36</th>\n",
       "      <td>In Ar-SiTAKA we used the model Arabic-SKIP-G300 provided by BIBREF13 . Arabic-SKIP-G300 has been trained on a large corpus of Arabic text collected from different sources such as Arabic Wikipedia, Arabic Gigaword Corpus, Ksucorpus, King Saud University Corpus, Microsoft crawled Arabic Corpus, etc. It contains 300-dimensional vectors for 6M words and phrases.</td>\n",
       "      <td>12,284 English-language tweets and 6100 Arabic-language tweets</td>\n",
       "      <td>[Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e801b6a6048175d3b1f3440852386adb220bcb36</th>\n",
       "      <td>The system has been tested on 12,284 English-language tweets and 6100 Arabic-language tweets provided by the organizers. The golden answers of all the test tweets were omitted by the organizers. The official evaluation results of our system are reported along with the top 10 systems and the baseline results in Table 2 and 3. Our system ranks 8th among 38 systems in the English-language tweets and ranks 2nd among 8 systems in the Arabic language tweets. The baselines 1, 2 and 3 stand for case when the system classify all the tweets as positive, negative and neutral respectively.</td>\n",
       "      <td>12,284 English-language tweets and 6100 Arabic-language tweets</td>\n",
       "      <td>[Unanswerable, Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0a736e0e3305a50d771dfc059c7d94b8bd27032e</th>\n",
       "      <td>The purpose of this research is to classify each review into one of the above 8 categories. In order to build reasonable classifiers, first we need to obtain a labeled dataset. Each of the TV series reviews was labeled by at least two individuals, and only those reviews with the same assigned label were selected in our training and testing data. This approach ensures that reviews with human biases are filtered out. As a result, we have 5000 for each TV series that matches the selection criteria.</td>\n",
       "      <td>5000 for each TV series</td>\n",
       "      <td>[Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series., Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0a736e0e3305a50d771dfc059c7d94b8bd27032e</th>\n",
       "      <td>What we are interested in are the reviews of the hottest or currently broadcasted TV series, so we select one of the most influential movie and TV series sharing websites in China, Douban. For every movie or TV series, you can find a corresponding section in it. For the sake of popularity, we choose “The Journey of Flower”, “Nirvana in Fire” and “Good Time” as parts of our movie review dataset, which are the hottest TV series from summer to fall 2015. Reviews of each episode have been collected for the sake of dataset comprehensiveness.</td>\n",
       "      <td>5000 for each TV series</td>\n",
       "      <td>[Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series., Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0a736e0e3305a50d771dfc059c7d94b8bd27032e</th>\n",
       "      <td>After the labelled cleaned data has been generated, we are now ready to process the dataset. One problem is that the vocabulary size of our corpus will be quite large. This could result in overfitting with the training data. As the dimension of the feature goes up, the complexity of our model will also increase. Then there will be quite an amount of difference between what we expect to learn and what we will learn from a particular dataset. One common way of dealing with the issue is to do feature selection. Here we applied DRC and INLINEFORM0 mentioned in related work. First let's define a contingency table for each word INLINEFORM1 like in Table TABREF13 . If INLINEFORM2 , it means the appearance of word INLINEFORM3 .</td>\n",
       "      <td>5000 for each TV series</td>\n",
       "      <td>[Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series., Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0a736e0e3305a50d771dfc059c7d94b8bd27032e</th>\n",
       "      <td>Dataset is another important factor influencing the performance of our classifiers. Most of the public available movie review data is in English, like the IMDB dataset collected by Pang/Lee 2004 BIBREF10 . Although it covers all kinds of movies in IMDB website, it only has labels related with the sentiment. Its initial goal was for sentiment analysis. Another intact movie review dataset is SNAP BIBREF11 , which consists of reviews from Amazon but only bearing rating scores. However, what we need is the content or aspect tags that are being discussed in each review. In addition, our review text is in Chinese. Therefore, it is necessary for us to build the review dataset by ourselves and label them into generic categories, which is one of as one of the contributions of this paper.</td>\n",
       "      <td>5000 for each TV series</td>\n",
       "      <td>[Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series., Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0a736e0e3305a50d771dfc059c7d94b8bd27032e</th>\n",
       "      <td>We can see that most of the reviews are focused on discussing the roles and analyzing the plots in the movie, i.e., 6th and 7th topics in Figure FIGREF30 , while quite a few are just following the posts, like the 4th and 5th topic in the figure. Based on the findings, we generate the category definition shown in Table TABREF11 . Then 5000 out of each TV series reviews, with no label bias between readers, are selected to make up our final data set.</td>\n",
       "      <td>5000 for each TV series</td>\n",
       "      <td>[Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series., Unanswerable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576a3ed6e4faa4c3893db632e97a52ac6e864aac</th>\n",
       "      <td>To address this shortcoming, we present the Treebank of Learner English (TLE), a first of its kind resource for non-native English, containing 5,124 sentences manually annotated with POS tags and dependency trees. The TLE sentences are drawn from the FCE dataset BIBREF1 , and authored by English learners from 10 different native language backgrounds. The treebank uses the Universal Dependencies (UD) formalism BIBREF2 , BIBREF3 , which provides a unified annotation framework across different languages and is geared towards multilingual NLP BIBREF4 . This characteristic allows our treebank to support computational analysis of ESL using not only English based but also multilingual approaches which seek to relate ESL phenomena to native language syntax.</td>\n",
       "      <td>5,124 sentences</td>\n",
       "      <td>[5124,  5,124 sentences (97,681 tokens)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576a3ed6e4faa4c3893db632e97a52ac6e864aac</th>\n",
       "      <td>After applying the resolutions produced by the judges, we queried the corpus with debugging tests for specific linguistics constructions. This additional testing phase further reduced the number of annotation errors and inconsistencies in the treebank. Including the training period, the treebank creation lasted over a year, with an aggregate of more than 2,000 annotation hours.</td>\n",
       "      <td>5,124 sentences</td>\n",
       "      <td>[5124,  5,124 sentences (97,681 tokens)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576a3ed6e4faa4c3893db632e97a52ac6e864aac</th>\n",
       "      <td>Syntactic annotations for ESL were previously developed by Nagata et al. nagata2011, who annotate an English learner corpus with POS tags and shallow syntactic parses. Our work departs from shallow syntax to full syntactic analysis, and provides annotations on a significantly larger scale. Furthermore, differently from this annotation effort, our treebank covers a wide range of learner native languages. An additional syntactic dataset for ESL, currently not available publicly, are 1,000 sentences from the EFCamDat dataset BIBREF8 , annotated with Stanford dependencies BIBREF19 . This dataset was used to measure the impact of grammatical errors on parsing by comparing performance on sentences with grammatical errors to error free sentences. The TLE enables a more direct way of estimating the magnitude of this performance gap by comparing performance on the same sentences in their original and error corrected versions. Our comparison suggests that the effect of grammatical errors on parsing is smaller that the one reported in this study.</td>\n",
       "      <td>5,124 sentences</td>\n",
       "      <td>[5124,  5,124 sentences (97,681 tokens)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576a3ed6e4faa4c3893db632e97a52ac6e864aac</th>\n",
       "      <td>Table TABREF16 presents tagging and parsing results on a test set of 500 TLE sentences (9,591 original tokens, 9,700 corrected tokens). Results are provided for three different training regimes. The first regime uses the training portion of version 1.3 of the EWT, the UD English treebank, containing 12,543 sentences (204,586 tokens). The second training mode uses 4,124 training sentences (78,541 original tokens, 79,581 corrected tokens) from the TLE corpus. In the third setup we combine these two training corpora. The remaining 500 TLE sentences (9,549 original tokens, 9,695 corrected tokens) are allocated to a development set, not used in this experiment. Parsing of the test sentences was performed on predicted POS tags.</td>\n",
       "      <td>5,124 sentences</td>\n",
       "      <td>[5124,  5,124 sentences (97,681 tokens)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576a3ed6e4faa4c3893db632e97a52ac6e864aac</th>\n",
       "      <td>20 the DET DT 21 det</td>\n",
       "      <td>5,124 sentences</td>\n",
       "      <td>[5124,  5,124 sentences (97,681 tokens)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16b816925567deb734049416c149747118e13963</th>\n",
       "      <td>Datasets. In order for the results to be consistent with previous works, we experimented with the benchmark datasets from SemEval 2014 task 4 BIBREF30 and SemEval 2016 task 5 BIBREF34 competitions. The laptop dataset is taken from SemEval 2014 and is used for both AE and ASC tasks. However, the restaurant dataset for AE is a SemEval 2014 dataset while for ASC is a SemEval 2016 dataset. The reason for the difference is to be consistent with the previous works. A summary of these datasets can be seen in Tables TABREF8 and TABREF8.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences, Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16b816925567deb734049416c149747118e13963</th>\n",
       "      <td>To perform the ablation study, first we initialize our model with post-trained BERT which has been trained on uncased version of $\\mathbf {BERT_{BASE}}$. We attempt to discover what number of training epochs and which dropout probability yield the best performance for BERT-PT. Since one and two training epochs result in very low scores, results of 3 to 10 training epochs have been depicted for all experiments. For AE, we experiment with 10 different dropout values in the fully connected (linear) layer. The results can be seen in Figure FIGREF6 for laptop and restaurant datasets. To be consistent with the previous work and because of the results having high variance, each point in the figure (F1 score) is the average of 9 runs. In the end, for each number of training epochs, a dropout value, which outperforms the other values, is found. In our experiments, we noticed that the validation loss increases after 2 epochs as has been mentioned in the original paper. However, the test results do not follow the same pattern. Looking at the figures, it can be seen that as the number of training epochs increases, better results are produced in the restaurant domain while in the laptop domain the scores go down. This can be attributed to the selection of validation sets as for both domains the last 150 examples of the SemEval training set were selected. Therefore, it can be said that the examples in the validation and test sets for laptop have more similar patterns than those of restaurant dataset. To be consistent with BERT-PT, we performed the same selection.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences, Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16b816925567deb734049416c149747118e13963</th>\n",
       "      <td>From the ablation studies, we extract the best results of BERT-PT and compare them with those of BAT. These are summarized in Tables TABREF11 and TABREF11 for aspect extraction and aspect sentiment classification, respectively. As can be seen in Table TABREF11, the best parameters for BERT-PT have greatly improved its original performance on restaurant dataset (+2.72) compared to laptop (+0.62). Similar improvements can be seen in ASC results with an increase of +2.16 in MF1 score for restaurant compared to +0.81 for laptop which is due to the increase in the number of training epochs for restaurant domain since it exhibits better results with more training while the model reaches its peak performance for laptop domain in earlier training epochs. In addition, applying adversarial training improves the network's performance in both tasks, though at different rates. While for laptop there are similar improvements in both tasks (+0.69 in AE, +0.61 in ASC), for restaurant we observe different enhancements (+0.81 in AE, +0.12 in ASC). This could be attributed to the fact that these are two different datasets whereas the laptop dataset is the same for both tasks. Furthermore, the perturbation size plays an important role in performance of the system. By choosing the appropriate ones, as was shown, better results are achieved.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences, Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16b816925567deb734049416c149747118e13963</th>\n",
       "      <td>Implementation details. We performed all our experiments on a GPU (GeForce RTX 2070) with 8 GB of memory. Except for the code specific to our model, we adapted the codebase utilized by BERT-PT. To carry out the ablation study of BERT-PT model, batches of 32 were specified. However, to perform the experiments for our proposed model, we reduced the batch size to 16 in order for the GPU to be able to store our model. For optimization, the Adam optimizer with a learning rate of $3e-5$ was used. From SemEval's training data, 150 examples were chosen for the validation and the remaining was used for training the model.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences, Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16b816925567deb734049416c149747118e13963</th>\n",
       "      <td>BIBREF20 design a seven-layer CNN architecture and make use of both part of speech tagging and word embeddings as features. BIBREF21 use convolutional neural networks and domain-specific data for AE and ASC. They show that adding the word embeddings produced from the domain-specific data to the general purpose embeddings semantically enriches them regarding the task at hand. In a recent work BIBREF1, the authors also show that using in-domain data can enhance the performance of the state-of-the-art language model (BERT). Similarly, BIBREF22 also fine-tune BERT on domain-specific data for ASC. They perform a two-stage process, first of which is self-supervised in-domain fine-tuning, followed by supervised task-specific fine-tuning. Working on the same task, BIBREF23 apply graph convolutional networks taking into consideration the assumption that in sentences with multiple aspects, the sentiment about one aspect can help determine the sentiment of another aspect.</td>\n",
       "      <td>Unanswerable</td>\n",
       "      <td>[SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences, Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10edfb9428b8a4652274c13962917662fdf84f8a</th>\n",
       "      <td>Common Vulnerabilities and Exposures (CVE) is a list of publicly known cybersecurity vulnerabilities, each with an identification number. These entries are used in the National Vulnerability Database (NVD), the U.S. government repository of standards based vulnerability management data. The NVD suffers from poor coverage, as it contains only 10% of the open-source vulnerabilities that have received a CVE identifier BIBREF2. This could be due to the fact that a number of security vulnerabilities are discovered and fixed through informal communication between maintainers and their users in an issue tracker. To make things worse, these public databases are too slow to add vulnerabilities as they lag behind a private database such as Snyk's DB by an average of 92 days BIBREF0 All of the above pitfalls of public vulnerability management databases (such as NVD) call for a mechanism to automatically infer the presence of security threats in open-source projects, and their corresponding fixes, in a timely manner.</td>\n",
       "      <td>808, 265, and 264 commits</td>\n",
       "      <td>[almost doubles the number of commits in the training split to 1493, validation, and test splits containing 808, 265, and 264 commits, 2022]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10edfb9428b8a4652274c13962917662fdf84f8a</th>\n",
       "      <td>Deep learning models are known for scaling well with more data. However, with less than 1,000 ground-truth training samples and around 1,800 augmented training samples, we are unable to exploit the full potential of deep learning. A reflection on the current state of labelled datasets in software engineering (or the lack thereof) throws light on limited practicality of deep learning models for certain software engineering tasks BIBREF29. As stated by BIBREF30, just as research in NLP changed focus from brittle rule-based expert systems to statistical methods, software engineering research should augment traditional methods that consider only the formal structure of programs with information about the statistical properties of code. Ongoing research on pre-trained code embeddings that don't require a labelled dataset for training is a step in the right direction. Drawing parallels with the recent history of NLP research, we are hoping that further study in the domain of code embeddings will considerably accelerate progress in tackling software problems with deep learning.</td>\n",
       "      <td>808, 265, and 264 commits</td>\n",
       "      <td>[almost doubles the number of commits in the training split to 1493, validation, and test splits containing 808, 265, and 264 commits, 2022]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10edfb9428b8a4652274c13962917662fdf84f8a</th>\n",
       "      <td>For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively. In order to minimize the occurrence of duplicate commits in two of these splits (such as in both training and test), commits from no repository belong to more than one split. However, 808 commits may not be sufficient to train deep learning models. Hence, in order to answer RQ4, we augment the training split with commits mined using regular expression matching on the commit messages from the same set of open-source Java projects. This almost doubles the number of commits in the training split to 1493. We then repeat our experiments for the first three research questions on the augmented dataset, and evaluate our trained models on the same validation and test splits.</td>\n",
       "      <td>808, 265, and 264 commits</td>\n",
       "      <td>[almost doubles the number of commits in the training split to 1493, validation, and test splits containing 808, 265, and 264 commits, 2022]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10edfb9428b8a4652274c13962917662fdf84f8a</th>\n",
       "      <td>This section details the methodology used in this study to build the training dataset, the models used for classification and the evaluation procedure. All of the experiments are conducted on Python 3.7 running on an Intel Core i7 6800K CPU and a Nvidia GTX 1080 GPU. All the deep learning models are implemented in PyTorch 0.4.1 BIBREF21, while Scikit-learn 0.19.2 BIBREF22 is used for computing the tf–idf vectors and performing logistic regression.</td>\n",
       "      <td>808, 265, and 264 commits</td>\n",
       "      <td>[almost doubles the number of commits in the training split to 1493, validation, and test splits containing 808, 265, and 264 commits, 2022]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10edfb9428b8a4652274c13962917662fdf84f8a</th>\n",
       "      <td>The removal of tokens whose length is greater than or equal to 64 characters.</td>\n",
       "      <td>808, 265, and 264 commits</td>\n",
       "      <td>[almost doubles the number of commits in the training split to 1493, validation, and test splits containing 808, 265, and 264 commits, 2022]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ead7704a64447dccd504951618d3be463eba86bf</th>\n",
       "      <td>The data set for the coding of death certificates is called the CépiDC corpus. Three CSV files (AlignedCauses) were provided by task organizers containing annotated death certificates for different periods : 2006 to 2012, 2013 and 2014. This training set contained 125383 death certificates. Each certificate contains one or more lines of text (medical causes that led to death) and some metadata. Each CSV file contains a \"Raw Text\" column entered by a physician, a \"Standard Text\" column entered by a human coder that supports the selection of an ICD-10 code in the last column. Table TABREF2 presents an excerpt of these files. Zero to multiples ICD-10 codes can be assigned to each line of a death certificate.</td>\n",
       "      <td>125383 death certificates</td>\n",
       "      <td>[125383, 125383 death certificates]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ead7704a64447dccd504951618d3be463eba86bf</th>\n",
       "      <td>The present study is part of the Drugs Systematized Assessment in real-liFe Environment (DRUGS-SAFE) research platform that is funded by the French Medicines Agency (Agence Nationale de Sécurité du Médicament et des Produits de Santé, ANSM). This platform aims at providing an integrated system allowing the concomitant monitoring of drug use and safety in France. The funder had no role in the design and conduct of the studies ; collection, management, analysis, and interpretation of the data ; preparation, review, or approval of the manuscript ; and the decision to submit the manuscript for publication. This publication represents the views of the authors and does not necessarily represent the opinion of the French Medicines Agency.</td>\n",
       "      <td>125383 death certificates</td>\n",
       "      <td>[125383, 125383 death certificates]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ead7704a64447dccd504951618d3be463eba86bf</th>\n",
       "      <td>The first dictionary contained 42439 terms and 3,539 ICD-10 codes (run2) and the second one 148448 terms and 6,392 ICD-10 codes (run1).</td>\n",
       "      <td>125383 death certificates</td>\n",
       "      <td>[125383, 125383 death certificates]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ead7704a64447dccd504951618d3be463eba86bf</th>\n",
       "      <td>We submitted two runs on the CépiDC test set, one used all the terms entered by human coders in the training set only (run 2), the other (run 1) added the 2015 ICD-10 dictionary provided by the task organizers to the set the terms of run 1. We obtained our best precision (0.794) and recall (0.779) with run 2.</td>\n",
       "      <td>125383 death certificates</td>\n",
       "      <td>[125383, 125383 death certificates]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ead7704a64447dccd504951618d3be463eba86bf</th>\n",
       "      <td>english</td>\n",
       "      <td>125383 death certificates</td>\n",
       "      <td>[125383, 125383 death certificates]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 fewshot_evidence  \\\n",
       "quids                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             We used two pre-trained embedding models in En-SiTAKA. The first one is word2vec which is provided by Google. It is trained on part of the Google News dataset (about 100 billion words) and it contains 300-dimensional vectors for 3M words and phrases BIBREF11 . The second one is SSWEu, which has been trained to capture the sentiment information of sentences as well as the syntactic contexts of words BIBREF12 . The SSWEu model contains 50-dimensional vectors for 100K words.   \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          We used two set of clusters in En-SiTAKA to represent the English-language tweets by mapping each tweet to a set of clusters. The first one is the well known set of clusters provided by the Ark Tweet NLP tool which contains 1000 clusters produced with the Brown clustering algorithm from 56M English-language tweets. These 1000 clusters are used to represent each tweet by mapping each word in the tweet to its cluster. The second one is Word2vec cluster ngrams, which is provided by BIBREF21 . They used the word2vec tool to learn 40-dimensional word embeddings of 255,657 words from a Twitter dataset and the K-means algorithm to cluster them into 4960 clusters. We were not able to find publicly available semantic clusters to be used in Ar-SiTAKA.   \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Up to now, support vector machines (SVM) BIBREF24 have been used widely and reported as the best classifier in the sentiment analysis problem. Thus, we trained a SVM classifier on the training sets provided by the organizers. For the English-language we combined the training sets of SemEval13-16 and testing sets of SemEval13-15, and used them as a training set. Table TABREF20 shows the numerical description of the datasets used in this work. We used the linear kernel with the value 0.5 for the cost parameter C. All the parameters and the set of features have been experimentally chosen based on the development sets.   \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 In Ar-SiTAKA we used the model Arabic-SKIP-G300 provided by BIBREF13 . Arabic-SKIP-G300 has been trained on a large corpus of Arabic text collected from different sources such as Arabic Wikipedia, Arabic Gigaword Corpus, Ksucorpus, King Saud University Corpus, Microsoft crawled Arabic Corpus, etc. It contains 300-dimensional vectors for 6M words and phrases.   \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The system has been tested on 12,284 English-language tweets and 6100 Arabic-language tweets provided by the organizers. The golden answers of all the test tweets were omitted by the organizers. The official evaluation results of our system are reported along with the top 10 systems and the baseline results in Table 2 and 3. Our system ranks 8th among 38 systems in the English-language tweets and ranks 2nd among 8 systems in the Arabic language tweets. The baselines 1, 2 and 3 stand for case when the system classify all the tweets as positive, negative and neutral respectively.   \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The purpose of this research is to classify each review into one of the above 8 categories. In order to build reasonable classifiers, first we need to obtain a labeled dataset. Each of the TV series reviews was labeled by at least two individuals, and only those reviews with the same assigned label were selected in our training and testing data. This approach ensures that reviews with human biases are filtered out. As a result, we have 5000 for each TV series that matches the selection criteria.   \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           What we are interested in are the reviews of the hottest or currently broadcasted TV series, so we select one of the most influential movie and TV series sharing websites in China, Douban. For every movie or TV series, you can find a corresponding section in it. For the sake of popularity, we choose “The Journey of Flower”, “Nirvana in Fire” and “Good Time” as parts of our movie review dataset, which are the hottest TV series from summer to fall 2015. Reviews of each episode have been collected for the sake of dataset comprehensiveness.   \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                After the labelled cleaned data has been generated, we are now ready to process the dataset. One problem is that the vocabulary size of our corpus will be quite large. This could result in overfitting with the training data. As the dimension of the feature goes up, the complexity of our model will also increase. Then there will be quite an amount of difference between what we expect to learn and what we will learn from a particular dataset. One common way of dealing with the issue is to do feature selection. Here we applied DRC and INLINEFORM0 mentioned in related work. First let's define a contingency table for each word INLINEFORM1 like in Table TABREF13 . If INLINEFORM2 , it means the appearance of word INLINEFORM3 .   \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Dataset is another important factor influencing the performance of our classifiers. Most of the public available movie review data is in English, like the IMDB dataset collected by Pang/Lee 2004 BIBREF10 . Although it covers all kinds of movies in IMDB website, it only has labels related with the sentiment. Its initial goal was for sentiment analysis. Another intact movie review dataset is SNAP BIBREF11 , which consists of reviews from Amazon but only bearing rating scores. However, what we need is the content or aspect tags that are being discussed in each review. In addition, our review text is in Chinese. Therefore, it is necessary for us to build the review dataset by ourselves and label them into generic categories, which is one of as one of the contributions of this paper.   \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We can see that most of the reviews are focused on discussing the roles and analyzing the plots in the movie, i.e., 6th and 7th topics in Figure FIGREF30 , while quite a few are just following the posts, like the 4th and 5th topic in the figure. Based on the findings, we generate the category definition shown in Table TABREF11 . Then 5000 out of each TV series reviews, with no label bias between readers, are selected to make up our final data set.   \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  To address this shortcoming, we present the Treebank of Learner English (TLE), a first of its kind resource for non-native English, containing 5,124 sentences manually annotated with POS tags and dependency trees. The TLE sentences are drawn from the FCE dataset BIBREF1 , and authored by English learners from 10 different native language backgrounds. The treebank uses the Universal Dependencies (UD) formalism BIBREF2 , BIBREF3 , which provides a unified annotation framework across different languages and is geared towards multilingual NLP BIBREF4 . This characteristic allows our treebank to support computational analysis of ESL using not only English based but also multilingual approaches which seek to relate ESL phenomena to native language syntax.   \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             After applying the resolutions produced by the judges, we queried the corpus with debugging tests for specific linguistics constructions. This additional testing phase further reduced the number of annotation errors and inconsistencies in the treebank. Including the training period, the treebank creation lasted over a year, with an aggregate of more than 2,000 annotation hours.   \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Syntactic annotations for ESL were previously developed by Nagata et al. nagata2011, who annotate an English learner corpus with POS tags and shallow syntactic parses. Our work departs from shallow syntax to full syntactic analysis, and provides annotations on a significantly larger scale. Furthermore, differently from this annotation effort, our treebank covers a wide range of learner native languages. An additional syntactic dataset for ESL, currently not available publicly, are 1,000 sentences from the EFCamDat dataset BIBREF8 , annotated with Stanford dependencies BIBREF19 . This dataset was used to measure the impact of grammatical errors on parsing by comparing performance on sentences with grammatical errors to error free sentences. The TLE enables a more direct way of estimating the magnitude of this performance gap by comparing performance on the same sentences in their original and error corrected versions. Our comparison suggests that the effect of grammatical errors on parsing is smaller that the one reported in this study.   \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Table TABREF16 presents tagging and parsing results on a test set of 500 TLE sentences (9,591 original tokens, 9,700 corrected tokens). Results are provided for three different training regimes. The first regime uses the training portion of version 1.3 of the EWT, the UD English treebank, containing 12,543 sentences (204,586 tokens). The second training mode uses 4,124 training sentences (78,541 original tokens, 79,581 corrected tokens) from the TLE corpus. In the third setup we combine these two training corpora. The remaining 500 TLE sentences (9,549 original tokens, 9,695 corrected tokens) are allocated to a development set, not used in this experiment. Parsing of the test sentences was performed on predicted POS tags.   \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     20 the DET DT 21 det   \n",
       "16b816925567deb734049416c149747118e13963                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Datasets. In order for the results to be consistent with previous works, we experimented with the benchmark datasets from SemEval 2014 task 4 BIBREF30 and SemEval 2016 task 5 BIBREF34 competitions. The laptop dataset is taken from SemEval 2014 and is used for both AE and ASC tasks. However, the restaurant dataset for AE is a SemEval 2014 dataset while for ASC is a SemEval 2016 dataset. The reason for the difference is to be consistent with the previous works. A summary of these datasets can be seen in Tables TABREF8 and TABREF8.   \n",
       "16b816925567deb734049416c149747118e13963  To perform the ablation study, first we initialize our model with post-trained BERT which has been trained on uncased version of $\\mathbf {BERT_{BASE}}$. We attempt to discover what number of training epochs and which dropout probability yield the best performance for BERT-PT. Since one and two training epochs result in very low scores, results of 3 to 10 training epochs have been depicted for all experiments. For AE, we experiment with 10 different dropout values in the fully connected (linear) layer. The results can be seen in Figure FIGREF6 for laptop and restaurant datasets. To be consistent with the previous work and because of the results having high variance, each point in the figure (F1 score) is the average of 9 runs. In the end, for each number of training epochs, a dropout value, which outperforms the other values, is found. In our experiments, we noticed that the validation loss increases after 2 epochs as has been mentioned in the original paper. However, the test results do not follow the same pattern. Looking at the figures, it can be seen that as the number of training epochs increases, better results are produced in the restaurant domain while in the laptop domain the scores go down. This can be attributed to the selection of validation sets as for both domains the last 150 examples of the SemEval training set were selected. Therefore, it can be said that the examples in the validation and test sets for laptop have more similar patterns than those of restaurant dataset. To be consistent with BERT-PT, we performed the same selection.   \n",
       "16b816925567deb734049416c149747118e13963                                                                                                                                                                                                                                            From the ablation studies, we extract the best results of BERT-PT and compare them with those of BAT. These are summarized in Tables TABREF11 and TABREF11 for aspect extraction and aspect sentiment classification, respectively. As can be seen in Table TABREF11, the best parameters for BERT-PT have greatly improved its original performance on restaurant dataset (+2.72) compared to laptop (+0.62). Similar improvements can be seen in ASC results with an increase of +2.16 in MF1 score for restaurant compared to +0.81 for laptop which is due to the increase in the number of training epochs for restaurant domain since it exhibits better results with more training while the model reaches its peak performance for laptop domain in earlier training epochs. In addition, applying adversarial training improves the network's performance in both tasks, though at different rates. While for laptop there are similar improvements in both tasks (+0.69 in AE, +0.61 in ASC), for restaurant we observe different enhancements (+0.81 in AE, +0.12 in ASC). This could be attributed to the fact that these are two different datasets whereas the laptop dataset is the same for both tasks. Furthermore, the perturbation size plays an important role in performance of the system. By choosing the appropriate ones, as was shown, better results are achieved.   \n",
       "16b816925567deb734049416c149747118e13963                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Implementation details. We performed all our experiments on a GPU (GeForce RTX 2070) with 8 GB of memory. Except for the code specific to our model, we adapted the codebase utilized by BERT-PT. To carry out the ablation study of BERT-PT model, batches of 32 were specified. However, to perform the experiments for our proposed model, we reduced the batch size to 16 in order for the GPU to be able to store our model. For optimization, the Adam optimizer with a learning rate of $3e-5$ was used. From SemEval's training data, 150 examples were chosen for the validation and the remaining was used for training the model.   \n",
       "16b816925567deb734049416c149747118e13963                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          BIBREF20 design a seven-layer CNN architecture and make use of both part of speech tagging and word embeddings as features. BIBREF21 use convolutional neural networks and domain-specific data for AE and ASC. They show that adding the word embeddings produced from the domain-specific data to the general purpose embeddings semantically enriches them regarding the task at hand. In a recent work BIBREF1, the authors also show that using in-domain data can enhance the performance of the state-of-the-art language model (BERT). Similarly, BIBREF22 also fine-tune BERT on domain-specific data for ASC. They perform a two-stage process, first of which is self-supervised in-domain fine-tuning, followed by supervised task-specific fine-tuning. Working on the same task, BIBREF23 apply graph convolutional networks taking into consideration the assumption that in sentences with multiple aspects, the sentiment about one aspect can help determine the sentiment of another aspect.   \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Common Vulnerabilities and Exposures (CVE) is a list of publicly known cybersecurity vulnerabilities, each with an identification number. These entries are used in the National Vulnerability Database (NVD), the U.S. government repository of standards based vulnerability management data. The NVD suffers from poor coverage, as it contains only 10% of the open-source vulnerabilities that have received a CVE identifier BIBREF2. This could be due to the fact that a number of security vulnerabilities are discovered and fixed through informal communication between maintainers and their users in an issue tracker. To make things worse, these public databases are too slow to add vulnerabilities as they lag behind a private database such as Snyk's DB by an average of 92 days BIBREF0 All of the above pitfalls of public vulnerability management databases (such as NVD) call for a mechanism to automatically infer the presence of security threats in open-source projects, and their corresponding fixes, in a timely manner.   \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Deep learning models are known for scaling well with more data. However, with less than 1,000 ground-truth training samples and around 1,800 augmented training samples, we are unable to exploit the full potential of deep learning. A reflection on the current state of labelled datasets in software engineering (or the lack thereof) throws light on limited practicality of deep learning models for certain software engineering tasks BIBREF29. As stated by BIBREF30, just as research in NLP changed focus from brittle rule-based expert systems to statistical methods, software engineering research should augment traditional methods that consider only the formal structure of programs with information about the statistical properties of code. Ongoing research on pre-trained code embeddings that don't require a labelled dataset for training is a step in the right direction. Drawing parallels with the recent history of NLP research, we are hoping that further study in the domain of code embeddings will considerably accelerate progress in tackling software problems with deep learning.   \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively. In order to minimize the occurrence of duplicate commits in two of these splits (such as in both training and test), commits from no repository belong to more than one split. However, 808 commits may not be sufficient to train deep learning models. Hence, in order to answer RQ4, we augment the training split with commits mined using regular expression matching on the commit messages from the same set of open-source Java projects. This almost doubles the number of commits in the training split to 1493. We then repeat our experiments for the first three research questions on the augmented dataset, and evaluate our trained models on the same validation and test splits.   \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      This section details the methodology used in this study to build the training dataset, the models used for classification and the evaluation procedure. All of the experiments are conducted on Python 3.7 running on an Intel Core i7 6800K CPU and a Nvidia GTX 1080 GPU. All the deep learning models are implemented in PyTorch 0.4.1 BIBREF21, while Scikit-learn 0.19.2 BIBREF22 is used for computing the tf–idf vectors and performing logistic regression.   \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The removal of tokens whose length is greater than or equal to 64 characters.   \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The data set for the coding of death certificates is called the CépiDC corpus. Three CSV files (AlignedCauses) were provided by task organizers containing annotated death certificates for different periods : 2006 to 2012, 2013 and 2014. This training set contained 125383 death certificates. Each certificate contains one or more lines of text (medical causes that led to death) and some metadata. Each CSV file contains a \"Raw Text\" column entered by a physician, a \"Standard Text\" column entered by a human coder that supports the selection of an ICD-10 code in the last column. Table TABREF2 presents an excerpt of these files. Zero to multiples ICD-10 codes can be assigned to each line of a death certificate.   \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The present study is part of the Drugs Systematized Assessment in real-liFe Environment (DRUGS-SAFE) research platform that is funded by the French Medicines Agency (Agence Nationale de Sécurité du Médicament et des Produits de Santé, ANSM). This platform aims at providing an integrated system allowing the concomitant monitoring of drug use and safety in France. The funder had no role in the design and conduct of the studies ; collection, management, analysis, and interpretation of the data ; preparation, review, or approval of the manuscript ; and the decision to submit the manuscript for publication. This publication represents the views of the authors and does not necessarily represent the opinion of the French Medicines Agency.   \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The first dictionary contained 42439 terms and 3,539 ICD-10 codes (run2) and the second one 148448 terms and 6,392 ICD-10 codes (run1).   \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We submitted two runs on the CépiDC test set, one used all the terms entered by human coders in the training set only (run 2), the other (run 1) added the 2015 ICD-10 dictionary provided by the task organizers to the set the terms of run 1. We obtained our best precision (0.794) and recall (0.779) with run 2.   \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  english   \n",
       "\n",
       "                                                                                     fewshot_pred_answer  \\\n",
       "quids                                                                                                      \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36  12,284 English-language tweets and 6100 Arabic-language tweets   \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36  12,284 English-language tweets and 6100 Arabic-language tweets   \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36  12,284 English-language tweets and 6100 Arabic-language tweets   \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36  12,284 English-language tweets and 6100 Arabic-language tweets   \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36  12,284 English-language tweets and 6100 Arabic-language tweets   \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                         5000 for each TV series   \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                         5000 for each TV series   \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                         5000 for each TV series   \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                         5000 for each TV series   \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                         5000 for each TV series   \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                 5,124 sentences   \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                 5,124 sentences   \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                 5,124 sentences   \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                 5,124 sentences   \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                 5,124 sentences   \n",
       "16b816925567deb734049416c149747118e13963                                                    Unanswerable   \n",
       "16b816925567deb734049416c149747118e13963                                                    Unanswerable   \n",
       "16b816925567deb734049416c149747118e13963                                                    Unanswerable   \n",
       "16b816925567deb734049416c149747118e13963                                                    Unanswerable   \n",
       "16b816925567deb734049416c149747118e13963                                                    Unanswerable   \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                       808, 265, and 264 commits   \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                       808, 265, and 264 commits   \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                       808, 265, and 264 commits   \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                       808, 265, and 264 commits   \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                       808, 265, and 264 commits   \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                       125383 death certificates   \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                       125383 death certificates   \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                       125383 death certificates   \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                       125383 death certificates   \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                       125383 death certificates   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                     gold_answers  \n",
       "quids                                                                                                                                                                                                                                                                                                                              \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36                                                                                                                                                                                                                                                             [Unanswerable, Unanswerable]  \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36                                                                                                                                                                                                                                                             [Unanswerable, Unanswerable]  \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36                                                                                                                                                                                                                                                             [Unanswerable, Unanswerable]  \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36                                                                                                                                                                                                                                                             [Unanswerable, Unanswerable]  \n",
       "e801b6a6048175d3b1f3440852386adb220bcb36                                                                                                                                                                                                                                                             [Unanswerable, Unanswerable]  \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                                                                                                                                                                  [Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series., Unanswerable]  \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                                                                                                                                                                  [Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series., Unanswerable]  \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                                                                                                                                                                  [Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series., Unanswerable]  \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                                                                                                                                                                  [Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series., Unanswerable]  \n",
       "0a736e0e3305a50d771dfc059c7d94b8bd27032e                                                                                                                                                                                  [Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series., Unanswerable]  \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                                                                                                                                                                                                                 [5124,  5,124 sentences (97,681 tokens)]  \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                                                                                                                                                                                                                 [5124,  5,124 sentences (97,681 tokens)]  \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                                                                                                                                                                                                                 [5124,  5,124 sentences (97,681 tokens)]  \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                                                                                                                                                                                                                 [5124,  5,124 sentences (97,681 tokens)]  \n",
       "576a3ed6e4faa4c3893db632e97a52ac6e864aac                                                                                                                                                                                                                                                 [5124,  5,124 sentences (97,681 tokens)]  \n",
       "16b816925567deb734049416c149747118e13963  [SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences, Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively.]  \n",
       "16b816925567deb734049416c149747118e13963  [SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences, Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively.]  \n",
       "16b816925567deb734049416c149747118e13963  [SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences, Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively.]  \n",
       "16b816925567deb734049416c149747118e13963  [SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences, Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively.]  \n",
       "16b816925567deb734049416c149747118e13963  [SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences, Semeval 2014 for ASC has total of  2951 and 4722 sentiments for Laptop and Restaurnant respectively, while SemEval 2016 for AE has total of 3857 and 5041 sentences on Laptop and Resaurant respectively.]  \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                                                                                                                             [almost doubles the number of commits in the training split to 1493, validation, and test splits containing 808, 265, and 264 commits, 2022]  \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                                                                                                                             [almost doubles the number of commits in the training split to 1493, validation, and test splits containing 808, 265, and 264 commits, 2022]  \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                                                                                                                             [almost doubles the number of commits in the training split to 1493, validation, and test splits containing 808, 265, and 264 commits, 2022]  \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                                                                                                                             [almost doubles the number of commits in the training split to 1493, validation, and test splits containing 808, 265, and 264 commits, 2022]  \n",
       "10edfb9428b8a4652274c13962917662fdf84f8a                                                                                                                                             [almost doubles the number of commits in the training split to 1493, validation, and test splits containing 808, 265, and 264 commits, 2022]  \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                                                                                                                                                                                                                                      [125383, 125383 death certificates]  \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                                                                                                                                                                                                                                      [125383, 125383 death certificates]  \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                                                                                                                                                                                                                                      [125383, 125383 death certificates]  \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                                                                                                                                                                                                                                      [125383, 125383 death certificates]  \n",
       "ead7704a64447dccd504951618d3be463eba86bf                                                                                                                                                                                                                                                      [125383, 125383 death certificates]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def highlight_max(x):\n",
    "    print(x.iloc[0])\n",
    "    return ['font-weight: bold' if 'section' in x.iloc[0] else '' for v in x]\n",
    "\n",
    "for count, (question, q_ids) in enumerate(general_test_questions.items()):\n",
    "    zeroshot_pred_answers = []\n",
    "    fewshot_pred_answers = []\n",
    "    fewshot_evidence = []\n",
    "    cot_pred_answers = []\n",
    "    golden_answers = []\n",
    "    for q_id in q_ids:\n",
    "        zeroshot_pred_answers.append(zeroshot_predictions_dict[q_id][\"predicted_answer\"])\n",
    "        fewshot_pred_answers.append(fewshot_predictions_dict[q_id][\"predicted_answer\"])\n",
    "        cot_pred_answers.append(cot_predictions_dict[q_id][\"predicted_answer\"])\n",
    "        golden_answers.append(fewshot_predictions_dict[q_id][\"golden_answers\"])\n",
    "        fewshot_evidence.append(fewshot_predictions__evidence_dict[q_id])\n",
    "\n",
    "    d = {\n",
    "        'fewshot_evidence' : fewshot_evidence,\n",
    "        'fewshot_pred_answer': fewshot_pred_answers,\n",
    "         'gold_answers': golden_answers,\n",
    "         'quids': q_ids\n",
    "         }\n",
    "         \n",
    "    df = pd.DataFrame(data=d)\n",
    "    # df.explode('fewshot_evidence')\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    if len(df) > 3:\n",
    "        print(\"Question: \", question)\n",
    "        # df.set_index('quids')\n",
    "        df = df.explode('fewshot_evidence').reset_index(drop=True)\n",
    "        display(df.set_index('quids'))\n",
    "        # df_styled.to_excel(\"test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/qasper/qasper-test-v0.3.json\") as f:\n",
    "    test_set = json.load(f)\n",
    "    \n",
    "URL_titles_dict = {}\n",
    "for k, v in test_set.items():\n",
    "    for qa in v[\"qas\"]:\n",
    "        URL_titles_dict[qa[\"question_id\"]] = {\n",
    "            \"title\" : v[\"title\"],\n",
    "            \"URL\" : f\"https://arxiv.org/pdf/{k}.pdf\",\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dict = {}\n",
    "chosen_q_ids = []\n",
    "for question, q_ids in general_test_questions.items():\n",
    "    evidence = []\n",
    "    if len(q_ids) > 3:\n",
    "        chosen_q_ids.extend(q_ids[:4])\n",
    "        answers= [fewshot_predictions_dict[q_id][\"predicted_answer\"] for q_id in q_ids[:4]]\n",
    "        evidence = [fewshot_predictions_dict[q_id][\"predicted_evidence\"] for q_id in q_ids[:4]]\n",
    "        titles = [URL_titles_dict[q_id][\"title\"] for q_id in q_ids[:4]]\n",
    "        URLs = [URL_titles_dict[q_id][\"URL\"] for q_id in q_ids[:4]]\n",
    "        query_dict[question] = {\n",
    "            \"q_ids\": q_ids[:4],\n",
    "            \"answers\": answers,\n",
    "            \"evidence\": evidence,\n",
    "            \"titles\": titles,\n",
    "            \"URLs\": URLs, \n",
    "        }            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"productivity experiment/qasper_data.json\", \"w\") as f:\n",
    "    json.dump(query_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5db69e486b1ad77ada501eb583d0ea38590d7392e592d7270e62511894061ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
