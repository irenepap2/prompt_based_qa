{"question_id": "397a1e851aab41c455c2b284f5e4947500d797f0", "predicted_answer": "", "predicted_evidence": ["An example of human-human chat on AntiScam dataset is shown in Table TABREF25."]}
{"question_id": "cc8b4ed3985f9bfbe1b5d7761b31d9bd6a965444", "predicted_answer": "", "predicted_evidence": ["To decouple syntactic and semantic information in utterances and provide detailed supervision, we design a hierarchical intent annotation scheme for non-collaborative tasks. We first separate on-task and off-task intents. As on-task intents are key actions that can vary among different tasks, we need to specifically define on-task intents for each task. On the other hand, since off-task content is too general to design task-specific intents, we choose common dialog acts as the categories. The advantage of this hierarchical annotation scheme is apparent when starting a new non-collaborative task: we only need to focus on designing the on-task categories and semantic slots which are the same as traditional task-oriented dialog systems. Consequently, we don't have to worry about the off-task annotation design since the off-task category is universal."]}
{"question_id": "f7662b11e87c1e051e13799413f3db459ac3e19c", "predicted_answer": "", "predicted_evidence": ["We compare MISSA mainly with two baseline models:"]}
{"question_id": "b584739622d0c53830e60430b13fd3ae6ff43669", "predicted_answer": "", "predicted_evidence": ["We compare MISSA mainly with two baseline models:"]}
{"question_id": "2849c2944c47cf1de62b539c5d3c396a3e8d283a", "predicted_answer": "", "predicted_evidence": ["We trained our classifier and its hyper-parameters by five-fold cross-validation on the training sets of the ISTEX and RSS datasets. We used GERBIL BIBREF23 to evaluate OpenTapioca against other approaches. We report the InKB micro and macro F1 scores on test sets, with GERBIL's weak annotation match method."]}
{"question_id": "1a6156189297b2fe17f174ef55cbd20341bb7dbf", "predicted_answer": "", "predicted_evidence": ["When increasing the detection delay to 12 and 24 hours, all three algorithms reach comparable performance with no statistically significant difference, as seen in table 4. For our approach, none of the features are computed retrospectively, which explains why the performance does not change when increasing the detection delay. The additional time allows Liu and Yang to collect repeated signals, which improves their detection accuracy. After 24 hours Liu performs the highest due to its retrospectively computed features. Note that after 24 hours rumours might have already spread far through social networks and potentially caused harm."]}
{"question_id": "3319d56556ae1597a86384057db0831e32774b90", "predicted_answer": "", "predicted_evidence": ["We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages."]}
{"question_id": "8cbe3fa4ec0f66071e3d6b829b09b6395b631c44", "predicted_answer": "", "predicted_evidence": ["Training Pseudo Feedback Features"]}
{"question_id": "85e417231a4bbb6691f7a89bd81710525f8fec4c", "predicted_answer": "", "predicted_evidence": ["We ordered the rumours and non-rumours chronologically and divided them in half, forming a training and test set. We ensured that each of the sets consists of 50% rumours and non-rumours. This is important when effectiveness is measured by accuracy. All training and optimization use the trainings set. Performance is then reported based on a single run on the test set."]}
{"question_id": "57ee20f494d8ce3fae46028c3f3551d180dba3e0", "predicted_answer": "", "predicted_evidence": ["Apart from novelty based features, we also apply a range of 51 context based features. The full list of features can be found in table 6 . The focus lies on features that can be computed instantly based only on the text of a message to keep the latency of our approach to a minimum. Most of these 51 features overlap with previous studies (Castillo et. al, 2011; Liu et. al, 2015; Qazvinian et. al, 2011; Yang et. al, 2012; Zhao et. al, 2015). This includes features based on the presence or number of URLs, hash-tags and user-names, POS tags, punctuation characters as well as 8 different categories of sentiment and emotions."]}
{"question_id": "2974237446d04da33b78ce6d22a477cdf80877b7", "predicted_answer": "", "predicted_evidence": ["Previous approaches to rumour detection rely on repeated signals to form propagation graphs or clustering methods. Beside causing a detection delay these methods are also blind to less popular rumours that don't go viral. In contrast, novelty based feature require only a single message enabling them to detect even the smallest rumours. Examples for such small rumours are shown in table 3 ."]}
{"question_id": "bc8526d4805e2554adb2e9c01736d3f3a3b19895", "predicted_answer": "", "predicted_evidence": ["The following baselines were used in our experiments:"]}
{"question_id": "a0fd0c0fe042ad045b8d5095c81643ef3a352b81", "predicted_answer": "", "predicted_evidence": ["Most of the sentences generated by both mechanisms are natural and semantically correlated with particular topics that are summarized in the first column of the table."]}
{"question_id": "6e040e80f2da69d50386a90a38ed6d2fa4f77bbd", "predicted_answer": "", "predicted_evidence": ["(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34."]}
{"question_id": "aebd1f0d728d0de5f76238844da044a44109f76f", "predicted_answer": "", "predicted_evidence": ["because $\\sin (-x)=-\\sin (x), \\cos (x)=\\cos (-x)$. This means for an offset $t$, the forward and backward relative positional encoding are the same with respect to the $\\cos (c_it)$ terms, but is the opposite with respect to the $\\sin (c_it)$ terms. Therefore, by using $R_{t-j}$, the attention score can distinguish different directions and distances."]}
{"question_id": "cb4086ad022197da79f28dc609d0de90108c4543", "predicted_answer": "", "predicted_evidence": ["We evaluate our model in two English NER datasets and four Chinese NER datasets."]}
{"question_id": "756a8a9125e6984e0ca768b653c6c760efa3db66", "predicted_answer": "", "predicted_evidence": ["For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy\u2014much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 ."]}
{"question_id": "fe52b093735bb456d7e699aa9a2b806d2b498ba0", "predicted_answer": "", "predicted_evidence": ["This thesis proposal provides an overview of KALM, a system for knowledge authoring. In addition, it introduces KALM-QA, the question answering part of KALM. Experimental results show that both KALM and KALM-QA achieve superior accuracy as compared to the state-of-the-art systems."]}
{"question_id": "7748c072e07d6c6db5a34be38b4a5e97ac6d7999", "predicted_answer": "", "predicted_evidence": ["For KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy\u2014much higher than the state-of-the-art machine learning approach BIBREF14 . Details of the evaluations can be found in BIBREF5 and BIBREF6 ."]}
{"question_id": "c97306c1be5d59cf27b1054adfa8f1da47d292ce", "predicted_answer": "", "predicted_evidence": ["We follow the formulation of the task as specified in the EVENTI exercise: determine the extent and the class of event mentions in a text, according to the It-TimeML $<$ EVENT $>$ tag definition (Subtask B in EVENTI)."]}
{"question_id": "e42916924b69cab1df25d3b4e6072feaa0ba8084", "predicted_answer": "", "predicted_evidence": ["As for the other parameters, the network maintains the optimized configurations used for the event detection task for English BIBREF14 : two LSTM layers of 100 units each, Nadam optimizer, variational dropout (0.5, 0.5), with gradient normalization ( $\\tau $ = 1), and batch size of 8. Character-level embeddings, learned using a Convolutional Neural Network (CNN) BIBREF22 , are concatenated with the word embedding vector to feed into the LSTM network. Final layer of the network is a CRF classifier."]}
{"question_id": "079ca5810060e1cdc12b5935d8c248492f0478b9", "predicted_answer": "", "predicted_evidence": ["Tables 1 and 1 report, respectively, the distribution of the events per token part-of speech (POS) and per event class. Not surprisingly, verbs are the largest annotated category, followed by nouns, adjectives, and prepositional phrases. Such a distribution reflects both a kind of \u201cnatural\u201d distribution of the realization of events in an Indo-european language, and, at the same time, specific annotation choices. For instance, adjectives have been annotated only when in a predicative position and when introduced by a copula or a copular construction. As for the classes, OCCURRENCE and STATE represent the large majority of all events, followed by the intensional ones (I_STATE and I_ACTION), expressing some factual relationship between the target events and their arguments, and finally the others (REPORTING, ASPECTUAL, and PERCEPTION)."]}
{"question_id": "a3e7d7389228a197c8c44e0c504a791b60f2c80d", "predicted_answer": "", "predicted_evidence": ["Candidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$."]}
{"question_id": "8b4bd0a962241ea548752212ebac145e2ced7452", "predicted_answer": "", "predicted_evidence": ["With word-level associations in hand, our next goals were to discover coherent clusters among the words and to automatically label those clusters."]}
{"question_id": "d39059340a79bdc0ebab80ad3308e3037d7d5773", "predicted_answer": "", "predicted_evidence": ["Two datasets for studying language and gender, each consisting of over 300K sentences."]}
{"question_id": "31d4b0204702907dc0cd0f394cf9c984649e1fbf", "predicted_answer": "", "predicted_evidence": ["Our contributions include:"]}
{"question_id": "371433bd3fb5042bacec4dfad3cfff66147c14f0", "predicted_answer": "", "predicted_evidence": ["4 Data-driven approaches:"]}
{"question_id": "f64449a21c452bc5395a0f0a49fb49825e6385f4", "predicted_answer": "", "predicted_evidence": ["In order to assess the perceived appropriateness of system responses we conduct a human study using crowd-sourcing on the FigureEight platform. We define appropriateness as \u201cacceptable behaviour in a work environment\u201d and the participants were made aware that the conversations took place between a human and a system. Ungrammatical (1a) and incoherent (1b) responses are excluded from this study. We collect appropriateness ratings given a stimulus (the prompt) and four randomly sampled responses from our corpus that the worker is to label following the methodology described in BIBREF21, where each utterance is rated relatively to a reference on a user-defined scale. Ratings are then normalised on a scale from [0-1]. This methodology was shown to produce more reliable user ratings than commonly used Likert Scales. In addition, we collect demographic information, including gender and age group. In total we collected 9960 HITs from 472 crowd workers. In order to identify spammers and unsuitable ratings, we use the responses from the adult-only bots as test questions: We remove users who give high ratings to sexual bot responses the majority (more than 55%) of the time.18,826 scores remain - resulting in an average of 7.7 ratings per individual system reply and 1568.8 ratings per response type as listed in Table TABREF14.Due to missing demographic data - and after removing malicious crowdworkers - we only consider a subset of 190 raters for our demographic study. The group is composed of 130 men and 60 women. Most raters (62.6%) are under the age of 44, with similar proportions across age groups for men and women. This is in-line with our target population: 57% of users of smart speakers are male and the majority are under 44 BIBREF22."]}
{"question_id": "3aeb25e334c8129b376f11c7077bcb2dd54f7e0e", "predicted_answer": "", "predicted_evidence": ["We then use these prompts to elicit responses from the following systems, following methodology from Amanda:EthicsNLP2018."]}
{"question_id": "c19e9fd2f1c969e023fb99b74e78eb1f3db8e162", "predicted_answer": "", "predicted_evidence": ["In the following, we describe the rationale behind the annotation of the dataset including the definition of the various semantic classes and the annotation guidelines."]}
{"question_id": "230ff86b7b90b87c33c53014bb1e9c582dfc107f", "predicted_answer": "", "predicted_evidence": ["We experiment with several languages with varying degrees of morphological richness and typology: Turkish, Finnish, Czech, German, Spanish, Catalan and English. Our experiments and analysis reveal insights such as:"]}
{"question_id": "dc23006d67f20f430f1483398de4a89c0be4efe2", "predicted_answer": "", "predicted_evidence": ["We experiment with several languages with varying degrees of morphological richness and typology: Turkish, Finnish, Czech, German, Spanish, Catalan and English. Our experiments and analysis reveal insights such as:"]}
{"question_id": "887d7f3edf37ccc6bf2e755dae418b04d2309686", "predicted_answer": "", "predicted_evidence": ["We use three types of units: (1) words (2) characters and character sequences and (3) outputs of morphological analysis. Words serve as a lower bound; while morphology is used as an upper bound for comparison. Table 1 shows sample outputs of various $\\rho $ functions."]}
{"question_id": "b8a3ab219be6c1e6893fe80e1fbf14f0c0c3c97c", "predicted_answer": "", "predicted_evidence": ["We have used the following datasets for training and evaluation:"]}
{"question_id": "780c7993d446cd63907bb38992a60bbac9cb42b1", "predicted_answer": "", "predicted_evidence": ["A study of the complexity of figure-caption correspondence compared to classical image-sentence matching."]}
{"question_id": "3da4606a884593f7702d098277b9a6ce207c080b", "predicted_answer": "", "predicted_evidence": ["Wikipedia. We used the January 2018 English Wikipedia dataset as one of the corpora on which to train Vecsigrafo. As opposed to SciGraph or SemScholar, specific of the scientific domain, Wikipedia is a source of general-purpose information."]}
{"question_id": "91336f12ab94a844b66b607f8621eb8bbd209f32", "predicted_answer": "", "predicted_evidence": ["We have used the following datasets for training and evaluation:"]}
{"question_id": "c5221bb28e58a4f13cf2eccce0e1b1bec7dd3c13", "predicted_answer": "", "predicted_evidence": ["Several distinct patterns emerge from the text. The text feature in the first column seems to focus on genetics and histochemistry, including terms like western blots or immunostaining and variations like immunoblot-s/ted/ting. Interestingly, it also seems to have learnt some type of is-a relations (western blot is a type of immunoblot). The second feature focuses on variations of the term radiograph, e.g. radiograph-y/s. The third feature specializes in text related to curve plots involving several statistic analysis, e.g. Real-time PCR, one-way ANOVA or Gaussian distribution. Sometimes (fourth figure from top) the caption does not mention the plot directly, but focuses on the analysis instead, e.g. \"the data presented here are mean values of duplicate experiments\", indicating transfer of knowledge from the visual part during training. The fourth feature extracts citations and models named after prominent scientists, e.g. Evans function (first and fourth figure), Manley (1992) (second), and Aliev-Panfilov model (third). The fifth feature extracts chromatography terminology, e.g. 3D surface plot, photomicrograph or color map and, finally, the right-most feature focuses on different types of named diagrams, like flow charts and state diagrams, e.g. phylogenetic trees."]}
{"question_id": "42a4ab4607a9eec42c427a817b7e898230d26444", "predicted_answer": "", "predicted_evidence": ["A study of the complexity of figure-caption correspondence compared to classical image-sentence matching."]}
{"question_id": "622efbecd9350a0f4487bdff2b8b362ef2541f3c", "predicted_answer": "", "predicted_evidence": ["Table TABREF3 summarizes the average, maximum and minimum sentence lengths for each dataset after we processed them with Twokenizer. We can see the four corpora offer similar characteristics in terms of length, with a cross dataset maximum length of 41 tokens. We also see there is an important vocabulary gap between the dataset and GloVe, with an average coverage of only 64.3 %. To tackle this issue, we used a set of binary features derived from POS tags to capture some of the semantics of the words that are not covered by the GloVe embeddings. We also include features for member mentions and hashtags as well as a feature to capture word elongation, based on regular expressions. Word elongation is very common in tweets, and is usually associated to strong sentiment. The following are the POS tag-derived rules we used to generate our binary features."]}
{"question_id": "f54e19f7ecece1bb0ef3171403ae322ad572ff00", "predicted_answer": "", "predicted_evidence": ["To validate the usefulness of our binary features, we performed an ablation experiment and trained our best models for each corpus without them. Table TABREF15 summarizes our results in terms of Pearson correlation on the development portion of the datasets. As seen, performance decreases in all cases, which shows that indeed these features are critical for performance, allowing the model to better capture the semantics of words missing in GloVe. In this sense, we think the usage of additional features, such as the ones derived from emotion or sentiment lexicons could indeed boost our model capabilities. This is proposed for future work."]}
{"question_id": "4137a82d7752be7a6c142ceb48ce784fd475fb06", "predicted_answer": "", "predicted_evidence": ["To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5."]}
{"question_id": "6c50871294562e4886ede804574e6acfa8d1a5f9", "predicted_answer": "", "predicted_evidence": ["To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5."]}
{"question_id": "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02", "predicted_answer": "", "predicted_evidence": ["To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5."]}
{"question_id": "ed44f7e698d6124cb86791841d02fc6f8b4d862a", "predicted_answer": "", "predicted_evidence": ["There is little consensus on the difference between profanity and hate speech and, how to define the latter BIBREF17. As shown in Figure FIGREF11, slurs are not an unequivocal indicator of hate speech and can be part of a non-aggressive conversation, while some of the most offensive comments may come in the form of subtle metaphors or sarcasm BIBREF18. Consequently, there is no existing human annotated vocabulary that explicitly reveals the presence of hate speech, which makes the available hate speech corpora sparse and noisy BIBREF19."]}
{"question_id": "d9e7633004ed1bc1ee45be58409bcc1fa6db59b2", "predicted_answer": "", "predicted_evidence": ["Furthermore, we perceived code-switching in English where Hindi, Spanish, and French tokens appear in the tweets. Some French tweets also contain Romanized dialectal Arabic tokens generated by, most likely, bilingual North African Twitter users. Hence, although we eliminated most of these tweets in order to avoid misleading the annotators, the possibly remaining ones still added noise to the data."]}
{"question_id": "c58ef13abe5fa91a761362ca962d7290312c74e4", "predicted_answer": "", "predicted_evidence": ["In this section, we present our data collection methodology and annotation process."]}
{"question_id": "9ef0d2365bde0d18054511fbb53cec5fa2cda5ee", "predicted_answer": "", "predicted_evidence": ["In this section, we present our data collection methodology and annotation process."]}
{"question_id": "cbb3c1c1e6e1818b6480f929f1c299eaa5ffd07a", "predicted_answer": "", "predicted_evidence": ["One possible solution to address the remaining issues of MT lies in the use of SWT, which have emerged over recent decades as a paradigm to make the semantics of content explicit so that it can be used by machines. It is believed that explicit semantic knowledge made available through these technologies can empower MT systems to supply translations with significantly better quality while remaining scalable. In particular, the disambiguated knowledge about real-world entities, their properties and their relationships made available on the LD Web can potentially be used to infer the right meaning of ambiguous sentences or words."]}
{"question_id": "9f74f3991b8681619d95ab93a7c8733a843ddffe", "predicted_answer": "", "predicted_evidence": ["One possible solution to address the remaining issues of MT lies in the use of SWT, which have emerged over recent decades as a paradigm to make the semantics of content explicit so that it can be used by machines. It is believed that explicit semantic knowledge made available through these technologies can empower MT systems to supply translations with significantly better quality while remaining scalable. In particular, the disambiguated knowledge about real-world entities, their properties and their relationships made available on the LD Web can potentially be used to infer the right meaning of ambiguous sentences or words."]}
{"question_id": "7c2c15ea3f1b1375b8aaef1103a001069d9915bb", "predicted_answer": "", "predicted_evidence": ["Although MT systems are now popular on the Web, they still generate a large number of incorrect translations. Recently, Popovi\u0107 BIBREF3 has classified five types of errors that still remain in MT systems. According to research, the two main faults that are responsible for 40% and 30% of problems respectively, are reordering errors and lexical and syntactic ambiguity. Thus, addressing these barriers is a key challenge for modern translation systems. A large number of MT approaches have been developed over the years that could potentially serve as a remedy. For instance, translators began by using methodologies based on linguistics which led to the family of RBMT. However, RBMT systems have a critical drawback in their reliance on manually crafted rules, thus making the development of new translation modules for different languages even more difficult."]}
{"question_id": "a77d38427639d54461ae308f3045434f81e497d0", "predicted_answer": "", "predicted_evidence": ["EEG signals were sampled at 1000Hz and a fourth order IIR band pass filter with cut off frequencies 0.1Hz and 70Hz was applied. A notch filter with cut off frequency 60 Hz was used to remove the power line noise. EEGlab's BIBREF17 Independent component analysis (ICA) toolbox was used to remove other biological signal artifacts like electrocardiography (ECG), electromyography (EMG), electrooculography (EOG) etc from the EEG signals. We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0 . So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel."]}
{"question_id": "010fd15696580d9924ac0275a4ff269005e5808d", "predicted_answer": "", "predicted_evidence": ["For data set A, we used data from first 8 subjects for training the model, remaining two subjects data for validation and test set respectively."]}
{"question_id": "d36a6447bfe58204e0d29f9213d84be04d875624", "predicted_answer": "", "predicted_evidence": ["For data set A, we used data from first 8 subjects for training the model, remaining two subjects data for validation and test set respectively."]}
{"question_id": "5ed02ae6c534cd49d405489990f0e4ba0330ff1b", "predicted_answer": "", "predicted_evidence": ["With model size of $2.5\\times $ reduction, LadaBERT-1 performs significantly better than BERT-PKD, boosting the performance by relative 8.9, 8.1, 6.1, 3.8 and 5.8 percentages on MNLI-m, MNLI-mm, SST-2, QQP and QNLI datasets respectively. Recall that BERT-PKD initializes the student model by selecting 3 of 12 layers in the pre-trained BERT-Base model. It turns out that the discarded layers have huge impact on the model performance, which is hard to be recovered by knowledge distillation. On the other hand, LadaBERT generates the student model by iterative pruning on the pre-trained teacher. In this way, the original knowledge in the teacher model can be preserved to the largest extent, and the benefit of which is complementary to knowledge distillation."]}
{"question_id": "f6346828c2f44529dc307abf04dd246bfeb4a9b2", "predicted_answer": "", "predicted_evidence": ["Weight pruning and matrix factorization are two simple baselines described in Section SECREF2. We evaluate both pruning methods in an iterative manner until the target compression ratio is reached."]}
{"question_id": "935873b97872820b7b6100d6a785fba286b94900", "predicted_answer": "", "predicted_evidence": ["The evaluation results of LadaBERT and state-of-the-art approaches are listed in Table TABREF40, where the models are ranked by parameter sizes for feasible comparison. As shown in the table, LadaBERT consistently outperforms the strongest baselines under similar model sizes. In addition, the performance of LadaBERT demonstrates the superiority of hybrid combination of SVD-based matrix factorization, weight pruning and knowledge distillation."]}
{"question_id": "f2bcfdbebb418e7da165c19b8c7167719432ee48", "predicted_answer": "", "predicted_evidence": ["Most extractive methods to date identify sentences based on human-engineered features. These include surface features such as sentence position and length BIBREF0 , the words in the title, the presence of proper nouns, content features such as word frequency BIBREF1 , and event features such as action nouns BIBREF2 . Sentences are typically assigned a score indicating the strength of presence of these features. Several methods have been used in order to select the summary sentences ranging from binary classifiers BIBREF3 , to hidden Markov models BIBREF4 , graph-based algorithms BIBREF5 , BIBREF6 , and integer linear programming BIBREF7 ."]}
{"question_id": "0fe49431db5ffaa24372919daf24d8f84117bfda", "predicted_answer": "", "predicted_evidence": ["In addition to the DUC 2002 and 500 DailyMail samples, we additionally report results on the entire DailyMail test set (Table 3 ). Since there is no established evaluation standard for this task, we experimented with three different ROUGE limits: 75 bytes, 275 bytes and full length."]}
{"question_id": "0f9c1586f1b4b531fa4fd113e767d06af90b1ae8", "predicted_answer": "", "predicted_evidence": ["In this section we present our experimental setup for assessing the performance of our summarization models. We discuss the datasets used for training and evaluation, give implementation details, briefly introduce comparison models, and explain how system output was evaluated."]}
{"question_id": "52faf319e37aa15fff1ab47f634a5a584dc42e75", "predicted_answer": "", "predicted_evidence": ["More than just curating a static collection of facts, we would like commonsense knowledge to be represented in a way that lends itself to machine reasoning and inference of missing information. We concern ourselves in this paper with the problem of learning commonsense knowledge representations."]}
{"question_id": "0c7cb3010ed92b8d46583a67e72946a6c0115f1f", "predicted_answer": "", "predicted_evidence": ["Recently, a thread of research on representation learning has aimed to create embedding spaces that automatically enforce consistency in these predictions using the intrinsic geometry of the embedding space BIBREF9 , BIBREF0 , BIBREF10 . In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models."]}
{"question_id": "9c2cacf77041e02d38f92a4c490df1e04552f96f", "predicted_answer": "", "predicted_evidence": ["In contrast to social media posts, sentiment analysis of news articles and blogs has received less attention BIBREF26 . This can be attributed to a more challenging task due to the nature of the domain since, for example, journalists will often refrain from using clearly positive or negative vocabulary when writing news articles BIBREF27 . However, certain aspects of these communication channels are still apt for sentiment analysis, such as column pieces BIBREF28 or political news BIBREF27 , BIBREF29 ."]}
{"question_id": "35cdaa0fff007add4a795850b139df80af7d1ffc", "predicted_answer": "", "predicted_evidence": ["Model performance during train is presented in Table 5 . While all the models outperformed the baselines, not all of them did so with a significant margin due to the robustness of the baselines selected. The ones found to be significantly better than the baselines were models IIb (Domain-specific) and IIc (Twitter-only) (permutation test, $n = 10^5$ both $p < 0.05$ ). The difference in precision between model IIb and IIc points out to the former making the wrong predictions for news articles. These errors are most likely in selecting the wrong supporting segment. Moreover, even though models IIa-c only produce negative labels, they still achieve improved performance over the state-of-the-art systems, highlighting the highly skewed nature of the training dataset."]}
{"question_id": "3de3a083b8ba3086792d38ae9667e095070f7f37", "predicted_answer": "", "predicted_evidence": ["Table 6 present the official evaluation results for English and Spanish. Some information is missing since at the time of submission only partial score had been made public. As previously mentioned, the pre-trained state-of-the-art models (model I) were directly applied to the evaluation data without any adaptation. These performed reasonably well for the English data. Among the submissions of the SEC Task pilot, our systems outperformed the other competitors for both languages."]}
{"question_id": "04914917d01c9cd8718cd551dc253eb3827915d8", "predicted_answer": "", "predicted_evidence": ["Table 6 present the official evaluation results for English and Spanish. Some information is missing since at the time of submission only partial score had been made public. As previously mentioned, the pre-trained state-of-the-art models (model I) were directly applied to the evaluation data without any adaptation. These performed reasonably well for the English data. Among the submissions of the SEC Task pilot, our systems outperformed the other competitors for both languages."]}
{"question_id": "20632fc4d2b693b5aabfbbc99ee5c1e9fc485dea", "predicted_answer": "", "predicted_evidence": ["This section introduces the two multimodal resources compared in this study and discusses related work, beginning with the crowd-sourced annotations in AI2D and continuing with the alternative expert annotations in AI2D-RST, which are built on top of the crowd-sourced descriptions and cover a 1000-diagram subset of the original data. Figure FIGREF1 provides an overview of the two datasets, explains their relation to each other and provides an overview of the experiments reported in Section SECREF4"]}
{"question_id": "a57e266c936e438aeeab5e8d20d9edd1c15a32ee", "predicted_answer": "", "predicted_evidence": ["hiippalaetal2019-ai2d show that the proposed annotation schema can be reliably applied to the data by measuring inter-annotator agreement between five annotators on random samples from the AI2D-RST corpus using Fleiss' $\\kappa $ BIBREF23. The results show high agreement on grouping ($N = 256, \\kappa = 0.84$), diagram types ($N = 119, \\kappa = 0.78$), connectivity ($N = 239, \\kappa = 0.88$) and discourse structure ($N = 227, \\kappa = 0.73$). It should be noted, however, that these measures may be affected by implicit knowledge that tends to develop among expert annotators who work towards the same task BIBREF24."]}
{"question_id": "27356a99290fcc01e3e5660af3405d2a6c6f6e7c", "predicted_answer": "", "predicted_evidence": ["This provides an interesting setting for comparison and evaluation, as non-expert annotations are cheap to produce and easily outnumber the expert-annotated data, whose production consumes both time and resources. Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic structures and thus justify their higher cost is one question that this study seeks to answer."]}
{"question_id": "6e37f43f4f54ffc77c785d60c6058fbad2147922", "predicted_answer": "", "predicted_evidence": ["Tasks related to grouping and connectivity annotation could be crowd-sourced relatively easily, whereas annotating diagram types and discourse relations may require multi-step procedures and assistance in the form of prompts, as yungetal2019 have recently shown for RST. Involving both expert and crowd-sourced annotators could also alleviate problems related to circularity by forcing domain experts to frame the tasks in terms understandable to crowd-sourced workers BIBREF24."]}
{"question_id": "fff1ed2435ba622d884ecde377ff2de127167638", "predicted_answer": "", "predicted_evidence": ["This provides an interesting setting for comparison and evaluation, as non-expert annotations are cheap to produce and easily outnumber the expert-annotated data, whose production consumes both time and resources. Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic structures and thus justify their higher cost is one question that this study seeks to answer."]}
{"question_id": "7ff7c286d3118a8be5688e2d18e9a56fe83679ad", "predicted_answer": "", "predicted_evidence": ["Lastly, we examine model performance with respect to human annotation using the human annotated dataset of BIBREF6 . We assume that models that perform similarly to human annotators are preferable. In Table TABREF20 , we present three Spearman correlation metrics to express model congruence with human annotations. Mean annotation expresses the correlation of model error rates with the controversy values attributed to a web page by human annotators, with positive values expressing greater error rates on controversial, and negative expressing higher error rates on non-controversial pages. Here, the HAN shows most unbiased (closest to zero) performance."]}
{"question_id": "1ecbbb60dc44a701e9c57c22167dd412711bb0be", "predicted_answer": "", "predicted_evidence": ["We use the Clueweb09 derived dataset of BIBREF0 for baseline comparison. For cross-temporal, cross-topic and cross-domain training & evaluation, we generate a new dataset based on Wikipedia crawl data. This dataset is gathered by using Wikipedia's `List of Contoversial articles' overview page of 2018 (time of writing) and 2009 (for comparison with baselines) . Using this as a `seed' set of controversial articles, we iteratively crawl the `See also', `References' and `External links' hyperlinks up to two hops from the seed list. The negative seed pages (i.e. non controversial) are gathered by using the random article endpoint. The snowball-sample approach includes general, non-Wikipedia, pages that are referred to from Wikipedia pages. The dataset thus extends beyond just the encyclopedia genre of texts. Labels are assumed to propagate: a page linked from a controversial issue is assumed to be controversial. The resulting dataset statistics are summarized in Table TABREF7 ."]}
{"question_id": "592df9831692b8fde213257ed1894344da3e0594", "predicted_answer": "", "predicted_evidence": ["Lastly, we examine model performance with respect to human annotation using the human annotated dataset of BIBREF6 . We assume that models that perform similarly to human annotators are preferable. In Table TABREF20 , we present three Spearman correlation metrics to express model congruence with human annotations. Mean annotation expresses the correlation of model error rates with the controversy values attributed to a web page by human annotators, with positive values expressing greater error rates on controversial, and negative expressing higher error rates on non-controversial pages. Here, the HAN shows most unbiased (closest to zero) performance."]}
{"question_id": "6822ca5f7a19866ffc3c985b790a4aadcecf2d1c", "predicted_answer": "", "predicted_evidence": ["To evaluate robustness towards unseen topics, 10-fold cross validation was used on the top ten largest topics present in the Wikipedia dataset in a leave-one-out fashion. The results are shown in table 4. In line with previous results, the language model scores best on Recall, beating all other models with a significant difference (p < 0.01). However in balancing Recall with Precision, the HAN model scores best, significantly outperforming both lexical models in F1 score (p < 0.05). Overall, when grouping together all neural and lexical results, the neural methods outperform the lexical models in Precision (p < 0.01), F1 (p < 0.05) and AUC (p < 0.01) with no significant difference found on the overall Recall scores. These results indicate that neural methods seem better able to generalize to unseen topics."]}
{"question_id": "60e6296ca2a697892bd67558a21a83ef01a38177", "predicted_answer": "", "predicted_evidence": ["To evaluate robustness towards unseen topics, 10-fold cross validation was used on the top ten largest topics present in the Wikipedia dataset in a leave-one-out fashion. The results are shown in table 4. In line with previous results, the language model scores best on Recall, beating all other models with a significant difference (p < 0.01). However in balancing Recall with Precision, the HAN model scores best, significantly outperforming both lexical models in F1 score (p < 0.05). Overall, when grouping together all neural and lexical results, the neural methods outperform the lexical models in Precision (p < 0.01), F1 (p < 0.05) and AUC (p < 0.01) with no significant difference found on the overall Recall scores. These results indicate that neural methods seem better able to generalize to unseen topics."]}
{"question_id": "9b868c7d17852f46a8fe725f24cb9548fdbd2b05", "predicted_answer": "", "predicted_evidence": ["Our RNNLM models consist of 2 LSTM layers, each containing 2048 units which are linearly projected to 512 units BIBREF19 . The word-piece and video embeddings are of size 512 each. We do not use dropout. During training, the batch size per worker is set to 256, and we perform full length unrolling to a max length of 70. The INLINEFORM0 -norms of the gradients are clipped to a max norm of INLINEFORM1 for the LSTM weights and to 10,000 for all other weights. We train with Synchronous SGD with the Adafactor optimizer BIBREF20 until convergence on a development set, created by randomly selecting INLINEFORM2 of all utterances."]}
{"question_id": "243cf21c4e34c4b91fcc4905aa4dc15a72087f0c", "predicted_answer": "", "predicted_evidence": ["Our RNNLM models consist of 2 LSTM layers, each containing 2048 units which are linearly projected to 512 units BIBREF19 . The word-piece and video embeddings are of size 512 each. We do not use dropout. During training, the batch size per worker is set to 256, and we perform full length unrolling to a max length of 70. The INLINEFORM0 -norms of the gradients are clipped to a max norm of INLINEFORM1 for the LSTM weights and to 10,000 for all other weights. We train with Synchronous SGD with the Adafactor optimizer BIBREF20 until convergence on a development set, created by randomly selecting INLINEFORM2 of all utterances."]}
{"question_id": "488e3c4fd1103c46e12815d1bf414a0356fb0d0e", "predicted_answer": "", "predicted_evidence": ["For a given video segment, we assume that there is a sequence of INLINEFORM0 video frames represented by features INLINEFORM1 , and the corresponding transcription INLINEFORM2 . In practice, we assume INLINEFORM3 since we can always assign a video frame to each word by replicating the video frames the requisite number of times. Thus, our visually-grounded language model models the probability of the next word given the history of previous words as well as video frames: INLINEFORM4 "]}
{"question_id": "84765903b8c7234ca2919d0a40e3c6a5bcedf45d", "predicted_answer": "", "predicted_evidence": ["For experiments with both data sets we used an established NMT architecture BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism."]}
{"question_id": "38363a7ed250bc729508c4c1dc975696a65c53cb", "predicted_answer": "", "predicted_evidence": ["After a small grid search we decided to inherit most of the hyperparameters of the model from the best results achieved in BIBREF3 where -to-Mizar translation is learned. We used relatively small LSTM cells consisting of 2 layers with 128 units. The \u201cscaled Luong\u201d version of the attention mechanism was used, as well as dropout with rate equal $0.2$. The number of training steps was 10000. (This setting was used for all our experiments described below.)"]}
{"question_id": "e862ebfdb1b3425af65fec81c8984edca6f89a76", "predicted_answer": "", "predicted_evidence": ["We also run an experiment on the joint set of all rewrite rules (consisting of 41396 examples). Here the task was more difficult as a model needed not only to apply rewriting correctly, but also choose \u201cthe right\u201d rewrite rule applicable for a given term. Nevertheless, the performance was also very good, reaching $83\\%$ of accuracy."]}
{"question_id": "ec8f39d32084996ab825debd7113c71daac38b06", "predicted_answer": "", "predicted_evidence": ["We have introduced a simple information theoretic approach to topic modeling that can leverage domain knowledge specified informally as anchors. Our framework uses a novel combination of CorEx and the information bottleneck. Preliminary results suggest it can extract more precise, interpretable topics through a lightweight interactive process. We next plan to perform further empirical evaluations and to extend the algorithm to handle complex latent structures present in health care data."]}
{"question_id": "a67a2d9acad1787b636ca2681330f4c29a0b0254", "predicted_answer": "", "predicted_evidence": ["We preprocessed each document with a standard biomedical text pipeline that extracts common medical terms and phrases (grouping neighboring words where appropriate) and detecting negation (\u201cnot\u201d is prepended to negated terms) BIBREF23 , BIBREF24 . We converted each document to a binary bag-of-words with a vocabulary of 4114 (possibly negated) medical phrases. We used the 60/40 training/test split from the competition."]}
{"question_id": "1efaf3bcd66d1b6bdfb124f0cec0cfeee27e6124", "predicted_answer": "", "predicted_evidence": ["The 20 Newsgroups data set is suitable for a straightforward evaluation of anchored topic models. The latent classes represent mutually exclusive categories, and each document is known to originate from a single category. We find that the correlation structure among the latent classes is less complex than in the Obesity Challenge data. Further, each category tends to exhibit some specialized vocabulary not used extensively in other categories (thus satisfying the anchor assumption from BIBREF1 )."]}
{"question_id": "fcdbaa08cccda9968f3fd433c99338cc60f596a7", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is the F-Score between corrected label sequence and predicted label sequence."]}
{"question_id": "2e4688205c8e344cded7a053b6014cce04ef1bd5", "predicted_answer": "", "predicted_evidence": ["The label sequence with the highest score can be obtained by carrying out viterbi algorithm. The regularized objective function is as follows: DISPLAYFORM0 INLINEFORM0 "]}
{"question_id": "fc436a4f3674e42fb280378314bfe77ba0c99f2e", "predicted_answer": "", "predicted_evidence": ["We use a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media. Details of the data are listed in Table TABREF19 . We also use the same unlabelled text as Peng and Dredze peng-dredze:2016:P16-2 from Sina Weibo service in China and the text is word segmented by a Chinese word segmentation system Jieba as Peng and Dredze peng-dredze:2016:P16-2 so that our results are more comparable to theirs."]}
{"question_id": "a71fb012631e6a8854d5945b6d0ab2ab8e7b7ee6", "predicted_answer": "", "predicted_evidence": ["The results of our experiments also suggest directions for future work. We can observe all models in Table TABREF23 achieve a much lower recall than precision BIBREF25 . So we need to design some methods to solve the problem."]}
{"question_id": "b70e4c49300dc3eab18e907ab903afd2a0c6075a", "predicted_answer": "", "predicted_evidence": ["In this section we provide the list of languages codes used throughout this paper and the statistics of the datasets used for the downstream tasks."]}
{"question_id": "088d42ecb1e15515f6a97a0da2fed81b61d61a23", "predicted_answer": "", "predicted_evidence": ["In this section, we consider some additional settings for comparing mBERT and MMTE. We also investigate the impact of the number of languages and the target language token on MMTE performance."]}
{"question_id": "8599d6d14ac157169920c73b98a79737c7a68cf5", "predicted_answer": "", "predicted_evidence": ["In this section, we consider some additional settings for comparing mBERT and MMTE. We also investigate the impact of the number of languages and the target language token on MMTE performance."]}
{"question_id": "f1d61b44105e651925d02a51e6d7ea10ea28ebd8", "predicted_answer": "", "predicted_evidence": ["In this section we provide the list of languages codes used throughout this paper and the statistics of the datasets used for the downstream tasks."]}
{"question_id": "108f99fcaf620fab53077812e8901870896acf36", "predicted_answer": "", "predicted_evidence": ["Asking humans to evaluate the quality of a dialogue model is challenging, especially when multiple models have to be compared. The likert score (a.k.a. 1 to 5 scoring) has been widely used to evaluate the interactive experience with conversational models BIBREF70, BIBREF65, BIBREF0, BIBREF1. In such evaluation, a human interacts with the systems for several turns, and then they assign a score from 1 to 5 based on three questions BIBREF0 about fluency, engagingness, and consistency. This evaluation is both expensive to conduct and requires many samples to achieve statistically significant results BIBREF6. To cope with these issues, BIBREF6 proposed ACUTE-EVAL, an A/B test evaluation for dialogue systems. The authors proposed two modes: human-model chats and self-chat BIBREF71, BIBREF72. In this work, we opt for the latter since it is cheaper to conduct and achieves similar results BIBREF6 to the former. Another advantage of using this method is the ability to evaluate multi-turn conversations instead of single-turn responses."]}
{"question_id": "6c8dc31a199b155e73c84173816c1e252137a0af", "predicted_answer": "", "predicted_evidence": ["We provide both cross-lingual and multilingual baselines and discuss their limitations to inspire future research."]}
{"question_id": "7125db8334a7efaf9f7753f2c2f0048a56e74c49", "predicted_answer": "", "predicted_evidence": ["Furthermore, we propose competitive baselines in two training settings, namely, cross-lingual and multilingual, and compare them with translation pipeline models. Our baselines leverage pre-trained cross-lingual BIBREF4 and multilingual BIBREF5 models."]}
{"question_id": "43729be0effb5defc62bae930ceacf7219934f1e", "predicted_answer": "", "predicted_evidence": ["We report more the mixed-language samples generated by M-CausalBert in Table TABREF61 and TABREF62."]}
{"question_id": "ae2142ee9e093ce485025168f4bcb3da4602739d", "predicted_answer": "", "predicted_evidence": ["Section SECREF14 describes how we extract our data set. Section SECREF26 explains how, given a set of contrastive examples, contrastive evaluation works."]}
{"question_id": "ebe1084a06abdabefffc66f029eeb0b69f114fd9", "predicted_answer": "", "predicted_evidence": ["We consider the following recurrent baselines:"]}
{"question_id": "cfdd583d01abaca923f5c466bb20e1d4b8c749ff", "predicted_answer": "", "predicted_evidence": ["We hope the test set will prove useful for empirically validating novel architectures for context-aware NMT. So far, we have only evaluated models that consider one sentence of context, but the nominal antecedent is more distant for a sizable proportion of the test set, and the evaluation of variable-size context models BIBREF7 , BIBREF10 is interesting future work."]}
{"question_id": "554d798e4ce58fd30820200c474d7e796dc8ba89", "predicted_answer": "", "predicted_evidence": ["For each sentence pair in the resulting test set, we introduce contrastive translations. A contrastive translation is a translation variant where the correct pronoun is swapped with an incorrect one. For an example, see Table TABREF19 , where the pronoun it in the original translation corresponds to sie because the antecedent bat is a feminine noun in German (Fledermaus). We produce wrong translations by replacing sie with one of the other pronouns (er, es)."]}
{"question_id": "91e361e85c6d3884694f3c747d61bfcef171bab0", "predicted_answer": "", "predicted_evidence": ["In this paper, we use a simple EL algorithm that directly links the mention to the entity with the greatest commonness score. Commonness BIBREF17, BIBREF18 is calculated base on the anchor links in Wikipedia. It estimates the probability of an entity given only the mention string. In our FET approach, the commonness score is also used as the confidence on the linking result (i.e., the $\\mathbf {g}$ used in the prediction part of Subsection SECREF5). Within a same document, we also use the same heuristic used in BIBREF19 to find coreferences of generic mentions of persons (e.g., \u201cMatt\u201d) to more specific mentions (e.g., \u201cMatt Damon\u201d)."]}
{"question_id": "6295951fda0cfa2eb4259d544b00bc7dade7c01e", "predicted_answer": "", "predicted_evidence": ["We use Ours (Full) to represent our full model, and also compare with five variants of our own approach: Ours (DirectTrain) is trained without adding random person types while obtaining the KB type representation, and $\\lambda _P$ is set to 1; Ours (NoEL) does not use entity linking, i.e., the KB type representation and the entity linking confidence score are removed, and the model is trained in DirectTrain style; Ours (NonDeep) uses one BiLSTM layer and replaces the MLP with a dense layer; Ours (NonDeep NoEL) is the NoEL version of Ours (NonDeep); Ours (LocAttEL) uses the entity linking approach proposed in BIBREF19 instead of our own commonness based approach. Ours (Full), Ours (DirectTrain), and Ours (NonDeep) all use our own commonness based entity linking approach."]}
{"question_id": "3f717e6eceab0a066af65ddf782c1ebc502c28c0", "predicted_answer": "", "predicted_evidence": ["The experimental results are listed in Table TABREF16. As we can see, our approach performs much better than existing approaches on both datasets."]}
{"question_id": "f5603271a04452cbdbb07697859bef2a2030d75c", "predicted_answer": "", "predicted_evidence": ["We presented a common-knowledge concept extractor for the Systems Engineer's Virtual Assistant (SEVA) system and showed how it can be beneficial for downstream tasks such as relation extraction and knowledge graph construction. We construct a word-level annotated dataset with the help of a domain expert by carefully defining a labelling scheme to train a sequence labelling task to recognize SE concepts. Further, we also construct some essential datasets from the SE domain which can be used for future research. Future directions include constructing a comprehensive common-knowledge relation extractor from SE handbook and incorporating such human knowledge into a more comprehensive machine-processable commonsense knowledge base for the SE domain."]}
{"question_id": "6575ffec1844e6fde5a668bce2afb16b67b65c1f", "predicted_answer": "", "predicted_evidence": ["SE concepts are less ambiguous as compared to generic natural language text. A word usually means one concept. For example, the word `system' usually means the same when referring to a `complex system', `system structure', or `management system' in the SE domain. In generic text, the meaning of terms like `evaluation', `requirement', or `analysis' may contextually differ. We would like domain specific phrases such as `system evaluation', `performance requirement', or `system analysis' to be single entities. Based on the operational and system concepts described in BIBREF0, we carefully construct a set of concept-labels for the SE handbook which is shown in the next section."]}
{"question_id": "77c3416578b52994227bae7f2529600f02183e12", "predicted_answer": "", "predicted_evidence": ["In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results."]}
{"question_id": "2abcff4fdedf9b17f76875cc338ba4ab8d1eccd3", "predicted_answer": "", "predicted_evidence": ["Relations from abbreviations are simple direct connections between the abbreviation and its full form described in the abbreviations dataset. Figure FIGREF25 shows a snippet of knowledge graph constructed using stands-for and subset-of relationships. Larger graphs are shown in the demo."]}
{"question_id": "6df57a21ca875e63fb39adece6a9ace5bb2b2cfa", "predicted_answer": "", "predicted_evidence": ["SE concepts are less ambiguous as compared to generic natural language text. A word usually means one concept. For example, the word `system' usually means the same when referring to a `complex system', `system structure', or `management system' in the SE domain. In generic text, the meaning of terms like `evaluation', `requirement', or `analysis' may contextually differ. We would like domain specific phrases such as `system evaluation', `performance requirement', or `system analysis' to be single entities. Based on the operational and system concepts described in BIBREF0, we carefully construct a set of concept-labels for the SE handbook which is shown in the next section."]}
{"question_id": "b39b278aa1cf2f87ad4159725dff77b387f2df84", "predicted_answer": "", "predicted_evidence": ["In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results."]}
{"question_id": "814e945668e2b6f31b088918758b120fb00ada7d", "predicted_answer": "", "predicted_evidence": ["The definition document consists of 241 SE definitions and their descriptions. We iteratively construct entities in increasing order of number of words in the definitions with the help of their parts-of-speech tags. This helps in creating subset-of relation between a lower-word entity and a higher-word entity. Each root entity is lemmatized such that entities like processes and process appear only once."]}
{"question_id": "d4456e9029fcdcb6e0149dd8f57b77d16ead1bc4", "predicted_answer": "", "predicted_evidence": ["The results were examined from the following aspects:"]}
{"question_id": "d0b967bfca2039c7fb05b931c8b9955f99a468dc", "predicted_answer": "", "predicted_evidence": ["When the model is set to AVGWVEC and the feature dimension is 100, the results computed from different training corpus were compared (ACL+AZ, MixedAbs and Brown corpus). ACL+AZ outperforms others and brown corpus is better than MixedAbs for most of the categories, but brown corpus is not as good as MixedAbs for the category of OWN."]}
{"question_id": "31e6062ba45d8956791e1b86bad7efcb6d1b191a", "predicted_answer": "", "predicted_evidence": ["The characteristics of word embeddings based on different model and dataset are listed in Table. TABREF12 ."]}
{"question_id": "38b29b0dcb87868680f9934af71ef245ebb122e4", "predicted_answer": "", "predicted_evidence": ["Training corpus affects the results. ACL+AZ outperforming others indicates that the topics of the training corpus are important factors in argumentative zoning. Although Brown corpus has more vocabularies, it doesn't win ACL+AZ."]}
{"question_id": "6e134d51a795c385d72f38f36bca4259522bcf51", "predicted_answer": "", "predicted_evidence": ["The learned word embeddings are input into a classifier as features under a supervised machine learning framework. Similar to sentiment classification using word embeddings BIBREF21 , where they try to predict each tweet to be either positive or negative, in the task of AZ, the embeddings are used to classify each sentence into one of the seven categories."]}
{"question_id": "0778cbbd093f8b779f7cf26302b2a8e081ccfb40", "predicted_answer": "", "predicted_evidence": ["Training corpus affects the results. ACL+AZ outperforming others indicates that the topics of the training corpus are important factors in argumentative zoning. Although Brown corpus has more vocabularies, it doesn't win ACL+AZ."]}
{"question_id": "578add9d3dadf86cd0876d42b03bf0114f83d0e7", "predicted_answer": "", "predicted_evidence": ["We use the following features based on the tweet content:"]}
{"question_id": "4d5b74499804ea5bc5520beb88d0f9816f67205a", "predicted_answer": "", "predicted_evidence": [" INLINEFORM0 : Sentiment score of the tweet obtained using SentiWordNet, ranging from -1 (negative) to +1 (positive)"]}
{"question_id": "baec99756b80eec7c0234a08bc2855e6770bcaeb", "predicted_answer": "", "predicted_evidence": ["In total, we collected INLINEFORM0 tweets posted on blackmarket sites. Out of these, we removed non-English tweets and tweets with a length of less than two characters. Finally, we were left with INLINEFORM1 blackmarket tweets. Then, from the timelines of the authors of these tweets, we randomly sampled INLINEFORM2 genuine tweets that were not posted on these blackmarket sites during the same period. Both the blackmarket and genuine tweets were also inspected manually."]}
{"question_id": "46d051b8924ad0ef8cfba9c7b5b84707ee72f26a", "predicted_answer": "", "predicted_evidence": ["blackAs studied in BIBREF0 , there are two prevalent models of blackmarket services, namely premium and freemium. Premium services are only available upon payment from customers, whereas freemium services offer both paid and unpaid options. The unpaid services are available to the users when they contribute to the blackmarket by providing appraisals for other users' content. Here, we mainly concentrate on freemium services. The freemium services can be further divided into three categories: (i) social-share services (request customers to spread the content on social media), (ii) credit-based services (customers earn credits by providing appraisals, and can then use the credits earned to gain appraisals for their content), and (iii) auto-time retweet services (customers need to provide their Twitter access tokens, upon which their content is retweeted 10-20 times for each 15-minute window)."]}
{"question_id": "dae2f135e50d77867c3f57fc3cb0427b2443e126", "predicted_answer": "", "predicted_evidence": ["In the fine-tuning procedure, let us assume that we only have English training data for downstream NLG tasks. According to whether the target language is English, the directions of NLG can be categorized into two classes: any languages to non-English languages (Any-to-Others), and any languages to English (Any-to-English)."]}
{"question_id": "38055717edf833566d912f14137b92a1d9c4f65a", "predicted_answer": "", "predicted_evidence": ["As shown in Table TABREF41, we use the En-En-QG and Zh-Zh-QG tasks to analyze the effects of using different fine-tuning strategies. It can be observed that fine-tuning encoder parameters, our model obtain an impressive performance for both English and Chinese QG, which shows the strong cross-lingual transfer ability of our model. When fine-tuning all the parameters, the model achieves the best score for English QG, but it suffers a performance drop when evaluating on Chinese QG. We find that fine-tuning decoder hurts cross-lingual decoding, and the model learns to only decodes English words. For only fine-tuning decoder, the performance degrades by a large margin for both languages because of the underfitting issue, which indicates the necessity of fine-tuning encoder."]}
{"question_id": "b6aa5665c981e3b582db4760759217e2979d5626", "predicted_answer": "", "predicted_evidence": ["As shown in Table TABREF41, we use the En-En-QG and Zh-Zh-QG tasks to analyze the effects of using different fine-tuning strategies. It can be observed that fine-tuning encoder parameters, our model obtain an impressive performance for both English and Chinese QG, which shows the strong cross-lingual transfer ability of our model. When fine-tuning all the parameters, the model achieves the best score for English QG, but it suffers a performance drop when evaluating on Chinese QG. We find that fine-tuning decoder hurts cross-lingual decoding, and the model learns to only decodes English words. For only fine-tuning decoder, the performance degrades by a large margin for both languages because of the underfitting issue, which indicates the necessity of fine-tuning encoder."]}
{"question_id": "c0355afc7871bf2e12260592873ffdb5c0c4c919", "predicted_answer": "", "predicted_evidence": ["We evaluate models with BLEU-4 (BL-4), ROUGE (RG) and METEOR (MTR) metrics. As shown in Table TABREF16, our model outperforms the baselines, which demonstrates that our pre-trained model provides a good initialization for NLG."]}
{"question_id": "afeceee343360d3fe715f405dac7760d9a6754a7", "predicted_answer": "", "predicted_evidence": ["We evaluate a number of baselines:"]}
{"question_id": "cc3dd701f3a674618de95a4196e9c7f4c8fbf1e5", "predicted_answer": "", "predicted_evidence": ["We evaluate a number of baselines:"]}
{"question_id": "d66550f65484696c1284903708b87809ea705786", "predicted_answer": "", "predicted_evidence": ["We evaluate a number of baselines:"]}
{"question_id": "29ba93bcd99c2323d04d4692d3672967cca4915e", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF27 shows the values of the fusion gates for an example story, averaged at each timestep. The pretrained seq2seq model acts similarly to a language model producing common words and punctuation. The second seq2seq model learns to focus on rare words, such as horned and robe."]}
{"question_id": "804bf5adc6dc5dd52f8079cf041ed3a710e03f8a", "predicted_answer": "", "predicted_evidence": ["The pretrained seq2seq model is the model in Section SECREF37 . The additional fused model has the following architecture:"]}
{"question_id": "f2dba5bf75967407cce5d0a9c2618269225081f5", "predicted_answer": "", "predicted_evidence": ["We collect a hierarchical story generation dataset from Reddit's WritingPrompts forum. WritingPrompts is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond. Each prompt can have multiple story responses. The prompts have a large diversity of topic, length, and detail. The stories must be at least 30 words, avoid general profanity and inappropriate content, and should be inspired by the prompt (but do not necessarily have to fulfill every requirement). Figure FIGREF1 shows an example."]}
{"question_id": "b783ec5cb9ad595da7db2c0ddf871152ae382c5f", "predicted_answer": "", "predicted_evidence": ["We collect a hierarchical story generation dataset from Reddit's WritingPrompts forum. WritingPrompts is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond. Each prompt can have multiple story responses. The prompts have a large diversity of topic, length, and detail. The stories must be at least 30 words, avoid general profanity and inappropriate content, and should be inspired by the prompt (but do not necessarily have to fulfill every requirement). Figure FIGREF1 shows an example."]}
{"question_id": "3eb107f35f4f5f5f527a93ffb487aa2e3fe51efd", "predicted_answer": "", "predicted_evidence": ["This section describes two experiments: i) compare our model against recent systems; ii) evaluate the efficiency of using multiple pre-trained word embeddings."]}
{"question_id": "47d54a6dd50cab8dab64bfa1f9a1947a8190080c", "predicted_answer": "", "predicted_evidence": ["Convolutional filters: we used 1600 filters. It is also the dimension of the word embedding concatenated from the five pre-trained word embeddings."]}
{"question_id": "67cb001f8ca122ea859724804b41529fea5faeef", "predicted_answer": "", "predicted_evidence": ["Our main contributions are as follows:"]}
{"question_id": "42eb7c5311fc1ac0344f0b38d3184ccd4faad3be", "predicted_answer": "", "predicted_evidence": ["Multiset mention overlap: Let $\\hat{M}_a$ be the multiset of all accounts mentioned by author $a$ (with repeats for each mention), and let $\\hat{M}_t$ be the multiset of all accounts mentioned by target $t$. We measure $\\frac{|\\hat{M}_a \\cap ^{*} \\hat{M}_t|}{|\\hat{M}_a \\cup \\hat{M}_t|}$ where $\\cap ^{*}$ takes the multiplicity of each element to be the sum of the multiplicity from $\\hat{M}_a $ and the multiplicity from $\\hat{M}_b$"]}
{"question_id": "8d14dd9c67d71494b4468000ff9683afdd11af7e", "predicted_answer": "", "predicted_evidence": ["We successfully recruited 170 workers to label all 6,897 available threads in our dataset. They labeled an average of 121.7 threads and a median of 7 threads each. They spent an average time of 3 minutes 50 seconds, and a median time of 61 seconds per thread. For each thread, we collected annotations from three different workers, and from this data we computed our reliability metrics using Fleiss's Kappa for inter-annotator agreement as shown in Table TABREF17."]}
{"question_id": "b857f3e3f1dad5df55f69d062978967fe023ac6f", "predicted_answer": "", "predicted_evidence": ["We successfully recruited 170 workers to label all 6,897 available threads in our dataset. They labeled an average of 121.7 threads and a median of 7 threads each. They spent an average time of 3 minutes 50 seconds, and a median time of 61 seconds per thread. For each thread, we collected annotations from three different workers, and from this data we computed our reliability metrics using Fleiss's Kappa for inter-annotator agreement as shown in Table TABREF17."]}
{"question_id": "5a473f86052cf7781dfe40943ddf99bc9fe8a4e4", "predicted_answer": "", "predicted_evidence": ["Downward overlap measures the number of two-hop paths from the author to the target along following relationships; upward overlap measures two-hop paths in the opposite direction. Inward overlap measures the similarity between the two users' follower sets, and outward overlap measures the similarity between their sets of friends. Bidirectional overlap then is a more generalized measure of social network similarity. We provide a graphical depiction for each of these features on the right side of Figure FIGREF18."]}
{"question_id": "235c7c7ca719068136928b18e19f9661e0f72806", "predicted_answer": "", "predicted_evidence": ["For instructional purposes, we provided five sample threads to demonstrate both positive and negative examples for each of the five criteria. Two of these threads are shown here. The thread in Figure FIGREF18 displays bullying behavior that is targeted against the green user, with all five cyberbullying criteria displayed. The thread includes repeated use of aggressive language such as \u201cshe really fucking tried\u201d and \u201cshe knows she lost.\u201d The bully's harmful intent is evident in the victim's defensive responses. And lastly, the thread is visible among four peers as three gang up against one, creating a power imbalance."]}
{"question_id": "c87966e7f497975b76a60f6be50c33d296a4a4e7", "predicted_answer": "", "predicted_evidence": ["Some researchers view cyberbullying as an extension of more \u201ctraditional\u201d bullying behaviors BIBREF16, BIBREF17, BIBREF18. In one widely-cited book, the psychologist Dan Olweus defines schoolyard bullying in terms of three criteria: repetition, harmful intent, and an imbalance of power BIBREF19. He then identifies bullies by their intention to \u201cinflict injury or discomfort\u201d upon a weaker victim through repeated acts of aggression."]}
{"question_id": "c9eae337edea0edb12030a7d4b01c3a3c73c16d3", "predicted_answer": "", "predicted_evidence": ["To evaluate the quality of the generated jokes, quotes, or tweets we rely on human judgment as there is no proven system to measure the quality of content objectively."]}
{"question_id": "9f1d81b2a6fe6835042a5229690e1951b97ff671", "predicted_answer": "", "predicted_evidence": ["The first experiment was training the model with just jokes."]}
{"question_id": "fae930129c2638ba6f9c9b3383e85aa130a73876", "predicted_answer": "", "predicted_evidence": ["In the above example we can see, out of total possible 261 linkages 100 linkages were without any p.p (post processing) error. We can also see the parser identifies the parts of speech of the words and the syntactic parsing is correct."]}
{"question_id": "1acfbdc34669cf19a778aceca941543f11b9a861", "predicted_answer": "", "predicted_evidence": ["We illustrate our proposed model in Figure FIGREF1 where embedding size: INLINEFORM0 , the number of filters: INLINEFORM1 , the number of neurons within the capsules in the first layer is equal to INLINEFORM2 , and the number of neurons within the capsule in the second layer: INLINEFORM3 . The length of the vector output INLINEFORM4 is used as the score for the input triple."]}
{"question_id": "864295caceb1e15144c1746ab5671d085d7ff7a1", "predicted_answer": "", "predicted_evidence": [" INLINEFORM0 We evaluate our CapsE for knowledge graph completion on two benchmark datasets WN18RR BIBREF17 and FB15k-237 BIBREF18 . CapsE obtains the best mean rank on WN18RR and the highest mean reciprocal rank and highest Hits@10 on FB15k-237."]}
{"question_id": "79e61134a6e29141cd19252571ffc92a0b4bc97f", "predicted_answer": "", "predicted_evidence": ["Under the strategy finetune-only, we use only single BERT.In order to adapt to different tasks, we will add a fully connected layer upon BERT. In the sequence labeling task, the BERT word embedding of each word passes through two fully connected layers, and the prediction probability of named entity can be obtained. In the next two verification tasks, we use \u201c[CLS]\u201d for prediction and add two fully connected layers subsequently. Under our strategy stack-and-finetune, we set different learning rates for the two phases. We tried to set the learning rate of the first stage to INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 and INLINEFORM4 , and set it to a smaller number in the latter stage, such as INLINEFORM5 , INLINEFORM6 , INLINEFORM7 and INLINEFORM8 . After our experiments, we found that it gets better results while the learning rate is set to 0.001 in the stage of training only the upper model and set to INLINEFORM9 in the later stage. Since BERT-Adam BIBREF1 has excellent performance, in our experiments, we use it as an optimizer with INLINEFORM10 , INLINEFORM11 -weight decay of INLINEFORM12 .We apply a dropout trick on all layers and set the dropout probability as 0.1."]}
{"question_id": "18fbfb1f88c5487f739aceffd23210a7d4057145", "predicted_answer": "", "predicted_evidence": ["We find the ensembled model enjoys a 0.72% improvements compared to the fine-tune only model and 0.005 improvement for the F1 score."]}
{"question_id": "5d3e87937ecebf0695bece08eccefb2f88ad4a0f", "predicted_answer": "", "predicted_evidence": ["We find the ensembled model enjoys a 0.72% improvements compared to the fine-tune only model and 0.005 improvement for the F1 score."]}
{"question_id": "7d539258b948cd5b5ad1230a15e4b739f29ed947", "predicted_answer": "", "predicted_evidence": ["tab:iaa-results shows raw agreement and Cohen's kappa across three annotators computed by averaging three pairwise comparisons. Agreement levels on scene role, function, and full construal are high for both phases, attesting to the validity of the annotation framework in Chinese. However, there is a slight decrease from Phase 1 to Phase 2, possibly due to the seven newly attested adpositions in Phase 2 and the 1-year interval between the two annotation phases."]}
{"question_id": "9c1f70affc87024b4280f0876839309b8dddd579", "predicted_answer": "", "predicted_evidence": ["Though parsing is not essential to this annotation project, we ran the StanfordNLP BIBREF40 dependency parser to obtain POS tags and dependency trees. These are stored alongside supersense annotations in the CoNLL-U-Lex format BIBREF41, BIBREF0. CoNLL-U-Lex extends the CoNLL-U format used by the Universal Dependencies BIBREF42 project to add additional columns for lexical semantic annotations."]}
{"question_id": "2694a679a703ccd6139897e4d9ff8e053dabd0f2", "predicted_answer": "", "predicted_evidence": ["There are a few observations in these distributions that are of particular interest. For some of the examples, we use an annotated subset of the English Little Prince corpus for qualitative comparisons, whereas all quantitative results in English refer to the larger STREUSLE corpus of English Web Treebank reviews BIBREF0."]}
{"question_id": "65c9aee2051ff7c47112b2aee0d928d9b6a8c2fe", "predicted_answer": "", "predicted_evidence": ["Ye Wang: Project Planning, DataSet Search, Code for TFIDF, PCA, Grid Search model running, ROC model running, Report Integration into Latex, Report Analysis of the Results (table creations), Report Analysis for the Outlier Removal, Random Forest, Report editing"]}
{"question_id": "f8264609a44f059b74168995ffee150182a0c14f", "predicted_answer": "", "predicted_evidence": ["https://www.cs.rutgers.edu/~mlittman/courses/ml03/iCML03/papers/ramos.pdf"]}
{"question_id": "c728fe6137f114c02e921f9be4a02a5bd83ae787", "predicted_answer": "", "predicted_evidence": ["In this section I present the extracted features partitioned in six groups and detail each of them separately."]}
{"question_id": "50bda708293532f07a3193aaea0519d433fcc040", "predicted_answer": "", "predicted_evidence": ["In order to capture this explicit ordering of INLINEFORM0 , the organisers proposed a cost measure that uses the confusion matrix of the prediction and prior knowledge in order to evaluate the performance of the system. In particular, the meaures uses writes as: DISPLAYFORM0 "]}
{"question_id": "46e660becd727c994a2a35c6587e15ea8bf8272d", "predicted_answer": "", "predicted_evidence": ["In this section I present the extracted features partitioned in six groups and detail each of them separately."]}
{"question_id": "d1a4529ea32aaab5ca3b9d9ae5c16f146c23af6b", "predicted_answer": "", "predicted_evidence": ["In this section I present the extracted features partitioned in six groups and detail each of them separately."]}
{"question_id": "7fba61426737394304e307cdc7537225f6253150", "predicted_answer": "", "predicted_evidence": ["While in terms of accuracy the system performed excellent achieving 98.2% in the test data, the question raised is whether there are any types of biases in the process. For instance, topic distributions learned with LDA were valuable features. One, however, needs to deeply investigate whether this is due to the expressiveness and modeling power of LDA or an artifact of the dataset used. In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics."]}
{"question_id": "46aa61557c8d20b1223a30366a0704d7af68bbbe", "predicted_answer": "", "predicted_evidence": ["Shorter sentence pairs are in general aligned correctly, irrespective of the score (compare examples with score $0.30$. $0.78$ and $1.57$, $2.44$ below). Longer sentences can include exact matches of longer substrings, however, they are scored based on a bag-of-words overlap (see the examples with scores $0.41$ and $0.84$ below)."]}
{"question_id": "b3b9d7c8722e8ec41cbbae40e68458485a5ba25c", "predicted_answer": "", "predicted_evidence": ["The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:"]}
{"question_id": "b569827ecd04ae8757dc3c9523ab97e3f47a6e00", "predicted_answer": "", "predicted_evidence": ["Each sentence in a bag can be regarded as independent individual and do not have any relationship with other sentences in the bag, which possibly leads to information loss among the multiple sentences in the bag when considering classification over bag level."]}
{"question_id": "0d42bd759c84cbf3a293ab58283a3d0d5e27d290", "predicted_answer": "", "predicted_evidence": ["Each sentence in a bag can be regarded as independent individual and do not have any relationship with other sentences in the bag, which possibly leads to information loss among the multiple sentences in the bag when considering classification over bag level."]}
{"question_id": "9f1e60ee86a5c46abe75b67ef369bf92a5090568", "predicted_answer": "", "predicted_evidence": ["Following previous works BIBREF3, BIBREF5, BIBREF6, BIBREF4, we use precision-recall (PR) curves, area under curve (AUC) and top-N precision (P@N) as metrics in our experiments on the held-out test set from the NYT dataset. To directly show the perfomance on one sentence bag, we also calculate the accuracy of classification (Acc.) on non-NA sentences."]}
{"question_id": "4dc4180127761e987c1043d5f8b94512bbe74d4f", "predicted_answer": "", "predicted_evidence": ["In addition to the above measures, we also found the following feature plane can improve the performance: DISPLAYFORM0 "]}
{"question_id": "420862798054f736128a6f0c4393c7f9cc648b40", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is the number of training pairs and the superscript INLINEFORM1 indicates the INLINEFORM2 -th sentence pair BIBREF10 ."]}
{"question_id": "ad8411edf11d3429c9bdd08b3e07ee671464d73c", "predicted_answer": "", "predicted_evidence": ["It's quite important to design a good topology for CNN to learn hidden features from heterogeneous feature planes. After several experiments, we found two topological graphs can be deployed in the architecture. Figure FIGREF20 and Figure FIGREF20 show the two CNN graphs. In Topology i@, we stack temporal convolution with kernel width as 1 and tanh activation on top of each feature plane. After that, we deploy another temporal convolution and tanh activation operation with kernel width as 2. In Topology ii@, however, we first stack temporal convolution and tanh activation with kernel width as 2. Then we deploy another temporal convolution and tanh activation operation with kernel width as 1. Experiment results demonstrate that the Topology i@ is slightly better than the Topology ii@. This conclusion is reasonable. The feature planes are heterogeneous. After conducting convolution and tanh activation transformation, it makes sense to compare values across different feature planes."]}
{"question_id": "11360385dff0a9d7b8f4b106ba2b7fe15ca90d7c", "predicted_answer": "", "predicted_evidence": ["Our primary system is the linear fusion of all the above six subsystems by BOSARIS Toolkit on SRE19 dev and eval BIBREF9. Before the fusion, each score is calibrated by PAV method (pav_calibrate_scores) on our development database. It is evaluated by the primary metric provided by NIST SRE 2019."]}
{"question_id": "875fbf4e5f93c3da63e28a233ce1d8405c7dfe63", "predicted_answer": "", "predicted_evidence": ["For the sake of clarity, the datasets notations are defined as in table 1 and the training data for the six subsystems are list in table 2, 3, and 4."]}
{"question_id": "56b66d19dbc5e605788166e168f36d25f5beb774", "predicted_answer": "", "predicted_evidence": ["Our primary system is the linear fusion of all the above six subsystems by BOSARIS Toolkit on SRE19 dev and eval BIBREF9. Before the fusion, each score is calibrated by PAV method (pav_calibrate_scores) on our development database. It is evaluated by the primary metric provided by NIST SRE 2019."]}
{"question_id": "2d924e888a92dc0b14cdb5584e73e87254c3d1ee", "predicted_answer": "", "predicted_evidence": ["As it was found before BIBREF14 , BIBREF16 , the addition of ngrams without accounting relations between their components considerably worsens the perplexity because of the vocabulary growth (for perplexity the less is the better) and practically does not change other automatic quality measures (Table 2)."]}
{"question_id": "3ed8ac1ba4df6609fa7de5077d83e820641edc5e", "predicted_answer": "", "predicted_evidence": ["Introducing information from domain-specific thesaurus EuroVoc led to improving the initial model without the additional assumption, which can be explained by the absence of general abstract words in such information-retrieval thesauri."]}
{"question_id": "e1ab241059ef1700738f885f051d724a7fcf283a", "predicted_answer": "", "predicted_evidence": ["The thesaurus topics seem to convey the contents in the most concentrated way. In the Syrian topic general word country is absent; instead of UN (United Nations), it contains word rebel, which is closer to the Syrian situation. In the Orthodox church topic, the unigram variant contains extra word year, relations of words Moscow and Kirill to other words in the topic can be inferred only from the encyclopedic knowledge."]}
{"question_id": "a4b77a20e067789691e0ab246bc5b11913d77ae1", "predicted_answer": "", "predicted_evidence": ["What constitutes hate speech and when does it differ from offensive language? No formal definition exists but there is a consensus that it is speech that targets disadvantaged social groups in a manner that is potentially harmful to them BIBREF0 , BIBREF1 . In the United States, hate speech is protected under the free speech provisions of the First Amendment, but it has been extensively debated in the legal sphere and with regards to speech codes on college campuses. In many countries, including the United Kingdom, Canada, and France, there are laws prohibiting hate speech, which tends to be defined as speech that targets minority groups in a way that could promote violence or social disorder. People convicted of using hate speech can often face large fines and even imprisonment. These laws extend to the internet and social media, leading many sites to create their own provisions against hate speech. Both Facebook and Twitter have responded to criticism for not doing enough to prevent hate speech on their sites by instituting policies to prohibit the use of their platforms for attacks on people based on characteristics like race, ethnicity, gender, and sexual orientation, or threats of violence towards others."]}
{"question_id": "ba39317e918b4386765f88e8c8ae99f9a098c935", "predicted_answer": "", "predicted_evidence": ["The best performing model has an overall precision 0.91, recall of 0.90, and F1 score of 0.90. Looking at Figure 1, however, we see that almost 40% of hate speech is misclassified: the precision and recall scores for the hate class are 0.44 and 0.61 respectively. Most of the misclassification occurs in the upper triangle of this matrix, suggesting that the model is biased towards classifying tweets as less hateful or offensive than the human coders. Far fewer tweets are classified as more offensive or hateful than their true category; approximately 5% of offensive and 2% of innocuous tweets have been erroneously classified as hate speech. To explore why these tweets have been misclassified we now look more closely at the tweets and their predicted classes."]}
{"question_id": "22c125c461f565f5437dac74bf19c2ef317bad86", "predicted_answer": "", "predicted_evidence": ["We first use a logistic regression with L1 regularization to reduce the dimensionality of the data. We then test a variety of models that have been used in prior work: logistic regression, na\u00efve Bayes, decision trees, random forests, and linear SVMs. We tested each model using 5-fold cross validation, holding out 10% of the sample for evaluation to help prevent over-fitting. After using a grid-search to iterate over the models and parameters we find that the Logistic Regression and Linear SVM tended to perform significantly better than other models. We decided to use a logistic regression with L2 regularization for the final model as it more readily allows us to examine the predicted probabilities of class membership and has performed well in previous papers BIBREF5 , BIBREF8 . We trained the final model using the entire dataset and used it to predict the label for each tweet. We use a one-versus-rest framework where a separate classifier is trained for each class and the class label with the highest predicted probability across all classifiers is assigned to each tweet. All modeling was performing using scikit-learn BIBREF12 ."]}
{"question_id": "4a91432abe3f54fcbdd00bb85dc0df95b16edf42", "predicted_answer": "", "predicted_evidence": ["We first use a logistic regression with L1 regularization to reduce the dimensionality of the data. We then test a variety of models that have been used in prior work: logistic regression, na\u00efve Bayes, decision trees, random forests, and linear SVMs. We tested each model using 5-fold cross validation, holding out 10% of the sample for evaluation to help prevent over-fitting. After using a grid-search to iterate over the models and parameters we find that the Logistic Regression and Linear SVM tended to perform significantly better than other models. We decided to use a logistic regression with L2 regularization for the final model as it more readily allows us to examine the predicted probabilities of class membership and has performed well in previous papers BIBREF5 , BIBREF8 . We trained the final model using the entire dataset and used it to predict the label for each tweet. We use a one-versus-rest framework where a separate classifier is trained for each class and the class label with the highest predicted probability across all classifiers is assigned to each tweet. All modeling was performing using scikit-learn BIBREF12 ."]}
{"question_id": "7c398615141ca416a32c9f72dbb785d3a6986a0f", "predicted_answer": "", "predicted_evidence": ["The central objective of our paper is, then, to determine how many of the last layers actually need fine-tuning. Why is this an important subject of study? Pragmatically, a reasonable cutoff point saves computational memory across fine-tuning multiple tasks, which bolsters the effectiveness of existing parameter-saving methods BIBREF5. Pedagogically, understanding the relationship between the number of fine-tuned layers and the resulting model quality may guide future works in modeling."]}
{"question_id": "441be93e2830cc0fc65afad6959db92754c9f5a8", "predicted_answer": "", "predicted_evidence": ["Within each variant, the two models display slight variability in parameter count\u2014110 and 125 million in the base variant, and 335 and 355 in the large one. These differences are mostly attributed to RoBERTa using many more embedding parameters\u2014exactly 63% more for both variants. For in-depth, layerwise statistics, see Table TABREF4."]}
{"question_id": "7f11f128fd39b8060f5810fa84102f000d94ea33", "predicted_answer": "", "predicted_evidence": ["Focusing on the results when smooth equals $0.01$ for SNLI and smooth equals $0.02$ for MultiNLI, we observe that the AUC of Hyp for all testing sets are approximately $0.5$, indicating Hyp's predictions are approximately equivalent to randomly guessing. Also, the gap between Hard and Easy for Norm significantly decreases comparing with the baseline. With the smooth, we can conclude that our method effectively alleviates the bias pattern."]}
{"question_id": "2a55076a66795793d79a3edfae1041098404fbc3", "predicted_answer": "", "predicted_evidence": ["Classifiers trained on NLI datasets are supposed to make predictions by understanding the semantic relationships between given sentence pairs. However, it is shown that models are unintentionally utilizing the annotation artifacts BIBREF4, BIBREF2. If the evaluation is conducted under a similar distribution as the training data, e.g., with the given testing set, models will enjoy additional advantages, making the evaluation results over-estimated. On the other hand, if the bias pattern cannot be generalized to the real-world, it may introduce noise to models, thus hurting the generalization ability."]}
{"question_id": "ecaa10a2d9927fa6ab6a954488f12aa6b42ddc1a", "predicted_answer": "", "predicted_evidence": ["Essentially speaking, the problem of the bias pattern is that the artifacts in hypotheses are distributed differently among labels, so balancing them across labels may be a good solution to alleviate the impacts BIBREF2."]}
{"question_id": "8b49423b7d1fa834128aa5038aa16c6ef3fdfa32", "predicted_answer": "", "predicted_evidence": ["We utilize SNLI BIBREF0, MultiNLI BIBREF1, JOCI BIBREF13 and SICK BIBREF14 for cross-dataset testing."]}
{"question_id": "0aca0a208a1e28857fab44e397dc7880e010dbca", "predicted_answer": "", "predicted_evidence": ["We also tested the annotation reliability (i.e., Fleiss\u2019 Kappa score) between using 3 workers vs. using 5 workers. The Fleiss\u2019 kappa score of 3 workers is 0.53 (95% CI [0.46, 0.61]. The Fleiss\u2019 kappa score of 5 workers is 0.56 (95% CI [0.51, 0.61]. Thus, using 3 workers vs. 5 workers does not make any difference on the annotation reliability, while it is obviously cheaper to use only 3 workers."]}
{"question_id": "471683ba6251b631f38a24d42b6dba6f52dee429", "predicted_answer": "", "predicted_evidence": ["Our data came from two different sources as shown in Table 1. First, we collected 2,803,164 tweets using the Twitter search API BIBREF27 from December 10, 2018 to December 26, 2018 base on a list of job loss-related keywords (n = 68). After filtering out duplicates and non-English tweets, 1,952,079 tweets were left. Second, we used the same list of keywords to identify relevant tweets from a database of historical random public tweets we collected from January 1, 2013 to December 30, 2017. We found 1,733,905 relevant tweets from this database. Due to the different mechanisms behind the two Twitter APIs (i.e., streaming API vs. search API), the volumes of the tweets from the two data sources were significantly different. For the Twitter search API, users can retrieve most of the public tweets related to the provided keywords within 10 to 14 days before the time of data collection; while the Twitter streaming API returns a random sample (i.e., roughly 1% to 20% varying across the years) of all public tweets at the time and covers a wide range of topics. After integrating the tweets from the two data sources, there were 3,685,984 unique tweets."]}
{"question_id": "5dfd58f91e7740899c23ebfe04b7176edce9ead2", "predicted_answer": "", "predicted_evidence": ["We used the IDN Tagged Corpus proposed in BIBREF11 . The corpus contains 10K sentences and 250K tokens that are tagged manually. Due to the small size, we used 5-fold cross-validation to split the corpus into training, development, and test sets. We did not split multi-word expressions but treated them as if they are a single token. All 5 folds of the dataset are available publicly to serve as a benchmark for future work."]}
{"question_id": "c09bceea67273c10a0621da1a83b409f53342fd9", "predicted_answer": "", "predicted_evidence": ["For the neural tagger, we set the size of the word, affix, and character embedding to 100, 20, and 30 respectively. We applied dropout regularization to the embedding layers. The max-pooled CNN has 30 filters for each filter width. We set the feedforward network and the biLSTM to have 100 hidden units. We put a dropout layer before the biLSTM input layer. We tuned the learning rate, dropout rate, context window size, and CNN filter width to the development set. As we said earlier, we experimented with different configurations in the embedding, encoding, and prediction step. We evaluated each configuration on the development set as well."]}
{"question_id": "732bd97ae34541f215c436e2a1b98db1649cba27", "predicted_answer": "", "predicted_evidence": ["Next, we present the result of evaluating the baselines and other comparisons on the test set in Table TABREF28 . The INLINEFORM0 scores are averaged over the 5 cross-validation folds. We see that Major baseline performs very poorly compared to the Memo baseline, which surprisingly achieves over 90 INLINEFORM1 points. This result suggests that Memo is a more suitable baseline for this dataset in contrast with Major. The result also provides evidence to the usefulness of our evaluation metric which heavily penalizes a simple majority vote model. Furthermore, we notice that the rule-based tagger by Rashel et al. BIBREF7 performs worse than Memo, indicating that Memo is not just suitable but also quite a strong baseline. Moving on, we observe how CRF has 6 points advantage over Memo, signaling that incorporating contextual features and modeling tag-to-tag transitions are useful. Lastly, the biLSTM with CRF tagger performs the best with 97.47 INLINEFORM2 score."]}
{"question_id": "183b385fb59ff1e3f658d4555a08b67c005a8734", "predicted_answer": "", "predicted_evidence": ["We used the IDN Tagged Corpus proposed in BIBREF11 . The corpus contains 10K sentences and 250K tokens that are tagged manually. Due to the small size, we used 5-fold cross-validation to split the corpus into training, development, and test sets. We did not split multi-word expressions but treated them as if they are a single token. All 5 folds of the dataset are available publicly to serve as a benchmark for future work."]}
{"question_id": "5f7f4a1d4380c118a58ed506c057d3b7aa234c1e", "predicted_answer": "", "predicted_evidence": ["Word embeddings are fixed-length vector representations for words BIBREF0 , BIBREF1 . In recent years, the morphology of words is drawing more and more attention BIBREF2 , especially for Chinese whose writing system is based on logograms."]}
{"question_id": "a79a23573d74ec62cbed5d5457a51419a66f6296", "predicted_answer": "", "predicted_evidence": ["We choose adagrad BIBREF23 as our optimizing algorithm, and we set the batch size as 4,096 and learning rate as 0.05. In practice, the slide window size INLINEFORM0 of stroke INLINEFORM1 -grams is set as INLINEFORM2 . The dimension of all word embeddings of different models is consistently set as 300. We use two test tasks to evaluate the performance of different models: one is word similarity, and the other is word analogy. A word similarity test consists of multiple word pairs and similarity scores annotated by humans. Good word representations should make the calculated similarity have a high rank correlation with human annotated scores, which is usually measured by the Spearman's correlation INLINEFORM3 BIBREF24 ."]}
{"question_id": "d427e9d181434078c78b7ee33a26b269f160f6d2", "predicted_answer": "", "predicted_evidence": ["Moreover, Chinese is a language originated from Oracle Bone Inscriptions (a kind of hieroglyphics). Its character glyphs have a spatial structure similar to graphs which can convey abundant semantics BIBREF4 . Additionally, the critical reason why Chinese characters are so rich in morphological information is that they are composed of basic strokes in a 2-D spatial order. However, different spatial configurations of strokes may lead to different semantics. As shown in the lower half of Figure 1, three Chinese characters \u201c\u5165\" (enter), \u201c\u516b\" (eight) and \u201c\u4eba\" (man) share exactly a common stroke sequence, but they have completely different semantics because of their different spatial configurations."]}
{"question_id": "0a5fd0e5f4ab12be57be20416a5ea7c3db5fb662", "predicted_answer": "", "predicted_evidence": ["What can be concluded is that the model did not have a flexibility in OOV words. Of course, this can also be an advantage, meaning that the model recognized the mismatch of a wrong word with its class."]}
{"question_id": "5d03a82a70f7b1ab9829891403ec31607828cbd5", "predicted_answer": "", "predicted_evidence": ["The main area of study for the experiments focuses on three important components. At first, we investigate the difference in results between part of speech taggers that classify morphological features and taggers that detect only the part of speech. Moreover, we explore the significance of pretrained vectors used from a model and their effect on the extraction of better results. Most importantly, the usage of subwords of tokens from a tagger as embeddings is issued. For the experiments, precision, recall and f1 score are used as evaluation metrics."]}
{"question_id": "6cad6f074b0486210ffa4982c8d1632f5aa91d91", "predicted_answer": "", "predicted_evidence": ["What can be concluded is that the model did not have a flexibility in OOV words. Of course, this can also be an advantage, meaning that the model recognized the mismatch of a wrong word with its class."]}
{"question_id": "d38b3e0896b105d171e69ce34c689e4a7e934522", "predicted_answer": "", "predicted_evidence": ["Different labels were found at the dataset and were matched to a label map, where for each label the part of the speech and their morphology are analyzed. In more detail, the first two characters refer to the part of speech and accordingly extend to more information about it. The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case BIBREF6. It must be mentioned that the extraction of morphological rules and the matching with the tags was done using the Greek version of the Universal Dependencies BIBREF7."]}
{"question_id": "4379a3ece3fdb93b71db43f62833f5f724c49842", "predicted_answer": "", "predicted_evidence": ["To identify bots in health-related social media data, we retrieved a sample of $10,417$ users from a database containing more than 400 million publicly available tweets posted by more than $100,000$ users who have announced their pregnancy on Twitter BIBREF4. This sample is based on related work for detecting users who have mentioned various pregnancy outcomes in their tweets. Two professional annotators manually categorized the $10,417$ users as \"bot,\" \"non-bot,\" or \"unavailable,\" based on their publicly available Twitter sites. Users were annotated broadly as \"bot\" if, in contrast to users annotated as \"non-bot,\" they do not appear to be posting personal information. Users were annotated as \"unavailable\" if their Twitter sites could not be viewed at the time of annotation, due to modifying their privacy settings or being removed or suspended from Twitter. Based on 1000 overlapping annotations, their inter-annotator agreement (IAA) was $\\kappa $ = $0.93$ (Cohen\u2019s kappa BIBREF21), considered \"almost perfect agreement\" BIBREF22. Their IAA does not include disagreements resulting from the change of a user's status to or from \"unavailable\" in the time between the first and second annotations. Upon resolving the disagreements, 413 $(4\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\%)$ as \"unavailable\"."]}
{"question_id": "0abc2499195185c94837e0340d00cd3b83ee795e", "predicted_answer": "", "predicted_evidence": ["Tweet Diversity. Considering that \"bot\" users may re-post the same tweets, we used the ratio of a user's unique tweets to the total number of tweets posted by the user, where 0 indicates that the user has posted only the same tweet multiple times, and 1 indicates that each tweet is unique and has been posted only once. As Figure 1 illustrates, a subset of \"bot\" users (in the training set) have posted more of the same tweets than \"non-bot\" users."]}
{"question_id": "138ad61b43c85d5db166ea9bd3d3b19bb2e2bbfb", "predicted_answer": "", "predicted_evidence": ["The objectives of this study are to (i) evaluate an existing bot detection system on user-level datasets selected for their health-related content, and (ii) extend the bot detection system for effective application within the health realm. Bot detection approaches have been published in the past few years, but most of the code and data necessary for reproducing the published results were not made available BIBREF17, BIBREF18, BIBREF19. The only system for which we found both operational code and data available, Botometer BIBREF20 (formerly BotOrNot), was chosen as the benchmark system for this study. To the best of our knowledge, this paper presents the first study on health-related bot detection. We have made the classification code and training set of annotated users available at (we will provide a URL with the camera-ready version of the paper)."]}
{"question_id": "7e906dc00e92088a25df3719104d1750e5a27485", "predicted_answer": "", "predicted_evidence": ["In recent years, social media has evolved into an important source of information for various types of health-related research. Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth. Twitter, for example, has 330 million monthly active users worldwide that generate almost 500 million micro-blogs (tweets) per day. For some years, the use of the platform to share personal health information has been growing, particularly amongst people living with one or more chronic conditions and those living with disability. Twenty percent of social network site users living with chronic conditions gather and share health information on the sites, compared with 12% of social network site users who report no chronic conditions. Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. Twitter is particularly popular in research due to the availability of the public streaming API, which releases a sample of publicly posted data in real time. While early health-related research from social media focused almost exclusively on population-level studies, some very recent research tasks have focused on performing longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women BIBREF4."]}
{"question_id": "0d9241e904bd2bbf5b9a6ed7b5fc929651d3e28e", "predicted_answer": "", "predicted_evidence": ["The baseline correctly picks 12.7% of all candidate responses, out of 100. Given that the dataset is focussed on support questions and multiple responses are likely to be relevant, this baseline already performs admirable. For reference, a BERT model on the OpenSubtitles dataset BIBREF22 achieves a 1-of-100 accuracy between 12.2% and 17.5%, depending on the model size BIBREF26."]}
{"question_id": "95646d0ac798dcfc15b43fa97a1908df9f7b9681", "predicted_answer": "", "predicted_evidence": ["The baseline correctly picks 12.7% of all candidate responses, out of 100. Given that the dataset is focussed on support questions and multiple responses are likely to be relevant, this baseline already performs admirable. For reference, a BERT model on the OpenSubtitles dataset BIBREF22 achieves a 1-of-100 accuracy between 12.2% and 17.5%, depending on the model size BIBREF26."]}
{"question_id": "12dc04e0ec1d3ba5ec543069fe457dfa4a1cac07", "predicted_answer": "", "predicted_evidence": ["The baseline correctly picks 12.7% of all candidate responses, out of 100. Given that the dataset is focussed on support questions and multiple responses are likely to be relevant, this baseline already performs admirable. For reference, a BERT model on the OpenSubtitles dataset BIBREF22 achieves a 1-of-100 accuracy between 12.2% and 17.5%, depending on the model size BIBREF26."]}
{"question_id": "647f6e6b168ec38fcdb737d3b276f78402282f9d", "predicted_answer": "", "predicted_evidence": ["Take for example the Ubuntu Dialog Corpus by BIBREF3, a commonly used corpus for multi-turn systems. This dataset was collected from an Internet Relay Chat (IRC) room casually discussing the operating system Ubuntu. IRC nodes usually support the ASCII text encoding, so there's no support for graphical emoji. However, in the 7,189,051 utterances, there are only 9946 happy emoticons (i.e. :-) and the cruelly denosed :) version) and 2125 sad emoticons."]}
{"question_id": "04796aaa59eeb2176339c0651838670fd916074d", "predicted_answer": "", "predicted_evidence": ["The results of the LGBM classifier for the different boosting frameworks and the scores from the base classifiers are illustrated in Table TABREF14 . The highest average ROC_AUC score of 0.9998 is obtained in the case of combining the two base learners along with the TF-IDF and QIEF features."]}
{"question_id": "ebb33f3871b8c2ffd2c451bc06480263b8e870e0", "predicted_answer": "", "predicted_evidence": ["Deep neural network models have increased in popularity in the field of NLP. They have pushed the state of the art of text representation and information retrieval. More specifically, these techniques enhanced NLP algorithms through the use of contextualized text embeddings at word, sentence, and paragraph levels BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 ."]}
{"question_id": "afd1c482c311e25fc42b9dd59cdc32ac542f5752", "predicted_answer": "", "predicted_evidence": ["The present work resulted in the creation of a PIO elements dataset, PICONET, and a classification tool. These constitute an important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated."]}
{"question_id": "ae1c4f9e33d0cd64d9a313c318ad635620303cdd", "predicted_answer": "", "predicted_evidence": ["Since our goal was to collect sequences that are uniquely representative of a description of Population, Intervention, and Outcome, we avoided a keyword-based approach such as in BIBREF12 . For example, using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label. Thus, we were able to extract portions of abstracts pertaining to P, I, and O categories while minimizing ambiguity and redundancy. Moreover, in the dataset from BIBREF12 , a section labeled as P that contained more than one sentence would be split into multiple P sentences to be included in the dataset. We avoided this approach and kept the full abstract sections. The full abstracts were kept in conjunction with our belief that keeping the full section retains more feature-rich sequences for each sequence, and that individual sentences from long abstract sections can be poor candidates for the corresponding label."]}
{"question_id": "7066f33c373115b1ead905fe70a1e966f77ebeee", "predicted_answer": "", "predicted_evidence": ["In this study, we introduce PICONET, a multi-label dataset consisting of sequences with labels Population/Problem (P), Intervention (I), and Outcome (O). This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories. The present approach is an improvement over a similar approach used in BIBREF12 ."]}
{"question_id": "018b81f810a39b3f437a85573d24531efccd835f", "predicted_answer": "", "predicted_evidence": ["For sections with labels such as population and intervention, we created a mutli-label. We also included negative examples by taking sentences from sections with headings such as aim. Furthermore, we cleaned the remaining data with various approaches including, but not limited to, language identification, removal of missing values, cleaning unicode characters, and filtering for sequences between 5 and 200 words, inclusive."]}
{"question_id": "e2c8d7f3ef5913582503e50244ca7158d0a62c42", "predicted_answer": "", "predicted_evidence": ["In figure FIGREF14, we see BERT's performance for each language. BERT performs well for the majority of languages, although some fare much worse than others. It is important to note that it is an unfair comparison because even though the datasets were curated using the same methodology, each language's dataset is different. It is possible, for example, that the examples we have for Basque are simply harder than they are for Portuguese."]}
{"question_id": "654fe0109502f2ed2dc8dad359dbbce4393e03dc", "predicted_answer": "", "predicted_evidence": ["The morphosyntactic feature in the agreement relation in (UNKREF2) is number, a feature that is cross-linguistically common in agreement systems. In addition to number, the most commonly involved in agreement relations are gender, case and person BIBREF22."]}
{"question_id": "da21bcaa8e3a9eadc8a5194fd57ae797e93c3049", "predicted_answer": "", "predicted_evidence": ["Datasets and Models We evaluate our adversarial attacks on different text classification datasets from tasks such as sentiment classification, subjectivity detection and question type classification. Amazon, Yelp, IMDB are sentence-level sentiment classification datasets which have been used in recent work BIBREF15 while MR BIBREF16 contains movie reviews based on sentiment polarity. MPQA BIBREF17 is a dataset for opinion polarity detection, Subj BIBREF18 for classifying a sentence as subjective or objective and TREC BIBREF19 is a dataset for question type classification."]}
{"question_id": "363a24ecb8ab45215134935e7e8165fff72ff90f", "predicted_answer": "", "predicted_evidence": ["of similarity)."]}
{"question_id": "74396ead9f88a9efc7626240ce128582ab69ef2b", "predicted_answer": "", "predicted_evidence": ["However, combining all of that same Twitter data with a much smaller amount of Amazon data (3998 Twitter training instances relative to 1003 Amazon training instances) and applying EasyAdapt to the combined dataset performed quite well ( INLINEFORM0 =0.780). The classifier was able to take advantage of a wealth of additional Twitter samples that had led to terrible performance on their own ( INLINEFORM1 =0.276). Thus, the high performance demonstrated when the EasyAdapt algorithm is applied to the training data from the two domains is particularly impressive. It shows that more data is indeed better data\u2014provided that the proper features are selected and the classifier is properly guided in handling it."]}
{"question_id": "8a7a9d205014c42cb0e24a0f3f38de2176fe74c0", "predicted_answer": "", "predicted_evidence": ["Finally, we include the best results reported by BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 on the same Amazon dataset. For a more direct comparison between our work and theirs, we also report the results from using all of our features under the same classification conditions as theirs (10-fold cross-validation using scikit-learn's Logistic Regression, tuning with an F1 objective). We refer to the latter case as Our Results, Same Classifier as Prior Best."]}
{"question_id": "eaed0b721cc3137b964f5265c7ecf76f565053e9", "predicted_answer": "", "predicted_evidence": ["The features used for each train/test scenario are shown in the first column of Table TABREF18 . Twitter Features refers to all features listed in Table TABREF11 preceded by the parenthetical (T), and Amazon Features to all features preceded by (A). General: Other Polarity includes the positive and negative percentages, average polarities, overall polarities, and largest polarity gap features from Table TABREF14 . General: Subjectivity includes the % strongly subjective positive words, % weakly subjective positive words, and their negative counterparts. We also include two baselines: the All Sarcasm case assumes that every instance is sarcastic, and the Random case randomly assigns each instance as sarcastic or non-sarcastic."]}
{"question_id": "ba7fea78b0b888a714cb7d89944b69c5038a1ef1", "predicted_answer": "", "predicted_evidence": ["Each model was tested on the Amazon test data (the model trained only on Twitter was also tested on the Twitter test set). Amazon reviews were selected as the target domain since the Twitter dataset was much larger than the Amazon dataset; this scenario is more consistent with the typically stated goal of domain adaptation (a large labeled out-of-domain source dataset and a small amount of labeled data in the target domain), and most clearly highlights the need for a domain-general approach. [6]Part-of-speech is considered in MPQA; Amazon and Twitter data was tagged using Stanford CoreNLP BIBREF20 and the Twitter POS-tagger BIBREF21 , respectively."]}
{"question_id": "38af3f25c36c3725a31304ab96e2c200c55792b4", "predicted_answer": "", "predicted_evidence": ["Finally, we include the best results reported by BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 on the same Amazon dataset. For a more direct comparison between our work and theirs, we also report the results from using all of our features under the same classification conditions as theirs (10-fold cross-validation using scikit-learn's Logistic Regression, tuning with an F1 objective). We refer to the latter case as Our Results, Same Classifier as Prior Best."]}
{"question_id": "9465d96a1368299fd3662d91aa94ba85347b4ccd", "predicted_answer": "", "predicted_evidence": ["Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features."]}
{"question_id": "e8c3f59313df20db0cdd49b84a37c44da849fe17", "predicted_answer": "", "predicted_evidence": ["Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features."]}
{"question_id": "f61268905626c0b2a715282478a5e373adda516c", "predicted_answer": "", "predicted_evidence": ["The performance of individual classifiers for offensive language identification with TF/IDF unigram features is demonstrated in table TABREF8 below. We can see that both linear classifiers (SVM and SGDC) outperform the other classifiers in terms of macro-F1, which does not take label imbalance into account. The Linear SVM and SGDC perform almost identically, with the Linear SVM performing slightly better in recall score for the Not Offensive class and SGDC in recall score for the Offensive class. Bernoulli Na\u00efve Bayes performs better than all classifiers in recall score for the Offensive class but yields the lowest precision score of all classifiers. While the RBF SVM and Multinomial Na\u00efve Bayes yield better recall score for the Not Offensive class, their recall scores for the Offensive class are really low. For a binary text classification task like offensive language detection, a high recall score for both classes, especially for the Offensive class, is important for a model to be considered successful. Thus, the Linear SVM can be considered the marginally best model trained with OGTD, as its weighted average precision and recall scores are higher."]}
{"question_id": "d9949dd4865e79c53284932d868ca8fd10d55e70", "predicted_answer": "", "predicted_evidence": ["We collected a set of 49,154 tweets. URLs, Emojis and Emoticons were removed, while usernames and user mentions were filtered as @USER following the same methodology described in OLID BIBREF5. Duplicate punctuation such as question and exclamation marks was normalized. After removing duplicate tweets, the dataset was comprised of 46,218 tweets of which 5,000 were randomly sampled for annotation. We used LightTag to annotate the dataset due to its simple and straightforward user interface and limitless annotations, provided by the software creators."]}
{"question_id": "de689a17b0b9fb6bbb80e9b85fb44b36b56de2fd", "predicted_answer": "", "predicted_evidence": ["We collected a set of 49,154 tweets. URLs, Emojis and Emoticons were removed, while usernames and user mentions were filtered as @USER following the same methodology described in OLID BIBREF5. Duplicate punctuation such as question and exclamation marks was normalized. After removing duplicate tweets, the dataset was comprised of 46,218 tweets of which 5,000 were randomly sampled for annotation. We used LightTag to annotate the dataset due to its simple and straightforward user interface and limitless annotations, provided by the software creators."]}
{"question_id": "5a90871856beeefaa69a1080e1b3c8b5d4b2b937", "predicted_answer": "", "predicted_evidence": ["The performance of individual classifiers for offensive language identification with TF/IDF unigram features is demonstrated in table TABREF8 below. We can see that both linear classifiers (SVM and SGDC) outperform the other classifiers in terms of macro-F1, which does not take label imbalance into account. The Linear SVM and SGDC perform almost identically, with the Linear SVM performing slightly better in recall score for the Not Offensive class and SGDC in recall score for the Offensive class. Bernoulli Na\u00efve Bayes performs better than all classifiers in recall score for the Offensive class but yields the lowest precision score of all classifiers. While the RBF SVM and Multinomial Na\u00efve Bayes yield better recall score for the Not Offensive class, their recall scores for the Offensive class are really low. For a binary text classification task like offensive language detection, a high recall score for both classes, especially for the Offensive class, is important for a model to be considered successful. Thus, the Linear SVM can be considered the marginally best model trained with OGTD, as its weighted average precision and recall scores are higher."]}
{"question_id": "6cb3007a09ab0f1602cdad20cc0437fbdd4d7f3e", "predicted_answer": "", "predicted_evidence": ["Every classical model was considered on the condition it could take matrices as input for fitting and was trained with the default settings because of the size of the dataset. Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features."]}
{"question_id": "211c242c028b35bb9cbd5e303bb6c750f859fd34", "predicted_answer": "", "predicted_evidence": ["The final Catalan corpus contains 567 annotated reviews and the final Basque corpus 343."]}
{"question_id": "9b05d5f723a8a452522907778a084b52e27fd924", "predicted_answer": "", "predicted_evidence": ["Statistics for the two corpora are shown in Table TABREF12 ."]}
{"question_id": "21175d8853fd906266f884bced85c598c35b1cbc", "predicted_answer": "", "predicted_evidence": ["Inter-annotator agreement is reported in Table TABREF17 ."]}
{"question_id": "87c00edc497274ae6a972c3097818de85b1b384f", "predicted_answer": "", "predicted_evidence": ["We begin with recognizing the main words (components) that play the most important roles in the sentence based on a given sentence structure. This is achieved by program $\\Pi _2$ (Listing ). The first four rules of $\\Pi _2$ determine the main subject and verb of the sentence whose structure is #1, #2, #3, or #5. Structure #4 requires a special treatment since the components following tobe can be of different forms. For instance, in \u201cCathy is gorgeous,\u201d the part after tobe is an adjective, but in \u201cCathy is a beautiful girl,\u201d the part after tobe is a noun, though, with adjective beautiful. This is done using the four last rules of $\\Pi _2$."]}
{"question_id": "de4e949c6917ff6933f5fa2a3062ba703aba014c", "predicted_answer": "", "predicted_evidence": ["GF has been used in a variety of applications, such as query-answering systems, voice communication, language learning, text analysis and translation, natural language generation BIBREF8, BIBREF9, automatic translation."]}
{"question_id": "4cf05da602669a4c09c91ff5a1baae6e30adefdf", "predicted_answer": "", "predicted_evidence": ["Since our language model is conditioned on a language vector, we can gain some intuitive understanding of the language space by generating text from different points in it. These points could be either one of the vectors learned during training, or some arbitrary other point. tab:interpolation shows text samples from different points along the line between Modern English [eng] and Middle English [enm]. Consistent with the results of Johnson2016zeroshot, it appears that the interesting region lies rather close to 0.5. Compare also to our fig:eng-deu, which shows that up until about a third of the way between English and German, the language model is nearly perfectly tuned to English."]}
{"question_id": "7380e62edcb11f728f6d617ee332dc8b5752b185", "predicted_answer": "", "predicted_evidence": ["Neural language models BIBREF0 , BIBREF1 , BIBREF2 have become an essential component in several areas of natural language processing (NLP), such as machine translation, speech recognition and image captioning. They have also become a common benchmarking application in machine learning research on recurrent neural networks (RNN), because producing an accurate probabilistic model of human language is a very challenging task which requires all levels of linguistic analysis, from pragmatics to phonology, to be taken into account."]}
{"question_id": "f37b01e0c366507308fca44c20d3f69621b94a6e", "predicted_answer": "", "predicted_evidence": ["We now take a look at the language vectors found during training with the full model of 990 languages. fig:germanic shows a hierarchical clustering of the subset of Germanic languages, which closely matches the established genetic relationships in this language family. While our experiments indicate that finding more remote relationships (say, connecting the Germanic languages to the Celtic) is difficult for the model, it is clear that the language vectors preserves similarity properties between languages."]}
{"question_id": "95af7aaea3ce9dab4cf64e2229ce9b98381dd050", "predicted_answer": "", "predicted_evidence": ["We are testing MagiCoder performances in the daily pharmacovigilance activities. Preliminary qualitative results show that MagiCoder drastically reduces the amount of work required for the revision of a report, allowing the pharmacovigilance stakeholders to provide high quality data about suspected ADRs."]}
{"question_id": "ab37ae82e38f64d3fa95782f2c791488f26cd43f", "predicted_answer": "", "predicted_evidence": ["Table TABREF58 shows the results of this first performance test. We group narrative descriptions by increasing length (in terms of characters). We note that reported results are computed considering terms at PT level. By moving to PT level, instead of using the LLT level, we group together terms that represent the same medical concept (i.e., the same adverse reaction). In this way, we do not consider an error when MagiCoder and the human expert use two different LLTs for representing the same adverse event. The use of the LLT level for reporting purpose and the PT level for analysis purpose is suggested also by MedDRA BIBREF5 . With common PT we mean the percentage of preferred terms retrieved by human reviewers that have been recognized also by MagiCoder. Reported performances are summarized also in FIGREF59 . Note that, false positive and false negative errors are required to be as small as possible, while common PT, recall, and precision have to be as large as possible."]}
{"question_id": "6c9b3b2f2e5aac1de1cbd916dc295515301ee2a2", "predicted_answer": "", "predicted_evidence": ["Let us now conclude this section by sketching the analysis of the computational complexity of MagiCoder."]}
{"question_id": "71413505d7d6579e2a453a1f09f4efd20197ab4b", "predicted_answer": "", "predicted_evidence": ["Let INLINEFORM0 be the input size (the length, in terms of words, of the narrative description). Let INLINEFORM1 be the cardinality of the dictionary (i.e., the number of terms). Moreover, let INLINEFORM2 be the number of distinct words occurring in the dictionary and let INLINEFORM3 be the length of the longest term in the dictionary. For MedDRA, we have about 75K terms ( INLINEFORM4 ) and 17K unique words ( INLINEFORM5 ). Notice that, reasonably, INLINEFORM6 is a small constant for any dictionary; in particular, for MedDRA we have INLINEFORM7 . We assume that all update operations on auxiliary data structures require constant time INLINEFORM8 ."]}
{"question_id": "3e6b6820e7843209495b4f9a72177573afaa4bc3", "predicted_answer": "", "predicted_evidence": ["Concurrently, it has been argued for mental health research that it would constitute a `valuable critical step' BIBREF10 to analyse first-hand accounts by individuals with lived experience of severe mental health issues in blog posts, tweets, and discussion forums. Several severe mental health difficulties, e.g., bipolar disorder (BD) and schizophrenia are considered as chronic and clinical recovery, defined as being relapse and symptom free for a sustained period of time BIBREF11 , is considered difficult to achieve BIBREF12 , BIBREF13 , BIBREF14 . Moreover, clinically recovered individuals often do not regain full social and educational/vocational functioning BIBREF15 , BIBREF16 . Therefore, research originating from initiatives by people with lived experience of mental health issues has been advocating emphasis on the individual's goals in recovery BIBREF17 , BIBREF18 . This movement gave rise to the concept of personal recovery BIBREF19 , BIBREF20 , loosely defined as a `way of living a satisfying, hopeful, and contributing life even with limitations caused by illness' BIBREF18 . The aspects of personal recovery have been conceptualised in various ways BIBREF21 , BIBREF22 , BIBREF23 . According to the frequently used CHIME model BIBREF24 , its main components are Connectedness, Hope and optimism, Identity, Meaning and purpose, and Empowerment. Here, we focus on BD, which is characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12 . Bipolar spectrum disorders were estimated to affect approximately 2% of the UK population BIBREF13 with rates ranging from 0.1%-4.4% across 11 other European, American and Asian countries BIBREF26 . Moreover, BD is associated with a high risk of suicide BIBREF27 , making its prevention and treatment important tasks for society. BD-specific personal recovery research is motivated by mainly two facts: First, the pole of positive/elevated mood and ongoing mood instability constitute core features of BD and pose special challenges compared to other mental health issues, such as unipolar depression BIBREF25 . Second, unlike for some other severe mental health difficulties, return to normal functioning is achievable given appropriate treatment BIBREF28 , BIBREF16 , BIBREF29 ."]}
{"question_id": "a926d71e6e58066d279d9f7dc3210cd43f410164", "predicted_answer": "", "predicted_evidence": ["Our research questions, which regard the experiences of different populations, lend themselves to several subprojects. First, we will collect and analyse English-language data from westerners. Then, we will address ethnically diverse English-speaking populations and finally multilingual accounts. This has the advantage that we can build data processing and methodological workflows along an increase in complexity of the data collection and analysis throughout the project."]}
{"question_id": "3d547a7dda18a2dd5dc89f12d25d7fe782d66450", "predicted_answer": "", "predicted_evidence": ["Linguistic Inquiry and Word Count (LIWC) BIBREF67 is a frequently used tool in social-science text analysis to analyse emotional and cognitive components of texts and derive features for classification models BIBREF47 , BIBREF46 , BIBREF68 , BIBREF69 . LIWC counts target words organised in a manually constructed hierarchical dictionary without contextual disambiguation in the texts under analysis and has been psychometrically validated and developed for English exclusively. While translations for several languages exist, e.g., Dutch BIBREF9 , and it is questionable to what extent LIWC concepts can be transferred to other languages and cultures by mere translation. We therefore aim to apply and develop methods that require less manual labour and are applicable to many languages and cultures. One option constitute unsupervised methods, such as topic modelling, which has been applied to explore cultural differences in mental-health related online data already BIBREF37 , BIBREF36 . The Differential Language Analysis ToolKit (DLATK) BIBREF70 facilitates social-scientific language analyses, including tools for preprocessing, such as emoticon-aware tokenisers, filtering according to meta data, and analysis, e.g. via robust topic modelling methods."]}
{"question_id": "4a32adb0d54da90434d5bd1c66cc03a7956d12a0", "predicted_answer": "", "predicted_evidence": ["We plan to collect data relevant for BD in general as well as for personal recovery in BD from three sources varying in their available amount versus depth of the accounts we expect to find: 1) Twitter, 2) Reddit (focusing on mental health-related content unlike previous work), 3) blogs authored by affected individuals. Twitter and Reddit users with a BD diagnosis will be identified automatically via self-reported diagnosis statements, such as `I was diagnosed with BD-I last week'. To do so, we will extend on the diagnosis patterns and terms for BD provided by BIBREF47 . Implicit consent is assumed from users on these platforms to use their public tweets and posts. SECREF3 Relevant blogs will be manually identified, and their authors will be contacted to obtain informed consent for using their texts."]}
{"question_id": "c17ece1dad42d92c78fca2e3d8afa9a20ff19598", "predicted_answer": "", "predicted_evidence": ["A substantial body of qualitative and quantitative research has shown the importance of personal recovery for individuals diagnosed with BD BIBREF22 , BIBREF25 , BIBREF30 , BIBREF31 , BIBREF23 . Qualitative evidence mainly comes from (semi-)structured interviews and focus groups and has been criticised for small numbers of participants BIBREF10 , lacking complementary quantitative evidence from larger samples BIBREF32 . Some quantitative evidence stems from the standardised bipolar recovery questionnaire BIBREF30 and a randomised control trial for recovery-focused cognitive-behavioural therapy BIBREF31 . Critically, previous research has taken place only in structured settings. What is more, the recovery concept emerged from research primarily conducted in English-speaking countries, mainly involving researchers and participants of Western ethnicity. This might have led to a lack of non-Western notions of wellbeing in the concept, such as those found in indigenous peoples BIBREF32 , limiting its the applicability to a general population. Indeed, the variation in BD prevalence rates from 0.1% in India to 4.4% in the US is striking. It has been shown that culture is an important factor in the diagnosis of BD BIBREF33 , as well as on the causes attributed to mental health difficulties in general and treatments considered appropriate BIBREF34 , BIBREF35 . While approaches to mental health classification from texts have long ignored the cultural dimension BIBREF36 , first studies show that online language of individuals affected by depression or related mental health difficulties differs significantly across cultures BIBREF37 , BIBREF36 ."]}
{"question_id": "c2ce25878a17760c79031a426b6f38931cd854b2", "predicted_answer": "", "predicted_evidence": ["After pre-processing, all the formatted poem samples will be sent to the poetry generation model for training, as illustrated in Figure 3."]}
{"question_id": "1d263356692ed8cdee2a13f103a82d98f43d66eb", "predicted_answer": "", "predicted_evidence": ["Chinese Classical poetry can be classified into two primary categories, SHI and CI. According to the statistical data from CCPC1.0, a Chinese Classical Poetry Corpus consisting of 834,902 poems in total (We believe it is almost a full collection of Chinese Classical poems). 92.87% poems in CCPC1.0 fall into the category of SHI and 7.13% fall into the category of CI. SHI and CI can be further divided into many different types in terms of their forms. We briefly introduce the related background knowledge as follows."]}
{"question_id": "68f1df3fb0703ff694a055d23e7ec3f6fb449b8d", "predicted_answer": "", "predicted_evidence": ["We use BLEU BIBREF26 as our evaluation metric. The performance of different systems are shown in Table TABREF34 and TABREF35 . On both the news and e-commerce domains, our system performs better than baseline systems."]}
{"question_id": "c7f43c95db3d0c870407cd0e7becdd802463683b", "predicted_answer": "", "predicted_evidence": ["Firstly, we briefly introduce the two architectures, i.e., skip-gram (SG) and continuous bag-of-words (CBOW) in Word2Vec BIBREF12. For a corpus with a word sequence $w_{1}, w_{2}, \\cdots , w_{T}$, skip-gram predicts the context word $w_{t+j}$ given the center word $w_t$, and maximizes the average log probability,"]}
{"question_id": "4e2b12cfc530a4682b06f8f5243bc9f64bd41135", "predicted_answer": "", "predicted_evidence": ["where $v_{w}$ and $v_{w}^{\\prime }$ are the vectors of the \u201cinput\u201d and \u201coutput\u201d words, and $|V|$ is the size of vocabulary."]}
{"question_id": "bc7081aaa207de2362e0bea7bc8108d338aee36f", "predicted_answer": "", "predicted_evidence": ["We follow the common practice and evaluate the performance of each model in terms of f-measure (F$_1$) at the top $N=10$ keyphrases, and apply stemming to reduce the number of mismatches. We also report the Mean Average Precision (MAP) scores of the ranked lists of keyphrases."]}
{"question_id": "c72e05dd41ed5a85335ffeca5a03e71514e60e84", "predicted_answer": "", "predicted_evidence": ["Although in this study we concentrate only on the textual content of the news articles, it is worth noting that the HTML pages also provide additional information that can be helpful in generating keyphrases such as text style properties (e.g. bold, italic), links to related articles, or news categorization (e.g. politics, science, technology)."]}
{"question_id": "07edc082eb86aecef3db5cad2534459c1310d6e8", "predicted_answer": "", "predicted_evidence": ["Position is a strong feature for keyphrase extraction, simply because texts are usually written so that the most important ideas go first BIBREF15. In news summarization for example, the lead baseline \u2013that is, the first sentences from the document\u2013, while incredibly simple, is still a competitive baseline BIBREF16. Similar to the lead baseline, we compute the FirstPhrases baseline that extracts the first $N$ keyphrase candidates from a document."]}
{"question_id": "eaacee4246f003d29a108fe857b5dd317287ecf1", "predicted_answer": "", "predicted_evidence": ["We train and evaluate several keyphrase generation models to understand the challenges of KPTimes and its usefulness for training models."]}
{"question_id": "3ea82a5ca495ffbd1e30e8655aef1be4ba423efe", "predicted_answer": "", "predicted_evidence": ["We explored the KPTimes dataset to better understand how it stands out from the existing ones. First, we looked at how editors tag news articles. Figure illustrates the difference between the annotation behaviour of readers, authors and editors through the number of times that each unique keyphrase is used in the gold standard. We see that non-expert annotators use a larger, less controlled indexing vocabulary, in part because they lack the higher level of domain expertise that editors have. For example, we observe that frequent keyphrases in KPTimes are close to topic descriptors (e.g. \u201cBaseball\u201c, \u201cPolitics and Government\u201c) while those appearing only once are very precise (e.g. \u201cMarley's Cafe\u201c, \u201cCatherine E. Connelly\u201c). Annotations in KPTimes are arguably more uniform and consistent, through the use of tag suggestions, which, as we will soon discuss in \u00a7SECREF12, makes it easier for supervised approaches to learn a good model."]}
{"question_id": "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b", "predicted_answer": "", "predicted_evidence": ["The hyper-parameters that were determined during the validation is presented in Table TABREF17 . The preprocessing of the data was conducted by lower-casing all the words in the documents and removing numbers. This results in a vocabulary size of INLINEFORM0 = 29044. Words not present in the training set are considered unknown during testing. Also, in order to have fixed-size contexts around the ambiguous words, the padding and truncating are applied to them whenever needed."]}
{"question_id": "4ae0b50c88a174cfc283b90cd3c9407de13fd370", "predicted_answer": "", "predicted_evidence": ["In the past few years, there has been an increasing interest in training neural word embeddings from large unlabeled corpora using neural networks BIBREF9 BIBREF10 . Word embeddings are typically represented as a dense real-valued low dimensional matrix INLINEFORM0 (i.e. a lookup table) of size INLINEFORM1 , where INLINEFORM2 is the predefined embedding dimension and INLINEFORM3 is the vocabulary size. Each column of the matrix is an embedding vector associated with a word in the vocabulary and each row of the matrix represents a latent feature. These vectors can subsequently be used to initialize the input layer of a neural network or some other NLP model. GloVe BIBREF2 is one of the existing unsupervised learning algorithms for obtaining these vector representations of the words in which training is performed on aggregated global word-word co-occurrence statistics from a corpus."]}
{"question_id": "a18d74109ed55ed14c33913efa62e12f207279c0", "predicted_answer": "", "predicted_evidence": [" where INLINEFORM0 and INLINEFORM1 are the weights and the bias of the classification layer (sigmoid), and INLINEFORM2 is the result of the merge layer (concatenation)."]}
{"question_id": "1d6d21043b9fd0ed3ccccdc6317dcf5a1347ef03", "predicted_answer": "", "predicted_evidence": ["The hyper-parameters that were determined during the validation is presented in Table TABREF17 . The preprocessing of the data was conducted by lower-casing all the words in the documents and removing numbers. This results in a vocabulary size of INLINEFORM0 = 29044. Words not present in the training set are considered unknown during testing. Also, in order to have fixed-size contexts around the ambiguous words, the padding and truncating are applied to them whenever needed."]}
{"question_id": "e90425ac05a15dc145bbf3034e78b56e7cec36ac", "predicted_answer": "", "predicted_evidence": ["Finally, the ICSI Meeting Corpus (Janin et al., 2003), which is annotated by Liu et al. (2009a), includes 161 meeting transcriptions. Unlike the other three datasets, the gold standard keys for the ICSI corpus are mostly unigrams."]}
{"question_id": "b677952cabfec0150e028530d5d4d708d796eedc", "predicted_answer": "", "predicted_evidence": ["We set up the score of a concept INLINEFORM0 in the subgraph INLINEFORM1 as following: DISPLAYFORM0 "]}
{"question_id": "d7799d26fe39302c4aff5b530aa691e8653fffe8", "predicted_answer": "", "predicted_evidence": ["The Inspec dataset is a collection of 2,000 abstracts from journal papers including the paper title. This is a relatively popular dataset for automatic keyphrase extraction, as it was first used by BIBREF3 and later by Mihalcea and BIBREF8 and BIBREF9 ."]}
{"question_id": "2711ae6dd532d136295c95253dbf202e37ecd3e7", "predicted_answer": "", "predicted_evidence": ["According to the case study in BIBREF5, the translations of the NART models contain incoherent phrases (e.g. repetitive words) and miss meaningful tokens on the source side, while these patterns do not commonly appear in ART models. After some empirical study, we find two non-obvious facts that lead to this phenomenon."]}
{"question_id": "96356c1affc56178b3099ce4b4aece995032e0ff", "predicted_answer": "", "predicted_evidence": ["While the ART models have achieved great success in terms of translation quality, the time consumption during inference is still far away from satisfactory. During training, the predictions at different positions can be estimated in parallel since the ground truth pair $(x,y)$ is exposed to the model. However, during inference, the model has to generate tokens sequentially as $y_{<t}$ must be inferred on the fly. Such autoregressive behavior becomes the bottleneck of the computational time BIBREF4."]}
{"question_id": "92fc94a4999d1b25a0593904025eb7b8953bb28b", "predicted_answer": "", "predicted_evidence": ["According to our empirical analysis, the percentage of repetitive words drops from 8.3% to 6.5% by our proposed methods on the IWSLT14 De-En test set, which is a 20%+ reduction. This shows that our proposed method effectively improve the quality of the translation outputs. We also provide several case studies in Appendix."]}
{"question_id": "e56c1f0e9eabda41f929d0dfd5cfa50edd69fa89", "predicted_answer": "", "predicted_evidence": ["In this paper, we proposed to use hints from a well-trained ART model to enhance the training of NART models. Our results on WMT14 En-De and De-En significantly outperform previous NART baselines, with one order of magnitude faster in inference than ART models. In the future, we will focus on designing new architectures and training methods for NART models to achieve comparable accuracy as ART models."]}
{"question_id": "a86758696926f2db71f982dc1a4fa4404988544e", "predicted_answer": "", "predicted_evidence": ["The evaluation is on two widely used public machine translation datasets: IWSLT14 German-to-English (De-En) BIBREF9, BIBREF1 and WMT14 English-to-German (En-De) dataset BIBREF4, BIBREF10. To compare with previous works, we also reverse WMT14 English-to-German dataset and obtain WMT14 German-to-English dataset."]}
{"question_id": "9262292ca4cc78de515b5617f6a91e540eb2678c", "predicted_answer": "", "predicted_evidence": ["In Table 8 we show the most discriminant features. The features are sorted by their information gain (IG). As can be seen, the highest gain is obtained by average, maximum and minimum, and standard deviation. On the other hand, probability and proportionality features has low information gain."]}
{"question_id": "d796a251792eca01cea31ba5cf3e54ff9acf543f", "predicted_answer": "", "predicted_evidence": ["We are interested in discovering which kind of features capture higher differences among varieties. Our hypothesis is that language varieties differ mainly in lexicographic clues. We show an example in Table 1 ."]}
{"question_id": "a526c63fc8dc1b79702b481b77e3922d7002d973", "predicted_answer": "", "predicted_evidence": ["During fine-tuning, we extract answer spans from the BioASQ training data by looking for occurrences of the gold standard answer in the provided snippets. Note that this approach is not perfect as it can produce false positives (e.g., the answer is mentioned in a sentence which does not answer the question) and false negatives (e.g., a sentence answers the question, but the exact string used is not in the synonym list)."]}
{"question_id": "0f9678e11079ee9ea1a1ce693f017177dd495ee5", "predicted_answer": "", "predicted_evidence": ["We train the network in two steps: First, the network is trained on SQuAD, following the procedure by weissenborn2017fastqa (pre-training phase). Second, we fine-tune the network parameters on BioASQ (fine-tuning phase). For both phases, we use the Adam optimizer BIBREF6 with an exponentially decaying learning rate. We start with learning rates of $10^{-3}$ and $10^{-4}$ for the pre-training and fine-tuning phases, respectively."]}
{"question_id": "0f1f81b6d4aa0da38b4cc8b060926e7df61bb646", "predicted_answer": "", "predicted_evidence": ["where $\\mathit {\\Phi }=(T_1, T_2, T_3, W, b)$ is the set of model parameters. The standard $L_2$ regularization is used, for which the weight $\\lambda $ is set as 0.0001. The algorithm goes over the training set for multiple iterations. For each training instance, if the loss $loss(E,E^r)=\\max (0,1-g(E)+g(E^r))$ is equal to zero, the online training algorithm continues to process the next event tuple. Otherwise, the parameters are updated to minimize the loss using back-propagation BIBREF9."]}
{"question_id": "ec62df859ad901bf0848f0a8b91eedc78dba5657", "predicted_answer": "", "predicted_evidence": ["In the training process, we calculate the similarity between a given event vector $\\mathbf {v}_e$ and its related intent vector $\\mathbf {v}_i$. For effectively training the model, we devise a ranking type loss function as follows:"]}
{"question_id": "ccec4f8deff651858f44553f8daa5a19e8ed8d3b", "predicted_answer": "", "predicted_evidence": ["where $\\mathbf {v}^{\\prime }_i$ is the incorrect intent for $\\mathbf {v}_e$, which is randomly selected from the annotated dataset."]}
{"question_id": "d38745a3910c380e6df97c7056a5dd9643fd365b", "predicted_answer": "", "predicted_evidence": ["We use mean average precision (MAP) as our evaluation metric. We first calculate precision at $N$ for each word, until all the gold standard morpheme boundaries have been recovered. Then, we average over $N$ to obtain the average precision (AP) for that word. We then calculate the mean of the APs across all words to obtain the MAP for the model."]}
{"question_id": "2b75df325c98b761faf2fecf6e71ac7366eb15ea", "predicted_answer": "", "predicted_evidence": ["We only report results for English. However, English is a morphologically impoverished language, with little inflection and relatively few productive patterns of derivation. Our morphology test set reflects this, with over half the words consisting of a simple morpheme, and over 90% having at most 2 morphemes."]}
{"question_id": "649e77ac2ecce42ab2efa821882675b5a0c993cb", "predicted_answer": "", "predicted_evidence": ["We use mean average precision (MAP) as our evaluation metric. We first calculate precision at $N$ for each word, until all the gold standard morpheme boundaries have been recovered. Then, we average over $N$ to obtain the average precision (AP) for that word. We then calculate the mean of the APs across all words to obtain the MAP for the model."]}
{"question_id": "0bc305d6b90f77f835bc4c904b22a4be07f963b2", "predicted_answer": "", "predicted_evidence": ["Next, we tested our model similarity scores against human similarity judgments. For these datasets, human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments BIBREF20 ."]}
{"question_id": "041529e15b70b21986adb781fd9b94b595e451ed", "predicted_answer": "", "predicted_evidence": ["We consider two evaluation metrics: accuracy (proportion of questions correctly answered) and NDCG INLINEFORM0 BIBREF20 . Unlike accuracy which evaluates if the question is correctly answered or not, NDCG INLINEFORM1 , being a measure of ranking quality, evaluates the position of the correct answer in our predicted ranking."]}
{"question_id": "da2350395867b5fd4dbf968b5a1cd6921ab6dd37", "predicted_answer": "", "predicted_evidence": ["HABCNN-QP and HABCNN-QAP make different use of INLINEFORM0 . HABCNN-QP compares INLINEFORM1 with answer representation INLINEFORM2 . HABCNN-QAP compares INLINEFORM3 with INLINEFORM4 . HABCNN-QAP projects D twice, once based on attention from Q, once based on attention from A and compares the two projected representations, shown in Figure FIGREF2 (top). HABCNN-QP only utilizes the Q-based projection of D and then compares the projected document with the answer representation, shown in Figure FIGREF2 (middle)."]}
{"question_id": "eb653a5c59851eda313ece0bcd8c589b6155d73e", "predicted_answer": "", "predicted_evidence": ["The phonetic sequence annotation of the corresponding text to each speech is also provided in the training set. There are 27 initials and 39 finals with 148 tones in the whole database."]}
{"question_id": "0caa3162abe588f576a568d63ab9fd0e9c46ceda", "predicted_answer": "", "predicted_evidence": ["The three-stage system, as shown in Figure FIGREF14 , has a more complex architecture. Firstly, we still train an AM whose architecture is the same as the first-stage in the two-stage system. This AM is used to generate temporal locations of each phoneme through CTC loss, so that we can train an another AM by using cross-entropy loss as the second-stage to predict the corresponding phonetic labels of the input frames, in which we only use ResNet14 without an RNN because we have the precise locations of each phoneme from the first stage. The third stage is similar, we use the intermediate features from the second stage to train an RNN network for LID task, also the loss in this stage is cross-entropy loss."]}
{"question_id": "cbe42bf7c99ee248cdb2c5d6cf86b41106e66863", "predicted_answer": "", "predicted_evidence": ["By analyzing the confusing matrices (Figure FIGREF19 ) of predicted results, we can find that the accuracy is high in several dialects' recognition, such as Shanghai (98.8%) and Hefei (99.8%), but the systems have some trouble while recognizing Minnan and Kekka, Hebei and Shanxi. The results accord with regional distribution of the dialects. For example, Minnan and Kekka are both in Fujian Province and have lots of cognate words, so it is hard to recognize them in reality."]}
{"question_id": "94d794df4a3109522c2ea09dad5d40e55d35df51", "predicted_answer": "", "predicted_evidence": ["We use neural LMs and neural machine translation (NMT) models in our restricted track entry. Our neural LM is as described in Sec. SECREF15 . Our LMs and NMT models share the same subword segmentation. We perform exploratory NMT experiments with the Base setup, but switch to the Big setup for our final models. Tab. TABREF21 shows the differences between both setups. Tab. TABREF22 lists some corpus statistics for the BEA-2019 training sets. In our experiments without fine-tuning we decode with the average of the 20 most recent checkpoints BIBREF26 . We use the SGNMT decoder BIBREF13 , BIBREF14 in all our experiments."]}
{"question_id": "044c66c6b7ff7378682f24887b05e1af79dcd04f", "predicted_answer": "", "predicted_evidence": ["We use neural LMs and neural machine translation (NMT) models in our restricted track entry. Our neural LM is as described in Sec. SECREF15 . Our LMs and NMT models share the same subword segmentation. We perform exploratory NMT experiments with the Base setup, but switch to the Big setup for our final models. Tab. TABREF21 shows the differences between both setups. Tab. TABREF22 lists some corpus statistics for the BEA-2019 training sets. In our experiments without fine-tuning we decode with the average of the 20 most recent checkpoints BIBREF26 . We use the SGNMT decoder BIBREF13 , BIBREF14 in all our experiments."]}
{"question_id": "903ac8686ed7e6e3269a5d863f06ff11c50e49e8", "predicted_answer": "", "predicted_evidence": ["We use neural LMs and neural machine translation (NMT) models in our restricted track entry. Our neural LM is as described in Sec. SECREF15 . Our LMs and NMT models share the same subword segmentation. We perform exploratory NMT experiments with the Base setup, but switch to the Big setup for our final models. Tab. TABREF21 shows the differences between both setups. Tab. TABREF22 lists some corpus statistics for the BEA-2019 training sets. In our experiments without fine-tuning we decode with the average of the 20 most recent checkpoints BIBREF26 . We use the SGNMT decoder BIBREF13 , BIBREF14 in all our experiments."]}
{"question_id": "ab95ca983240ad5289c123a2774f8e0db424f4a1", "predicted_answer": "", "predicted_evidence": ["We report M2 BIBREF24 scores on the CoNLL-2014 test set BIBREF1 and span-based ERRANT scores BIBREF25 on the BEA-2019 dev set BIBREF2 . On CoNLL-2014 we compare with the best published results with comparable amount of parallel training data. We refer to BIBREF2 for a full comparison of BEA-2019 systems. We tune our systems on BEA-2019 and only report the performance on CoNLL-2014 for comparison to prior work."]}
{"question_id": "fcf9377fc3fce529d4bab1258db3f46b15ae5872", "predicted_answer": "", "predicted_evidence": ["The low human performance can be attributed to the difficulty of identifying bias. Issues of bias are typically reserved for senior Wikipedia editors (Section SECREF14) and untrained workers performed worse (37.39%) on the same task in BIBREF2 (and can struggle on other tasks requiring linguistic knowledge BIBREF39). concurrent's encoder, which is architecturally identical to BERT, had similar performance to a stand-alone BERT system. The linguistic and category-related features in the modular detector gave it slight leverage over the plain BERT-based models."]}
{"question_id": "5422a3f2a083395416d6f99c57d28335eb2e44e1", "predicted_answer": "", "predicted_evidence": ["The distribution of errors is given in Table TABREF50. Most errors are due to the subtlety and complexity of language understanding required for bias neutralization, rather than the generation of fluent text. These challenges are particularly pronounced for neutralizing edits that involve the replacement of factive and assertive verbs. As column 2 shows, a large proportion of the errors, though disagreeing with the edit written by the Wikipedia editors, nonetheless successfully neutralize bias in the source."]}
{"question_id": "7c2d6bc913523d77e8fdc82c60598ee95b445d84", "predicted_answer": "", "predicted_evidence": ["We propose the task of neutralizing text, in which the algorithm is given an input sentence and must produce an output sentence whose meaning is as similar as possible to the input but with the subjective bias removed."]}
{"question_id": "1a0794ebbc9ee61bbb7ef2422d576a10576d9d96", "predicted_answer": "", "predicted_evidence": ["To gather individual samples from the continuous data, segmentation marks were interleaved through a user interface. This was later used to segment individual samples. These samples were further segmented using motion calculation of the wrist joint co-ordinates from skeletal data. Figure FIGREF7 (a) illustrates the distribution of number of samples per gesture class in GMU-ASL51 dataset. Figure FIGREF7 (b) shows the distribution of duration of videos in our dataset."]}
{"question_id": "256dfa501a71d7784520a527f43aec0549b1afea", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF30 shows three confusion matrices for a subset of twelve sign classes for a subject. The top matrix is for AI-LSTM network, middle one is for Max CNN-LSTM and bottom one is for Spatial AI-LSTM. As seen in Figure FIGREF10 the sign pairs Alarm/Doorbell are similar in skeletal motion but have different hand shapes. Since Max CNN-LSTM includes hand shapes, it can successfully recognize it while other two models struggles. Same is true for some other signs like Email, Event, List, Order and Weather . Some other signs are better recognized by Spatial AI-LSTM network. It should be mentioned here that accuracy listed in Table TABREF28 shows average accuracy across all test subjects, while Figure FIGREF30 presents confusion matrix for a single test subject. For this particular subject overall test accuracy is 58%, 70% and 69% for AI-LSTM, Max CNN-LSTM and Spatial AI-LSTM network respectively."]}
{"question_id": "f85520bbc594918968d7d9f33d11639055458344", "predicted_answer": "", "predicted_evidence": ["To deal with over-fitting, dropout was used for all networks except convolutional layers with probability of 0.5. In addition to dropout, L2 regularization was used for LSTM networks and for dense layers; $\\beta $ was set to 0.008 which controls the impact of regularization on the network. State size and number of layers of LSTM networks were 50 and 2, respectively. Learning rate for Max CNN-LSTM and LSTM networks were set to $0.00001$ and $0.00005$, respectively. We used Adam Optimizer for training our networks BIBREF24. All networks were run for a certain number of epochs (200-300) with a batch size of 64. We developed all of our models with Tensorflow 1.10 (python). Average time taken to train an AI-LSTM and an Spatial AI-LSTM are 25 and 30 minutes on an Intel(R) Core(TM) i5-7600 (3.50GHz) processor respectively. We trained 3D CNN and Max 3D CNN models on GPU (Tesla K80) and each model took around 20 hours to train."]}
{"question_id": "e4f2d59030b17867449cf5456118ab722296bebd", "predicted_answer": "", "predicted_evidence": ["Let's turn to our main question: what do character-level models learn about morphology? To answer it, we compare the oracle model to char-lstm, our best character-level model."]}
{"question_id": "e664b58ea034a638e7142f8a393a88aadd1e215e", "predicted_answer": "", "predicted_evidence": ["We trained our augmented model (oracle-attn) on Finnish, German, Czech, and Russian. Its accuracy is very similar to the oracle model (Table 8 ), so we obtain a more interpretable model with no change to our main results."]}
{"question_id": "c4b621f573bbb411bdaa84a7562c9c4795a7eb3a", "predicted_answer": "", "predicted_evidence": ["Let's turn to our main question: what do character-level models learn about morphology? To answer it, we compare the oracle model to char-lstm, our best character-level model."]}
{"question_id": "3ccc4ccebc3b0de5546b1208e8094a839fd4a4ab", "predicted_answer": "", "predicted_evidence": ["Character-level models are effective because they can represent OOV words and orthographic regularities of words that are consistent with morphology. But they depend on context to disambiguate words, and for some words this context is insufficient. Case syncretism is a specific example that our analysis identified, but the main results in Table 2 hint at the possibility that different phenomena are at play in different languages."]}
{"question_id": "2dba0b83fc22995f83e7ac66cc8f68bcdcc70ee9", "predicted_answer": "", "predicted_evidence": ["The sampled results in Figure FIGREF25 show that the Seq2Seq and the RL model can generate reasonable responses for intervention. However, as is to be expected with machine-generated text, in the other human evaluation we conducted, where Mechanical Turk workers were also presented with sampled human-written responses alongside the machine generated responses, the human-written responses were chosen as the most effective and diverse option a majority of the time (70% or more) for both datasets. This indicates that there is significant room for improvement while generating automated intervention responses."]}
{"question_id": "a8cc891bb8dccf0d32c1c9cd1699d5ead0eed711", "predicted_answer": "", "predicted_evidence": ["The experimental results of the detection task and the generative intervention task are shown in Table TABREF27 and Table TABREF29 separately. The results of the human evaluation are shown in Table TABREF30. Figure FIGREF25 shows examples of the generated responses."]}
{"question_id": "8330242b56b63708a23c6a92db4d4bcf927a4576", "predicted_answer": "", "predicted_evidence": ["Q1: Which posts or comments in this conversation are hate speech?"]}
{"question_id": "a4cf0cf372f62b2dbc7f31c600c6c66246263328", "predicted_answer": "", "predicted_evidence": ["4) BBFNMT (based) is comparable to the +global-deep context, the best comparison system, while BBFNMT (big) slightly outperformed +global-deep context by $0.16$ BLEU scores. In particular, the parameters of BBFNMT (base/big) model, which just increased $12.1/7.9$M over the Transformer (base/big), were only 70% of the +global-deep context model. This denotes that the BBFNMT model is more efficient than the +global-deep context model. In addition, the training speed of the proposed models slightly decreased ($8\\%$), compared to the corresponding baselines."]}
{"question_id": "f7b91b99279833f9f489635eb8f77c6d13136098", "predicted_answer": "", "predicted_evidence": ["To evaluate the quality of our sentence compression model, we conducted a horizontal comparison between the proposed sentence compression model and other sentence compression models in different settings. Table TABREF34 shows the comparison results. We observed that the proposed unsupervised ESC model performed substantially better than Fevry and BIBREF17's unsupervised method. The proposed supervised ESC model also substantially outperformed the RNN-based Seq2seq and BIBREF11's baseline method. That is, our supervised model gave +2.0 improvements on R-1, R-2, and R-L scores over the RNN-based Seq2seq. This means that the proposed Transformer-based approaches can generate compressed sentences of high quality."]}
{"question_id": "99e514acc0109b7efa4e3860ce1e8c455f5bb790", "predicted_answer": "", "predicted_evidence": ["4) BBFNMT (based) is comparable to the +global-deep context, the best comparison system, while BBFNMT (big) slightly outperformed +global-deep context by $0.16$ BLEU scores. In particular, the parameters of BBFNMT (base/big) model, which just increased $12.1/7.9$M over the Transformer (base/big), were only 70% of the +global-deep context model. This denotes that the BBFNMT model is more efficient than the +global-deep context model. In addition, the training speed of the proposed models slightly decreased ($8\\%$), compared to the corresponding baselines."]}
{"question_id": "2fec84a62b4028bbe6500754d9c058eefbc24d9a", "predicted_answer": "", "predicted_evidence": ["GazSelfAttn evaluations on CoNLL-03 and Ontonotes 5 datasets show F$_1$ score improvement over baseline model from 92.34 to 92.86 and from 89.11 to 89.32 respectively. Moreover, we perform ablation experiments to study the contribution of the different model components."]}
{"question_id": "2803709fba74e6098aae145abcbf0e9a3f4c35e5", "predicted_answer": "", "predicted_evidence": ["GazSelfAttn evaluations on CoNLL-03 and Ontonotes 5 datasets show F$_1$ score improvement over baseline model from 92.34 to 92.86 and from 89.11 to 89.32 respectively. Moreover, we perform ablation experiments to study the contribution of the different model components."]}
{"question_id": "ec39120fb879ae10452d3f244e1e32237047005a", "predicted_answer": "", "predicted_evidence": ["To extract gazetteers from Wikidata, we process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, example resulting tuples are Boston $\\rightarrow $ City and Massachusetts $\\rightarrow $ State. Each entity is associated with a set of aliases, we keep only the aliases that are less than seven tokens long. Example aliases for Boston are \u201cBeantown\u201d and \u201cThe Cradle of Liberty\u201d. If there are multiple types per alias, we use the sitelink count to keep the six most popular types. The sitelink filtering is important to reduce the infrequent meanings of an entity in the gazetteer data."]}
{"question_id": "ac87dd34d28c3edd9419fa0145f3d38c87d696aa", "predicted_answer": "", "predicted_evidence": ["For parallelizing the text and audio embeddings in Subsection SECREF14 , we projected the embeddings to the top 100 principle components, so the affine transformation matrices were INLINEFORM0 . The mini-batch size was 200, and INLINEFORM1 in ( EQREF15 ) was set to 0.5."]}
{"question_id": "e66a88eecf8d5d093caec1f487603534f88dd7e7", "predicted_answer": "", "predicted_evidence": ["We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the \u201cclean\" and \u201cothers\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features."]}
{"question_id": "fef5b65263c81299acc350a101dabaf5a8cb9c6e", "predicted_answer": "", "predicted_evidence": ["For parallelizing the text and audio embeddings in Subsection SECREF14 , we projected the embeddings to the top 100 principle components, so the affine transformation matrices were INLINEFORM0 . The mini-batch size was 200, and INLINEFORM1 in ( EQREF15 ) was set to 0.5."]}
{"question_id": "f40e23adc8245562c8677f0f86fa5175179b5422", "predicted_answer": "", "predicted_evidence": ["For parallelizing the text and audio embeddings in Subsection SECREF14 , we projected the embeddings to the top 100 principle components, so the affine transformation matrices were INLINEFORM0 . The mini-batch size was 200, and INLINEFORM1 in ( EQREF15 ) was set to 0.5."]}
{"question_id": "50bcbb730aa74637503c227f022a10f57d43f1f7", "predicted_answer": "", "predicted_evidence": ["where: $f(x)=w^Tx$ is a linear scoring function, $(x_u,x_v)$ is a pairwise and $\\xi _{u,v}^{(i)}$ is the loss. The document pairwise in our model is a pair of a query and an article."]}
{"question_id": "fac273ecb3e72f2dc94cdbc797582d7225a8e070", "predicted_answer": "", "predicted_evidence": ["Legal text, along with other natural language text data, e.g. scientific literature, news articles or social media, has seen an exponential growth on the Internet and in specialized systems. Unlike other textual data, legal texts contain strict logical connections of law-specific words, phrases, issues, concepts and factors between sentences or various articles. Those are for helping people to make a correct argumentation and avoid ambiguity when using them in a particular case. Unfortunately, this also makes information retrieval and question answering on legal domain become more complicated than others."]}
{"question_id": "7c561db6847fb0416bca8a6cb5eebf689a4b1438", "predicted_answer": "", "predicted_evidence": ["This finding seems to suggest that data size is more important than language relatedness for predicting the effects of pretraining. However, there are big differences even amongst the languages with similar amounts of pretraining data. Analyzing our results further, we found a striking correlation between the WER of the initial ASR model and the BLEU score of the AST system pretrained using that model, as shown in Figure FIGREF11. Therefore, although pretraining data size clearly influences AST performance, this appears to be mainly due to its effect on WER of the ASR model. We therefore hypothesize that WER is a better direct predictor of AST performance than either data size or language relatedness."]}
{"question_id": "13eb64957478ade79a1e81d32e36ee319209c19a", "predicted_answer": "", "predicted_evidence": ["Separate logistic regression models were trained on the representations from each layer of the encoder. Since convolutional layers have a stride of 2, the number of frames decreases at each convolutional layer. To label the frames after a convolutional layer we eliminated every other label (and corresponding frame) from the original label sequence. For example, given label sequence S$_{\\text{1}}$ = aaaaaaann at input layer, we get sequence S$_{\\text{2}}$ = aaaan at the first convolutional layer and sequence S$_{\\text{3}}$ = aan at the second convolutional layer and at the following recurrent layers."]}
{"question_id": "3cfe464052f0a248b6e22c9351279403dfe34f3c", "predicted_answer": "", "predicted_evidence": ["For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure FIGREF1: the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2. Details of the architecture and training parameters are described in Section SECREF9."]}
{"question_id": "119c404da6e42d4879eee10edeab4b2851162659", "predicted_answer": "", "predicted_evidence": ["To look at a range of languages with similar amounts of data, we used GlobalPhone corpora from seven languages BIBREF15, each with around 20 hours of speech: Mandarin Chinese (zh), Croatian (hr), Czech (cs), French (fr), Polish (pl), Portuguese (pt), and Swedish (sv). French and Portuguese, like the source language (Spanish), belong to the Romance family of languages, while the other languages are less related\u2014especially Chinese, which is not an Indo-European language. GlobalPhone consists of read speech recorded using similar conditions across languages, and the transcriptions for Chinese are Romanized, with annotated word boundaries."]}
{"question_id": "32f2aa2df0152050cbcd27dd2f408b2fa5894031", "predicted_answer": "", "predicted_evidence": ["Even with data augmentation, however, there is still a large gap between the WERs on near-field and far-field test sets. The bottom two rows of Table 3 show the performance of the methods introduced in this paper on the same test sets. An $L^1$ -distance penalty can lower the test set WER by 1.32% absolute. Using a GAN enhancer can reduce the WER by an additional 1.07%. Overall, the gap between near-field and far-field performance decreases by almost 27% compared to the model that only uses data augmentation."]}
{"question_id": "065623cc1d5f5b19ec1f84d286522fc2f805c6ce", "predicted_answer": "", "predicted_evidence": ["We created features for our model based on POS tags and their combinations. The sets of features and the combinations are learned automatically from annotated examples. We used these novel features to make our model more domain-independent."]}
{"question_id": "5c17559749810c67c50a7dbe34580d5e3b4f9acb", "predicted_answer": "", "predicted_evidence": ["The results show that generally random forest classifier seems to work best in extracting Condition-Action statements."]}
{"question_id": "1c0a575e289eb486d3e6375d6f783cc2bf18adf9", "predicted_answer": "", "predicted_evidence": ["Our data preparation process proceeded as follows: We started by converting the guidelines from PDF or html to text format, editing sentences only to manage conversion errors, the majority of which were bullet points. Tables and some figures pose a problem, and we are simply treating them as unstructured text. We are not dealing at this time with the ambiguities introduced by this approach; we do have plans to address it in future work."]}
{"question_id": "4efe0d62bba618803ec12b63f32debb8b757dd68", "predicted_answer": "", "predicted_evidence": ["However, completely automated extraction of condition-action statements does not seem possible. This is due among other things to the variety of linguistic expressions used in condition-action sentences. For example, they are not always in form of \"{if} condition {then} action\u201d. In the sentence \"Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation\u201d, we have a condition-action sentence without an \"{if}\" term."]}
{"question_id": "97708d93bccc832ea671dc31a76dad6a121fcd60", "predicted_answer": "", "predicted_evidence": ["In this section, we attempt to uncover why automatic metrics perform so poorly."]}
{"question_id": "f11856814a57b86667179e1e275e4f99ff1bcad8", "predicted_answer": "", "predicted_evidence": ["This paper shows that state-of-the-art automatic evaluation metrics for NLG systems do not sufficiently reflect human ratings, which stresses the need for human evaluations. This result is opposed to the current trend of relying on automatic evaluation identified in BIBREF1 ."]}
{"question_id": "0bb97991fc297aa5aed784568de52d5b9121f920", "predicted_answer": "", "predicted_evidence": ["The global + new-TF-IDF variant outperforms all but the DPP model in Rouge-1 recall. The global + N-first variant outperforms all other models in Rouge-2 recall. However, the Rouge scores of the SOTA methods and the introduced centroid variants are in a very similar range."]}
{"question_id": "7ba6330d105f49c7f71dba148bb73245a8ef2966", "predicted_answer": "", "predicted_evidence": ["Table TABREF9 shows the Rouge scores measured in our experiments."]}
{"question_id": "157de5175259d6f25db703efb299f948dae597b7", "predicted_answer": "", "predicted_evidence": [" BIBREF7 implement this original model with the following modifications:"]}
{"question_id": "cf3fab54b2b289b66e7dba4706c47a62569627c5", "predicted_answer": "", "predicted_evidence": ["A summary is selected by de-queuing the ranked list of sentences in decreasing order until the desired summary length is reached."]}
{"question_id": "000549a217ea24432c0656598279dbb85378c113", "predicted_answer": "", "predicted_evidence": ["We also investigate the occurrence of markers in the two platforms via frequency analysis (Table TABREF29 ). We report the mean of occurrence per utterance and the standard deviation (SD) of each marker. Table TABREF29 shows that markers such as hyperbole, punctuations, and interjections are popular in both platforms. Emojis and emoticons, although the two most popular markers in INLINEFORM0 are almost unused in INLINEFORM1 . Exclamations and INLINEFORM2 s are more common in the INLINEFORM3 corpus. Next, we combine each marker with the type they belong to (i.e., either trope, morpho-syntactic and typographic) and compare the means between each pair of types via independent t-tests. We found that the difference of means is significant ( INLINEFORM4 ) for all pair of types across the two platforms."]}
{"question_id": "63d2e97657419a0185127534f4ff9d0039cb1a63", "predicted_answer": "", "predicted_evidence": ["Tag questions - We built a list of tag questions (e.g.,, \u201cdidn't you?\u201d, \u201caren't we?\u201d) from a grammar site and use them as binary indicators."]}
{"question_id": "43f43b135109ebd1d2d1f9af979c64ce550b5f0f", "predicted_answer": "", "predicted_evidence": ["Three types of markers \u2014 tropes, morpho-syntactic, and typographic are used as features."]}
{"question_id": "e797634fa77e490783b349034f9e095ee570b7a9", "predicted_answer": "", "predicted_evidence": ["We present three contributions in this paper. First, we provide a detailed investigation of a set of theoretically-grounded irony markers (e.g., tropes, morpho-syntactic, and typographic markers) in social media. We conduct the classification and frequency analysis based on their occurrence. Second, we analyze and compare the use of irony markers on two social media platforms ( INLINEFORM0 and INLINEFORM1 ). Third, we provide an analysis of markers on topically different social media content (e.g., technology vs. political subreddits)."]}
{"question_id": "475e698a801be0ad9e4f74756d1fff4fe0728009", "predicted_answer": "", "predicted_evidence": ["For argument labeling, every token in the sentence is assigned one of the argument labels, or INLINEFORM0 if the model predicts it is not an argument to the indicated predicate."]}
{"question_id": "8246d1eee1482555d075127ac84f2e1d0781a446", "predicted_answer": "", "predicted_evidence": ["Despite the consistency of this format, there are significant differences between the training sets across languages. English uses PropBank role labels BIBREF2 . Catalan, Chinese, English, German, and Spanish include (but are not limited to) labels such as \u201carg INLINEFORM0 -agt\u201d (for \u201cagent\u201d) or \u201cA INLINEFORM1 \u201d that may correspond to some degree to each other and to the English roles. Catalan and Spanish share most labels (being drawn from the same source corpus, AnCora; BIBREF3 ), and English and German share some labels. Czech and Japanese each have their own distinct sets of argument labels, most of which do not have clear correspondences to English or to each other."]}
{"question_id": "1ec0be667a6594eb2e07c50258b120e693e040a8", "predicted_answer": "", "predicted_evidence": ["The CoNLL 2009 dataset includes seven different languages, allowing study of trends across the same. Unlike the Universal Dependencies dataset, however, the semantic label spaces are entirely language-specific, making our task more challenging. Nonetheless, the success of polyglot training in this setting demonstrates that sharing of statistical strength across languages does not depend on explicit alignment in annotation conventions, and can be done simply through parameter sharing. We show that polyglot training can result in better labeling accuracy than a monolingual parser, especially for low-resource languages. We find that even a simple combination of data is as effective as more complex kinds of polyglot training. We include a breakdown into label categories of the differences between the monolingual and polyglot models. Our findings indicate that polyglot training consistently improves label accuracy for common labels."]}
{"question_id": "e3bafa432cd3e1225170ff04de2fdf1ede38c6ef", "predicted_answer": "", "predicted_evidence": ["Unlike multilingual word representations, argument label sets are disjoint between language pairs, and correspondences are not clearly defined. Hence, we use separate label representations for each language's labels. Similarly, while (for example) eng:look and spa:mira may be semantically connected, the senses look.01 and mira.01 may not correspond. Hence, predicate sense representations are also language-specific."]}
{"question_id": "dde29d9ea5859aa5a4bcd613dca80aec501ef03a", "predicted_answer": "", "predicted_evidence": ["We thank anonymous reviewers for useful comments and Jingbo Zhu for sharing the MMD executable program. This paper is partially supported by the National Natural Science Foundation of China (NSFC Grant Nos. 61272343 and 61472006), the Doctoral Program of Higher Education of China (Grant No. 20130001110032), and the National Basic Research Program (973 Program No. 2014CB340405)."]}
{"question_id": "9b1382b44dc69f7ee20acf952f7ceb1c3ef83965", "predicted_answer": "", "predicted_evidence": ["A similar (but different) research problem is topic tracking in conversations, e.g., BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 . In these approaches, the goal is typically a classification problem with a few pre-defined conversation states/topics, and hence it can hardly be generalized to general-purpose session segmentation."]}
{"question_id": "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f", "predicted_answer": "", "predicted_evidence": ["Random. We randomly segmented conversation sessions. In this baseline, we were equipped with the prior probability of segmentation."]}
{"question_id": "6157567c5614e1954b801431fec680f044e102c6", "predicted_answer": "", "predicted_evidence": ["The questions collected from the web search engine may not be fluent or domain relevant; especially the domain relevance drops significantly as the iteration goes on. Here we adopt a skip-gram model BIBREF11 and a language model for evaluating the domain relevance and fluency of the expanded questions, respectively. For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as: "]}
{"question_id": "8ea4a75dacf6a39f9d385ba14b3dce715a47d689", "predicted_answer": "", "predicted_evidence": ["We test our domain-relevance evaluating method on the web snippet dataset, which is a commonly-used for domain classification of short documents. It contains 10,060 training and 2,280 test snippets (short documents) in 8 classes (domains), and each snippet has 18 words on average. There have been plenty of prior results BIBREF12 , BIBREF13 , BIBREF14 on the dataset."]}
{"question_id": "1e11e74481ead4b7635922bbe0de041dc2dde28d", "predicted_answer": "", "predicted_evidence": ["The last experiment is on our in-house KB in the power tool domain. It contains 67 distinct predicates, 293 distinct subjects and 279 distinct objects respectively. For the 67 predicates, we hand-craft 163 templates. Here we use the same language model as in our first experiment, and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance."]}
{"question_id": "597d3fc9b8c0c036f58cea5b757d0109d5211b2f", "predicted_answer": "", "predicted_evidence": ["where $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document. For fluency, we define the averaged language model score as: "]}
{"question_id": "f0404673085517eea708c5e91f32fb0f7728fa08", "predicted_answer": "", "predicted_evidence": ["We collect tennis press-conference transcripts from ASAP Sport's website (http://www.asapsports.com/), whose tennis collection dates back to 1992 and is still updated for current tournaments. For our study, we take post- game interviews for tennis singles matches played between Jan, 2000 to Oct 18, 2015. We also obtain easily-extractable match information from a dataset provided by Tennis-Data, which covers the majority of the matches played on the men's side from 2000-2015 and on the women's side from 2007-2015."]}
{"question_id": "d6b0c71721ed24ef1d9bd31ed3a266b0c7fc9b57", "predicted_answer": "", "predicted_evidence": ["Driving data is collected from pairs of human workers on Mechanical Turk. Workers received the following description of the task:"]}
{"question_id": "63cdac43a643fc1e06da44910458e89b2c7cd921", "predicted_answer": "", "predicted_evidence": ["The data was validated by a separate set of crowdsourcers. All audios deemed by the crowdsourcers to be unintelligible or contain the wrong phrase were removed. The total number of speakers, utterances, and hours of audio remaining is shown in Table TABREF12 ."]}
{"question_id": "37ac705166fa87dc74fe86575bf04bea56cc4930", "predicted_answer": "", "predicted_evidence": ["We show the category-wise performance in Figure FIGREF11."]}
{"question_id": "90aba75508aa145475d7cc9a501bbe987c0e8413", "predicted_answer": "", "predicted_evidence": ["We show the category-wise performance in Figure FIGREF11."]}
{"question_id": "e6204daf4efeb752fdbd5c26e179efcb8ddd2807", "predicted_answer": "", "predicted_evidence": ["The automatic evaluation aims to evaluate both the grammatical correctness and the consistency of the speech in terms of its content. For evaluating the grammatical correctness we identify for each sentence of the speech its POS tags. Then we check all sentences of the entire corpus whether one has the same sequence of POS tags. Having a sentence with the same POS tag structure does not necessarily mean that the grammar is correct. Neither does the lack of finding a matching sentence imply the existence of an error. But it points in a certain direction. Furthermore, we let the system output the sentence for which it could not find a matching sentence so that we can evaluate those sentences manually."]}
{"question_id": "95c3907c5e8f57f239f3b031b1e41f19ff77924a", "predicted_answer": "", "predicted_evidence": ["In this section we present the results from our experiments. Table TABREF15 shows the results from the manual evaluation. Note that each criterion scores between 0 and 3 which leads to a maximum total score of 12. The achieved total score range from 5 to 10 with an average of 8.1. In particular, the grammatical correctness and the sentence transitions were very good. Each of them scored on average 2.3 out of 3. The speech content yielded the lowest scores. This indicates that the topic model may need some improvement."]}
{"question_id": "b900122c7d6c2d6161bfca8a95eae11952d1cb58", "predicted_answer": "", "predicted_evidence": [" INLINEFORM0 tells how likely this word is to occur after the previous 5 ones. This value can be directly obtained by the language model of the specified class. INLINEFORM1 tells how likely the word w is to occur in a speech which covers the current topics INLINEFORM2 . The following equation shows the definition of INLINEFORM3 where INLINEFORM4 denotes our dataset and INLINEFORM5 is the subset containing only speeches of class INLINEFORM6 . INLINEFORM7 "]}
{"question_id": "5206b6f40a91fc16179829041c1139a6c6d91ce7", "predicted_answer": "", "predicted_evidence": ["For the manual evaluation we have defined a list of evaluation criteria. That is, a generated speech is evaluated by assessing each of the criterion and assigning a score between 0 and 3 to it. Table TABREF13 lists all evaluation criteria and describes the meaning of the different scores."]}
{"question_id": "48ff9645a506aa2c17810d2654d1f0f0d9e609ee", "predicted_answer": "", "predicted_evidence": ["Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14)."]}
{"question_id": "84ee6180d3267115ad27852027d147fb86a33135", "predicted_answer": "", "predicted_evidence": ["Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus BIBREF9. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model BIBREF2 required 1 day of training on 1024 32GB V100."]}
{"question_id": "c7ffef8bf0100eb6148bd932d0409b21759060b1", "predicted_answer": "", "predicted_evidence": ["The other dataset is the GlobalPhone corpus BIBREF47 , which includes French (FRE), German (GER), Czech (CZE), and Spanish (ESP). The four languages from GlobalPhone were used as the low-resource target languages. In Section 6.2, 20 thousand segments for each language were used to calculate the average cosine similarity. For the experiments of STD, the 20 thousands segments served as the database to be retrieved, and the other 1 thousand used for query and 4 thousand for fine-tuning."]}
{"question_id": "1ff0ffeb2d0b2e150abdb2f559d8b31f4dd8aa2c", "predicted_answer": "", "predicted_evidence": ["Besides analyzing the cosine similarity of the learned representations, we also apply them to the query-by-example STD task. Here we compare the retrieval performance in MAP of INLINEFORM0 with different levels of accessibility to the low-resource target language along with two baseline models, INLINEFORM1 and INLINEFORM2 trained purely by the target languages. For the four target languages, the total available amount of audio word segments in the training set were 4 thousands for each language. In Table TABREF20 , we took different partitions of the target language training sets to fine tune the INLINEFORM3 pretrained by the source languages. The amount of audio word segments in these partitions are: 1K, 2K, 3K, 4K, and 0, which means no fine-tuning."]}
{"question_id": "3cc0d773085dc175b85955e95911a2cfaab2cdc4", "predicted_answer": "", "predicted_evidence": ["In Fig. FIGREF14 , the cosine similarities of the segment pairs get smaller as the edit distances increase, and the trend is observed in all languages. The gap between each edit distance groups, i.e. (0,1), (1,2), (2,3), (3,4), is obvious. This means that INLINEFORM0 learned from English can successfully encode the sequential phonetic structures into fixed-length vector for the target languages to some good extend even though it has never seen any audio data of the target languages. Another interesting fact is the corresponding variance between languages. In the source language, English, the variances of the five edit distance groups are fixed at 0.030, which means that the cosine similarity in each edit distance group is centralized. However, the variances of the groups in the target languages vary. In French and German, the variance grows from 0.030 to 0.060 as the edit distance increases from 0 to 4. For Czech/Spanish, the variance starts at a larger value of 0.040/0.050 and increases to 0.050/0.073. We suspect that the fluctuating variance is related to the similarity between languages. English, German and French are more similar compared with Czech and Spanish. Among the four target languages, German has the highest lexical similarity with English (0.60) and the second highest is French (0.27), while for Czech and Spanish, the lexical similarity scores is 0 BIBREF48 ."]}
{"question_id": "dfd07a8e2de80c3a8d075a0f400fb13a1f1d4c60", "predicted_answer": "", "predicted_evidence": ["Supervised Baselines"]}
{"question_id": "2e70d25f14357ad74c085a9454a2ce33bb988a6f", "predicted_answer": "", "predicted_evidence": ["Recently, reproducibility is becoming a growing concern for the NLP community BIBREF14. In fact, the majority of the papers we consider in this study fail to report the validation set results. To address these issues, apart from the F1 scores on the test sets we also report the F1 scores for the validation sets."]}
{"question_id": "de84972c5d1bbf664d0f8b702fce5f161449ec23", "predicted_answer": "", "predicted_evidence": ["Ultimately, we try different classifiers in order to assess the impact of the segmentation method. As part of the models of the first type, the resulting document vector is output from a batch normalisation layer. A linear transformation is then applied to that and this output is passed through a softmax classifier in order to acquire the multi-class probabilities. This final process is summarised in the following formula:"]}
{"question_id": "bab4e8881f4d75e266bce6fbfa4c3bcd3eacf30f", "predicted_answer": "", "predicted_evidence": ["All of the models were trained by RMSprop BIBREF12 with mini-batches of 100 samples. The learning rate and decay term were set as 0.001 and 0.9 respectively, also tuned on the development set."]}
{"question_id": "11dd2913d1517a1d47b367acb29fe9d79a9c95d1", "predicted_answer": "", "predicted_evidence": ["The noisy channel approach applies Bayes' rule to model $p(y|x) = p(x|y) p(y)/ p(x)$, that is, the channel model $p(x|y)$ operating from the target to the source and a language model $p(y)$. We do not model $p(x)$ since it is constant for all $y$. We compute the channel model probabilities as follows:"]}
{"question_id": "8701ec7345ccc2c35eca4e132a8e16d58585cd63", "predicted_answer": "", "predicted_evidence": ["For English-German (En-De) we train on WMT'17 data, validate on news2016 and test on news2017. For reranking, we train models with a 40K joint byte pair encoding vocabulary (BPE; BIBREF11). To be able to use the language model during online decoding, we use the vocabulary of the langauge model on the target side. For the source vocabulary, we learn a 40K byte pair encoding on the source portion of the bitext; we find using LM and bitext vocabularies give similar accuracy. For Chinese-English (Zh-En), we pre-process WMT'17 data following BIBREF12, we develop on dev2017 and test on news2017. For IWSLT'14 De-En we follow the setup of BIBREF13 and measure case-sensitive tokenized BLEU. For WMT De-En, En-De and Zh-En we measure detokenized BLEU BIBREF14."]}
{"question_id": "d20fd6330cb9d03734e2632166d6c8f780359a94", "predicted_answer": "", "predicted_evidence": ["We plot the averaged performances on varying amounts of training data for each target domain in Figure 3 . Note that the improvements are even higher for the experiments with smaller training data. In particular, ZAT shows an improvement of $14.67$ in absolute F1-score over CRF when training with 500 instances. ZAT achieves an F1-score of 76.04% with only 500 training instances, while even with 2000 training instances the CRF model achieves an F1-score of only 75%. Thus the ZAT model achieves better F1-score with only one-fourth the training data."]}
{"question_id": "1a1d94c981c58e2f2ee18bdfc4abc69fd8f15e14", "predicted_answer": "", "predicted_evidence": ["According to the authors Medagoda et al. BIBREF0 there has being a continuous research going on in the English language but the research carried out in the indigenous languages is less. Also, the researches in indigenous languages follow the techniques used for the English language but this has one disadvantage which is, techniques have properties which are specific to a language. Hence It is really important to understand and analyze Indigenous language data because it can give meaningful insights to the companies. For example, India and China have world's largest population and are rich in diverse languages, analysing these indigenous language will be useful to companies because they have large share of users in India and China. In the current study, the types of languages i.e. indigenous languages and code mix languages are discussed prior to the approaches, methodologies used by the researchers and challenges faced by them."]}
{"question_id": "5d790459b05c5a3e6f1e698824444e55fc11890c", "predicted_answer": "", "predicted_evidence": ["with $c_r$ and $c_v$ as semantic recipe-class and semantic image-class, respectively, while $c_r=c_v$ if the food image and recipe text are a positive pair."]}
{"question_id": "1ef6471cc3e1eb10d2e92656c77020ca1612f08e", "predicted_answer": "", "predicted_evidence": ["where $\\beta \\in [0,1]$ weights between quadratic and linear loss, $\\alpha \\in [0,2]$ is the margin and $\\gamma \\in [0,1]$ weights between semantic- and sample-loss. The triplet loss encourages the embedding vectors of a matching pair to be larger by a margin above its non-matching counterpart. Further, the semantic loss encourages the model to form clusters of dishes, sharing the same class. We chose $\\beta $ to be $0.1$, $\\alpha $ to be $0.3$ and $\\gamma $ to be $0.3$."]}
{"question_id": "d976c22e9d068e4e31fb46e929023459f8290a63", "predicted_answer": "", "predicted_evidence": ["In this method, paragraphs are encoded separately, and the concatenation of the resulted encoding is going through the classifier. First, each paragraph is encoded with LSTM. The hidden state at the end of each sentence is extracted, and the resulting matrix is going through gated CNN BIBREF1 for extraction of single encoding for each paragraph. The accuracy is barely above $50\\%$ , which depicts that this method is not very promising."]}
{"question_id": "a1ac4463031bbc42c80893b57c0055b860f12e10", "predicted_answer": "", "predicted_evidence": ["Hi 48 PDX Cantrell $<$LINK$>$ $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ $<$NET$>$ ECT ECT $<$NET$>$ F $<$NET$>$ ECT ECT $<$NET$>$ G Slaughter 06 07 03 57 DEVELOPMENT 06 09 2000 07 01 $<$NET$>$ $<$NET$>$ ECT ENRON 09 06 03 10 23 PM To $<$NET$>$ $<$NET$>$ ECT ECT cc $<$NET$>$ $<$NET$>$ ECT ECT Subject Wow Do not underestimate the employment group contains Socal study about recession impact $<$NET$>$ will note else to you for a revised Good credit period I just want to bring the afternoon $<$NET$>$ I spoke to $<$NET$>$ Let me know if"]}
{"question_id": "3216dfc233be68206bd342407e2ba7da3843b31d", "predicted_answer": "", "predicted_evidence": ["A few observations from the datasets above: the malicious content is relatively more verbose than than the legitimate counterparts. Moreover, the size of the malicious data is comparatively higher compared to the legitimate content."]}
{"question_id": "4f57ac24f3f4689a2f885715cd84b7d867fe3f12", "predicted_answer": "", "predicted_evidence": ["Despite the incoherent nature of the generated emails, the text-based classifiers do not achieve a 100% accuracy as well as F1-scores."]}
{"question_id": "46146ff3ef3430924e6b673a28df96ccb869dee4", "predicted_answer": "", "predicted_evidence": ["We investigate the best performing model by making use of the confusion matrix (see Figure FIGREF20) and by inspecting all errors made by the model on the test set (see Table TABREF21)."]}
{"question_id": "3499d5feeb3a45411d8e893516adbdc14e72002a", "predicted_answer": "", "predicted_evidence": ["Rearrange the words to be in the new positions, to which their original indices have moved by Step 2."]}
{"question_id": "d0048ef1cba3f63b5d60c568d5d0ba62ac4d7e75", "predicted_answer": "", "predicted_evidence": ["In this paper, we integrate context information into word-by-word translation by combining a language model (LM) with cross-lingual word embedding. Let $f$ be a source word in the current position and $e$ a possible target word. Given a history $h$ of target words before $e$ , the score of $e$ to be the translation of $f$ would be:"]}
{"question_id": "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c", "predicted_answer": "", "predicted_evidence": ["We compared VarNDRR against the following two different baseline methods:"]}
{"question_id": "4e63454275380787ebd0e38aa885977332ab33af", "predicted_answer": "", "predicted_evidence": ["The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported."]}
{"question_id": "dfaeb8faf04505a4178945c933ba217e472979d8", "predicted_answer": "", "predicted_evidence": ["The definition of the terminology related to the dataset is given as follows."]}
{"question_id": "342ada55bd4d7408e1fcabf1810b92d84c1dbc41", "predicted_answer": "", "predicted_evidence": ["The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported."]}
{"question_id": "86d1c990c1639490c239c3dbf5492ecc44ab6652", "predicted_answer": "", "predicted_evidence": ["Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other)."]}
{"question_id": "b065c2846817f3969b39e355d5d017e326d6f42e", "predicted_answer": "", "predicted_evidence": ["In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics."]}
{"question_id": "9536e4a2455008007067f23cc873768374c8f664", "predicted_answer": "", "predicted_evidence": ["For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format."]}
{"question_id": "cfa44bb587b0c05906d8325491ca9e0f024269e8", "predicted_answer": "", "predicted_evidence": ["There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS."]}
{"question_id": "b3dc9a35e8c3ed7abcc4ca0bf308dea75be9c016", "predicted_answer": "", "predicted_evidence": ["The definition of the terminology related to the dataset is given as follows."]}
{"question_id": "693cdb9978749db04ba34d9c168e71534f00a226", "predicted_answer": "", "predicted_evidence": ["After that, LGI learned the syntax \u2018the size is big/small\u2019, followed by \u2018the size is not small/big\u2019. Figure 5 illustrates that LGI could correctly categorize whether the digit size was small or big with proper text output. And we witness that, based on the syntax of \u2018the size is big/small\u2019 (train steps =1000), the negative adverb \u2018not\u2019 in the language text \u2018the size is not small/big\u2019 was much easier to be learned (train steps =200, with same hyper-parameters). This is quite similar to the cumulative learning process of the human being."]}
{"question_id": "71fd0efea1b441d86d9a75255815ba3efe09779b", "predicted_answer": "", "predicted_evidence": ["After that, LGI learned the syntax \u2018the size is big/small\u2019, followed by \u2018the size is not small/big\u2019. Figure 5 illustrates that LGI could correctly categorize whether the digit size was small or big with proper text output. And we witness that, based on the syntax of \u2018the size is big/small\u2019 (train steps =1000), the negative adverb \u2018not\u2019 in the language text \u2018the size is not small/big\u2019 was much easier to be learned (train steps =200, with same hyper-parameters). This is quite similar to the cumulative learning process of the human being."]}
{"question_id": "fb9e333a4e5d5141fe8e97b24b8f7e5685afbf09", "predicted_answer": "", "predicted_evidence": ["Language is the most remarkable characteristics distinguishing mankind from animals. Theoretically, all kinds of information such as object properties, tasks and goals, commands and even emotions can be described and conveyed by language [21]. We trained with LGI eight different syntaxes (in other word, eight different tasks), and LGI demonstrates its understanding by correctly interacting with the vision system. After learning \u2018this is 9\u2019, it is much easier to learn \u2018give me a 9\u2019; after learning the \u2018size is big\u2019, it is much easier to learn \u2018the size is not small\u2019. Maybe some digested words or syntaxes were represented by certain PFC units, which could be shared with the following sentence learning."]}
{"question_id": "cb029240d4dedde74fcafad6a46c1cfc2621b934", "predicted_answer": "", "predicted_evidence": ["[3] Miller, E. K. & Cohen, J. D. (2001). An integrative theory of prefrontal cortex function. Annual review of neuroscience, 24(1), 167-202."]}
{"question_id": "11a8531699952f5a2286a4311f0fe80ed1befa1e", "predicted_answer": "", "predicted_evidence": ["The language processing component first binarizes the input text symbol-wise into a sequence of binary vectors INLINEFORM0 , where T is the text length. To improve the language command recognition, we added one LSTM layer to extract the quantity information of the text (for example, suppose text = \u2018move left 12\u2019, the expected output INLINEFORM1 is 1 dimensional quantity 12 at the last time point). This layer mimics the number processing functionality of human Intra-Parietal Sulcus (IPS), so it is given the name IPS layer. The PFC outputs the desired activation of INLINEFORM2 , which can either be decoded by the \u2018texitizer\u2019 into predicted text or serve as INLINEFORM3 for the next iteration of the imagination process. Here, we propose a textizer (a rounding operation, followed by symbol mapping from binary vector, whose detailed discussion can be referred to the Supplementary section A) to classify the predicted symbol instead of softmax operation which has no neuroscience foundation."]}
{"question_id": "bcf222ad4bb537b01019ed354ea03cd6bf2c1f8e", "predicted_answer": "", "predicted_evidence": ["Imagination is another key component of human thinking. For the game Go [22, 23], the network using a reinforcement learning strategy has to be trained with billions of games in order to acquire a feeling (Q value estimated for each potential action) to move the chess. As human beings, after knowing the rule conveyed by language, we can quickly start a game with proper moves using a try-in-imagination strategy without requiring even a single practice. With imagination, people can change the answering contents (or even tell good-will lies) by considering or imagining the consequence of the next few output sentences. Machine equipped with the unique ability of imagination could easily select clever actions for multiple tasks without being trained heavily."]}
{"question_id": "af45ff2c4209f14235482329d0729864fb2bd4b0", "predicted_answer": "", "predicted_evidence": ["Due to this lack of large-scale datasets, many research studies BIBREF4, BIBREF2, BIBREF5 resort to automatic generation of artificial errors (also called pseudo-errors). Although such methods are efficient and have seen some success, they do not guarantee that generated errors reflect the range and the distribution of true errors made by humans BIBREF6."]}
{"question_id": "d2451d32c5a11a0eb8356a5e9d94a9231b59f198", "predicted_answer": "", "predicted_evidence": ["See Figure FIGREF33 for an overview of the distributions of these computed statistics per category for English. We observed similar trends for other two languages (Chinese and Japanese), except for a slightly larger number of spell edits, mainly due to the non-Latin character conversion errors. We also confirmed that the difference of perplexities between the source and the target for typo edits (i.e., mechanical, spell, and grammatical edits) was statistically significant for all three languages (two-tailed t-test, $p < .01$). This means that these edits, on average, turn the source text into a more fluent text in the target."]}
{"question_id": "90dde59e1857a0d2b1ee4615ab017fee0741f29f", "predicted_answer": "", "predicted_evidence": ["Filter out edits that are not written in human language"]}
{"question_id": "811b67460e65232b8f363dc3f329ffecdfcc4ab2", "predicted_answer": "", "predicted_evidence": ["Although GitHub provides a set of APIs (application programming interfaces) that allow end-users to access its data in a programmatic manner, it doesn't allow flexible querying on the repository meta data necessary for our data collection purposes. Therefore, we turn to GH Archive, which collects all the GitHub event data and make them accessible through flexible APIs. Specifically, we collected every repository from GH Archive that:"]}
{"question_id": "68aa460ad357b4228b16b31b2cbec986215813bf", "predicted_answer": "", "predicted_evidence": ["The rationale behind the third feature is that we observed that purely numerical changes always end up being tagged as semantic edits."]}
{"question_id": "4542b162a5be00206fd14570898a7925cb267599", "predicted_answer": "", "predicted_evidence": ["As one can see from Figure FIGREF45, simple spelling edits such as inserting \u201cs\u201d and deleting \u201ce\u201d dominate the lists. In fact, many of the frequent atomic edits even in Chinese and Japanese are made against English words (see Figure FIGREF27 for examples\u2014you notice many English words such as \u201cGB-18030\u201d and \u201cGemfile\u201d in non-English text). You also notice a number of grammatical edits in Chinese (e.g., confusion between the possessive particle de and the adjectival particle de) and Japanese (e.g., omissions of case particles such as wo, no, and ni). This demonstrates that the dataset can serve as a rich source of not only spelling but also naturally-occurring grammatical errors."]}
{"question_id": "a17fc7b96753f85aee1d2036e2627570f4b50c30", "predicted_answer": "", "predicted_evidence": ["Datasets: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16. They cover both factoid and non-factoid QA and different average passage length. The statistics of the four datasets are provided in the Appendix. To generate passage-level question-answering data from Quasart-t and SearchQA, we used the retrieved passages for each question from OpenQA, and generated question-passage relevance label based on whether the ground truth answer is contained in the passage."]}
{"question_id": "c6170bb09ba2a416f8fa9b542f0ab05a64dbf2e4", "predicted_answer": "", "predicted_evidence": ["Experiment Setting: We use the same pooling methods as in the sentence embedding experiment to extract passage embeddings, and make sure that the passage length is within BERT's maximum sequence length. Different methods of combining query embeddings with answer passage embeddings were explored including: cosine similarity (no trainable parameter), bilinear function, concatenation, and $(u, v, u * v, |u - v|)$ where $u$ and $v$ are query embedding and answer embedding, respectively. A logistic regression layer or an MLP layer is added on top of the embeddings to output a ranking score. We apply the pairwise rank hinge loss $l(q, +a, -a; \\theta ) = max\\lbrace 0, - S(q, +a; \\theta )+S(q, -a; \\theta )\\rbrace $ to every tuple of $(query, +answer, -answer)$. Ranking metrics such as MRR (mean reciprocal rank), MAP (mean average precision), Precision@K and Recall@K are used to measure the performance. We compared BERT passage embeddings against the baseline of BM25, other state-of-the-art models, and a fine-tuned BERT on in-domain supervised data which serves as the upper bound. For in-domain BERT fine-tuning, we feed the hidden state of the [CLS] token from the top layer into a two-layer MLP which outputs a relevance score between the question and candidate answer passage. We fine-tune all BERT parameters except the word embedding layers."]}
{"question_id": "fe080c6393f126b55ae456b81133bfc8ecbe85c2", "predicted_answer": "", "predicted_evidence": ["Effect of Pooling Methods: We examined different methods of extracting BERT hidden state activations. The pooling methods we evaluated include: CLS-pooling (the hidden state corresponding to the [CLS] token), SEP-pooling (the hidden state corresponding to the [SEP] token), Mean-pooling (the average of the hidden state of the encoding layer on the time axis), and Max-pooling (the maximum of the hidden state of the encoding layer on the time axis). To eliminate the layer-wise effects, we averaged the performance of each pooling method over different layers. The results are summarized in Table TABREF2, where the score for each task category is calculated by averaging the normalized values for the tasks within each category. Although the activations of [CLS] token hidden states are often used in fine-tuning BERT for classification tasks, Mean-pooling of hidden states performs the best in all task categories among all the pooling methods."]}
{"question_id": "53a8c3cf22d6bf6477bc576a85a83d8447ee0484", "predicted_answer": "", "predicted_evidence": ["Effect of Encoder Layer: We compare the performance of embeddings extracted from different encoder layers of a pre-trained BERT using bert-as-service BIBREF10. Since we are interested in the linguistic information encoded in the embeddings, we only add a logistic regression layer on top of the embeddings for each classification task. The results of using [CLS] token activations as embeddings are presented in Figure FIGREF1. The raw values are provided in the Appendix. In the heatmap, the raw values of metrics are normalized by the best performance of a particular task from all the models we evaluated including BERT. The tasks in the figure are grouped by task category. For example, all semantic similarity related tasks are placed at the top of the figure."]}
{"question_id": "3a33512d253005ac280ee9ca4f9dfa69aa38d48f", "predicted_answer": "", "predicted_evidence": ["Datasets: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16. They cover both factoid and non-factoid QA and different average passage length. The statistics of the four datasets are provided in the Appendix. To generate passage-level question-answering data from Quasart-t and SearchQA, we used the retrieved passages for each question from OpenQA, and generated question-passage relevance label based on whether the ground truth answer is contained in the passage."]}
{"question_id": "f7f2968feb28c2907266c892f051ae9f7d6286e6", "predicted_answer": "", "predicted_evidence": ["We use the SentEval toolkit to evaluate the quality of sentence representations from BERT activations. The evaluation encompasses a variety of downstream and probing tasks. Downstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity. Probing tasks use single sentence embedding as input, are designed to probe sentence-level linguistic phenomena, from superficial properties of sentences to syntactic information to semantic acceptability. For details about the tasks, please refer to BIBREF8 and BIBREF9. We compare the BERT embeddings against two state-of-the-art sentence embeddings, Universal Sentence Encoder BIBREF5, InferSent BIBREF2, and a baseline of averaging GloVe word embeddings."]}
{"question_id": "38289bd9592db4d3670b65a0fef1fe8a309fee61", "predicted_answer": "", "predicted_evidence": ["Accuracy scores using other learning algorithms were significantly lower (see Table TABREF10 ), therefore, we report only the results of the logistic regression classifier in the subsequent sections."]}
{"question_id": "cb7a00233502c4b7801d34bc95d6d22d79776ae8", "predicted_answer": "", "predicted_evidence": ["In the following sections, we first describe our datasets (section SECREF2 ) and features (section SECREF3 ), then we present the details and the results of our experiments in section SECREF4 . Finally, section SECREF5 concludes our work and outlines further directions of research within this area."]}
{"question_id": "35d2eae3a7c9bed54196334a09344591f9cbb5c8", "predicted_answer": "", "predicted_evidence": ["In the following sections, we first describe our datasets (section SECREF2 ) and features (section SECREF3 ), then we present the details and the results of our experiments in section SECREF4 . Finally, section SECREF5 concludes our work and outlines further directions of research within this area."]}
{"question_id": "a70656fc61bf526dd21db7d2ec697b29a5a9c24e", "predicted_answer": "", "predicted_evidence": ["Morphological (Morph): We included the variation (the ratio of a category to the ratio of lexical tokens - i.e. nouns, verbs, adjectives and adverbs) and the IncSc of all lexical categories together with the IncSc of punctuations, particles, sub- and conjunctions (#34, #51). Some additional features, using insights from L2 teaching material BIBREF20 , captured fine-grained inflectional information such as the IncSc of neuter gender nouns and the ratio of different verb forms to all verbs (#52 - #56). Instead of simple type-token ratio (TTR) we used a bilogarithmic and a square root TTR as in BIBREF3 . Moreover, nominal ratio BIBREF4 , the ratio of pronouns to prepositions BIBREF13 , and two lexical density features were also included: the ratio of lexical words to all non-lexical categories (#48) and to all tokens (#49). Relative structures (#57) consisted of relative adverbs, determiners, pronouns and possessives."]}
{"question_id": "f381b0ef693243d67657f6c34bbce015f6b1fd07", "predicted_answer": "", "predicted_evidence": ["In the future, a more detailed investigation is needed to understand the performance drop between document and sentence level. Acquiring more sentence-level annotated data and exploring new features relying on lexical-semantic resources for Swedish would be interesting directions to pursue. Furthermore, we intend to test the utility of this approach in a real-world web application involving language learners and teachers."]}
{"question_id": "c176eb1ccaa0e50fb7512153f0716e60bf74aa53", "predicted_answer": "", "predicted_evidence": ["To understand more the NLI features performance, given their high performance comparing to the other features, we extract the top important tokens for each of the NLI feature subsets (see Figure FIGREF37). Some of the obtained results confirmed what was found previously. For instance, the authors in BIBREF19 found that Russians write English tweets with more prepositions comparing to native speakers of other languages (e.g. as, about, because in (c) Stop-words and RP in (a) POS in Figure FIGREF37). Further research must be conducted to investigate in depth the rest of the results."]}
{"question_id": "e0b54906184a4ad87d127bed22194e62de38222b", "predicted_answer": "", "predicted_evidence": ["For the theme-based features, we use the following features that we believe that they change based on the themes:"]}
{"question_id": "1f8044487af39244d723582b8a68f94750eed2cc", "predicted_answer": "", "predicted_evidence": ["Why do the thematic information help? The Flip-Flop behavior. As an example, let's considering the fear and joy emotions in Figure FIGREF34. We can notice that all the themes that used to nudge the division issues have a decreasing dashed line, where others such as Supporting Trump theme has an extremely increasing dashed line. Therefore, we manually analyzed the tweets of some IRA accounts and we found this observation clear, as an example from user $x$:"]}
{"question_id": "595fe416a100bc7247444f25b11baca6e08d9291", "predicted_answer": "", "predicted_evidence": ["For the theme-based features, we use the following features that we believe that they change based on the themes:"]}
{"question_id": "1f011fa772ce802e74eda89f706cdb1aa2833686", "predicted_answer": "", "predicted_evidence": ["To understand more the NLI features performance, given their high performance comparing to the other features, we extract the top important tokens for each of the NLI feature subsets (see Figure FIGREF37). Some of the obtained results confirmed what was found previously. For instance, the authors in BIBREF19 found that Russians write English tweets with more prepositions comparing to native speakers of other languages (e.g. as, about, because in (c) Stop-words and RP in (a) POS in Figure FIGREF37). Further research must be conducted to investigate in depth the rest of the results."]}
{"question_id": "181027f398a6b79b1ba44d8d41cc1aba0d6f5212", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 and INLINEFORM1 denotes the log likelihood functions of Reader and Encoder, respectively."]}
{"question_id": "ab097db03652b8b38edddc074f23e2adf9278cba", "predicted_answer": "", "predicted_evidence": [" In this section, we describe the learning process of the parameters of KeyVec. Similarly to most neural network models, KeyVec can be trained using Stochastic Gradient Descent (SGD), where the Neural Reader and Neural Encoder are jointly optimized. In particular, the parameters of Reader and Encoder are learned simultaneously by maximizing the joint likelihood of the two components:"]}
{"question_id": "5d4190403eb800bb17eec71e979788e11cf74e67", "predicted_answer": "", "predicted_evidence": ["In document INLINEFORM0 , the set of key words INLINEFORM1 is composed of top 30 words in INLINEFORM2 (i.e., the gold summary of INLINEFORM3 ) with the highest TF-IDF scores. Encoder's objective is then formalized by maximizing the probability of predicting the key words in INLINEFORM4 using the document embedding INLINEFORM5 :"]}
{"question_id": "56d41e0fcc288c1e65806ae77097d685c83e22db", "predicted_answer": "", "predicted_evidence": ["Given the embeddings of sentences INLINEFORM0 in a document INLINEFORM1 , Neural Reader computes the probability of each sentence INLINEFORM2 being a key sentence, denoted as INLINEFORM3 ."]}
{"question_id": "1237b6fcc64b43901415f3ded17cc210a54ab698", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 and INLINEFORM1 denotes the log likelihood functions of Reader and Encoder, respectively."]}
{"question_id": "31cba86bc45970337ba035ecf36d8954a9a5206a", "predicted_answer": "", "predicted_evidence": ["Besides the role of the executive of the research and development programme itself, Almannar\u00f3mur will conduct communication between the executing parties and the local industry, as well as foreign companies and institutions. Together with the executing parties, Almannar\u00f3mur will also host conferences and events to promote the programme and bring together interested parties."]}
{"question_id": "3a25f82512d56d9e1ffba72f977f515ae3ba3cca", "predicted_answer": "", "predicted_evidence": ["As mentioned above, a number of language resources have been made available at the repository m\u00e1lf\u00f6ng. Most of these are now also available at the CLARIN-IS website and will be integrated into the CLARIN Virtual Language Observatory. Below we give a brief and non-exhaustive overview of language resources for Icelandic which will be developed in the programme."]}
{"question_id": "b59f3a58939f7ac007d3263a459c56ebefc4b49a", "predicted_answer": "", "predicted_evidence": ["After studying somewhat similar national programmes in other European countries, we have defined the most important factors that in our opinion will help lead to the success of the programme: First, we have defined core projects that comprise the most important language resources and software tools necessary for various LT applications. Second, all deliverables will be published under as open licenses as possible and all resources and software will be easily accessible. The deliverables will be packaged and published for use in commercial applications, where applicable. Third, from the beginning of the programme, we encourage innovation projects from academia and industry through a competitive R&D fund, and fourth, constant communication with users and industry through conferences, events and direct interaction will be maintained, with the aim of putting deliverables to use in products as soon as possible. The cooperation between academia and industry is also reflected in the consortium of universities, institutions, associations, and private companies, performing the R&D work for all core projects."]}
{"question_id": "b4b7333805cb6fdde44907747887a971422dc298", "predicted_answer": "", "predicted_evidence": ["In Estonia, three consecutive national programmes have been launched. The third national programme, Estonian Language Technology 2018\u20132027, is currently under way. While the Estonian Ministry of Education and Research has been responsible for the programmes, the universities in Tallinn and Tartu, together with the Institute of the Estonian Language, led the implementation."]}
{"question_id": "871f7661f5a3da366b0b5feaa36f54fd3dedae8e", "predicted_answer": "", "predicted_evidence": ["The history of Icelandic LT is usually considered to have begun around the turn of the century, even though a couple of LT resources and products were developed in the years leading up to that. Following the report of an expert group appointed by the Minister of Education, Science and Culture BIBREF7, the Icelandic Government launched a special LT Programme in the year 2000, with the aim of supporting institutions and companies to create basic resources for Icelandic LT work. This initiative resulted in a few projects which laid the ground for future work in the field. The most important of these were a 25 million token, balanced, tagged corpus, a full-form database of Icelandic inflections, a training model for PoS taggers, an improved speech synthesiser, and an isolated word speech recogniser BIBREF8."]}
{"question_id": "3fafde90eebc1c00ba6c3fb4c5b984009393ce7f", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF19 shows the agreement between our aspects and that of the dataset. We assumed two aspects as equal when they were textually the same. We made some experiments using text distance metrics, such as the Jaro-Winkler distance, but the results did not differ significantly from an exact matching. We fitted the importance factor value (on the X axis) so as to enrich final aspects set: a higher factor resulted in a larger aspects set and a higher value of precision metric, with slowly decreasing recall. First results (blue line on charts) were not satisfactory, so we removed a sentiment filtering step of analysis (orange line on chart), which doubled the precision value, with nearly the same value of recall. The level of precision for whole dataset (computer, router, and speaker) was most of the time at the same level. However, the recall of router was significantly worse than speaker and computer sets."]}
{"question_id": "e6bc11bd6cfd4b2138c29602b9b56fc5378a4293", "predicted_answer": "", "predicted_evidence": ["A sentiment analysis can be made at the level of (1) the whole document, (2) the individual sentences, or (what is currently seen as the most attractive approach) (3) at the level of individual fragments of text. Regarding document level analysis BIBREF8 , BIBREF9 - the task at this level is to classify whether a full opinion expresses a positive, negative or neutral attitude. For example, given a product review, the model determines whether the text shows an overall positive, negative or neutral opinion about the product. The biggest disadvantage of document level analysis is an assumption that each document expresses views on a single entity. Thus, it is not applicable to documents which evaluate or compare multiple objects. As for sentence level analysis BIBREF10 - The task at this level relates to sentences and determines whether each sentence expressed a positive, negative, or neutral opinion. This level of analysis is closely related to subjectivity classification which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express subjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions. With feature/aspect level analysis BIBREF11 - both the document level and the sentence level analyses do not discover what exactly people liked and did not like. A finer-grained analysis can be performed at aspect level. Aspect level was earlier called feature/aspect level. Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). As a result, we can aggregate the opinions. For example, the phone display gathers positive feedback, but the battery is often rated negatively. The aspect-based level of analysis is much more complex since it requires more advanced knowledge representation than at the level of entire documents only. Also, the documents often consist of multiple sentences, so saying that the document is positive provides only partial information. In the literature, there exists some initial work related to aspects. There exist initial solutions that use SVM-based algorithms BIBREF12 or conditional random field classifiers BIBREF13 with manually engineered features. There also exist some solutions based on deep neural networks, such as connecting sentiments with the corresponding aspects based on the constituency parse tree BIBREF11 ."]}
{"question_id": "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521", "predicted_answer": "", "predicted_evidence": ["The last step covers summary (abstract) generation in natural language. Natural language generation models use parameterized templates (very limited and dependent on the size of the rule-based system responsible for the completions of the text), or deep neural networks BIBREF17 ."]}
{"question_id": "4748a50c96acb1aa03f7efd1b43376c193b2450a", "predicted_answer": "", "predicted_evidence": ["We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 . Aspects in these review datasets were annotated manually."]}
{"question_id": "acac0606aab83cae5d13047863c7af542d58e54c", "predicted_answer": "", "predicted_evidence": ["Table TABREF14 compares the accuracy reported on Dataset I for the methods suggested in BIBREF1, BIBREF4 with the accuracy obtained by our methods, as well as the latter on Dataset II, using 10-fold cross-validation in all cases. Table TABREF14 reports accuracy results of the more stringent analysis described in section SECREF13."]}
{"question_id": "2ee4ecf98ef7d02c9e4d103968098fe35f067bbb", "predicted_answer": "", "predicted_evidence": ["We consider three datasets, two of which are a contribution of this work."]}
{"question_id": "82f8843b59668567bba09fc8f93963ca7d1fe107", "predicted_answer": "", "predicted_evidence": ["We consider three datasets, two of which are a contribution of this work."]}
{"question_id": "376e8ed6e039e07c892c77b7525778178d56acb7", "predicted_answer": "", "predicted_evidence": ["Dataset III is extracted from 3561 concepts whose Wikipedia pages are under edit protection, assuming that many of them are likely to be controversial. They were then crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10. We used this dataset for testing the models trained on Dataset I."]}
{"question_id": "4de6bcddd46726bf58326304b0490fdb9e7e86ec", "predicted_answer": "", "predicted_evidence": ["Nearest neighbors (NN) Estimator: We used the pre-trained GloVe embeddings BIBREF11 of concepts to implement a nearest-neighbor estimator as follows. Given a concept $c$, we extract all labeled concepts within a given radius $r$ (cosine similarity $0.3$). In one variant, $c$'s controversiality score is taken to be the fraction of controversial concepts among them. In another variant, labeled concepts are weighted by their cosine similarity to $c$."]}
{"question_id": "e831ce6c406bf5d1c493162732e1b320abb71b6f", "predicted_answer": "", "predicted_evidence": ["We are grateful to Shiri Dori-Hacohen and Hoda Sepehri Rad for sharing their data with us and giving us permission to use it."]}
{"question_id": "634a071b13eb7139e77872ecfdc135a2eb2f89da", "predicted_answer": "", "predicted_evidence": ["Table TABREF14 compares the accuracy reported on Dataset I for the methods suggested in BIBREF1, BIBREF4 with the accuracy obtained by our methods, as well as the latter on Dataset II, using 10-fold cross-validation in all cases. Table TABREF14 reports accuracy results of the more stringent analysis described in section SECREF13."]}
{"question_id": "8861138891669a45de3955c802c55a37be717977", "predicted_answer": "", "predicted_evidence": ["We are grateful to Shiri Dori-Hacohen and Hoda Sepehri Rad for sharing their data with us and giving us permission to use it."]}
{"question_id": "267d70d9f3339c56831ea150d2213643fbc5129b", "predicted_answer": "", "predicted_evidence": ["Finally, we present the visual results in Figure FIGREF14 , which includes examples of funny captions obtained using NJM. Although the original caption is in Japanese, we also translated the captions into English. Enjoy!"]}
{"question_id": "477da8d997ff87400c6aad19dcc74f8998bc89c3", "predicted_answer": "", "predicted_evidence": ["We conducted evaluations to confirm the effectiveness of the proposed method. We describe the experimental method in Section SECREF11 , and the experimental results are presented in Section SECREF12 ."]}
{"question_id": "4485e32052741972877375667901f61e602ec4de", "predicted_answer": "", "predicted_evidence": ["We are currently posting funny captions generated by the NJM to the Bokete Ogiri website in order to evaluate the proposed method. Here, we compare the proposed method with STAIR captions. As reported by Bokete users, the funny captions generated by STAIR caption averaged 1.71 stars, whereas the NJM averaged 3.23 stars. Thus, the NJM is funnier than the baseline STAIR caption according to Bokete users. We believe that this difference is the result of using (i) Funny Score to effectively train the generator regarding funny captions and (ii) the self-collected BoketeDB, which is a large-scale database for funny captions."]}
{"question_id": "df4895c6ae426006e75c511a304d56d37c42a1c7", "predicted_answer": "", "predicted_evidence": ["Chandrasekaran et al. conducted a humor enhancement of an image BIBREF4 by constructing an analyzer to quantify \u201cvisual humor\u201d in an image input. They also constructed datasets including interesting (3,200) and non-interesting (3,200) human-labeled images to evaluate visual humor. The \u201cfunniness\u201d of an image can be trained by defining five stages."]}
{"question_id": "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e", "predicted_answer": "", "predicted_evidence": ["We present the results of our experiments with both the Evo-Devo and the Machine Learning mechanisms described earlier using the U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12 ."]}
{"question_id": "54b0d2df6ee27aaacdaf7f9c76c897b27e534667", "predicted_answer": "", "predicted_evidence": ["General-purpose ASR engines when used for enterprise domains may output erroneous text, especially when encountering domain-specific terms. One may have to adapt/repair the ASR output in order to do further natural language processing such as question-answering. We have presented two mechanisms for adaptation/repair of ASR-output with respect to a domain. The Evo-Devo mechanism provides a bio-inspired abstraction to help structure the adaptation and repair process. This is one of the main contribution of this paper. The machine learning mechanism provides a means of adaptation and repair by examining the feature-space of the ASR output. The results of the experiments show that both these mechanisms are promising and may need further development."]}
{"question_id": "b9a3836cff16af7454c7a8b0e5ff90206d0db1f5", "predicted_answer": "", "predicted_evidence": ["In the machine learning technique of adaptation, we considers INLINEFORM0 pairs as the predominant entity and tests the accuracy of classification of errors."]}
{"question_id": "99554d0c76fbaef90bce972700fa4c315f961c31", "predicted_answer": "", "predicted_evidence": ["The ASR output ( INLINEFORM0 ) are then given as input in the Evo-Devo and Machine Learning mechanism of adaptation."]}
{"question_id": "5370a0062aae7fa4e700ae47aa143be5c5fc6b22", "predicted_answer": "", "predicted_evidence": ["Our experiments therefore aim to evaluate on a wider range of target languages, and to explore the effects of both the amount of labeled data, and the number of languages from which it is obtained."]}
{"question_id": "9a52a33d0ae5491c07f125454aea9a41b9babb82", "predicted_answer": "", "predicted_evidence": ["Next we investigate how labeled data from high-resource languages can be used to obtain improved features on a target zero-resource language for which no labeled data is available."]}
{"question_id": "8c46a26f9b0b41c656b5b55142d491600663defa", "predicted_answer": "", "predicted_evidence": ["To determine whether these gains come from the diversity of training languages or just the larger amount of training data, we trained models on the 15 hour subset and the full 81 hours of the English wsj corpus, which corresponds to the amount of data of four GlobalPhone languages. More data does help to some degree, as Figure FIGREF21 shows. But, except for Mandarin, training on just two languages (46 hours) already works better."]}
{"question_id": "e5f8d2fc1332e982a54ee4b4c1f7f55e900d0b86", "predicted_answer": "", "predicted_evidence": ["These systems score better than all of our features, but are not directly comparable for several reasons. Firstly, it is unclear how these systems were optimized, since there was no separate development set in zrsc 2015. Secondly, our features are all 39-dimensional to be directly comparable with MFCCs, whereas the other two systems have higher dimensionality (and indeed the winning system from zrsc 2017 was even greater, with more than 1000 dimensions BIBREF17 ). Such higher dimensional features may be useful in some circumstances, but lower dimensional features are often more efficient to work with and we don't know whether the competing systems would work as well with fewer dimensions."]}
{"question_id": "19578949108ef72603afe538059ee55b4dee0751", "predicted_answer": "", "predicted_evidence": ["As our focus was on deep methods for MDS, we only tested several non-neural baselines. However, other classical methods deserve more attention, for which we refer the reader to Hong14 and leave the implementation of these methods on Multi-News for future work."]}
{"question_id": "44435fbd4087fa711835d267036b6a1f82336a22", "predicted_answer": "", "predicted_evidence": ["In this section we describe additional methods we compare with and present our assumptions and experimental process."]}
{"question_id": "86656aae3c27c6ea108f5600dd09ab7e421d876a", "predicted_answer": "", "predicted_evidence": ["The number of collected Wayback links for summaries and their corresponding cited articles totals over 250,000. We only include examples with between 2 and 10 source documents per summary, as our goal is MDS, and the number of examples with more than 10 sources was minimal. The number of source articles per summary present, after downloading and processing the text to obtain the original article text, varies across the dataset, as shown in Table TABREF4 . We believe this setting reflects real-world situations; often for a new or specialized event there may be only a few news articles. Nonetheless, we would like to summarize these events in addition to others with greater news coverage."]}
{"question_id": "22488c8628b6db5fd708b6471c31a8eac31f66df", "predicted_answer": "", "predicted_evidence": ["Our dataset, which we call Multi-News, consists of news articles and human-written summaries of these articles from the site newser.com. Each summary is professionally written by editors and includes links to the original articles cited. We will release stable Wayback-archived links, and scripts to reproduce the dataset from these links. Our dataset is notably the first large-scale dataset for MDS on news articles. Our dataset also comes from a diverse set of news sources; over 1,500 sites appear as source documents 5 times or greater, as opposed to previous news datasets (DUC comes from 2 sources, CNNDM comes from CNN and Daily Mail respectively, and even the Newsroom dataset BIBREF6 covers only 38 news sources). A total of 20 editors contribute to 85% of the total summaries on newser.com. Thus we believe that this dataset allows for the summarization of diverse source documents and summaries."]}
{"question_id": "1f2952cd1dc0c891232fa678b6c219f6b4d31958", "predicted_answer": "", "predicted_evidence": ["Experiments #7, #8, #9 and #10 with comparison to #6 showed that reducing vocabulary too much also negatively affects BLEU. Though Experiment #9 with $1k$ target vocabulary has the lowest $D$ favoring the $C$, in comparison to others, the BLEU is still lower than the others. An explanation for this reduction is that $\\mu $ is higher and unfavorable to $R$. Hence a strictly smaller vocabulary is not the best choice either."]}
{"question_id": "23fe8431058f2a7b7588745766fc715f271aad07", "predicted_answer": "", "predicted_evidence": ["Machine translation is commonly defined as the task of transforming sequences from the form $x = x_1 x_2 x_3 ... x_m$ to $y = y_1 y_2 y_3 ... y_n$, where $x$ is from source language $X$ and $y$ is from target language $Y$ respectively. NMT accomplishes the translation objective using artificial neural networks."]}
{"question_id": "e5b2eb6a49c163872054333f8670dd3f9563046a", "predicted_answer": "", "predicted_evidence": ["We categorize the related work into the subsections as following:"]}
{"question_id": "73760a45b23b2ec0cab181f82953fb296bb6cd19", "predicted_answer": "", "predicted_evidence": ["Experiments #7, #8, #9 and #10 with comparison to #6 showed that reducing vocabulary too much also negatively affects BLEU. Though Experiment #9 with $1k$ target vocabulary has the lowest $D$ favoring the $C$, in comparison to others, the BLEU is still lower than the others. An explanation for this reduction is that $\\mu $ is higher and unfavorable to $R$. Hence a strictly smaller vocabulary is not the best choice either."]}
{"question_id": "ec990c16896793a819766bc3168c02556ef69971", "predicted_answer": "", "predicted_evidence": ["Experiments #7, #8, #9 and #10 with comparison to #6 showed that reducing vocabulary too much also negatively affects BLEU. Though Experiment #9 with $1k$ target vocabulary has the lowest $D$ favoring the $C$, in comparison to others, the BLEU is still lower than the others. An explanation for this reduction is that $\\mu $ is higher and unfavorable to $R$. Hence a strictly smaller vocabulary is not the best choice either."]}
{"question_id": "11c4071d9d7efeede84f47892b1fa0c6a93667eb", "predicted_answer": "", "predicted_evidence": ["When a model is used in a domain mismatch scenario, i.e. where a test set's distribution does not match the training set's distribution, model performance generally degrades. It is not surprising that frequency-biased classifiers show particular degradation in domain mismatch scenarios, as types that were infrequent in the training distribution and were ignored by learning algorithm may appear with high frequency in the newer domain. koehn2017sixchallenges showed empirical evidence of poor generalization of NMT to out-of-domain datasets."]}
{"question_id": "9aa751aebf6a449d95fb04ceec71688f2ed2cea2", "predicted_answer": "", "predicted_evidence": ["For a comparison, word and character segmentation have no influence on $\\mu $. However, the trim size of word and character vocabulary has an effect on class imbalance $D$ and Out-of-Vocabulary (OOV) tokens and is presented in Figures FIGREF9 and FIGREF9, respectively. The summary of word, character, and BPE with respect to $D$ and $\\mu $ is presented in Table TABREF10."]}
{"question_id": "2929e92f9b4939297b4d0f799d464d46e8d52063", "predicted_answer": "", "predicted_evidence": ["Table TABREF22 shows the entity-chunking ablation results on OntoNotes 5.0 development set. Both Att and Baseline models were taken without re-training for this subtask. The $HC^{all}$ column lists the performance of Att-BiLSTM-CNN on each chunking tag. Other columns list the performance compared to $HC^{all}$. Columns $H$ to $C^5$ are when the full model is deprived of all other information in testing time by forcefully zeroing all vectors except the one specified by the column header. The figures shown in the table are per-token recalls for each chunking tag, which tells if a part of the model is responsible for signaling the whole model to predict that tag. Colors mark relatively high and low values of interest."]}
{"question_id": "1dcfcfa46dbcffc2fc7be92dd57df9620258097b", "predicted_answer": "", "predicted_evidence": ["Table TABREF17 shows results among different entity lengths. It could be seen that cross-structures were much better at dealing with multi-token mentions (1.8%/2.3%/8.7%/2.6%) compared to the prevalently used, problematic Baseline."]}
{"question_id": "77bbe1698e001c5889217be3164982ea36e85752", "predicted_answer": "", "predicted_evidence": ["All models in the experiments use the same set of raw features: character embedding, character type, word embedding, and word capitalization."]}
{"question_id": "b537832bba2eb6d34702a9d71138e661c05a7c3a", "predicted_answer": "", "predicted_evidence": ["We use the following dataset orders (chosen randomly) for text classification:"]}
{"question_id": "1002bd01372eba0f3078fb4a951505278ed45f2e", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF34 shows INLINEFORM0 score and accuracy of various models on the test set corresponding to the first dataset seen during training as the models are trained on more datasets. The figure illustrates how well each model retains its previously acquired knowledge as it learns new knowledge. We can see that MbPA++ is consistently better compared to other methods."]}
{"question_id": "3450723bf66956486de777f141bde5073e4a7694", "predicted_answer": "", "predicted_evidence": ["MbPA++: our episodic memory model described in \u00a7 SECREF2 ."]}
{"question_id": "36cb7ebdd39e0b8a89ff946d3a3aef8a76a6bb43", "predicted_answer": "", "predicted_evidence": ["On the other hand, the improved model (RNNwA + n-gram), where neural and hand-crafted features are concatenated, increases the accuracy of the proposed model by approximately $0,5$% on English and approximately 2% in Spanish and Arabic. This also supports our intuition that the performance of neural models can be improved by hand-crafted features, which is based on the study of BIBREF14. As can be seen in Table TABREF11, the improved model outperforms the state-of-the-art method of BIBREF3 in English and produces competitive results in Spanish and Arabic."]}
{"question_id": "28e50459da60ceda49fe1578c12f3f805b288bd0", "predicted_answer": "", "predicted_evidence": ["On the other hand, the improved model (RNNwA + n-gram), where neural and hand-crafted features are concatenated, increases the accuracy of the proposed model by approximately $0,5$% on English and approximately 2% in Spanish and Arabic. This also supports our intuition that the performance of neural models can be improved by hand-crafted features, which is based on the study of BIBREF14. As can be seen in Table TABREF11, the improved model outperforms the state-of-the-art method of BIBREF3 in English and produces competitive results in Spanish and Arabic."]}
{"question_id": "e1f61500eb733f2b95692b6a9a53f8aaa6f1e1f6", "predicted_answer": "", "predicted_evidence": ["where $W_\\alpha $ is a learnable weight matrix that is used to multiply each output of the RNN, $t_i$ is the feature vector of $i$th tweet, $b$ is a learnable bias vector, $w_i$ is a learnable attention weight, $A_i$ is the attention context vector, $v_i$ is the attention value for $i$th tweet, $o_i$ is attention output vector for the corresponding tweet, $K$ is the output vector for user. Matrix $W_\\alpha $ and vectors $w_i$ and $b$ are learned parameters."]}
{"question_id": "da4d07645edaf7494a8cb5216150a00690da01f7", "predicted_answer": "", "predicted_evidence": ["where $T_D$ is the dynamic cache built on top of $T_P$. $T_D$ may need to copy some states from $T_P$ if we need to update information for those states in $T_P$."]}
{"question_id": "c0cebef0e29b9d13c165b6f19f6ca8393348c671", "predicted_answer": "", "predicted_evidence": ["In previous work, a method was proposed to do a pre-initialized composition for a non-class LM BIBREF3. However, it the dynamic part is still expanded on-the-fly. In this work, we propose two improvements in order to best leverage class language models. First, we use simpler methods for pre-initialization which do not need to pre-generate decoder state statistics. Second, we propose a two-layer pre-initialization mechanism that also avoids performing dynamic expansion on per user basis. In the two-layer pre-initialization method, we make use of a class LM with class tag. We build a personalized FST that contains the members of the class for each user. Using the FST replacement algorithm, we obtain a personalized language transducer BIBREF4. We perform a pre-composition for all FST states whose transitions do not contain class tags. By doing so, the actual on-demand composition is only required for the states in personalized FST. For a multi-threaded service, the pre-composed FST can be shared by all threads, since it does not contain personalized FST states (non-terminals). The personalized part will be shared for all utterances from the same user, which will take full advantage of memory usage."]}
{"question_id": "5695908a8c6beb0e3863a1458a1b93aab508fd34", "predicted_answer": "", "predicted_evidence": ["Speech input is now a common feature for smart devices. In many cases, the user's query involves entities such as a name from a contact list, a location, or a music title. Recognizing entities is particularly challenging for speech recognition because many entities are infrequent or out of the main vocabulary of the system. One way to improve performance is such cases is through the use of a personal language model (LM) which contains the expected user-specific entities. Because each user can have their own personalized LM, it is vital that the speech decoder be able to efficiently load the model on the fly, so it can be used in decoding, without any noticeable increase in latency."]}
{"question_id": "fa800a21469a70fa6490bfc67cabdcc8bf086fb5", "predicted_answer": "", "predicted_evidence": ["Author profile (auth). In order to test whether community-based information of authors is in itself sufficient to correctly classify the content produced by them, we utilize just the author profiles we generated to train a gbdt classifier."]}
{"question_id": "6883767bbdf14e124c61df4f76335d3e91bfcb03", "predicted_answer": "", "predicted_evidence": ["Author profiling has been leveraged in several ways for a variety of purposes in nlp. For instance, many studies have relied on demographic information of the authors. Amongst these are Hovy et al. hovy2015demographic and Ebrahimi et al. ebrahimi2016personalized who extracted age and gender-related information to achieve superior performance in a text classification task. Pavalanathan and Eisenstein pavalanathan2015confounds, in their work, further showed the relevance of the same information to automatic text-based geo-location. Researching along the same lines, Johannsen et al. johannsen2015cross and Mirkin et al. mirkin2015motivating utilized demographic factors to improve syntactic parsing and machine translation respectively."]}
{"question_id": "11679d1feba747c64bbbc62939a20fbb69ada0f3", "predicted_answer": "", "predicted_evidence": ["The author profiling features on their own (auth) achieve impressive results overall and in particular on the sexism class, where their performance is typical of a community-based generalization, i.e., low precision but high recall. For the racism class on the other hand, the performance of auth on its own is quite poor. This contrast can be explained by the fact that tweets in the racism class come from only 5 unique authors who: (i) are isolated in the community graph, or (ii) have also authored several tweets in the sexism class, or (iii) are densely connected to authors from the sexism and none classes which possibly camouflages their racist nature."]}
{"question_id": "e0c80d31d590df46d33502169b1d32f0aa1ea6e3", "predicted_answer": "", "predicted_evidence": ["A user's profile is a single document, also represented as a sequence of tokens. For each user, we populate the profile input using the plain text user description associated with their account, which often contains terms which express self-identity such as \u201crepublican\u201d or \u201cathiest.\u201d"]}
{"question_id": "7a8b24062a5bb63a8b4c729f6247a7fd2fec7f07", "predicted_answer": "", "predicted_evidence": ["What a person does says a lot about who they are. Information about the types of activities that a person engages in can provide insights about their interests BIBREF0 , personality BIBREF1 , physical health BIBREF2 , the activities that they are likely to do in the future BIBREF3 , and other psychological phenomena like personal values BIBREF4 . For example, it has been shown that university students who exhibit traits of interpersonal affect and self-esteem are more likely to attend parties BIBREF5 , and those that value stimulation are likely to watch movies that can be categorized as thrillers BIBREF6 ."]}
{"question_id": "cab082973e1648b0f0cc651ab4e0298a5ca012b5", "predicted_answer": "", "predicted_evidence": ["Given a set of activity clusters and knowledge about the users who have reported to have participated in these activities, we explore the ability of machine learning models to make inferences about which activities are likely to be next performed by a user. Here we describe the supervised learning setup, evaluation, and neural architecture used for the prediction task."]}
{"question_id": "1cc394bdfdfd187fc0af28500ad47a0a764d5645", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is a set of activity clusters, INLINEFORM1 , INLINEFORM2 , and INLINEFORM3 are vectors that represent the user's history, profile, and attributes, respectively, and INLINEFORM4 is the target cluster. The target cluster is the cluster label of an activity cluster that contains an activity that is known to have been performed by the user."]}
{"question_id": "16cc37e4f8e2db99eaf89337a3d9ada431170d5b", "predicted_answer": "", "predicted_evidence": ["We split our data at the user-level, and from our set of valid users we use 200,000 instances for training data, 10,000 as test data, and the rest as our validation set."]}
{"question_id": "cc78a08f5bfe233405c99cb3dac1f11f3a9268b1", "predicted_answer": "", "predicted_evidence": ["While we do not expect to know exactly what a person is doing at any given time, it is fairly common for people to publicly share the types of activities that they are doing by making posts, written in natural language, on social media platforms like Twitter. However, when taking a randomly sampled stream of tweets, we find that only a small fraction of the content was directly related to activities that the users were doing in the real world \u2013 instead, most instances are more conversational in nature, or contain the sharing of opinions about the world or links to websites or images. Using such a random sample would require us to filter out a large percentage of the total data collected, making the data collection process inefficient."]}
{"question_id": "101d7a355e8bf6d1860917876ee0b9971eae7a2f", "predicted_answer": "", "predicted_evidence": ["Bag Of Words (BOW) [17] - This simple representation captures the TF-IDF value of an n-gram. We pick top 50K n-grams, with the value of `n' going up to 5."]}
{"question_id": "4288621e960ffbfce59ef1c740d30baac1588b9b", "predicted_answer": "", "predicted_evidence": ["In this section we list down the set of models considered in the study."]}
{"question_id": "c3befe7006ca81ce64397df654c31c11482dafbe", "predicted_answer": "", "predicted_evidence": ["This work proposed a set of elementary property prediction tasks to understand different tweet representations in an application independent, fine-grained fashion. The open nature of social media not only poses a plethora of opportunities to understand the basic characteristics of the posts, but also helped us draw novel insights about different representation models. We observed that among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models. Tweet length affects the task prediction accuracies, but we found that all models behave similarly under variation in tweet length. Finally while LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive to word order."]}
{"question_id": "5d0a3f8ca3882f87773cf8c2ef1b4f72b9cc241e", "predicted_answer": "", "predicted_evidence": ["Finally, instead of tuning the word reward using grid search, we introduce a way to learn it using a perceptron-like tuning method. We show that the optimal value is sensitive both to task and beam size, implying that it is important to tune for every model trained. Fortunately, tuning is a quick post-training step."]}
{"question_id": "dce27c49b9bf1919ca545e04663507d83bb42dbe", "predicted_answer": "", "predicted_evidence": ["Here, we first show that the beam problem is indeed the brevity problem. We then demonstrate that solving the length problem does solve the beam problem. Tables TABREF10 , TABREF11 , and TABREF12 show the results of our German\u2013English, Russian\u2013English, and French\u2013English systems respectively. Each table looks at the impact on BLEU, METEOR, and the ratio of the lengths of generated sentences compared to the gold lengths BIBREF22 , BIBREF23 . The baseline method is a standard model without any length correction. The reward method is the tuned constant word reward discussed in the previous section. Norm refers to the normalization method, where a hypothesis' score is divided by its length."]}
{"question_id": "991ea04072b3412928be5e6e903cfa54eeac3951", "predicted_answer": "", "predicted_evidence": ["We have explored simple and effective ways to alleviate or eliminate the beam problem. We showed that the beam problem can largely be explained by the brevity problem, which results from the locally-normalized structure of the model. We compared two corrections to the model and introduced a method to learn the parameters of these corrections. Because this method is helpful and easy, we hope to see it included to make stronger baseline NMT systems."]}
{"question_id": "a82a12a22a45d9507bc359635ffe9574f15e0810", "predicted_answer": "", "predicted_evidence": ["Beyond lexical cues from text inputs, other research has also utilized speakers' acoustic cues BIBREF2 , BIBREF5 . These studies have typically used audio tracks from TV shows and their corresponding captions in order to categorize characters' speaking turns as humorous or non-humorous. Utterances prior to canned laughter that was manually inserted into the shows were treated as humorous, while other utterances were treated as negative cases."]}
{"question_id": "355cf303ba61f84b580e2016fcb24e438abeafa7", "predicted_answer": "", "predicted_evidence": ["When building conventional models, we developed our own feature extraction scripts and used the SKLL python package for building Random Forest models. When implementing CNN, we used the Keras Python package. Regarding hyper-parameter tweaking, we utilized the Tree Parzen Estimation (TPE) method as detailed in TPE. After running 200 iterations of tweaking, we ended up with the following selection: INLINEFORM0 is 6 (entailing that the various filter sizes are INLINEFORM1 ), INLINEFORM2 is 100, INLINEFORM3 is INLINEFORM4 and INLINEFORM5 is INLINEFORM6 , optimization uses Adam BIBREF13 . When training the CNN model, we randomly selected INLINEFORM7 of the training data as the validation set for using early stopping to avoid over-fitting."]}
{"question_id": "88757bc49ccab76e587fba7521f0981d6a1af2f7", "predicted_answer": "", "predicted_evidence": ["Humor recognition refers to the task of deciding whether a sentence/spoken-utterance expresses a certain degree of humor. In most of the previous studies BIBREF1 , BIBREF2 , BIBREF3 , humor recognition was modeled as a binary classification task. In the seminal work BIBREF1 , a corpus of INLINEFORM0 \u201cone-liners\" was created using daily joke websites to collect humorous instances while using formal writing resources (e.g., news titles) to obtain non-humorous instances. Three humor-specific stylistic features, including alliteration, antonymy, and adult slang were utilized together with content-based features to build classifiers. In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website. BIBREF3 explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec BIBREF4 distributed representations were utilized in the model building."]}
{"question_id": "2f9a31f5a2b668acf3bce8958f5daa67ab8b2c83", "predicted_answer": "", "predicted_evidence": ["The remainder of the paper is organized as follows: Section SECREF2 briefly reviews the previous related research; Section SECREF3 describes the corpus we collected from TED talks; Section SECREF4 describes the text classification methods; Section SECREF5 reports on our experiments; finally, Section SECREF6 discusses the findings of our study and plans for future work."]}
{"question_id": "4830459e3d1d204e431025ce7e596ef3f8d757d2", "predicted_answer": "", "predicted_evidence": ["Stemming from the present study, we envision that more research is worth pursuing: (a) for presentations, cues from other modalities such as audio or video will be included, similar to Bertero2016LREC; (b) context information from multiple utterances will be modeled by using sequential modeling methods."]}
{"question_id": "74ebfba06f37cc95dfe59c3790ebe6165e6be19c", "predicted_answer": "", "predicted_evidence": ["We collected INLINEFORM0 TED Talk transcripts. An example transcription is given in Figure FIGREF4 . The collected transcripts were split into sentences using the Stanford CoreNLP tool BIBREF11 . In this study, sentences containing or immediately followed by `(Laughter)' were used as `Laughter' sentences, as shown in Figure FIGREF4 ; all other sentences were defined as `No-Laughter' sentences. Following BIBREF1 and BIBREF3 , we selected the same numbers ( INLINEFORM1 ) of `Laughter' and `No-Laughter' sentences. To minimize possible topic shifts between positive and negative instances, for each positive instance, we picked one negative instance nearby (the context window was 7 sentences in this study). For example, in Figure FIGREF4 , a negative instance (corresponding to `sent-2') was selected from the nearby sentences ranging from `sent-7' to `sent+7'."]}
{"question_id": "3a01dc85ac983002fd631f1c28fc1cbe16094c24", "predicted_answer": "", "predicted_evidence": ["(2) Integrating commonsense knowledge into conversational models boosts model performance, as Tri-LSTM outperforms Dual-LSTM by a certain margin."]}
{"question_id": "00ffe2c59a3ba18d6d2b353d6ab062a152c88526", "predicted_answer": "", "predicted_evidence": ["In recent years, data-driven approaches to building conversation models have been made possible by the proliferation of social media conversation data and the increase of computing power. By relying on a large number of message-response pairs, the Seq2Seq framework BIBREF0 attempts to produce an appropriate response based solely on the message itself, without any memory module."]}
{"question_id": "042800c3336ed5f4826203616a39747c61382ba6", "predicted_answer": "", "predicted_evidence": ["In our experiment, ConceptNet is used as the commonsense knowledge base. Preprocessing of this knowledge base involves removing assertions containing non-English characters or any word outside vocabulary $V$ . 1.4M concepts remain. 0.8M concepts are unigrams, 0.43M are bi-grams and the other 0.17M are tri-grams or more. Each concept is associated with an average of 4.3 assertions. More than half of the concepts are associated with only one assertion."]}
{"question_id": "52868394eb2b3b37eb5f47f51c06ad53061f4495", "predicted_answer": "", "predicted_evidence": ["In this section, we first discuss about the data collection process (Section SECREF8), followed by general descriptive statistics (Section SECREF12). Finally, Section SECREF18 analyzes the overall rating and sub-ratings."]}
{"question_id": "59dc6b1d3da74a2e67a6fb1ce940b28d9e3d8de0", "predicted_answer": "", "predicted_evidence": ["In future work, we could easily increase the dataset with other languages and use it for multilingual recommendation. We release HotelRec for further research: https://github.com/Diego999/HotelRec."]}
{"question_id": "713e1c7b0ab17759ba85d7cd2041e387831661df", "predicted_answer": "", "predicted_evidence": ["In this section, we first discuss about the data collection process (Section SECREF8), followed by general descriptive statistics (Section SECREF12). Finally, Section SECREF18 analyzes the overall rating and sub-ratings."]}
{"question_id": "00db191facf903cef18fb1727d1cab638c277e0a", "predicted_answer": "", "predicted_evidence": ["In these experiments, we only applied char3-MS-vec to EncDec but BIBREF38 indicated that combining multiple kinds of subword units can improve the performance. We will investigate the effect of combining several character INLINEFORM0 -gram embeddings in future work."]}
{"question_id": "1edfe390828f02a2db9a88454421c7f3d4cdd611", "predicted_answer": "", "predicted_evidence": ["We explored the effectiveness of multi-dimensional self-attention for word embedding construction. Table TABREF24 shows perplexities of using several encoders on the PTB dataset. As in BIBREF8 , we applied CNN to construct word embeddings (charCNN in Table TABREF24 ). Moreover, we applied the summation and standard self-attention, which computes the scalar value as a weight for a character INLINEFORM0 -gram embedding, to construct word embeddings (char INLINEFORM1 -Sum-vec and char INLINEFORM2 -SS-vec, respectively). For CNN, we used hyperparameters identical to BIBREF8 (\u201cOriginal Settings\u201d in Table TABREF24 ) but the setting has two differences from other architectures: 1. The dimension of the computed vectors is much larger than the dimension of the baseline word embeddings and 2. The dimension of the input character embeddings is much smaller than the dimension of the baseline word embeddings. Therefore, we added two configurations: assigning the dimension of the computed vectors and input character embeddings a value identical to the baseline word embeddings (in Table TABREF24 , \u201cSmall CNN result dims\u201d and \u201cLarge embedding dims\u201d, respectively)."]}
{"question_id": "3dad6b792044018bb968ac0d0fd4628653f9e4b7", "predicted_answer": "", "predicted_evidence": ["On the other hand, in the field of word embedding construction, some previous researchers found that character INLINEFORM0 -grams are more useful than single characters BIBREF0 , BIBREF10 . In particular, BIBREF0 demonstrated that constructing word embeddings from character INLINEFORM1 -gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM)."]}
{"question_id": "a28c73a6a8c46a43a1eec2b42b542dd7fde1e30e", "predicted_answer": "", "predicted_evidence": ["For headline generation, we used sentence-headline pairs extracted from the annotated English Gigaword corpus BIBREF35 in the same manner as BIBREF2 . The training set contains about 3.8M sentence-headline pairs. For evaluation, we exclude the test set constructed by BIBREF2 because it contains some invalid instances, as reported in BIBREF33 . We instead used the test sets constructed by BIBREF33 and BIBREF34 ."]}
{"question_id": "5f1ffaa738fedd5b6668ec8b58a027ddea6867ce", "predicted_answer": "", "predicted_evidence": ["Table TABREF15 shows perplexities of the baselines and the proposed method. We varied INLINEFORM0 for char INLINEFORM1 -MS-vec from 2 to 4. For the baseline, we also applied two word embeddings to investigate the performance in the case where we use more kinds of word embeddings. In detail, we prepared INLINEFORM2 and used INLINEFORM3 instead of INLINEFORM4 in Equation . Table TABREF15 also shows the number of character INLINEFORM5 -grams in each dataset. This table indicates that char INLINEFORM6 -MS-vec improved the performance of state-of-the-art models except for char4-MS-vec on WT103. These results indicate that char INLINEFORM7 -MS-vec can raise the quality of word-level language models. In particular, Table TABREF15 shows that char3-MS-vec achieved the best scores consistently. In contrast, an additional word embedding did not improve the performance. This fact implies that the improvement of char INLINEFORM8 -MS-vec is caused by using character INLINEFORM9 -grams. Thus, we answer yes to the first research question."]}
{"question_id": "8e26c471ca0ee1b9779da04c0b81918fd310d0f3", "predicted_answer": "", "predicted_evidence": ["On the other hand, in the field of word embedding construction, some previous researchers found that character INLINEFORM0 -grams are more useful than single characters BIBREF0 , BIBREF10 . In particular, BIBREF0 demonstrated that constructing word embeddings from character INLINEFORM1 -gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM)."]}
{"question_id": "a398c9b061f28543bc77c2951d0dfc5d1bee9e87", "predicted_answer": "", "predicted_evidence": ["We start with an explanation of the various types of embeddings we have used and proceed to describe the various components of our model, both individually and together. Finally, we cover how the parameters are learned."]}
{"question_id": "dae9caf8434ce43c9bc5913ebf062bc057a27cfe", "predicted_answer": "", "predicted_evidence": ["We use binary cross-entropy as the loss optimization function for our model. The cross-entropy method BIBREF18 is an iterative procedure where each iteration can be divided into two stages:"]}
{"question_id": "e9b6b14b8061b71d73a73d8138c8dab8eda4ba3f", "predicted_answer": "", "predicted_evidence": ["Word vectors and character vectors have been used across various approaches proposed to solve this problem. However, we suggest the use of subword representations to better analyse the morphology of possible clickbait-y words. We also attempt to model the interaction between the title of an article and its text."]}
{"question_id": "76e17e648a4d1f386eb6bf61b0c24f134af872be", "predicted_answer": "", "predicted_evidence": ["All methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important \u201cunbiased\u201d features."]}
{"question_id": "7572f6e68a2ed2c41b87c5088ba8680afa0c0a0b", "predicted_answer": "", "predicted_evidence": ["Based on this criterion and results from Section SECREF13 , we choose the abt dataset as source and st dataset as target for bias fine-tuning experiments."]}
{"question_id": "5d2bbcc3aa769e639dc21893890bc36b76597a33", "predicted_answer": "", "predicted_evidence": ["Based on this criterion and results from Section SECREF13 , we choose the abt dataset as source and st dataset as target for bias fine-tuning experiments."]}
{"question_id": "4ddc53afffaf1622d97695347dd1b3190d156dee", "predicted_answer": "", "predicted_evidence": ["Based on this criterion and results from Section SECREF13 , we choose the abt dataset as source and st dataset as target for bias fine-tuning experiments."]}
{"question_id": "5d93245832d90b31aee42ea2bf1e7704c22ebeca", "predicted_answer": "", "predicted_evidence": ["We also compare different pre-trained embeddings, word2vec BIBREF10 trained on Google News corpus, FastText BIBREF16 ) trained on Wikipedia corpus, and randomly initialized embeddings (random) to analyze their effects on the biases. Experiments were run 10 times and averaged."]}
{"question_id": "c0dbf3f1957f3bff3ced5b48aff60097f3eac7bb", "predicted_answer": "", "predicted_evidence": ["We experiment and discuss various methods to reduce gender biases identified in Section SECREF13 ."]}
{"question_id": "ed7ce13cd95f7664a5e4fc530dcf72dc3808dced", "predicted_answer": "", "predicted_evidence": ["As Figure FIGREF13 shows, in the forward propagation, Hungarian algorithm works out the aligned position pairs, according to which, neural components are dynamically connected to the next layer. For the example of Figure FIGREF13 , the 1st source and 2nd target word representations are jointly linked to the 1st aligned position of concatenation layer. Once the computational graph has been dynamically constructed in the forward pass, the backward process could propagate through the dynamically constructed links between layers, without any branching and non-differentiated issues. For the example in Figure FIGREF13 , the backward pass firstly propagates to the 1st aligned position of concatenation layer, then respectively propagates to 1st source and 2nd target word representations. In this way, the optimization framework could still adjust the parameters of neural architectures in an end-to-end manner."]}
{"question_id": "26eceba0e6e4c0b6dfa94e5708dd74b63f701731", "predicted_answer": "", "predicted_evidence": ["Our method outperforms all the baselines, which illustrates the effectiveness of our model."]}
{"question_id": "ff69b363ca604f80b2aa7afdc6a32d2ffd2d1f85", "predicted_answer": "", "predicted_evidence": ["Our method outperforms all the baselines, which illustrates the effectiveness of our model."]}
{"question_id": "ee19fd54997f2eec7c87c7d4a2169026fe208285", "predicted_answer": "", "predicted_evidence": ["In addition to automatic evaluation, we also assessed system output by eliciting human judgments. Participants compared summaries produced from the best extractive baseline (SummaRunner), and the best EA- and CA-based systems (SummaRunner+S2S+Copy and AE+Att+Copy+Salient, respectively). As an upper bound, we also included Gold standard summaries. The study was conducted on the Amazon Mechanical Turk platform using Best-Worst Scaling (BWS; BIBREF42), a less labor-intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales BIBREF43. Specifically, participants were shown the movie title and basic background information (i.e., synopsis, release year, genre, director, and cast). They were also presented with three system summaries and asked to select the best and worst among them according to Informativeness (i.e., does the summary convey opinions about specific aspects of the movie in a concise manner?), Correctness (i.e., is the information in the summary factually accurate and does it correspond to the information given about the movie?), and Grammaticality (i.e., is the summary fluent and grammatical?). Examples of system summaries are shown in Figure FIGREF1 and Figure FIGREF37. We randomly selected 50 movies from the test set and compared all possible combinations of summary triples for each movie. We collected three judgments for each comparison. The order of summaries and movies was randomized per participant. The score of a system was computed as the percentage of times it was chosen as best minus the percentage of times it was selected as worst. The scores range from -1 (worst) to 1 (best) and are shown in Table TABREF36. Perhaps unsurprisingly, the human-generated gold summaries were considered best, whereas our model (AE+Att+Copy+Salient) was ranked second, indicating that humans find its output more informative, correct, and grammatical compared to other systems. SummaRunner was ranked third followed by SummaRunner+S2S+Copy. We inspected the summaries produced by the latter system and found they were factually incorrect bearing little correspondence to the movie (examples shown in Figure FIGREF37), possibly due to the huge information loss at the extraction stage. All pairwise system differences are statistically significant using a one-way ANOVA with posthoc Tukey HSD tests ($p < 0.01$)."]}
{"question_id": "74fcb741d29892918903702dbb145fef372d1de3", "predicted_answer": "", "predicted_evidence": ["We use two objective functions to train the Abstract model. Firstly, we use a maximum likelihood loss to optimize the generation probability distribution $p(y^{\\prime }_t)$ based on gold summaries $Y=\\lbrace y_1,y_2,...,y_L\\rbrace $ provided at training time:"]}
{"question_id": "de0d135b94ba3b3a4f4a0fb03df38a84f9dc9da4", "predicted_answer": "", "predicted_evidence": ["We performed experiments on the Rotten Tomatoes dataset provided in BIBREF16. It contains 3,731 movies; for each movie we are given a large set of reviews (99.8 on average) written by professional critics and users and a gold-standard consensus, i.e. a summary written by an editor (see an example in Figure FIGREF1). On average, reviews are 19.7 tokens long, while the summary length is 19.6 tokens. The dataset is divided into 2,458 movies for training, 536 movies for development, and 737 movies for testing. Following previous work BIBREF16, we used a generic label for movie titles during training which we replace with the original movie names at test time."]}
{"question_id": "6a20a3220c4edad758b912e2d3e5b99b0b295d96", "predicted_answer": "", "predicted_evidence": ["We now obtain cWeight as we did previously, and formulate cumulative summary, capturing the consensus of different models. We hence used a supervised learning algorithm to capture the mean performances of different models over the training data to fine-tune our summary."]}
{"question_id": "c2745e44ebe7dd57126b784ac065f0b7fc2630f1", "predicted_answer": "", "predicted_evidence": ["As we discussed earlier, summarization models are field selective. Some models tend to perform remarkably better than others in certain fields. So, instead of assigning uniform weights to all models we can go by the following approach."]}
{"question_id": "d5dcc89a08924bed9772bc431090cbb52fb7836f", "predicted_answer": "", "predicted_evidence": ["In the Table 1, we try different model pairs with weights trained on corpus for Task 2. We have displayed mean ROUGE-2 scores for base Models. We have calculated final scores taking into consideration all normalizations, stemming, lemmatizing and clustering techniques, and the ones providing best results were used. We generally expected WordNet, Glove based semantic models to perform better given they better capture crux of the sentence and compute similarity using the same, but instead, they performed average. This is attributed to the fact they assigned high similarity scores to not so semantically related sentences. We also observe that combinations with TF/IDF and Similarity Matrices(Jaccard/Cosine) offer nearly same results. The InferSent based Summarizer performed exceptionally well. We initially used pre-trained features to generate sentence vectors through InferSent."]}
{"question_id": "d418bf6595b1b51a114f28ac8a6909c278838aeb", "predicted_answer": "", "predicted_evidence": ["The broad field of QA includes research ranging from retrieval-based BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 to generative BIBREF6 , BIBREF7 , as well as, from closed-domain BIBREF8 , BIBREF9 to open-domain QA BIBREF7 , BIBREF10 , BIBREF11 , BIBREF12 . We focus on the notion of improving an already deployed system."]}
{"question_id": "6d6b0628d8a942c57d7af1447a563021be79bc64", "predicted_answer": "", "predicted_evidence": ["a train- and test split of the evaluation corpus INLINEFORM0 , each including QA-pairs as tuples INLINEFORM1 ; the pre-trained baseline QA model for initial ranking INLINEFORM2 and the untrained re-ranking model INLINEFORM3 . evaluation metrics. training of the re-ranking model INLINEFORM4 INLINEFORM5 INLINEFORM6 INLINEFORM7 *R contains top-10 results INLINEFORM8 continue with next QA pair add positive sample INLINEFORM9 *confidence for INLINEFORM10 INLINEFORM11 INLINEFORM12 add negative sample INLINEFORM13 random INLINEFORM14 INLINEFORM15 INLINEFORM16 INLINEFORM17 INLINEFORM18 evaluation of the re-ranking model INLINEFORM19 INLINEFORM20 INLINEFORM21 *top-10 baseline ranking INLINEFORM22 *apply re-ranking INLINEFORM23 INLINEFORM24 Evaluation Procedure (per Data Split)"]}
{"question_id": "b21245212244ad7adf7d321420f2239a0f0fe56b", "predicted_answer": "", "predicted_evidence": ["a train- and test split of the evaluation corpus INLINEFORM0 , each including QA-pairs as tuples INLINEFORM1 ; the pre-trained baseline QA model for initial ranking INLINEFORM2 and the untrained re-ranking model INLINEFORM3 . evaluation metrics. training of the re-ranking model INLINEFORM4 INLINEFORM5 INLINEFORM6 INLINEFORM7 *R contains top-10 results INLINEFORM8 continue with next QA pair add positive sample INLINEFORM9 *confidence for INLINEFORM10 INLINEFORM11 INLINEFORM12 add negative sample INLINEFORM13 random INLINEFORM14 INLINEFORM15 INLINEFORM16 INLINEFORM17 INLINEFORM18 evaluation of the re-ranking model INLINEFORM19 INLINEFORM20 INLINEFORM21 *top-10 baseline ranking INLINEFORM22 *apply re-ranking INLINEFORM23 INLINEFORM24 Evaluation Procedure (per Data Split)"]}
{"question_id": "4a201b8b9cc566b56aedb5ab45335f202bc41845", "predicted_answer": "", "predicted_evidence": ["The second metric is the Type and Category Test (TCT), based on the assumption that two entities which share types and categories should be close in the vector space. This assumption is suggested by the human bias for which rdf:type and dct:subject would be predicates with a higher weight than the others. Although this does not happen, we compute it for a mere sake of comparison with the NST metric. The TCT formula is equal to Equation 19 except for sets $C_K(e)$ , which are replaced by sets of types and categories $TC_K(e)$ ."]}
{"question_id": "6a90135bd001be69a888076aff1b149b78adf443", "predicted_answer": "", "predicted_evidence": ["In Figure 3 , we show the CPU, Memory, and disk consumption for KG2Vec on the larger model of DBpedia 2016-04. All three subphases of the algorithm are visible in the plot. For 2.7 hours, tokens are counted; then, the learning proceeds for 7.7 hours; finally in the last 2.3 hours, the model is saved."]}
{"question_id": "1f40adc719d8ccda81e7e90525b577f5698b5aad", "predicted_answer": "", "predicted_evidence": ["assigns a vector of dimensionality $d$ to an entity, a relation, or a literal. However, some approaches consider only the vector representations of entities or subjects (i.e, $\\lbrace s \\in E : \\exists (s, p, o) \\in K \\rbrace $ ). For instance, in approaches based on Tensor Factorization, given a relation, its subjects and objects are processed and transformed into sparse matrices; all the matrices are then combined into a tensor whose depth is the number of relations. For the final embedding, current approaches rely on dimensionality reduction to decrease the overall complexity BIBREF9 , BIBREF12 , BIBREF2 . The reduction is performed through an embedding map $\\Phi : \\mathbb {R}^d \\rightarrow \\mathbb {R}^k$ , which is a homomorphism that maps the initial vector space into a smaller, reduced space. The positive value $k < d$ is called the rank of the embedding. Note that each dimension of the reduced common space does not necessarily have an explicit connection with a particular relation. Dimensionality reduction methods include Principal Component Analysis techniques BIBREF9 and generative statistical models such as Latent Dirichlet Allocation BIBREF19 , BIBREF20 ."]}
{"question_id": "f92c344e9b1a986754277fd0f08a47dc3e5f9feb", "predicted_answer": "", "predicted_evidence": ["The metrics that take into account the context can also be considered. Such metrics can come in the form of an evaluation model that is learned from data. This model can be either a discriminative model that attempts to distinguish between model and human responses or a model that uses data collected from the human survey in order to provide human-like scores to proposed responses."]}
{"question_id": "b10388e343868ca8e5c7c601ebb903f52e756e61", "predicted_answer": "", "predicted_evidence": ["The metrics that take into account the context can also be considered. Such metrics can come in the form of an evaluation model that is learned from data. This model can be either a discriminative model that attempts to distinguish between model and human responses or a model that uses data collected from the human survey in order to provide human-like scores to proposed responses."]}
{"question_id": "e8cdeb3a081d51cc143c7090a54c82d393f1a2ca", "predicted_answer": "", "predicted_evidence": ["According to them, the metrics (like Kiros et al, 2015 BIBREF32 ) that are based on distributed sentence representations hold the most promise for the future. It is because word-overlap metrics like BLEU simply require too many ground-truth responses to find a significant match for a reasonable response due to the high diversity of dialogue responses. Similarly, the metrics that are embedding-based consist of basic averages of vectors obtained through distributional semantics and so they are also insufficiently complex for modeling sentence-level compositionality in dialogue."]}
{"question_id": "833d3ae7613500f2867ed8b33d233d71781014e7", "predicted_answer": "", "predicted_evidence": ["Recently, generative adversarial networks are being explored and how they can be used in the dialog agents. Although generative adversarial networks are a topic in itself to explore. However, the paper mentioned below used uses reinforcement learning along with generative adversarial network so we cover it here inside the reinforcement learning methods. They can be used by the applications to generate dialogues similar to humans."]}
{"question_id": "a1a0365bf6968cbdfd1072cf3923c26250bc955c", "predicted_answer": "", "predicted_evidence": ["After exploring the neural methods in a lot of detail, the researchers have also begun exploring, in the current decade, how to use the reinforcement learning methods in the dialogue and personal agents."]}
{"question_id": "64f7337970e8d1989b2e1f7106d86f73c4a3d0af", "predicted_answer": "", "predicted_evidence": ["The metrics that take into account the context can also be considered. Such metrics can come in the form of an evaluation model that is learned from data. This model can be either a discriminative model that attempts to distinguish between model and human responses or a model that uses data collected from the human survey in order to provide human-like scores to proposed responses."]}
{"question_id": "8fdb4f521d3ba4179f8ccc4c28ba399aab6c3550", "predicted_answer": "", "predicted_evidence": ["Next came the era of using machine learning methods in the area of conversation agents which totally revolutionized this field."]}
{"question_id": "a0d45b71feb74774cfdc0d5c6e23cd41bc6bc1f2", "predicted_answer": "", "predicted_evidence": ["Next came the era of using machine learning methods in the area of conversation agents which totally revolutionized this field."]}
{"question_id": "89414ef7fcb2709c47827f30a556f543b9a9e6e0", "predicted_answer": "", "predicted_evidence": ["Laszlo and Petrovi\u0107 BIBREF11 also commented on the state of the art of the time, noting the USA prototype efforts from 1954 and the publication of a collection of research papers in 1955 as well as the USSR efforts starting from 1955 and the UK prototype from 1956. They do not detail or cite the articles they mention. However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrovi\u0107 described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort). Another interesting moment, which they delineated in BIBREF11, is that the group soon discovered that some experimental work had already been done in 1957 at the Institute of Telecommunications (today a part of the Faculty of Electrical Engineering and Computing at the University of Zagreb) by Vladimir Matkovi\u0107. Because of this, they decided to include him in the research group of the Faculty of Humanities and Social Sciences at the University of Zagreb. The work done by Matkovi\u0107 was documented in his doctoral dissertation but remained unpublished until 1959."]}
{"question_id": "faffcc6ef27c1441e6528f924e320368430d8da3", "predicted_answer": "", "predicted_evidence": ["Laszlo and Petrovi\u0107 BIBREF11 also commented on the state of the art of the time, noting the USA prototype efforts from 1954 and the publication of a collection of research papers in 1955 as well as the USSR efforts starting from 1955 and the UK prototype from 1956. They do not detail or cite the articles they mention. However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrovi\u0107 described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort). Another interesting moment, which they delineated in BIBREF11, is that the group soon discovered that some experimental work had already been done in 1957 at the Institute of Telecommunications (today a part of the Faculty of Electrical Engineering and Computing at the University of Zagreb) by Vladimir Matkovi\u0107. Because of this, they decided to include him in the research group of the Faculty of Humanities and Social Sciences at the University of Zagreb. The work done by Matkovi\u0107 was documented in his doctoral dissertation but remained unpublished until 1959."]}
{"question_id": "afad388a0141bdda5ca9586803ac53d5f10f41f6", "predicted_answer": "", "predicted_evidence": ["Laszlo and Petrovi\u0107 BIBREF11 considered cybernetics (as described in BIBREF13 by Wiener, who invented the term \u201ccybernetics\u201d) to be the best approach for machine translation in the long run. The question is whether Laszlo's idea of cybernetics would drive the research of the group towards artificial neural networks. Laszlo and his group do not go into neural network details (bear in mind that this is 1959, the time of Rosenblatt), but the following passage offers a strong suggestion about the idea they had (bearing in mind that Wiener relates McCulloch and Pitts' ideas in his book): \"Cybernetics is the scientific discipline which studies analogies between machines and living organisms\" (BIBREF11, p. 107). They fully commit to the idea two pages later (BIBREF11, p. 109): \"An important analogy is the one between the functioning of the machine and that of the human nervous system\". This could be taken to mean a simple computer brain analogy in the spirit of BIBREF14 and later BIBREF15, but Laszlo and Petrovi\u0107 specifically said that thinking of cybernetics as the \"theory of electronic computers\" (as they are made) is wrong BIBREF11, since the emphasis should be on modelling analogical processes. There is a very interesting quote from BIBREF11, where Laszlo and Petrovi\u0107 note that \"today, there is a significant effort in the world to make fully automated machine translation possible; to achieve this, logicians and linguists are making efforts on ever more sophisticated problems\". This seems to suggest that they were aware of the efforts of logicians (such as Bar Hillel, and to some degree Pitts, since Wiener specifically mentions logicians-turned-cyberneticists in his book BIBREF13), but still concluded that a cybernetic approach would probably be a better choice."]}
{"question_id": "baaa6ad7148b785429a20f38786cd03ab9a2646e", "predicted_answer": "", "predicted_evidence": ["Andreev's approach was in a sense \"external\". The modelling would be statistical, but its purpose would not be to mimic the stochasticity of the human thought process, but rather to produce a working machine translation system. Kulagina and Melchuk disagreed with this approach as they thought that more of what is presently called \"philosophical logic\" was needed to model the human thought process at the symbolic level, and according to them, the formalization of the human thought process was a prerequisite for developing a machine translation system (cf. BIBREF6). We could speculate that sub-symbolic processing would have been acceptable too, since that approach is also rooted in philosophical logic as a way of formalizing human cognitive functions and is also \"internal\" in the same sense symbolic approaches are."]}
{"question_id": "de346decb1fbca8746b72c78ea9d1208902f5e0a", "predicted_answer": "", "predicted_evidence": ["In Yugoslavia, organized effort in machine translation started in 1959, but the first individual effort was made by Vladimir Matkovi\u0107 from the Institute for Telecommunications in Zagreb in 1957 in his PhD thesis on entropy in the Croatian language BIBREF10. The main research group in machine translation was formed in 1958, at the Circle for Young Linguists in Zagreb, initiated by a young linguist Bulcsu Laszlo, who graduated in Russian language, Southern Slavic languages and English language and literature at the University of Zagreb in 1952. The majority of the group members came from different departments of the Faculty of Humanities and Social Sciences of the University of Zagreb, with several individuals from other institutions. The members from the Faculty of Humanities and Social Sciences were: Svetozar Petrovi\u0107 (Department of Comparative Literature), Stjepan Babi\u0107 (Department of Serbo-Croatian Language and Literature), Krunoslav Pranji\u0107 (Department of Serbo-Croatian Language and Literature), \u017deljko Bujas (Department of English Language and Literature), Malik Muli\u0107 (Department of Russian Language and Literature) and Bulcsu Laszlo (Department of Comparative Slavistics). The members of the research group from outside the Faculty of Humanities and Social Sciences were: Bo\u017eidar Finka (Institute for Language of the Yugoslav Academy of Sciences and Arts), Vladimir Vrani\u0107 (Center for Numerical Research of the Yugoslav Academy of Sciences and Arts), Vladimir Matkovi\u0107 (Institute for Telecommunications), Vladimir Muljevi\u0107 (Institute for Regulatory and Signal Devices) BIBREF10."]}
{"question_id": "0bde3ecfdd7c4a9af23f53da2cda6cd7a8398220", "predicted_answer": "", "predicted_evidence": ["NSE gives us unrestricted access to the entire source sequence stored in the memory. As such, the encoder may attend to relevant words when encoding each word. The sequence INLINEFORM0 is then used as the sequence INLINEFORM1 in Section SECREF2 ."]}
{"question_id": "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b", "predicted_answer": "", "predicted_evidence": ["On WikiSmall, NseLstm-B performed best on both Fluency and Adequacy. On WikiLarge, LstmLstm-B achieved the highest Fluency score while NseLstm-B received the highest Adequacy score. In terms of Simplicity and Average, NseLstm-S outperformed all other systems on both WikiSmall and WikiLarge."]}
{"question_id": "051034cc94f2c02d3041575c53f969b3311c9ea1", "predicted_answer": "", "predicted_evidence": ["Table TABREF20 shows the correlations between the scores assigned by humans and the automatic evaluation measures. There is a positive significant correlation between Fluency and Adequacy (0.69), but a negative significant correlation between Adequacy and Simplicity (-0.64). BLEU correlates well with Fluency (0.63) and Adequacy (0.90) while SARI correlates well with Simplicity (0.73). BLEU and SARI show a negative significant correlation (-0.54). The results reflect the challenge of managing the trade-off between Fluency, Adequacy and Simplicity in sentence simplification."]}
{"question_id": "511e46b5aa8e1ee9e7dc890f47fa15ef94d4a0af", "predicted_answer": "", "predicted_evidence": ["Table TABREF20 shows the correlations between the scores assigned by humans and the automatic evaluation measures. There is a positive significant correlation between Fluency and Adequacy (0.69), but a negative significant correlation between Adequacy and Simplicity (-0.64). BLEU correlates well with Fluency (0.63) and Adequacy (0.90) while SARI correlates well with Simplicity (0.73). BLEU and SARI show a negative significant correlation (-0.54). The results reflect the challenge of managing the trade-off between Fluency, Adequacy and Simplicity in sentence simplification."]}
{"question_id": "6b4006a90aeaaff8914052d72d28851a9c0c0146", "predicted_answer": "", "predicted_evidence": ["On WikiSmall, Hybrid \u2013 the current state-of-the-art \u2013 achieved best BLEU (53.94) and SARI (30.46) scores. Among neural models, NseLstm-B yielded the highest BLEU score (53.42), while NseLstm-S performed best on SARI (29.75). On WikiLarge, again, NseLstm-B had the highest BLEU score of 92.02. Sbmt-Sari \u2013 that was trained on a huge corpus of 106M sentence pairs and 2B words \u2013 scored highest on SARI with 39.96, followed by Dress-Ls (37.27), Dress (37.08), and NseLstm-S (36.88)."]}
{"question_id": "eccbbe3684d0cf6b794cb4eef379bb1c8bcc33bf", "predicted_answer": "", "predicted_evidence": ["The rest of this document is structured as follows: se:work introduces the related work. Then, in se:IMT we present our protocol. se:exp describes the experiments conducted in order to assess our proposal. The results of those experiments are presented and discussed in se:res. Finally, in se:conc, conclusions are drawn."]}
{"question_id": "a3705b53c6710b41154c65327b7bbec175bdfae7", "predicted_answer": "", "predicted_evidence": ["As a future work, we want to further research the behavior of the neural systems. For that, we would like to explore techniques for enriching the training corpus with additional data, and the incorrect generation of words due to subwords. We would also like to develop new protocols based on successful IMT approaches. Finally, we should test our proposal with real users to obtain actual measures of the effort reduction."]}
{"question_id": "b62b7ec5128219f04be41854247d5af992797937", "predicted_answer": "", "predicted_evidence": ["As a future work, we want to further research the behavior of the neural systems. For that, we would like to explore techniques for enriching the training corpus with additional data, and the incorrect generation of words due to subwords. We would also like to develop new protocols based on successful IMT approaches. Finally, we should test our proposal with real users to obtain actual measures of the effort reduction."]}
{"question_id": "e8fa4303b36a47a5c87f862458442941bbdff7d9", "predicted_answer": "", "predicted_evidence": ["This neural network usually follows an encoder-decoder architecture, featuring recurrent networks BIBREF23, BIBREF24, convolutional networks BIBREF25 or attention mechanisms BIBREF26. Model parameters are jointly estimated on large parallel corpora, using stochastic gradient descent BIBREF27, BIBREF28. At decoding time, the system obtains the most likely translation using a beam search method."]}
{"question_id": "51e9f446d987219bc069222731dfc1081957ce1f", "predicted_answer": "", "predicted_evidence": ["SMT systems were trained with Moses BIBREF29, following the standard procedure: we estimated a 5-gram language model\u2014smoothed with the improved KneserNey method\u2014using SRILM BIBREF30, and optimized the weights of the log-linear model with MERT BIBREF31."]}
{"question_id": "13fb28e8b7f34fe600b29fb842deef75608c1478", "predicted_answer": "", "predicted_evidence": ["All of the tasks were evaluated using the standard metrics of precision(P), recall(R) and INLINEFORM0 : DISPLAYFORM0 "]}
{"question_id": "d5bce5da746a075421c80abe10c97ad11a96c6cd", "predicted_answer": "", "predicted_evidence": ["All of the tasks were evaluated using the standard metrics of precision(P), recall(R) and INLINEFORM0 : DISPLAYFORM0 "]}
{"question_id": "930733efb3b97e1634b4dcd77123d4d5731e8807", "predicted_answer": "", "predicted_evidence": ["All of the tasks were evaluated using the standard metrics of precision(P), recall(R) and INLINEFORM0 : DISPLAYFORM0 "]}
{"question_id": "11f9c207476af75a9272105e646df02594059c3f", "predicted_answer": "", "predicted_evidence": ["All of the tasks were evaluated using the standard metrics of precision(P), recall(R) and INLINEFORM0 : DISPLAYFORM0 "]}
{"question_id": "b32de10d84b808886d7a91ab0c423d4fc751384c", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is a bias term and INLINEFORM1 is a non-linear function such as the hyperbolic tangent. This filter is applied to each possible window of words in the sequence INLINEFORM2 to produce the feature map: DISPLAYFORM0 "]}
{"question_id": "9ea3669528c2b295f21770cb7f70d0c4b4389223", "predicted_answer": "", "predicted_evidence": ["Table 2 shows the evaluation results. The rule based RB gives fairly high precision but with low recall. CB, the common-sense based method, achieves the highest recall. Yet, its precision is the worst. RB+CB, the combination of RB and CB gives higher the F-measure But, the improvement of 1.27% is only marginal compared to RB."]}
{"question_id": "9863f5765ba70f7ff336a580346ef70205abbbd8", "predicted_answer": "", "predicted_evidence": ["We compare with the following baseline methods:"]}
{"question_id": "ced63053eb631c78a4ddd8c85ec0f3323a631a54", "predicted_answer": "", "predicted_evidence": ["Multi-kernel: This is the state-of-the-art method using the multi-kernel method BIBREF31 to identify the emotion cause. We use the best performance reported in their paper."]}
{"question_id": "f13a5b6a67a9b10fde68e8b33792879b8146102c", "predicted_answer": "", "predicted_evidence": ["[id=lq]Details of the corpus are shown in Table 1. The metrics we used in evaluation follows lee2010text. It is commonly accepted so that we can compare our results with others. If a proposed emotion cause clause covers the annotated answer, the word sequence is considered correct. The precision, recall, and F-measure are defined by INLINEFORM0 "]}
{"question_id": "67c16ba64fe27838b1034d15194c07a9c98cdebe", "predicted_answer": "", "predicted_evidence": ["SVM: This is a SVM classifier using the unigram, bigram and trigram features. It is a baseline previously used in BIBREF24 , BIBREF31 "]}
{"question_id": "58a3cfbbf209174fcffe44ce99840c758b448364", "predicted_answer": "", "predicted_evidence": ["Shallow LSTMs do especially well here. Deeper models have gradually degrading perplexity, with RHNs lagging all of them by a significant margin. NAS is not quite up there with the LSTM suggesting its architecture might have overfitted to Penn Treebank, but data for deeper variants would be necessary to draw this conclusion."]}
{"question_id": "6c6e06f7bfb6d30003fd3801fdaf34649ef1b8f4", "predicted_answer": "", "predicted_evidence": ["There are a few notable things about the results. First, in our environment (Tensorflow with a single GPU) even with the same seed as the one used by the tuner, the effect of UID19 is almost as large as that of UID19 and UID20 combined. Second, the variance induced by UID19 and UID20 together is roughly equivalent to an absolute difference of 0.4 in perplexity on Penn Treebank and 0.5 on Wikitext-2. Third, the validation perplexities of the best checkpoints are about one standard deviation lower than the sample mean of the reruns, so the tuner could fit the noise only to a limited degree."]}
{"question_id": "b6e97d1b1565732b1b3f1d74e6d2800dd21be37a", "predicted_answer": "", "predicted_evidence": ["There are a few notable things about the results. First, in our environment (Tensorflow with a single GPU) even with the same seed as the one used by the tuner, the effect of UID19 is almost as large as that of UID19 and UID20 combined. Second, the variance induced by UID19 and UID20 together is roughly equivalent to an absolute difference of 0.4 in perplexity on Penn Treebank and 0.5 on Wikitext-2. Third, the validation perplexities of the best checkpoints are about one standard deviation lower than the sample mean of the reruns, so the tuner could fit the noise only to a limited degree."]}
{"question_id": "4f8b078b9f60be30520fd32a3d8601ab3babb5c0", "predicted_answer": "", "predicted_evidence": ["Naturally, NAS benefitted only to a limited degree from our tuning, since the numbers of BIBREF1 were already produced by employing similar regularisation methods and a grid search. The small edge can be attributed to the suboptimality of grid search (see Section SECREF23 )."]}
{"question_id": "54517cded8267ea6c9a3f3cf9c37a8d24b3f7c2c", "predicted_answer": "", "predicted_evidence": ["Shallow LSTMs do especially well here. Deeper models have gradually degrading perplexity, with RHNs lagging all of them by a significant margin. NAS is not quite up there with the LSTM suggesting its architecture might have overfitted to Penn Treebank, but data for deeper variants would be necessary to draw this conclusion."]}
{"question_id": "803babb71e1bdaf507847d6c712585f4128e9f47", "predicted_answer": "", "predicted_evidence": ["For all experiments, we use the Transformer Big BIBREF26 as implemented in Fairseq, with the hyperparameters of BIBREF27. Training is done on 8 GPUs, with accumulated gradients over 10 batches BIBREF27, and a max batch size of 3500 tokens per GPU. We train for 20 epochs, while saving a checkpoint every 2500 updates ($\\approx \\frac{2}{5}$ epoch on UGC) and average the 5 best checkpoints according to their perplexity on a validation set (a held-out subset of UGC)."]}
{"question_id": "5fd112980d0dd7f7ce30e6273fe6e7b230b13225", "predicted_answer": "", "predicted_evidence": ["This in-domain data is concatenated to the out-of-domain parallel data and used for training."]}
{"question_id": "eaae11ffd4ff955de2cd6389b888f5fd2c660a32", "predicted_answer": "", "predicted_evidence": ["We conduct a human evaluation to confirm the observations with BLEU and to overcome some of the limitations of this metric."]}
{"question_id": "290ebf0d1c49b67a6d1858366be751d89086a78b", "predicted_answer": "", "predicted_evidence": ["We conduct a human evaluation to confirm the observations with BLEU and to overcome some of the limitations of this metric."]}
{"question_id": "806fefe0e331ddb3c17245d6a9fa7433798e367f", "predicted_answer": "", "predicted_evidence": ["Regarding robustness, we found many of the same errors listed by BIBREF0 as noise in social media text: SMS language (\u00e9 qd g vu sa), typos and phonetic spelling (pattes), repeated letters (trooop, merciiii), slang (nickel, bof, mdr), missing or wrong accents (tres), emoticons (`:-)') and emojis, missing punctuation, wrong or non-standard capitalization (lowercase proper names, capitalized words for emphasis). Regarding domain aspects, there are polysemous words with typical specific meaning carte $\\rightarrow $ map, menu; cadre $\\rightarrow $ frame, executive, setting), idiomatic expressions (\u00e0 tomber par terre $\\rightarrow $ to die for), and venue-related named entities (La Bo\u00eete \u00e0 Sardines)."]}
{"question_id": "458e5ed506883bfec6623102ec9f43c071f0616f", "predicted_answer": "", "predicted_evidence": ["A shallow qualitative analysis of the classification output provided insight into some of the classification mistakes."]}
{"question_id": "85ab5f773b297bcf48a274634d402a35e1d57446", "predicted_answer": "", "predicted_evidence": ["Essentially, the annotation scheme describes two levels of annotation. Firstly, the annotators were asked to indicate, at the post level, whether the post under investigation was related to cyberbullying. If the post was considered a signal of cyberbullying, annotators identified the author's role. Secondly, at the subsentence level, the annotators were tasked with the identification of a number of fine-grained text categories related to cyberbullying. More concretely, they identified all text spans corresponding to one of the categories described in the annotation scheme. To provide the annotators with some context, all posts were presented within their original conversation when possible. All annotations were done using the Brat rapid annotation tool BIBREF52 , some examples of which are presented in Table TABREF33 ."]}
{"question_id": "5154f63c50729b8ac04939588c2f5ffeb916e3df", "predicted_answer": "", "predicted_evidence": ["When applied to the training data, this resulted in INLINEFORM0 and INLINEFORM1 features for English and Dutch, respectively."]}
{"question_id": "2aeabec8a734a6e8ca9e7a308dd8c9a1011b3d6e", "predicted_answer": "", "predicted_evidence": ["When applied to the training data, this resulted in INLINEFORM0 and INLINEFORM1 features for English and Dutch, respectively."]}
{"question_id": "f2b8a2ed5916d75cf568a931829a5a3cde2fc345", "predicted_answer": "", "predicted_evidence": ["Term lists: one binary feature derived for each one out of six lists, indicating the presence of an item from the list in a post: proper names, `allness' indicators (e.g. always, everybody), diminishers (e.g. slightly, relatively), intensifiers (e.g. absolutely, amazingly), negation words and aggressive language and profanity words. Person alternation is a binary feature indicating whether the combination of a first and second person pronoun occurs in order to capture interpersonal intent."]}
{"question_id": "c0af44ebd7cd81270d9b5b54d4a40feed162fa54", "predicted_answer": "", "predicted_evidence": ["In spite of these efforts, a lot of undesirable and hurtful content remains online. BIBREF1 analysed a body of quantitative research on cyberbullying and observed cybervictimisation rates among teenagers between 20% and 40%. BIBREF5 focused on 12 to 17 year olds living in the United States and found that no less than 72% of them had encountered cyberbullying at least once within the year preceding the questionnaire. BIBREF6 surveyed 9 to 26 year olds in the United States, Canada, the United Kingdom and Australia, and found that 29% of the respondents had ever been victimised online. A study among 2,000 Flemish secondary school students (age 12 to 18) revealed that 11% of them had been bullied online at least once in the six months preceding the survey BIBREF7 . Finally, the 2014 large-scale EU Kids Online Report BIBREF8 published that 20% of 11 to 16 year olds had been exposed to hate messages online. In addition, youngsters were 12% more likely to be exposed to cyberbullying as compared to 2010, clearly demonstrating that cyberbullying is a growing problem."]}
{"question_id": "a4a9971799c8860b50f219c93f050ebf6a627b3d", "predicted_answer": "", "predicted_evidence": ["Another angle for analyzing written text is by looking at the psychological properties that can be inferred regarding their authors. This is typically called psycholinguistics, where one examines how the use of the language can be indicative of different psychological states. Examples of such psychological properties include introversion, extroversion, sensitivity, and emotions. One of the tools that automates the process of extracting psychological meaning from text is the Linguistic Inquiry and Word Count (LIWC) BIBREF8 tool. This approach has been used in the literature to study the behaviour of different groups and to predict their psychological states, such as predicting depression BIBREF9 . More recently, it has also been applied to uncover different psychological properties of extremist groups and understand their intentions behind the recruitment campaigns BIBREF10 ."]}
{"question_id": "778c6a27182349dc5275282c3e9577bda2555c3d", "predicted_answer": "", "predicted_evidence": ["We investigated which features contribute most to the classification task to distinguish between radical and non-radical tweets. We used the mean decrease impurity method of random forests BIBREF27 to identify the most important features in each feature category. The ten most important features are shown in Table TABREF22 . We found that the most important feature for distinguishing radical tweets is the psychological feature distance measure. This measures how similar the Twitter user is to the average psychological profile calculated from the propaganda magazine articles. Following this is the Us-them dichotomy which looks at the total number of pronouns used (I,they, we, you). This finding is in line with the tactics reported in the radicalization literature with regards to emphasizing the separation between the radical group and the world."]}
{"question_id": "42dcf1bb19b8470993c05e55413eed487b0f2559", "predicted_answer": "", "predicted_evidence": ["In order to understand how radical messages are constructed and used, as mentioned earlier, we analyze content of ISIS propaganda material published in Dabiq magazine. Dabiq is an online magazine published by ISIS terrorist groups with the purpose of recruiting people and promoting their propaganda and ideology. Using this data source, we investigate what topics, textual properties, and linguistic cues exist in these magazines. Our intuition is that utilising these linguistic cues from the extremist propaganda would allow us to detect supporters of ISIS group who are influenced by their propaganda."]}
{"question_id": "2ecd12069388fd58ad5f8f4ae7ac1bb4f56497b9", "predicted_answer": "", "predicted_evidence": ["Feature engineering is the process of exploring large spaces of heterogeneous features with the aim of discovering meaningful features that may aid in modeling the problem at hand. We explore three categories of information to identify relevant features to detect radical content. Some features are user-based while others are message-based. The three categories are: 1) Radical language (Textual features INLINEFORM0 ); 2) Psychological signals (Psychological features INLINEFORM1 ); and 3) Behavioural features ( INLINEFORM2 ). In the following, we detail each of these categories."]}
{"question_id": "824629b36a75753b1500d9dcaee0fc3c758297b1", "predicted_answer": "", "predicted_evidence": ["Another angle for analyzing written text is by looking at the psychological properties that can be inferred regarding their authors. This is typically called psycholinguistics, where one examines how the use of the language can be indicative of different psychological states. Examples of such psychological properties include introversion, extroversion, sensitivity, and emotions. One of the tools that automates the process of extracting psychological meaning from text is the Linguistic Inquiry and Word Count (LIWC) BIBREF8 tool. This approach has been used in the literature to study the behaviour of different groups and to predict their psychological states, such as predicting depression BIBREF9 . More recently, it has also been applied to uncover different psychological properties of extremist groups and understand their intentions behind the recruitment campaigns BIBREF10 ."]}
{"question_id": "31894361833b3e329a1fb9ebf85a78841cff229f", "predicted_answer": "", "predicted_evidence": ["Feature engineering is the process of exploring large spaces of heterogeneous features with the aim of discovering meaningful features that may aid in modeling the problem at hand. We explore three categories of information to identify relevant features to detect radical content. Some features are user-based while others are message-based. The three categories are: 1) Radical language (Textual features INLINEFORM0 ); 2) Psychological signals (Psychological features INLINEFORM1 ); and 3) Behavioural features ( INLINEFORM2 ). In the following, we detail each of these categories."]}
{"question_id": "cef3a26d8b46cd057bcc2abd3d648dc15336a2bf", "predicted_answer": "", "predicted_evidence": ["where $\\mathbf {V}_{e_{\\scriptstyle {t}}}$ is the enriched embedding of $h_t$, $\\mathbf {W}_{i,:}$ is $i^{\\text{th}}$ row of $W_{\\scriptstyle _{NCE}}$ matrix, $\\mathcal {N}_c = \\lbrace h_i| 1 \\le i \\le N, h_i \\sim P_n(h_c)\\rbrace $ is the set of negative examples, and $P_n(h_c)$ is the distribution which we use to pick the negative samples. We train our model by maximizing equation DISPLAY_FORM10 using batch stochastic gradient descent."]}
{"question_id": "636ac549cf4917c5922cd09a655abf278924c930", "predicted_answer": "", "predicted_evidence": ["We show significant gains over previous work based on click-embedding in several experimental studies."]}
{"question_id": "c61c0b25f9de4a7ca2013d2e4aba8a5047e14ce4", "predicted_answer": "", "predicted_evidence": ["We show significant gains over previous work based on click-embedding in several experimental studies."]}
{"question_id": "1d047286ac63e5dca1ab811172b89d7d125679e5", "predicted_answer": "", "predicted_evidence": ["We tune the hyperparameters for all models, including the baseline session-only model, on the validation set. We search for a learning rate from $\\lbrace 0.01, 0.1, 0.5, 1.0, 2.5\\rbrace $ and embedding dimensions from $\\lbrace 32, 128\\rbrace $. To train the model weights, we use stochastic gradient descent (SGD) with exponential decay since it performs better than other optimizers in our case, and a batch size of 4096."]}
{"question_id": "6d17dc00f7e5331128b6b585e78cac0b9082e13d", "predicted_answer": "", "predicted_evidence": ["The annotation was performed by several student assistants with a background in linguistics and with Norwegian as their native language. 100 documents containing 2065 sentences were annotated doubly and disagreements were resolved before moving on. The remaining documents were annotated by one annotator. The doubly annotated documents were adjudicated by a third annotator different from the two first annotators. In the single annotation phase, all annotators were given the possibility to discuss difficult choices in joint annotator meetings, but were encouraged to take independent decisions based on the guidelines if possible. Annotation was performed using the web-based annotation tool Brat BIBREF22."]}
{"question_id": "de0154affd86c608c457bf83d888bbd1f879df93", "predicted_answer": "", "predicted_evidence": ["Below we review some of the most important principles described in our annotation guidelines relating to the targets of polarity."]}
{"question_id": "9887ca3d25e2109f41d1da80eeea05c465053fbc", "predicted_answer": "", "predicted_evidence": ["While most fine-grained sentiment datasets are in English, there are datasets available in several languages, such as German BIBREF5, Czech BIBREF6, Arabic, Chinese, Dutch, French, Russian, Spanish, Turkish BIBREF7, Hungarian BIBREF8, and Hindi BIBREF9. Additionally, there has been an increased effort to create fine-grained resources for low-resource languages, such as Basque and Catalan BIBREF10. No datasets for fine-grained SA have previously been created for Norwegian, however."]}
{"question_id": "87b65b538d79e1218fa19aaac71e32e9b49208df", "predicted_answer": "", "predicted_evidence": ["Table TABREF37 shows the results of the proportional and binary Overlap measures for precision, recall, and $\\text{F}_1$. The baseline model achieves modest results when compared to datasets that do not involve multiple domains BIBREF11, BIBREF10, with .41, .31, and .31 Proportional $\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively (.41, .36, .56 Binary $\\text{F}_1$). However, this is still better than previous results on cross-domain datasets BIBREF33. The domain variation between documents leads to a lower overlap between Holders, Targets, and Polar Expressions seen in training and those at test time (56%, 28%, and 50%, respectively). We argue, however, that this is a more realistic situation regarding available data, and that it is important to move away from simplifications where training and test data are taken from the same distribution."]}
{"question_id": "075d6ab5dd132666e85d0b6ad238118271dfc147", "predicted_answer": "", "predicted_evidence": ["As shown in Table , editing the gold query consistently improves both question match and interaction match accuracy. This shows the editing approach is indeed helpful to improve the generation quality when the previous query is the oracle."]}
{"question_id": "f2b1e87f61c65aaa99bcf9825de11ae237260270", "predicted_answer": "", "predicted_evidence": ["On both Spider and SParC, we use the exact set match accuracy between the gold and the predicted queries . To avoid ordering issues, instead of using simple string matching, yu2018spider decompose predicted queries into different SQL clauses such as SELECT, WHERE, GROUP BY, and ORDER BY and compute scores for each clause using set matching separately. On SparC, we report two metrics: question match accuracy which is the score average over all questions and interaction match accuracy which is average over all interactions."]}
{"question_id": "78c7318b2218b906a67d8854f3e511034075f79a", "predicted_answer": "", "predicted_evidence": ["Currently the standard approach in chitchat dialogue is to perform human evaluations BIBREF2, BIBREF20, BIBREF21, BIBREF4, BIBREF5, BIBREF7, typically reporting a judgment such as conversation quality or appropriateness via a Likert scale or pairwise comparison. While conversations are naturally multi-turn, pairwise setups typically consider single turn evaluations, taking the \u201cgold\u201d dialogue history from human-human logs, and only consider altering a single utterance. A more complete multi-turn evaluation is typically measured with a Likert scale (usually 1-4 or 1-5) after the conversation takes place. Some works such as BIBREF6 ask a series of questions relating to different aspects of conversational ability. There are some notable variants from these standard setups. BIBREF22 provide a method that combines continuous scales and relative assessments, but in single-turn, rather than multi-turn evaluation. BIBREF19 compare human evaluations to automatic metrics computed on self-chats. Note that we also use self-chats in this work, but we evaluate these with humans, rather than automatic metrics."]}
{"question_id": "697c5d2ba7e019ddb91a1de5031a90fe741f2468", "predicted_answer": "", "predicted_evidence": ["We use crowdworkers for our annotations. We recommend limiting the number of annotations a single worker may complete to be only a few pairs (in our experiments, if we are making $N$ model comparisons then we allow $N$ annotations). In preliminary trials, we found that limiting the influence of any one worker was important for replicability, but that results were highly consistent across multiple runs with this limitation."]}
{"question_id": "e25b73f700e8c958b64951f14a71bc60d225125c", "predicted_answer": "", "predicted_evidence": ["Cross-Domain BLEU vs. Cluster Proximity An interesting observation can be made with respect to the visual analysis of the domain clusters as depicted in Figure FIGREF15: as the Medical cluster (in Yellow), Law cluster (in Purple) and IT cluster (in Red) are close to each other in the embedding space, their cross-domain BLEU scores are also higher. For example, note how in the results for the Medical domain-specific model (first row in Table TABREF28), the BLEU scores on the Law and IT test sets are much higher in comparison to those on the Koran and Subtitles test sets, which clusters are farther away in the visualized embedding space. Similarly, as the Subtitles cluster (Blue) is closer to the Koran cluster (Green), the highest cross-domain BLEU score on the Koran test set is from the Subtitles model. To further quantify this phenomenon, we plot and measure Pearson's correlation between the cosine similarity of the centroids for the English BERT-based dev sentence representations for each domain pair, and the cross-domain BLEU score for this domain pair. This is shown in Figure FIGREF29. We can see the general trend where the closer the domain centroids are (with a similarity of 1 for training and evaluating on the same domain), the higher the cross-domain BLEU is between those domains, resulting in a Pearson's correlation of 0.81 (strong correlation). This suggests that such preliminary visual analysis can be a useful tool for understanding the relationship between diverse datasets, and motivates the use of pre-trained language model representations for domain data selection in MT."]}
{"question_id": "908ba58d26d15c14600623498d4e86c9b73b14b2", "predicted_answer": "", "predicted_evidence": ["px Previous works used n-gram LMs for data selection BIBREF4, BIBREF5 or other count-based methods BIBREF43, BIBREF44, BIBREF45, BIBREF46. While such methods work well in practice, they cannot generalize beyond the N-grams observed in the in-domain datasets, which are usually small."]}
{"question_id": "3e0fd1a3944e207edbbe7c7108239dbaf3bccd4f", "predicted_answer": "", "predicted_evidence": ["The definition of domain is many times vague and over-simplistic (e.g. \u201cmedical text\u201d may be used for biomedical research papers and for clinical conversations between doctors and patients, although the two vary greatly in topic, formality etc.). A common definition treats a domain as a data source: \u201ca domain is defined by a corpus from a specific source, and may differ from other domains in topic, genre, style, level of formality, etc.\u201d BIBREF8. We claim that a more data-driven definition should take place, as different data sources may have sentences with similar traits and vice versa - a single massive web-crawled corpus contains texts in numerous styles, topics and registers. Our analysis in Section SECREF2 shows examples for such cases, e.g. a sentence discussing \u201cViruses and virus-like organisms\u201d in a legal corpus."]}
{"question_id": "c0847af3958d791beaa14c4040ada2d364251c4d", "predicted_answer": "", "predicted_evidence": ["The proliferation of massive pretrained neural language models such as ELMo BIBREF9, BERT BIBREF10 or RoBERTa BIBREF11 has enabled great progress on many NLP benchmarks BIBREF12, BIBREF13. Larger and larger models trained on billions of tokens of raw text are released in an ever-increasing pace BIBREF3, enabling the NLP community to fine-tune them for the task of interest. While many works tried to \u201cprobe\u201d those models for the morphological, syntactic and semantic information they capture BIBREF14, BIBREF15, BIBREF16, an important aspect of language remained overlooked in this context \u2013 the domain the data comes from, often referred to as the \u201cdata distribution\u201d."]}
{"question_id": "2f142cd11731d29d0c3fa426e26ef80d997862e0", "predicted_answer": "", "predicted_evidence": ["Table TABREF30 provides the experimental results of these methods on RumourEval and PHEME datasets. We have the following observations:"]}
{"question_id": "ce23849e9e9a22626965f1ca8ca948a5c87280e9", "predicted_answer": "", "predicted_evidence": ["MT-trans The only difference between MT-trans and MT-lstm is that encoder of MT-trans is composed of transformer encoder."]}
{"question_id": "d9a45fea8539aac01dec01f29b7d04b44b9c2ca6", "predicted_answer": "", "predicted_evidence": ["As the last layer, softmax functions are applied to achieve the classification of different tasks, which emits the prediction of probability distribution for the specific task $i$."]}
{"question_id": "246e924017c48fa1f069361c44133fdf4f0386e1", "predicted_answer": "", "predicted_evidence": ["where $\\hat{{\\rm \\textbf {y}}}_i$ is the predictive result, ${\\rm \\textbf {F}}_i$ is the concatenation of private features ${\\rm \\textbf {H}}_i$ of task $i$ and the outputs ${\\rm \\textbf {SSL}}_i$ of selected sharing layer for task $i$. ${\\rm \\textbf {W}}_i$ and ${\\rm \\textbf {b}}_i$ are trainable parameters."]}
{"question_id": "96459b02efa82993a0b413530ed0b517c6633eea", "predicted_answer": "", "predicted_evidence": ["Exact search under length normalization does not suffer from the length deficiency anymore (last row in Tab. TABREF19 ), but it is not able to match our best BLEU score under Beam-10 search. This suggests that while length normalization biases search towards translations of roughly the correct length, it does not fix the fundamental modelling problem."]}
{"question_id": "6c1614991647705265fb348d28ba60dd3b63b799", "predicted_answer": "", "predicted_evidence": ["We then constrained exact search to either the length of the best Beam-10 hypothesis or the reference length. Tab. TABREF18 shows that exact search constrained to the Beam-10 hypothesis length does not improve over beam search, suggesting that any search errors between beam search score and global best score for that length are insignificant enough so as not to affect the BLEU score. The oracle experiment in which we constrained exact search to the correct reference length (last row in Tab. TABREF18 ) improved the BLEU score by 0.9 points."]}
{"question_id": "b948bb86855b2c0bfc8fad88ff1e29cd94bb6ada", "predicted_answer": "", "predicted_evidence": ["To better evaluate the quality of the generated utterances, we performed manual evaluation."]}
{"question_id": "157284acedf13377cbc6d58c8f3648d3a62f5db5", "predicted_answer": "", "predicted_evidence": ["Model-based: The idea is to use an additional meta-learner to learn to update the original learner with a few training examples. BIBREF17 developed a meta-learner based on LSTMs. Hypernetwork BIBREF18 , MetaNet BIBREF19 , and TCML BIBREF20 also learn a separate set of representations for fast model adaptation. BIBREF21 proposed an LSTM-based meta-learner to learn the optimization algorithm (gradients) used to train the original network."]}
{"question_id": "e4ea0569b637d5f56f63e933b8f269695fe1a926", "predicted_answer": "", "predicted_evidence": ["where $n_c$ indicates the number of correct questions, and $N$ is the total number of questions generated for the given claim. The label is assigned based on the correctness score ($s$) and the derived threshold ($\\phi $) as:"]}
{"question_id": "e3c44964eb6ddc554901244eb6595f26a9bae47e", "predicted_answer": "", "predicted_evidence": ["Here, the classification threshold ($\\phi $) is derived empirically based on the precision-recall curve."]}
{"question_id": "905a8d775973882227549e960c7028e4a3561752", "predicted_answer": "", "predicted_evidence": ["We fine-tune our model (BERT) on the masked language modeling task on the wiki-text provided along with the FEVER dataset for 2 epochs."]}
{"question_id": "76f90c88926256e7f90d2104a88acfdd7fc5475e", "predicted_answer": "", "predicted_evidence": ["In our case, FEVER claims are derived from Wikipedia. We first collect all the claims from the FEVER dataset along with \u201cid\u201d, \u201clabel\u201d and \u201cverifiable\u201d fields. We don't perform any normalization on the claims such as lowercasing, transforming the spaces to underscore or parenthesis to special characters as it may decrease the accuracy of the NER tagger. These claims are then processed by the NER tagger to identify the named entities and their type. The named entities are then used to generate the questions by masking the entities for the subsequent stage."]}
{"question_id": "182eb91090017a7c8ea38a88b219b641842664e4", "predicted_answer": "", "predicted_evidence": ["Table TABREF6 summarizes the statistics of the final dataset. The vocabulary size is 8.4K. We can see that the training set contains over 31K instances. Each content record contains around 5 tuples, each of which takes one of the 34 data types."]}
{"question_id": "0ef114d24a7a32821967e912dff23c016c4eab41", "predicted_answer": "", "predicted_evidence": ["Adversarial Style Transfer (AdvST) BIBREF12 . As another latest style transfer approach capable of handling more than one attributes, the model also mixes back-translation with auto-encoding as the above method, and additionally uses adversarial training to disentangle content and style representations."]}
{"question_id": "67672648e7ebcbef18921006e2c8787966f8cdf2", "predicted_answer": "", "predicted_evidence": ["We compare with a diverse set of approaches:"]}
{"question_id": "c32fc488f0527f330273263fa8956788bd071efc", "predicted_answer": "", "predicted_evidence": ["Ours w/o Coverage. For ablation study, we compare with a model variant that omits the content coverage constraint. That is, the model is trained by maximizing only Eq.( EQREF13 )."]}
{"question_id": "8908d1b865137bc309dde10a93735ec76037e5f9", "predicted_answer": "", "predicted_evidence": ["Official ranking: Our system is ranked fourth over 38 systems in terms of macro-average recall. Table 4 shows the results of our system on the test set of 2016 and 2017."]}
{"question_id": "d207f78beb6cd754268881bf575c8f98000667ea", "predicted_answer": "", "predicted_evidence": ["Polarity classification is the basic task of sentiment analysis in which the polarity of a given text should be classified into three categories: positive, negative or neutral. In Twitter where the tweet is short and written in informal language, this task needs more attention. SemEval has proposed the task of Message Polarity Classification in Twitter since 2013, the objective is to classify a tweet into one of the three polarity labels BIBREF0 ."]}
{"question_id": "35c01dc0b50b73ee5ca7491d7d373f6e853933d2", "predicted_answer": "", "predicted_evidence": ["Training. Differently from BIBREF17, which has paired sentence $S$ and corresponding ground-truth image $I$ for training text-guided image generation models to learn the mapping $S$ $\\rightarrow $ $I$, existing datasets such as COCO BIBREF11 and CUB BIBREF10 with natural language descriptions do not provide paired training data ($I$, $S^{\\prime }$) $\\rightarrow $ $I^{\\prime }_\\text{gt}$ for training text-guided image manipulation models, where $S^{\\prime }$ is a text describing new attributes, and $I^{\\prime }_\\text{gt}$ is the corresponding ground truth modified image."]}
{"question_id": "c077519ea42c9649fb78da34485de2262a0df779", "predicted_answer": "", "predicted_evidence": ["Effectiveness of the detail correction module and main module. As shown in Fig. FIGREF16 (f), our model without DCM misses some attributes (e.g., the bird missing the tail in the second row, the zebra missing the mouth in the third row), or generates new contents (e.g., new background in the first row, different appearance of the bus in the fourth row), which indicates that our DCM can correct inappropriate attributes and reconstruct text-irrelevant contents. Fig. FIGREF16 (e) shows that without main module, our model fails to do image manipulation on both datasets, which just achieves an identity mapping. This is mainly because the model fails to correlate words with corresponding attributes, which has been done in the main module. Table TABREF11 also illustrates the identity mapping, as our model without main module gets the lowest $L_{1}$ pixel difference value."]}
{"question_id": "a51c680a63ee393792d885f66de75484dc6bc9bc", "predicted_answer": "", "predicted_evidence": ["Best results are marked in bold; second best are underlined in the table"]}
{"question_id": "e752dc4d721a2cf081108b6bd71e3d10b4644354", "predicted_answer": "", "predicted_evidence": ["In order to make a systematic comparison, three groups of baselines are used in the evaluation. Group 1 includes all commonly used feature sets mentioned in Chen et al. chen2016neural including Majority, Trigram, Text features (TextFeatures), and AveWordvec. All feature sets in Group 1 except Majority use the SVM classifier."]}
{"question_id": "c79f168503a60d1b08bb2c9aac124199d210b06d", "predicted_answer": "", "predicted_evidence": ["Individual Layers: Only a single layer is used for the downstream task."]}
{"question_id": "9dd8ce48a2a59a63ae6366ab8b2b8828e5ae7f35", "predicted_answer": "", "predicted_evidence": ["In our experiment, we don't observe a difference between the computation of an unweighted average and of learning a task-specific weighted average. For four datasets, the unweighted average yielded better performance, while for the other four other datasets, the learned weighted average yielded better performance. However, the differences are insignificant."]}
{"question_id": "5cc5e2db82f5d40a5244224dad94da50b4f673db", "predicted_answer": "", "predicted_evidence": ["Occupation Data: We gathered occupation lists from different sources on the internet including crowdsourced lists and government lists. Then, we classified the occupations into 2 categories - gender-specific occupation and gender-neutral occupations. These are used in the algorithm for bias checking which will be explained in the next sub-section."]}
{"question_id": "ab975efc916c34f55e1144b1d28e7dfdc257e371", "predicted_answer": "", "predicted_evidence": ["Occupation Data: We gathered occupation lists from different sources on the internet including crowdsourced lists and government lists. Then, we classified the occupations into 2 categories - gender-specific occupation and gender-neutral occupations. These are used in the algorithm for bias checking which will be explained in the next sub-section."]}
{"question_id": "e7ce612f53e9be705cdb8daa775eae51778825ef", "predicted_answer": "", "predicted_evidence": ["Hence, the observation is that when we change the year and location parameters in the tool, the tool can automatically respond to the change. Therefore the system is sensitive to the subjectivity of bias in various cultural contexts and timeframes."]}
{"question_id": "6c5a64b5150305c584326882d37af5b0e58de2fd", "predicted_answer": "", "predicted_evidence": ["De-biasing the model after training as a way to remove bias focuses on \"fixing\" the model after training is complete. BIBREF8 in their famous work on gender bias in word embeddings take this approach to \"fix\" the embeddings after training."]}
{"question_id": "f7a27de3eb6447377eb48ef6d2201205ff943751", "predicted_answer": "", "predicted_evidence": ["In practice, a satisfactory stylistic dialogue system should express the desired style on the premise of the response quality. Based on the criterion of human evaluation metric, 3 is the marginal score of acceptance. So we deem a response as marginally acceptable by actual users when both quality and style expression scores are greater or equal to 3. On the other hand, 4 is the score that well satisfies the users, so responses with both scores greater or equal to 4 are deemed as satisfying to actual users."]}
{"question_id": "2df3cd12937591481e85cf78c96a24190ad69e50", "predicted_answer": "", "predicted_evidence": ["As shown in Figure FIGREF55, some of the strong baselines exhibit a drastic drop in response quality after domain variation such as GPT2-FT and PS w/o R. In contrast, the PS model successfully maintains high response quality in spite of domain variation. The model seems to benefit from leveraging retrieved results to bridge the gap between the two different domains. This can also be observed in the results of RST and RRe which also use the retrieved results and get a even higher performance when facing domain variation."]}
{"question_id": "fcb0ac1934e2fd9f58f4b459e6853999a27844f9", "predicted_answer": "", "predicted_evidence": ["The aforementioned approaches explicitly incorporate the language style information into the model configuration either via embeddings or memory modules to control the process of response generation. In our replication experiments, we found that these approaches tend to overemphasise the importance of the language style. As a result, the generated responses tend to be generic and non-informative BIBREF17, but they do express a distinct style; e.g., they generate a generic response: \u201cI am happy to hear that.\" that conveys a `happy' emotion to different queries."]}
{"question_id": "fc9aa04de4018b7d55e19a39663a2e9837328de7", "predicted_answer": "", "predicted_evidence": ["To fully evaluate the proposed approach, we conduct extensive experiments on three benchmark datasets. Results of both human and automatic evaluation show that the proposed approach significantly outperforms several strong baselines. In addition, we also conduct an extensive cross-domain experiment to demonstrate that the proposed approach is more robust than such baselines."]}
{"question_id": "044cb5ef850c0a2073682bb31d919d504667f907", "predicted_answer": "", "predicted_evidence": ["As shown in Table TABREF14, the versification-based models yield a very high accuracy with the recognition of Shakespeare and Fletcher (0.97 to 1 with the exception of Valentinian), yet slightly lower accuracy with the recognition of Massinger (0.81 to 0.88). The accuracy of words-based models remains very high across all three authors (0.95 to 1); in three cases it is nevertheless outperformed by the combined model. We thus may conclude that combined models provide a reliable discriminator between Shakespeare\u2019s, Fletcher\u2019s and Massinger\u2019s styles."]}
{"question_id": "c845110efee2f633d47f5682573bc6091e8f5023", "predicted_answer": "", "predicted_evidence": ["The probability that the text of H8 is a result of collaboration between Shakespeare and Fletcher is very high: with 7 scenes all the 30 models agree upon Shakespeare\u2019s authorship, with 5 scenes all the 30 models agree upon Fletcher\u2019s authorship."]}
{"question_id": "2301424672cb79297cf7ad95f23b58515e4acce8", "predicted_answer": "", "predicted_evidence": ["While the stylistic dissimilarity of Henry VIII (henceforth H8) to Shakespeare\u2019s other plays had been pointed out before BIBREF2, it was not until the mid-nineteenth century that Shakespeare\u2019s sole authorship was called into question. In 1850 British scholar James Spedding published an article BIBREF3 attributing several scenes to John Fletcher. Spedding supported this with data from the domain of versification, namely the ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d), pointing out that the distribution of values across scenes is strongly bimodal."]}
{"question_id": "6c05376cd0f011e00d1ada0254f6db808f33c3b7", "predicted_answer": "", "predicted_evidence": ["For the sake of comparison of the attribution power of both feature subsets, cross-validations are performed not only of the combined models (500 words $\\cup $ 500 rhythmic types), but also of the words-based models (500 words) and versification-based models (500 rhythmic types) alone."]}
{"question_id": "9925e7d8757e8fd7411bcb5250bc08158a244fb3", "predicted_answer": "", "predicted_evidence": ["In the present study, with regard to the aforementioned studies, Shakespeare, Fletcher, and Massinger are considered as candidates to the authorship of H8."]}
{"question_id": "fa468c31dd0f9095d7cec010f2262eeed565a7d2", "predicted_answer": "", "predicted_evidence": ["Scenes 5.2, 5.3, 5.4 and 5.5 are Fletcher\u2019s according to word-based models and combined models. Rhythmic types indicate the possibility of Shakespeare\u2019s share in 5.4."]}
{"question_id": "8c89f1d1b3c2a45c0254c4c8d6e700ab9a4b4ffb", "predicted_answer": "", "predicted_evidence": ["Yet another possibility is open consent, in which individuals make their data publicly available. Initiatives like Personal Genome Project may have an exemplary role, however, they can only provide limited data and they represent a biased population sample BIBREF33 ."]}
{"question_id": "f5bc07df5c61dcb589a848bd36f4ce9c22abd46a", "predicted_answer": "", "predicted_evidence": ["The ethics discussion is gaining momentum in general NLP BIBREF8 . We aim in this paper to gather the ethical challenges that are especially relevant for clinical NLP, and to stimulate discussion about those in the broader NLP community. Although enhancing privacy through restricted data access has been the norm, we do not only discuss the right to privacy, but also draw attention to the social impact and biases emanating from clinical notes and their processing. The challenges we describe here are in large part not unique to clinical NLP, and are applicable to general data science as well."]}
{"question_id": "8126c6b8a0cab3e22661d3d71d96aa57360da65c", "predicted_answer": "", "predicted_evidence": ["To measure the quality of outline generated by our model and the baselines, we employ three automatic metrics, namely"]}
{"question_id": "2f01d3e5120d1fef4b01028536cb5fe0abad1968", "predicted_answer": "", "predicted_evidence": ["To measure the quality of outline generated by our model and the baselines, we employ three automatic metrics, namely"]}
{"question_id": "b78bb6fe817c2d4bc69236df998f546e94c3ee21", "predicted_answer": "", "predicted_evidence": ["For each target emotion (i.e., intended emotion of generated sentences) we conducted an initial MANOVA, with human ratings of affect categories the DVs (dependent variables) and the affect strength parameter $\\beta $ the IV (independent variable). We then conducted follow-up univariate ANOVAs to identify which DV changes significantly with $\\beta $ . In total we conducted 5 MANOVAs and 30 follow-up ANOVAs, which required us to update the significance level to p $<$ 0.001 following a Bonferroni correction."]}
{"question_id": "1a419468d255d40ae82ed7777618072a48f0091b", "predicted_answer": "", "predicted_evidence": ["Affect is a term that subsumes emotion and longer term constructs such as mood and personality and refers to the experience of feeling or emotion BIBREF0 . BIBREF1 picard1997affective provides a detailed discussion of the importance of affect analysis in human communication and interaction. Within this context the analysis of human affect from text is an important topic in natural language understanding, examples of which include sentiment analysis from Twitter BIBREF2 , affect analysis from poetry BIBREF3 and studies of correlation between function words and social/psychological processes BIBREF4 . People exchange verbal messages which not only contain syntactic information, but also information conveying their mental and emotional states. Examples include the use of emotionally colored words (such as furious and joy) and swear words. The automated processing of affect in human verbal communication is of great importance to understanding spoken language systems, particularly for emerging applications such as dialogue systems and conversational agents."]}
{"question_id": "52f5249a9a2cb7210eeb8e52cb29d18912f6c3aa", "predicted_answer": "", "predicted_evidence": ["The gate $f$ consists of a projection layer to one dimension and an activation function. The resulting weight is multiplied by each element of the output of layer $L_{g_{k+1}}$ to produce the output for task $g_{k+1}$:"]}
{"question_id": "baad4b6f834d5944f61bd12f30908e3cf3739dcd", "predicted_answer": "", "predicted_evidence": ["We depart from BERT BIBREF12, and we design three baselines."]}
{"question_id": "37b972a3afae04193411dc569f672d802c16ad71", "predicted_answer": "", "predicted_evidence": ["We have argued for a new way to study propaganda in news media: by focusing on identifying the instances of use of specific propaganda techniques. Going at this fine-grained level can yield more reliable systems and it also makes it possible to explain to the user why an article was judged as propagandistic by an automatic system."]}
{"question_id": "a01af34c7f630ba0e79e0a0120d2e1c92d022df5", "predicted_answer": "", "predicted_evidence": ["In future work, we plan to include more media sources, especially from non-English-speaking media and regions. We further want to extend the tool to support other propaganda techniques."]}
{"question_id": "0c4e419fe57bf01d58a44f3e263777c22cdd90dc", "predicted_answer": "", "predicted_evidence": ["We depart from BERT BIBREF12, and we design three baselines."]}
{"question_id": "7b76b8b69246525a48c0a8ca0c42db3319cd10a5", "predicted_answer": "", "predicted_evidence": ["These results suggest that there are only slight differences between bigram and trigram models, and that the type and quantity of corpora used to train the models is what really determines the results."]}
{"question_id": "8b1af67e3905244653b4cf66ba0acec8d6bff81f", "predicted_answer": "", "predicted_evidence": ["An N-gram model can predict the next word from a sequence of N-1 previous words. A trigram Language Model (LM) predicts the conditional probability of the next word using the following approximation: DISPLAYFORM0 "]}
{"question_id": "9a7aeecbecf5e30ffa595c233fca31719c9b429f", "predicted_answer": "", "predicted_evidence": ["In this section we present the results from our development stage (Table 2), the evaluation stage (Table 3), and two post-evaluation results (Table 3). Since we implemented both bigram and trigam language models during the development stage but only results from trigram language models were submitted to the task, we evaluated bigram language models in the post-evaluation stage. Note that the accuracy and distance measurements listed in Table 2 and Table 3 are defined by the task organizers BIBREF6 ."]}
{"question_id": "3605ea281e72e9085a0ac0a7270cef25fc23063f", "predicted_answer": "", "predicted_evidence": ["Table 3 shows the results of our system during the task evaluation. We submitted two runs, one with a trigram language model trained on the tweet data, and another with a trigram language model trained on the news data. In addition, after the evaluation was concluded we also decided to run the bigram language models as well. Contrary to what we observed in the development data, the bigram language model actually performed somewhat better than the trigram language model. In addition, and also contrary to what we observed with the development data, the news data proved generally more effective in the post\u2013evaluation runs than the tweet data."]}
{"question_id": "21f6cb3819c85312364dd17dd4091df946591ef0", "predicted_answer": "", "predicted_evidence": ["For Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets. For each pair, if the first tweet was funnier than the second, the system would output the tweet_ids for the pair followed by a \u201c1\u201d. If the second tweet is funnier it outputs the tweet_ids followed by a \u201c0\u201d. For Subtask B, the system outputs all the tweet_ids for a hashtag file starting from the funniest."]}
{"question_id": "fd8a8eb69f07c584a76633f8802c2746f7236d64", "predicted_answer": "", "predicted_evidence": ["To find the ground truth, we collected annotations from AMT workers. We asked these workers to determine whether or not a given sentence expressed a given relation. If the majority answer was no, then we labeled that sentence as expressing no relation. (We denote no relation as NA in WikiGenderBias.) Each sentence was annotated by three different workers. Each worker was paid 15 cents per annotation. We only accepted workers from England, the US or Australia and with HIT Approval Rate greater than $95\\%$ and Number of HITs greater than 100. We found the pairwise inter-annotator agreement as measured by Fleiss' Kappa BIBREF26 $\\kappa $ to be 0.44, which is consistent across both genders and signals moderate agreement. We note that our $\\kappa $ value is affected by asking workers to make binary classifications, which limits the degree of agreement that is attainable above chance. We also found the pairwise inter-annotator agreement to be 84%."]}
{"question_id": "452e978bd597411b65be757bf47dc6a78f3c67c9", "predicted_answer": "", "predicted_evidence": ["While these findings will help future work avoid gender biases, this study is preliminary. We only consider binary gender, but future work should consider non-binary genders. Additionally, future work should further probe the source of gender bias in the model's predictions, perhaps by visualizing attention or looking more closely at the model's outputs."]}
{"question_id": "159025c44c0115ab4cdc253885384f72e592e83a", "predicted_answer": "", "predicted_evidence": ["Since a form of measurement is required for observation, prior work has created methods for measuring gender bias BIBREF16, BIBREF9, BIBREF10, BIBREF17, BIBREF11, BIBREF12, BIBREF18. Gender bias has been measured mainly in training sets and in predictions. Measuring the latter is simple: measure the difference in performance of the model on male and female datapoints, with the definition of the gender of a datapoint being domain-dependent BIBREF11, BIBREF12. Other metrics have been proposed to evaluate fairness of predictors and allocative bias BIBREF19, BIBREF7, such as Equality of Opportunity. We use both of these methods to evaluate NRE models."]}
{"question_id": "6590055fb033cb32826f2afecb3d7f607dd97d57", "predicted_answer": "", "predicted_evidence": ["Name Anonymization surprisingly substantially increases F1 score gap for the hypernym relation, but slightly decreases F1 score gap for all other relations. Name Anonymization appears to be effective at debiasing all relations aside from hypernym, though not as effective as either Gender-Swapping or using Debiased Embeddings. These results indicate that entity bias likely does not contribute very much the gender bias in the models' original predictions."]}
{"question_id": "3435e365adf7866e45670c865dc33bb7d2a6a0c6", "predicted_answer": "", "predicted_evidence": ["To generate WikiGenderBias, we use a variant of the Distant Supervision assumption: for a given relation between two entities, assume that any sentence from an article written about one of those entities that mentions the other entity expresses the relation. For instance, if we know (Barack, spouse, Michelle) is a relation and we find the sentence He and Michelle were married in Barack's Wikipedia article, then we assume that sentence expresses the (Barack, spouse, Michelle) relation. This assumption is similar to that made by BIBREF20 and allows us to scalably create the dataset."]}
{"question_id": "cd82bdaa0c94330f8cccfb1c59b4e6761a5a4f4d", "predicted_answer": "", "predicted_evidence": ["This section describes the original data from the Snopes platform, followed by a detailed report on our corpus annotation methodology."]}
{"question_id": "753a187c1dd8d96353187fbb193b5f86293a796c", "predicted_answer": "", "predicted_evidence": ["FGE Annotation. Similar to the stance annotation, we used the approach of BIBREF12 to compute the agreement. The inter-annotator agreement between the crowd workers in this case is $\\kappa = 0.55$ Cohen's Kappa. We compared the annotations of FGE in 200 ETSs by experts with the annotations by crowd workers, reaching an agreement of $\\kappa = 0.56$. This is considered as moderate inter-annotator agreement BIBREF15."]}
{"question_id": "29794bda61665a1fbe736111e107fd181eacba1b", "predicted_answer": "", "predicted_evidence": ["Snopes is a large-scale fact-checking platform that employs human fact-checkers to validate claims. A simple fact-checking instance from the Snopes website is shown in Figure FIGREF14. At the top of the page, the claim and the verdict (rating) are given. The fact-checkers additionally provide a resolution (origin), which backs up the verdict. Evidence in the resolution, which we call evidence text snippets (ETSs), is marked with a yellow bar. As additional validation support, Snopes fact-checkers provide URLs for original documents (ODCs) from which the ETSs have been extracted or which provide additional information."]}
{"question_id": "dd80a38e578443496d3720d883ad194ce82c5f39", "predicted_answer": "", "predicted_evidence": ["CLEF-2018 Another corpus concerned with political debates was introduced by BIBREF11 and used for the CLEF-2018 shared task. The corpus consists of transcripts of political debates in English and Arabic and provides annotations for two tasks: identification of check-worthy statements (claims) in the transcripts, and validation of 150 statements (claims) from the debates. However, as for the corpus PolitiFact17, no evidence for the validation of these claims is available."]}
{"question_id": "9a9774eacb8f75bcfa07a4e60ed5eb02646467e3", "predicted_answer": "", "predicted_evidence": ["This section describes the original data from the Snopes platform, followed by a detailed report on our corpus annotation methodology."]}
{"question_id": "4ed58d828cd6bb9beca1471a9fa9f5e77488b1d1", "predicted_answer": "", "predicted_evidence": ["The contributions of our work are as follows:"]}
{"question_id": "de580e43614ee38d2d9fc6263ff96e6ca2b54eb5", "predicted_answer": "", "predicted_evidence": ["1) We provide a substantially sized mixed-domain corpus of natural claims with annotations for different fact-checking tasks. We publish a web crawler that reconstructs our dataset including all annotations. For research purposes, we are allowed to share the original corpus."]}
{"question_id": "ae89eed483c11ccd70a34795e9fe416af8a35da2", "predicted_answer": "", "predicted_evidence": ["As the example illustrates, there is a gradual transition between sentences that can be considered as essential for the validation of the claim and those which just provide minor negligible details or unrelated information. Nevertheless, even though the inter-annotator agreement for the annotation of FGE is lower than for the annotation of ETS stance, compared to other annotation problems BIBREF16, BIBREF17, BIBREF18 that are similar to the annotation of FGE, our framework leads to a better agreement."]}
{"question_id": "fc62549a8f0922c09996a119b2b6a8b5e829e989", "predicted_answer": "", "predicted_evidence": ["The characteristics of a user speech can mainly be distinguished by the word dictionary. Thus, this metric tries to measure the differences of the word dictionary among the comparing set. Table 1 shows the quantitative measure results from the dialogue set of the main characters in drama data from \u201cFriends,\" a famous American television sitcom. In the figures, \u201ccharacter_1\" to \u201ccharacter_6\" are the main characters of the drama (Chandler, Joey, Monica, Phoebe, Rachel, and Ross, respectively). The dialogues were measured against one another by using the cross entropy metric. As shown in the table, the lower cross entropy value among the same character's dialogue was calculated, and the higher value was calculated among the different character's dialogues as expected. This result demonstrates that the cross entropy metric can be used to measure the similarities among the members of the set."]}
{"question_id": "e2a507749a4a3201edd6413c77ad0d4c23e9c6ce", "predicted_answer": "", "predicted_evidence": ["It is certain that our proposed method reveals the applicability of the RNN-based language model in a user device with the preservation of privacy. Furthermore, with our method the personalized language model can be generated with a smaller amount of user data than the huge amount of training data that is usually required in the traditional deep neural network discipline. In the future work, we aim to visualize the deep neural network and to investigate the specific relationship among users' language styles and the LSTM cells in the network. This approach seems likely to uncover enhanced learning schemes that require less data than was previously necessary."]}
{"question_id": "a3a867f7b3557c168d05c517c468ff6c7337bff9", "predicted_answer": "", "predicted_evidence": ["The \u201cpersonalized language model 2\" in Table 2 shows the sample output of the personalized language model trained with another style of document data, English bible data. As shown in Table 2, the output of the personalized language model contains more bible-like vocabulary and sentence styles."]}
{"question_id": "8bb2280483af8013a32e0d294e97d44444f08ab0", "predicted_answer": "", "predicted_evidence": ["The characteristics of a user speech can mainly be distinguished by the word dictionary. Thus, this metric tries to measure the differences of the word dictionary among the comparing set. Table 1 shows the quantitative measure results from the dialogue set of the main characters in drama data from \u201cFriends,\" a famous American television sitcom. In the figures, \u201ccharacter_1\" to \u201ccharacter_6\" are the main characters of the drama (Chandler, Joey, Monica, Phoebe, Rachel, and Ross, respectively). The dialogues were measured against one another by using the cross entropy metric. As shown in the table, the lower cross entropy value among the same character's dialogue was calculated, and the higher value was calculated among the different character's dialogues as expected. This result demonstrates that the cross entropy metric can be used to measure the similarities among the members of the set."]}
{"question_id": "a68acd8364764d5601dc12e4b31d9102fb7d5f7e", "predicted_answer": "", "predicted_evidence": ["An output of a personalized language model can be measured by calculating the cross entropy between the word distribution of the model output and that of the target data. Word distribution can be acquired by normalizing a word histogram which is calculated based on word counts in the target corpus. Equation (3) shows the metric formulation. "]}
{"question_id": "6d55e377335815b7ad134d1a2977d231ad34a25b", "predicted_answer": "", "predicted_evidence": ["The characteristics of a user speech can mainly be distinguished by the word dictionary. Thus, this metric tries to measure the differences of the word dictionary among the comparing set. Table 1 shows the quantitative measure results from the dialogue set of the main characters in drama data from \u201cFriends,\" a famous American television sitcom. In the figures, \u201ccharacter_1\" to \u201ccharacter_6\" are the main characters of the drama (Chandler, Joey, Monica, Phoebe, Rachel, and Ross, respectively). The dialogues were measured against one another by using the cross entropy metric. As shown in the table, the lower cross entropy value among the same character's dialogue was calculated, and the higher value was calculated among the different character's dialogues as expected. This result demonstrates that the cross entropy metric can be used to measure the similarities among the members of the set."]}
{"question_id": "0035b351df63971ec57e36d4bfc6f7594bed41ae", "predicted_answer": "", "predicted_evidence": ["These basic features are extracted from the text. They are the following:"]}
{"question_id": "2b021e1486343d503bab26c2282f56cfdab67248", "predicted_answer": "", "predicted_evidence": ["The evaluation metrics used by the task organizers were the macroaveraged recall ( INLINEFORM0 ), the F1 averaged across the positives and the negatives INLINEFORM1 and the accuracy ( INLINEFORM2 ) BIBREF25 ."]}
{"question_id": "e801b6a6048175d3b1f3440852386adb220bcb36", "predicted_answer": "", "predicted_evidence": ["These basic features are extracted from the text. They are the following:"]}
{"question_id": "3699927c6c1146f5057576034d226a99946d52cb", "predicted_answer": "", "predicted_evidence": ["Comparing results across languages we observe that scores for English exceed scores for all other languages. At the same time, for almost all models and languages the IAA scores fall under the category of `fair agreement' ( $0.20 < \\kappa < 0.40$ ) indicating that the elicitation task was feasible for crowdworkers. This applies to both evaluations (Tables 5 and 6 ). We observed a similar pattern in the results of Experiment 1 (Table 3 ). We believe there are two reasons for this drop. Firstly, in order to perform cross-linguistic experiments, we translated English categories into other languages. As mentioned in Sections \"Results\" and \"Results\" , such a direct correspondence may not always exist. Consequently, annotators for languages other than English are faced with a noisier (and potentially harder) task. Secondly, while it is straightforward to recruit English native speakers on crowd sourcing platforms, it has proven more challenging for the other languages. We suspect that our effort to recruit native speakers, might not have been entirely fail-safe for languages other that English, and that the language competence of those crowdworkers might have impacted the quality of their judgments."]}
{"question_id": "6606160e210d05b94f7cbd9c5ff91947339f9d02", "predicted_answer": "", "predicted_evidence": ["Even though much empirical research glosses over this observation, there is strong evidence that human conceptual representations are structured (see BIBREF37 for a recent critique and overview of cognitive studies of categorization). Categories mentally represent the complex structure of the environment. They allow to make inferences about concepts or categories that go beyond their perceived similarities capturing abstract and potentially complex properties (for example, the nutritional value of food items, or the emotional benefits of pets). Much research on human categorization is based on laboratory experiments where subjects are presented with artificial stimuli represented by a restricted set of task-relevant features. Observations of natural concepts, however, are often noisy or incomplete so that a notion of systematic relations among features might be more important here than under artificial conditions in the lab BIBREF38 ."]}
{"question_id": "0dc9050c832a6091bc9db3f7fa7be72139f51177", "predicted_answer": "", "predicted_evidence": ["Categories such as animal or furniture are fundamental cognitive building blocks allowing humans to efficiently represent and communicate the complex world around them. Concepts (e.g., dog, table) are grouped into categories based on shared properties pertaining, for example, to their behavior, appearance, or function. Categorization underlies other cognitive functions such as perception BIBREF0 , BIBREF1 or language BIBREF2 , BIBREF3 , and there is evidence that categories are not only shaped by the world they represent, but also by the language through which they are communicated BIBREF4 , BIBREF5 . Although mental categories exist across communities and cultures, their exact manifestations differ BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 . For example, American English speakers prefer taxonomic categorizations (e.g., mouse,squirrel) while Chinese speakers tend to prefer to categorize objects relationally (e.g., tree, squirrel; BIBREF7 )."]}
{"question_id": "4beb50ba020f624446ff1ef5bf4adca5ed318b98", "predicted_answer": "", "predicted_evidence": ["The ESuLMo is evaluated in two ways, task independent and task dependent. For the former, we examine the perplexity of the pre-trained language models. For the latter, we examine on four benchmark NLP tasks, syntactic dependency parsing, semantic role labeling, implicity discourse relation recognition, and textual entailment."]}
{"question_id": "9bf60073fbb69fbf860196513fc6fd2f466535f6", "predicted_answer": "", "predicted_evidence": ["While applying our pre-trained ESuLMo to other NLP tasks, we have two different strategies: (1) Fine-tuning our ESuLMo while training other NLP tasks; (2) Fixing our ESuLMo while training other NLP tasks. During the experiment, we find there is no significant difference between these two strategies. However, the first strategy consumes much more resource than the second one. Therefore, we choose the second strategy to conduct all the remaining experiments."]}
{"question_id": "7d503b3d4d415cf3e91ab08bd5a1a2474dd1047b", "predicted_answer": "", "predicted_evidence": ["We also analyze the subword vocabularies from two algorithms and find that the overlapping rates for 500, 1K and 2K sizes are 60.2%, 55.1% and 51.9% respectively. This indicates subword mechanism can stably work in different vocabularies."]}
{"question_id": "1c8958ec50976a9b1088c51e8f73a767fb3973fa", "predicted_answer": "", "predicted_evidence": ["In total we experimented with 11 different setups of the proposed scheme, each with a different ensemble of classifiers, see Table TABREF17 ."]}
{"question_id": "363d0cb0cd5c9a0b0364d61d95f2eff7347d5a36", "predicted_answer": "", "predicted_evidence": ["The research question we address in this work is:"]}
{"question_id": "cf0b7d8a2449d04078f69ec9717a547adfb67d17", "predicted_answer": "", "predicted_evidence": ["To answer this question, our main goals can be summarized as follows:"]}
{"question_id": "8de0e1fdcca81b49615a6839076f8d42226bf1fe", "predicted_answer": "", "predicted_evidence": ["The above methods allow the annotators to create hundreds of intents efficiently, with thousands of examples, allowing millions of distinct potential phrases to be matched. When combined with the ability for customers to configure entities and select a subset of intents that are relevant to their business, this approach produces highly customer-specific repositories of domain knowledge."]}
{"question_id": "909ecf675f874421eecc926a9f7486475aa1423c", "predicted_answer": "", "predicted_evidence": ["The results confirm our assumptions presented in Section SECREF20. The longer the intent annotation, the more likely it is to be correct due to stronger contextuality of the annotation. Intent annotations which span at least three words are more likely to rescore the lattice correctly than to introduce a false positive. These results also lead us to a practical heuristic, that an intent annotation which spans only one or two words should not be considered for rescoring. Application of this heuristic results in an estimated accuracy of 77%. We use this heuristic in further experiments. A stricter heuristic would require at least four words span, with an accuracy of 87.7%. Calibration of this threshold is helpful when the algorithm is adapted to a downstream task, where a different precision/recall ratio may be required. We present some examples of successful lattice rescoring in Table TABREF19."]}
{"question_id": "29477c8e28a703cacb716a272055b49e2439a695", "predicted_answer": "", "predicted_evidence": ["Last but not least, production ASR systems impose strict constraints on the additional computation that can be performed. Since we operate in a near real-time environment, this precludes the use of computationally expensive language models which could compensate for some of the ASR errors."]}
{"question_id": "9186b2c5b7000ab7f15a46a47da73ea45544bace", "predicted_answer": "", "predicted_evidence": ["In the second phase, hard-EM is used for parameter estimation. At the end of each iteration, the least frequent subwords are selected as candidates for pruning. For each candidate subword, the change in likelihood when removing the subword is estimated by resegmenting all words in which the subword occurs. After each pruned subword, the parameters of the model are updated. Pruning ends when the goal lexicon size is reached or the change in likelihood no longer exceeds a given threshold."]}
{"question_id": "d30b2fb5b29faf05cf5e04d0c587a7310a908d8c", "predicted_answer": "", "predicted_evidence": ["Figure shows the Precision\u2013Recall curves for the primary systems, for all four languages. While increasing the Morfessor cost, forced splitting improves BPR. Tables to show test set Boundary Precision, Recall and F$_{1}$-score (BPR) results at the optimal tuning point (selected using a development set) for each model, for English, Finnish, Turkish and North S\u00e1mi, respectively. The default Morfessor EM+Prune configuration (\u201csoft\u201d EM, full prior, forcesplit) significantly outperforms Morfessor Baseline w.r.t. the F-score for all languages except North S\u00e1mi, for which there is no significant difference between the methods."]}
{"question_id": "526dc757a686a1fe41e77f7e3848e3507940bfc4", "predicted_answer": "", "predicted_evidence": ["Each iteration begins with 3 sub-iterations of EM. In the pruning phase of each iteration, the subwords in the current lexicon are sorted in ascending order according to the estimated change in the cost function if the subword is removed from the lexicon. Subwords consisting of a single character are always kept, to retain the ability to represent an open vocabulary without OOV issues. The list is then pruned according to one of three available pruning criteria:"]}
{"question_id": "2d91554c3f320a4bcfeb00aa466309074a206712", "predicted_answer": "", "predicted_evidence": ["The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU BIBREF35 , METEOR BIBREF36 and chrF BIBREF37 metrics."]}
{"question_id": "53362c2870cf76b7981c27b3520a71eb1e3e7965", "predicted_answer": "", "predicted_evidence": ["The left part of Table TABREF27 illustrates the size of the wordnets provided by the Open Multilingual Wordnet. The right part of the table shows statistics of the Polylingual Wordnet."]}
{"question_id": "5138121b9e9bd56962e69bfe49d5df5301cb7745", "predicted_answer": "", "predicted_evidence": ["METEOR (Metric for Evaluation of Translation with Explicit ORdering) is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. In addition to exact word (or phrase) matching it has additional features, i.e. stemming, paraphrasing and synonymy matching. In contrast to BLEU, the metric produces good correlation with human judgement at the sentence or segment level."]}
{"question_id": "25e6ba07285155266c3154d3e2ca1ae05c2f7f2d", "predicted_answer": "", "predicted_evidence": ["See the following tables for sample dialog histories and generated host responses from each of our baseline and speaker-conditioned dialog models."]}
{"question_id": "d68cc9aaf0466b97354600a5646c3be4512fc096", "predicted_answer": "", "predicted_evidence": ["See the following tables for sample dialog histories and generated host responses from each of our baseline and speaker-conditioned dialog models."]}
{"question_id": "d038e5d2a6f85e68422caaf8b96cb046db6599fa", "predicted_answer": "", "predicted_evidence": ["We additionally explore two tasks that are facilitated by speaker role annotations in Interview: 1) generating appropriate responses for a specific role given a conversation history (speaker role modeling); and 2) predicting whether a new speaker will interject on the next sentence of a conversation. These tasks are crucial components to building fluent and role-specific dialog systems, for settings such as healthcare and customer service."]}
{"question_id": "c66e0aa86b59bbf9e6a1dc725fb9785473bfa137", "predicted_answer": "", "predicted_evidence": ["We collect a novel dataset of 105K multi-party interview transcripts for 7 programs on National Public Radio (NPR) over 20 years (1999\u20132019), total of 10k hours. These transcripts contain a total of 3M turns comprising 7.5M sentences (127M words) from 184K speakers, of which 287 are hosts. To investigate role-play in media dialog, we curate a subset, Interview 2P, with two roles: a host and a guest, comprising 23K two-party conversations encompassing 455K turns, with 1.24M sentences and 21.7M words."]}
{"question_id": "369d7bc5351409910c7a5e05c0cbb5abab8e50ec", "predicted_answer": "", "predicted_evidence": ["We collect a novel dataset of 105K multi-party interview transcripts for 7 programs on National Public Radio (NPR) over 20 years (1999\u20132019), total of 10k hours. These transcripts contain a total of 3M turns comprising 7.5M sentences (127M words) from 184K speakers, of which 287 are hosts. To investigate role-play in media dialog, we curate a subset, Interview 2P, with two roles: a host and a guest, comprising 23K two-party conversations encompassing 455K turns, with 1.24M sentences and 21.7M words."]}
{"question_id": "b9d9803ba24127f91ba4d7cff4da11492da20f09", "predicted_answer": "", "predicted_evidence": ["See the following tables for sample dialog histories and generated host responses from each of our baseline and speaker-conditioned dialog models."]}
{"question_id": "7625068cc22a095109580b83eff48616387167c2", "predicted_answer": "", "predicted_evidence": ["See the following tables for sample dialog histories and generated host responses from each of our baseline and speaker-conditioned dialog models."]}
{"question_id": "be0b438952048fe6bb91c61ba48e529d784bdcea", "predicted_answer": "", "predicted_evidence": ["The US Defense Advanced Research Projects Agency (DARPA) has undertaken efforts to collect broadcast and informal conversation from public and private sources including messaging boards, SMS BIBREF15, and broadcast newswire content BIBREF16, BIBREF17. However, it proves difficult to adopt these datasets as widely available benchmarks on dialog modeling tasks, as they come with a substantial cost ($100-$1000 per dataset/year, covering up to a hundred hours of transcribed conversation). In this vein, we contribute an open-access large-scale corpus of cleanly annotated broadcast media dialog."]}
{"question_id": "a97137318025a6642ed0634f7159255270ba3d4f", "predicted_answer": "", "predicted_evidence": ["The US Defense Advanced Research Projects Agency (DARPA) has undertaken efforts to collect broadcast and informal conversation from public and private sources including messaging boards, SMS BIBREF15, and broadcast newswire content BIBREF16, BIBREF17. However, it proves difficult to adopt these datasets as widely available benchmarks on dialog modeling tasks, as they come with a substantial cost ($100-$1000 per dataset/year, covering up to a hundred hours of transcribed conversation). In this vein, we contribute an open-access large-scale corpus of cleanly annotated broadcast media dialog."]}
{"question_id": "a24b2269b292fd0ee81d50303d1315383c594382", "predicted_answer": "", "predicted_evidence": ["The US Defense Advanced Research Projects Agency (DARPA) has undertaken efforts to collect broadcast and informal conversation from public and private sources including messaging boards, SMS BIBREF15, and broadcast newswire content BIBREF16, BIBREF17. However, it proves difficult to adopt these datasets as widely available benchmarks on dialog modeling tasks, as they come with a substantial cost ($100-$1000 per dataset/year, covering up to a hundred hours of transcribed conversation). In this vein, we contribute an open-access large-scale corpus of cleanly annotated broadcast media dialog."]}
{"question_id": "7d8cd7d6c86349ef0bd4fdbd84c8dc49c7678f46", "predicted_answer": "", "predicted_evidence": ["In the experiments, three regular text datasets and three short text datasets were used:"]}
{"question_id": "0fee37ebe0a010cf8bd665fa566306d8e7d12631", "predicted_answer": "", "predicted_evidence": ["In this section, we review three lines of related work: models with document meta information, models with word meta information, and models for short texts."]}
{"question_id": "f8bba20d1781ce2b14fad28d6eff024e5a6c2c02", "predicted_answer": "", "predicted_evidence": ["Perplexity is a measure that is widely used BIBREF23 to evaluate the modelling accuracy of topic models. The lower the score, the higher the modelling accuracy. To compute perplexity, we randomly selected some documents in a dataset as the training set and the remaining as the test set. We first trained a topic model on the training set to get the word distributions of each topic INLINEFORM0 ( INLINEFORM1 ). Each test document INLINEFORM2 was split into two halves containing every first and every second words respectively. We then fixed the topics and trained the models on the first half to get the topic proportions ( INLINEFORM3 ) of test document INLINEFORM4 and compute perplexity for predicting the second half. In regard to MetaLDA, we fixed the matrices INLINEFORM5 and INLINEFORM6 output from the training procedure. On the first half of test document INLINEFORM7 , we computed the Dirichlet prior INLINEFORM8 with INLINEFORM9 and the labels INLINEFORM10 of test document INLINEFORM11 (See Step UID12 ), and then point-estimated INLINEFORM12 . We ran all the models 5 times with different random number seeds and report the average scores and the standard deviations."]}
{"question_id": "252599e53f52b3375b26d4e8e8b66322a42d2563", "predicted_answer": "", "predicted_evidence": ["At the word level, MetaLDA-def-wf performed the best among the models with word features only. Moreover, our model has obvious advantage in running speed (see Table TABREF66 ). Furthermore, comparing MetaLDA-def-wf with MetaLDA-def-def and MetaLDA-0.1-wf with LDA, we can see using the word features indeed improved perplexity."]}
{"question_id": "e12166fa9d6f63c4e92252c95c6a7bc96977ebf4", "predicted_answer": "", "predicted_evidence": ["We relied on the textual representations\u2014a feature space of n-grams (unigrams, bigrams and trigrams)\u2014for training. Due to the noisy nature of Twitter, where users frequently write short, informal spellings and grammars, we pre-processed input data as the following steps: (1) utilized a revised Twokenizer system which was specially trained on Twitter texts BIBREF20 to tokenize raw messages, (2) completed stemming and lemmatization using WordNet Lemmatizer BIBREF21 ."]}
{"question_id": "d4cb704e93086a2246a8caa5c1035e8297b8f4c0", "predicted_answer": "", "predicted_evidence": ["These studies are valuable in their own right, but one evident limitation is that each dataset is limited to depicting a particular company and excludes the populations who have no access to such restricted networks (e.g., people who are not employees of that company). Moreover, the workers may be unwilling to express, e.g., negative feelings about work (\u201cI don't wanna go to work today\u201d), unprofessional behavior (\u201cGot drunk as hell last night and still made it to work\u201d), or a desire to work elsewhere (\u201cI want to go work at Disney World so bad\u201d) on platforms controlled by their employers."]}
{"question_id": "a11b5eb928a6db9a0e3bb290ace468ff1685d253", "predicted_answer": "", "predicted_evidence": ["To assess the labeling quality of multiple annotators in crowdsourced annotation rounds (R1, R2 and R4), we calculated Fleiss' kappa BIBREF22 and Krippendorff's alpha BIBREF23 measures using the online tool BIBREF24 to assess inter-annotator reliability among the five annotators of each HIT. And then we calculated the average and standard deviation of inter-annotator scores for multiple HITs per round. Table TABREF36 records the inter-annotator agreement scores in three rounds of crowdsourced annotations."]}
{"question_id": "275b2c22b6a733d2840324d61b5b101f2bbc5653", "predicted_answer": "", "predicted_evidence": ["Twitter has drawn much attention from researchers in various disciplines in large part because of the volume and granularity of publicly available social data associated with massive information. This micro-blogging website, which was launched in 2006, has attracted more than 500 million registered users by 2012, with 340 million tweets posted every day. Twitter supports directional connections (followers and followees) in its social network, and allows for geographic information about where a tweet was posted if a user enables location services. The large volume and desirable features provided by Twitter makes it a well-suited source of data for our task."]}
{"question_id": "f1f7a040545c9501215d3391e267c7874f9a6004", "predicted_answer": "", "predicted_evidence": ["The mean and the standard deviation (between parentheses) for each extractor and each corpus are presented in Table TABREF48 . They will be used to test statistical hypotheses about the mean difference value of INLINEFORM0 , INLINEFORM1 and INLINEFORM2 between PAMPO and the other three extractors."]}
{"question_id": "b6f4fd6bc76bfcbc15724a546445908afa6d922c", "predicted_answer": "", "predicted_evidence": ["Equation ( EQREF32 ) defines another measure commonly used to assess the quality of the process, INLINEFORM0 . This measure allows interpreting the global quality, taking into account the decrease of INLINEFORM1 and the increase of INLINEFORM2 . The second phase of the PAMPO process increases the value of INLINEFORM3 from 0.72 to 0.87. DISPLAYFORM0 "]}
{"question_id": "3614c1f1435b7c1fd1f7f0041219eebf5bcff473", "predicted_answer": "", "predicted_evidence": ["The language is an important factor to be taken in consideration in the NER task. Most of the services are devoted to English and few support NER on Portuguese texts. The first reference to work developed in Portuguese texts was published in 1997 BIBREF14 ; the authors perform the NER task and compute some measures in a Portuguese corpus and other five corpora. Until now, we have only identified the Rembrandt tool as a service developed and devoted to extract named entities in Portuguese texts. Other tools (AlchemyAPI, NERD and Zemanta) have been adapted to work and accept Portuguese texts but were not specifically developed for that purpose. As recently pointed out by Taba and Caseli BIBREF15 , the Portuguese language still lacks high quality linguistic resources and tools."]}
{"question_id": "c316d7d0c80b8f720ff90a8bb84a8b879a3ef7ea", "predicted_answer": "", "predicted_evidence": ["Table TABREF27 shows the most frequent `candidate entities' from the whole book, as extracted by Algorithm 1 and which of those candidate entities were considered as actual `named entities' by Algorithm 2."]}
{"question_id": "a786cceba4372f6041187c426432853eda03dca6", "predicted_answer": "", "predicted_evidence": ["We used two evaluation measures: accuracy, and macro-averaged F1 (the harmonic average of precision and recall). In the supervised scenario, we performed 5-fold cross-validation. In the distant-supervision scenario, we propagated labels from the media to the users. Therefore, in the latter case the user labels were only used for evaluation."]}
{"question_id": "a837dcbd339e27a974e28944178c790a5b0b37c0", "predicted_answer": "", "predicted_evidence": ["Table TABREF24 shows some basic statistics about the resulting media dataset. Similarly to the IRA dataset, the distribution is right-heavy."]}
{"question_id": "c135e1f8ecaf7965f6a6d3e30b537eb37ad74230", "predicted_answer": "", "predicted_evidence": ["We propagate labels from the given media to the troll user that mentions them according to the following media-to-user mapping:"]}
{"question_id": "16a10c1681dc5a399b6d34b4eed7bb1fef816dd0", "predicted_answer": "", "predicted_evidence": ["Our main dataset contains 2973371 tweets by 2848 Twitter users, which the US House Intelligence Committee has linked to the Russian Internet Research Agency (IRA). The data was collected and published by BIBREF0, and then made available online. The time span covers the period from February 2012 to May 2018."]}
{"question_id": "2ca3ca39d59f448e30be6798514709be7e3c62d8", "predicted_answer": "", "predicted_evidence": ["Basic statistics about the CNN, Daily Mail and CBT datasets are summarized in Table TABREF2 ."]}
{"question_id": "df7fb8e6e44c9c5af3f19dde762c75cbf2f8452f", "predicted_answer": "", "predicted_evidence": ["For single models we are reporting results for the best model as well as the average of accuracies for the best 20% of models with best performance on validation data since single models display considerable variation of results due to random weight initialization even for identical hyperparameter values. Single model performance may consequently prove difficult to reproduce."]}
{"question_id": "20e2b517fddb0350f5099c39b16c2ca66186d09b", "predicted_answer": "", "predicted_evidence": ["Basic statistics about the CNN, Daily Mail and CBT datasets are summarized in Table TABREF2 ."]}
{"question_id": "70512cc9dcd45157e40c8d1f85e82d21ade7645b", "predicted_answer": "", "predicted_evidence": ["For single models we are reporting results for the best model as well as the average of accuracies for the best 20% of models with best performance on validation data since single models display considerable variation of results due to random weight initialization even for identical hyperparameter values. Single model performance may consequently prove difficult to reproduce."]}
{"question_id": "fd556a038c36abc88a800d9d4f2cfa0aef6f5aba", "predicted_answer": "", "predicted_evidence": ["To generate human results for our dataset, we hire annotators proficient in German on Amazon Mechanical Turk."]}
{"question_id": "9119fbfba84d298014d1b74e0e3d30330320002c", "predicted_answer": "", "predicted_evidence": ["In Figure FIGREF20, we explore the effect of argument order. Despite the fact that all argument orderings should be equally valid from a grammatical perspective, we find that humans tend to favour more 'canonical' orders, with nominative-accusative-dative being the preferred order. Models also assign higher log probability scores to the canonical order compared to others. It is likely that some orders occur more frequently than others in German, thus leading to a frequency bias for both models and humans. Although sentences with shuffled argument order have the same meaning as those without shuffled order, we find a similar bias for the meaningfulness scores."]}
{"question_id": "058b6e3fdbb607fa7dbfc688628b3e13e130c35a", "predicted_answer": "", "predicted_evidence": ["In Table TABREF26, we show correlations between human judgments of grammaticality, meaningfulness and LSTM log probabilities. Unsurprisingly, all variables are positively correlated, which supports our earlier findings. More surprising is that the LSTM is more correlated with both grammaticality and meaningfulness than meaningfulness is with grammaticality. Note that meaningfulness and grammaticality have been annotated by different annotators, which might help explain this finding."]}
{"question_id": "5b95665d44666a1dc9e568d2471e5edf8614859f", "predicted_answer": "", "predicted_evidence": ["Surprisingly, humans performed only slightly better than the LSTM. We believe that this is due two factors. First, we presented the sentences in a scrambled order and asked for an absolute grammaticality judgment. It may be more difficult to put a sentence on a 1 to 10 scale than making pairwise judgments. Second, our sentences may be particularly challenging. The grammatical sentences contained both unusual argument orders and semantically odd situations, thus inciting participants to rate them low. While these factors could be expected to impact the LSTM, it is more surprising that they impact humans, despite precise instructions to rate on grammaticality rather than meaning or frequency. In addition, as can be seen in Figure FIGREF11b, some ungrammatical sentences were rated as highly grammatical by humans. We suspect that these are cases of inattention, as in our test set the distinction between grammatical and ungrammatical rest on a single word, and even a single character (the distinction between 'der' and 'den', for instance)."]}
{"question_id": "b9686a168366aafbab1737df426e031ad74a6284", "predicted_answer": "", "predicted_evidence": ["The second important finding is that, given what ground-truth language-use data is available, there are in general very few false positives: cases where the corpora suggest a language is frequently used in a country but census-based data does not. While uncommon, there are more false positives in Twitter data. This is significant because it means that, in general, these corpora do not predict language use that is not actually present."]}
{"question_id": "740cc392c0c8bfadfe6b3a60c0be635c03e17f2a", "predicted_answer": "", "predicted_evidence": ["The distribution of language data in Figures 2 and 3 raises an important distinction between types of users: locals vs. non-locals. For example, from internet usage statistics we know that many countries in Africa have less access to web-sites and thus produce much less web-crawled data. This is reflected in Figure 3. But many African countries are over-represented in Twitter data. Are these Twitter users members of the local populations or do they represent visitors? Note that Figure 2 does not reflect the popularity of Twitter as a platform because we start by normalizing the Twitter output for each country against the total Twitter usage. The over-represented countries in Figure 2, then, represent places where Twitter data is produced at a higher rate than expected. It has nothing to do with the relative popularity of the platform (e.g., Twitter vs. web pages)."]}
{"question_id": "845bdcd900c0f96b2ae091d086fb1ab8bb1063f0", "predicted_answer": "", "predicted_evidence": ["Some countries are not available because their top-level domains are used for other purposes (i.e., .ai, .fm, .io, .ly, .ag, .tv). Domains that do not contain geographic information are also removed from consideration (e.g., .com sites). The Common Crawl dataset covers 2014 through the end of 2017, totalling 81.5 billion web pages. As shown in Table 1, after processing this produces a corpus of 16.65 billion words. Table 1 also shows the number of countries represented in the web corpus against the number of countries in the ground-truth UN dataset and in the collected Twitter corpus. Countries may be missing from the web dataset (i) because their domains are used for a different purpose or (ii) their domains are not widely used or the country does not produce a significant amount of data on the open internet."]}
{"question_id": "8d1b6c88f06ee195d75af32ede85dbd6477c8497", "predicted_answer": "", "predicted_evidence": ["To what degree do these datasets represent majority languages? This is an important question because, with only language labels available, the prevalence of only a few languages will obscure important demographic information. Table 3 shows the top twenty languages (chosen from the web corpus) by their relative proportion of each dataset and, at the bottom, by their combined percent of the overall dataset. The two datasets do not agree in top languages given only the total number of words; however, these twenty languages make up a similar percent of each dataset."]}
{"question_id": "bc05503eef25c732f1785e29d59b6022f12ba094", "predicted_answer": "", "predicted_evidence": ["Both datasets contain abstractive gold summaries, which are not readily suited to training extractive summarization models. A greedy algorithm was used to generate an oracle summary for each document. The algorithm greedily select sentences which can maximize the ROUGE scores as the oracle sentences. We assigned label 1 to sentences selected in the oracle summary and 0 otherwise."]}
{"question_id": "a6603305f4fd3dd0010ac31243c40999a116537e", "predicted_answer": "", "predicted_evidence": ["We use interval segment embeddings to distinguish multiple sentences within a document. For $sent_i$ we will assign a segment embedding $E_A$ or $E_B$ conditioned on $i$ is odd or even. For example, for $[sent_1, sent_2, sent_3, sent_4, sent_5]$ we will assign $[E_A, E_B, E_A,E_B, E_A]$ ."]}
{"question_id": "2ba4477d597b1fd123d14be07a7780ccb5c4819b", "predicted_answer": "", "predicted_evidence": ["Like in the original BERT paper, the Simple Classifier only adds a linear layer on the BERT outputs and use a sigmoid function to get the predicted score: "]}
{"question_id": "027814f3a879a6c7852e033f9d99519b8729e444", "predicted_answer": "", "predicted_evidence": ["$$\\nonumber lr = 2e^{-3}\\cdot min(step^{-0.5}, step \\cdot warmup^{-1.5})$$   (Eq. 17) "]}
{"question_id": "00df1ff914956d4d23299d02fd44e4c985bb61fa", "predicted_answer": "", "predicted_evidence": ["$$\\nonumber lr = 2e^{-3}\\cdot min(step^{-0.5}, step \\cdot warmup^{-1.5})$$   (Eq. 17) "]}
{"question_id": "b57ad10468e1ba2a7a34396688dbb10a575d89f5", "predicted_answer": "", "predicted_evidence": ["$$loss = -\\frac{1}{|D|}\\sum _{(t_a, q) \\in D} \\log (f_{nn}(t_a,q)) \\nonumber $$   (Eq. 20) "]}
{"question_id": "9d6d17120c42a834b2b5d96f2120d646218ed4bb", "predicted_answer": "", "predicted_evidence": ["Unlike existing studies in database community BIBREF1 , BIBREF2 that utilize surrounding text of a table or pagerank score of a web page, we focus on making a thorough exploration of table content in this work. We believe that content-based table retrieval has the following challenges. The first challenge is how to effectively represent a table, which is semi-structured and includes many aspects such as headers, cells and caption. The second challenge is how to build a robust model that measures the relevance between an unstructured natural language query and a semi-structured table. Table retrieval could be viewed as a multi-modal task because the query and the table are of different forms. Moreover, to the best of our knowledge, there is no publicly available dataset for table retrieval. Further progress towards improving this area requires richer training and evaluation resources."]}
{"question_id": "965e0ce975a0b8612a30cfc31bbfd4b8a57aa138", "predicted_answer": "", "predicted_evidence": ["Since headers and cells have similar characteristics, we use a similar way to measure the relevance between a query and table cells. Specifically, we derive three memories $M_{cel}$ , $M_{row}$ and $M_{col}$ from table cells in order to match from cell level, row level and column level. Each memory cell in $M_{cel}$ represents the embedding of a table cell. Each cell in $M_{row}$ represent the vector a row, which is computed with weighted average over the embeddings of cells in the same row. We derive the column memory $M_{col}$ in an analogous way. We use the same module $NN_1()$ to calculate the relevance scores for these three memories. "]}
{"question_id": "8dfdd1ed805bb23c774fbb032ef1d97c6802e07c", "predicted_answer": "", "predicted_evidence": ["To the best of our knowledge, there is no publicly available dataset for table retrieval. We introduce WebQueryTable, an open-domain dataset consisting of query-table pairs. We use search logs from a commercial search engine to get a list of queries that could be potentially answered by web tables. Each query in query logs is paired with a list of web pages, ordered by the number of user clicks for the query. We select the tables occurred in the top ranked web page, and ask annotators to label whether a table is relevant to a query or not. In this way, we get 21,113 query-table pairs. In the real scenario of table retrieval, a system is required to find a table from a huge collection of tables. Therefore, in order to enlarge the search space of our dataset, we extract 252,703 web tables from Wikipedia and regard them as searchable tables as well. Data statistics are given in Table 1 ."]}
{"question_id": "c21675d8a90bda624d27e5535d1c10f08fcbc16b", "predicted_answer": "", "predicted_evidence": ["It is helpful to note that tables from the web are not always \u201cregular\u201d. We regard a table as a \u201cregular\u201d table if it contains header, cell and caption, and the number of cells in each row is equal to the number of header cells. In this work, we make a comprehensive study of table retrieval on regular tables, and would like to release benchmark datasets of good quality. It is trivial to implement heuristic rules so as to convert the irregular tables to regular one, so we leave it to the future work."]}
{"question_id": "da077b385d619305033785af5b204696d6145bd8", "predicted_answer": "", "predicted_evidence": ["In the following subsections, we will introduce our proposed Query-bag Matching (QBM) model which output is the matching probability indicating whether the query and bag are asking the same questions. The basic Q-Q (query-question) matching model hybrid CNN (hCNN) BIBREF5 is presented as the background. Then we will show the base model and its two components designed to promote the performance: Mutual Coverage and Bag Representation. For better understanding, the whole model is shown in Figure FIGREF2."]}
{"question_id": "6d8a51e2790043497ed2637a1abc36bdffb39b71", "predicted_answer": "", "predicted_evidence": ["We conduct experiments on two datasets: AliMe and Quora. The AliMe dataset is collected from the AliMe intelligent assistant system and the Quora dataset is composed of a public dataset."]}
{"question_id": "de4cc9e7fa5d700f5046d60789770f47911b3dd7", "predicted_answer": "", "predicted_evidence": ["Effectiveness of the Mutual Coverage To intuitively learn the coverage weight, we sample some words with their weights in Table TABREF17. It shows that the words like \u201cThe\u201d have low weight, which confirms that they contribute little to the matching. \u201cRefund\u201d in E-commerce is a very important element in a user query sentence. And \u201cAmerica\u201d is important in Quora, because question like \u201cwhat is the capital in <location>?\u201d is highly related to location \u201c<location>\u201d."]}
{"question_id": "8ad5ebca2f69023b60ccfa3aac0ed426234437ac", "predicted_answer": "", "predicted_evidence": ["To prove the effectiveness of our models, we propose two baselines from different aspects: the Q-Q matching based baseline and the query-bag matching based baseline."]}
{"question_id": "4afd4cfcb30433714b135b977baff346323af1e3", "predicted_answer": "", "predicted_evidence": ["We conduct experiments on two datasets: AliMe and Quora. The AliMe dataset is collected from the AliMe intelligent assistant system and the Quora dataset is composed of a public dataset."]}
{"question_id": "b2dc0c813da92cf13d86528bd32c12286ec9b9cd", "predicted_answer": "", "predicted_evidence": ["Table TABREF12 shows the performance of our cross-lingual models in German, Italian and Dutch. We summarize the results as follows:"]}
{"question_id": "c4c06f36454fbfdc5d218fb84ce74eaf7f78fc98", "predicted_answer": "", "predicted_evidence": ["Table TABREF12 shows the performance of our cross-lingual models in German, Italian and Dutch. We summarize the results as follows:"]}
{"question_id": "347dc2fd6427b39cf2358d43864750044437dff8", "predicted_answer": "", "predicted_evidence": ["In order to make the model directly transferable to the German, Italian and Dutch test data, we use the following language-independent features."]}
{"question_id": "6911e8724dfdb178fa81bf58019947b71ef8fbe7", "predicted_answer": "", "predicted_evidence": ["Can we train a semantic parser in a language where annotation is available?. In this paper we show that this is indeed possible and we propose a zero-shot cross-lingual semantic parsing method based on language-independent features, where a parser trained in English \u2013 where labelled data is available, is used to parse sentences in three languages, Italian, German and Dutch."]}
{"question_id": "b012df09fa2a3d6b581032d68991768cf4bc9d7b", "predicted_answer": "", "predicted_evidence": ["To show how this approach performs, we focus on the Parallel Meaning Bank BIBREF13 \u2013 a multilingual semantic bank, where sentences in English, German, Italian and Dutch have been annotated with their meaning representations. The annotations in the PMB are based on Discourse Representation Theory BIBREF3, a popular theory of meaning representation designed to account for intra and inter-sentential phenomena, like temporal expressions and anaphora. Figure 1 shows an example DRT for the sentence `I sat down and opened my laptop' in its canonical `box' representation. A DRS is a nested structure with the top part containing the discourse references and the bottom with unary and binary predicates, as well as semantic constants (e.g. `speaker'). DRS can be linked to each other via logic operator (e.g. $\\lnot $, $\\rightarrow $, $\\diamond $) or, as in this case, discourse relations (e.g. CONTINUATION, RESULT, ELABORATION, etc.)."]}
{"question_id": "62edffd051d056cf60e17deafcc55a8c9af398cb", "predicted_answer": "", "predicted_evidence": ["Multilingual word embeddings. We use the MUSE BIBREF17 pre-trained multilingual word embeddings and keep them fixed during training."]}
{"question_id": "d5c393df758dec6ea6827ae5b887eb6c303a4f4d", "predicted_answer": "", "predicted_evidence": ["Lexicons: number of words present in each lexicon"]}
{"question_id": "11a3af3f056e0fb5559fe5cbff1640e022732735", "predicted_answer": "", "predicted_evidence": ["Porting lexicons to other languages has also been studied: use aligned thesauri and propagate at the sense level BIBREF13 , BIBREF14 , translate the lexicon directly BIBREF15 , BIBREF16 , take advantage of off-the-shelf translation and include sample word context to get better translations BIBREF17 or use crowd sourcing to quickly bootstrap lexicons in non-english languages BIBREF18 ."]}
{"question_id": "07a214748a69b31400585aef7aba6af3e3d9cce2", "predicted_answer": "", "predicted_evidence": ["In order to extract features on those corpora, polarity lexicons are translated from English using the method described in Section SECREF3 . The following lexicons are translated:"]}
{"question_id": "44bf3047ff7e5c6b727b2aaa0805dd66c907dcd6", "predicted_answer": "", "predicted_evidence": ["In our paper we have studied the challenges of abstractive dialogue summarization. We have addressed a major factor that prevents researchers from engaging into this problem: the lack of a proper dataset. To the best of our knowledge, this is the first attempt to create a comprehensive resource of this type which can be used in future research. The next step could be creating an even more challenging dataset with longer dialogues that not only cover one topic, but span over numerous different ones."]}
{"question_id": "c6f2598b85dc74123fe879bf23aafc7213853f5b", "predicted_answer": "", "predicted_evidence": ["We noticed a few annotations (7 for news and 4 for dialogues) with opposite marks (i.e. one annotator judgement was $-1$, whereas the second one was 1) and decided to have them annotated once again by another annotator who had to resolve conflicts. For the rest, we calculated the linear weighted Cohen's kappa coefficient BIBREF22 between annotators' scores. For news examples, we obtained agreement on the level of $0.371$ and for dialogues \u2013 $0.506$. The annotators' agreement is higher on dialogues than on news, probably because of structures of those data \u2013 articles are often long and it is difficult to decide what the key-point of the text is; dialogues, on the contrary, are rather short and focused mainly on one topic."]}
{"question_id": "bdae851d4cf1d05506cf3e8359786031ac4f756f", "predicted_answer": "", "predicted_evidence": ["We evaluate models with the standard ROUGE metric BIBREF21, reporting the $F_1$ scores (with stemming) for ROUGE-1, ROUGE-2 and ROUGE-L following previous works BIBREF5, BIBREF4. We obtain scores using the py-rouge package."]}
{"question_id": "894bbb1e42540894deb31c04cba0e6cfb10ea912", "predicted_answer": "", "predicted_evidence": ["As shown, summarization of dialogues is much more challenging than of news. In order to perform well, it may require designing dedicated tools, but also new, non-standard measures to capture the quality of abstractive dialogue summaries in a relevant way. We hope to tackle these issues in future work."]}
{"question_id": "75b3e2d2caec56e5c8fbf6532070b98d70774b95", "predicted_answer": "", "predicted_evidence": ["Each dialogue was created by one person. After collecting all of the conversations, we asked language experts to annotate them with summaries, assuming that they should (1) be rather short, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person. Each dialogue contains only one reference summary. Validation. Since the SAMSum corpus contains dialogues created by linguists, the question arises whether such conversations are really similar to those typically written via messenger apps. To find the answer, we performed a validation task. We asked two linguists to doubly annotate 50 conversations in order to verify whether the dialogues could appear in a messenger app and could be summarized (i.e. a dialogue is not too general or unintelligible) or not (e.g. a dialogue between two people in a shop). The results revealed that 94% of examined dialogues were classified by both annotators as good i.e. they do look like conversations from a messenger app and could be condensed in a reasonable way. In a similar validation task, conducted for the existing dialogue-type datasets (described in the Initial approach section), the annotators agreed that only 28% of the dialogues resembled conversations from a messenger app."]}
{"question_id": "573b8b1ad919d3fd0ef7df84e55e5bfd165b3e84", "predicted_answer": "", "predicted_evidence": ["Adversarial examples are generally minimal perturbations applied to the input data in an effort to expose the regions of the input space where a trained model performs poorly. Prior works BIBREF0, BIBREF1 have demonstrated the ability of an adversary to evade state-of-the-art classifiers by carefully crafting attack examples which can be even imperceptible to humans. Following such approaches, there has been a number of techniques aimed at generating adversarial examples BIBREF2, BIBREF3. Depending on the degree of access to the target model, an adversary may operate in one of the two different settings: (a) black-box setting, where an adversary doesn't have access to target model's internal architecture or its parameters, (b) white-box setting, where an adversary has access to the target model, its parameters, and input feature representations. In both these settings, the adversary cannot alter the training data or the target model itself. Depending on the purpose of the adversary, adversarial attacks can be categorized as (a) targeted attack and (b) non-targeted attack. In a targeted attack, the output category of a generated example is intentionally controlled to a specific target category with limited change in semantic information. While a non-targeted attack doesn't care about the category of misclassified results."]}
{"question_id": "07d98dfa88944abd12acd45e98fb7d3719986aeb", "predicted_answer": "", "predicted_evidence": ["Let us consider a target model $T$ and $(x,l)$ refers to the samples from the dataset. Given an instance $x$, the goal of the adversary is to generate adversarial examples $x^{\\prime }$ such that $T(x^{\\prime }) \\ne l$, where $l$ denotes the true label i.e take one of the $K$ classes of the target classification model. The changes made to $x$ to get $x^{\\prime }$ are called perturbations. We would like to have $x^{\\prime }$ close to the original instance $x$. In a black box setting, we do not have knowledge about the internals of the target model or its training data. Previous work by Papernot et al. BIBREF14 train a separate substitute classifier such that it can mimic the decision boundaries of the target classifier. The substitute classifier is then used to craft adversarial examples. While these techniques have been applied for image classification models, such methods have not been explored extensively for text."]}
{"question_id": "3a40559e5a3c2a87c7b9031c89e762b828249c05", "predicted_answer": "", "predicted_evidence": ["In this section, we describe the evaluation setup used to measure the effectiveness of our model in generating adversarial examples. The success of our model lies in its ability to fool the target classifier. We pretrain our models with dataset that generates a number of character and word perturbations. We elaborate on the experimental setup and the results below."]}
{"question_id": "5db47bbb97282983e10414240db78154ea7ac75f", "predicted_answer": "", "predicted_evidence": ["We adopt a self-critical sequence training technique to train our model to generate examples that can fool or increase the probability of misclassification in text classifiers."]}
{"question_id": "c589d83565f528b87e355b9280c1e7143a42401d", "predicted_answer": "", "predicted_evidence": ["We evaluate our models on two different datasets associated with two different tasks: IMDB sentiment classification and AG's news categorization task. We run ablation studies on various components of the model and provide insights into decisions of our model."]}
{"question_id": "7f90e9390ad58b22b362a57330fff1c7c2da7985", "predicted_answer": "", "predicted_evidence": ["No-RL: We use our pretrained model without the reinforcement learning objective."]}
{"question_id": "3e3e45094f952704f1f679701470c3dbd845999e", "predicted_answer": "", "predicted_evidence": ["We propose a black-box non-targeted attack strategy by combining ideas of substitute network and adversarial example generation. We formulate it as a reinforcement learning task."]}
{"question_id": "475ef4ad32a8589dae9d97048166d732ae5d7beb", "predicted_answer": "", "predicted_evidence": ["However, cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-Bert's multilingual representation is not able to generalize equally well in all cases. A possible explanation for this, as we will see in section SECREF18 , is typological similarity. English and Japanese have a different order of subject, verb and object, while English and Bulgarian have the same, and M-Bert may be having trouble generalizing across different orderings."]}
{"question_id": "3fd8eab282569b1c18b82f20d579b335ae70e79f", "predicted_answer": "", "predicted_evidence": ["Because M-Bert uses a single, multilingual vocabulary, one form of cross-lingual transfer occurs when word pieces present during fine-tuning also appear in the evaluation languages. In this section, we present experiments probing M-Bert's dependence on this superficial form of generalization: How much does transferability depend on lexical overlap? And is transfer possible to languages written in different scripts (no overlap)?"]}
{"question_id": "8e9561541f2e928eb239860c2455a254b5aceaeb", "predicted_answer": "", "predicted_evidence": ["However, cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-Bert's multilingual representation is not able to generalize equally well in all cases. A possible explanation for this, as we will see in section SECREF18 , is typological similarity. English and Japanese have a different order of subject, verb and object, while English and Bulgarian have the same, and M-Bert may be having trouble generalizing across different orderings."]}
{"question_id": "50c1bf8b928069f3ffc7f0cb00aa056a163ef336", "predicted_answer": "", "predicted_evidence": ["We would like to thank Mark Omernick, Livio Baldini Soares, Emily Pitler, Jason Riesa, and Slav Petrov for the valuable discussions and feedback."]}
{"question_id": "2ddfb40a9e73f382a2eb641c8e22bbb80cef017b", "predicted_answer": "", "predicted_evidence": ["We would like to thank Mark Omernick, Livio Baldini Soares, Emily Pitler, Jason Riesa, and Slav Petrov for the valuable discussions and feedback."]}
{"question_id": "65b39676db60f914f29f74b7c1264422ee42ad5c", "predicted_answer": "", "predicted_evidence": ["We explain our data preparation methods in the next section. It is followed by our research methodology in Section III. We present our results in Section IV, which is followed by discussion and conclusion in Section V and VI."]}
{"question_id": "a2baa8e266318f23f43321c4b2b9cf467718c94a", "predicted_answer": "", "predicted_evidence": ["We explain our data preparation methods in the next section. It is followed by our research methodology in Section III. We present our results in Section IV, which is followed by discussion and conclusion in Section V and VI."]}
{"question_id": "97ff88c31dac9a3e8041a77fa7e34ce54eef5a76", "predicted_answer": "", "predicted_evidence": ["The paper is organized as follows: Section SECREF2 describes our model, while Section SECREF3 summarizes the evaluation of the model on the two above-mentioned tasks and the comparison with respect to state-of-the-art approaches. Section SECREF4 gives an overview of the literature of both QA and recommender systems, while final remarks and our long-term vision are reported in Section SECREF5 ."]}
{"question_id": "272defe245d1c5c091d3bc51399181da2da5e5f0", "predicted_answer": "", "predicted_evidence": ["Differently from BIBREF2 , the relevant knowledge base facts, taken from the knowledge base in triple form distributed with the dataset, are retrieved by INLINEFORM0 implemented by exploiting the Elasticsearch engine and not according to an hash lookup operator which applies a strict filtering procedure based on word frequency. In our work, INLINEFORM1 returns at most the top 30 relevant facts for INLINEFORM2 . Each entity in questions and documents is recognized using the list of entities provided with the dataset and considered as a single word of the dictionary INLINEFORM3 ."]}
{"question_id": "860257956b83099cccf1359e5d960289d7d50265", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is the hidden layer size, INLINEFORM1 and INLINEFORM2 are weight matrices, INLINEFORM3 , INLINEFORM4 are bias vectors, INLINEFORM5 is the sigmoid function and INLINEFORM6 is the ReLU activation function, which are applied pointwise to the given input vector."]}
{"question_id": "1b1849ad0bdd79c6645572849fe7873ec7bd7e6d", "predicted_answer": "", "predicted_evidence": ["However, while it has advanced the field in so many ways, it has also introduced almost imperceptible biases: why is newswire considered more standard or more canonical than other text types? Journalists are trained writers who make fewer errors and adhere to a codified norm. But let us pause for a minute. If NLP had emerged only in the last decade, would newswire data still be our canon? Or would, say, Wikipedia be considered canonical? User-generated data is less standardized, but is highly available. If we take this thought further and start over today, maybe we would be in an `inverted' world: social media is standard and newswire with its `headlinese' is the `bad language' BIBREF0 . It is easy to collect large quantities of social media data. Whatever we consider canonical, all data comes with its biases, even more democratic media like Wikipedia carry their own peculiarities."]}
{"question_id": "deb0c3524a3b3707e8b20abd27f54ad6188d6e4e", "predicted_answer": "", "predicted_evidence": ["There has been several works on generating embeddings for words, most famously Word2Vec by Mikolov et al. BIBREF2 ). There has also been a number of different works that use encoder-decoder models based on long short-term memory (LSTM) BIBREF3 , and gated recurrent neural networks (GRU) BIBREF4 . These methods have been used mostly in the context of machine translation. The encoder maps the sentence from the source language to a vector representation, while the decoder conditions on this encoded vector for translating it to the target language. Perhaps the work most related to ours is the work of Le and Mikolov le2014distributed, where they extended the Word2Vec model to generate representations for sentences (called ParagraphVec). However, these models all function at the word level, making them ill-suited to the extremely noisy and idiosyncratic nature of tweets. Our character-level model, on the other hand, can better deal with the noise and idiosyncrasies in tweets. We plan to make our model and the data used to train it publicly available to be used by other researchers that work with tweets."]}
{"question_id": "d7e43a3db8616a106304ac04ba729c1fee78761d", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 refers to the character at time-step INLINEFORM1 , INLINEFORM2 represents the one-hot vector of the character at time-step INLINEFORM3 . The result from the softmax is a decoded tweet matrix INLINEFORM4 , which is eventually compared with the actual tweet or a synonym-replaced version of the tweet (explained in Section SECREF3 ) for learning the parameters of the model."]}
{"question_id": "0ba8f04c3fd64ee543b9b4c022310310bc5d3c23", "predicted_answer": "", "predicted_evidence": ["The second evaluation is based on the SemEval 2015-Task 10B: Twitter Message Polarity Classification BIBREF12 . Given a tweet, the task is to classify it as either positive, negative or neutral in sentiment. The size of the training and test sets were 9,520 tweets and 2,380 tweets respectively ( INLINEFORM0 positive, INLINEFORM1 negative, and INLINEFORM2 neutral)."]}
{"question_id": "b7d02f12baab5db46ea9403d8932e1cd1b022f79", "predicted_answer": "", "predicted_evidence": ["The first evaluation is based on the SemEval 2015-Task 1: Paraphrase and Semantic Similarity in Twitter BIBREF10 . Given a pair of tweets, the goal is to predict their semantic equivalence (i.e., if they express the same or very similar meaning), through a binary yes/no judgement. The dataset provided for this task contains 18K tweet pairs for training and 1K pairs for testing, with INLINEFORM0 of these pairs being paraphrases, and INLINEFORM1 non-paraphrases. We first extract the vector representation of all the tweets in the dataset using our Tweet2Vec model. We use two features to represent a tweet pair. Given two tweet vectors INLINEFORM2 and INLINEFORM3 , we compute their element-wise product INLINEFORM4 and their absolute difference INLINEFORM5 and concatenate them together (Similar to BIBREF11 ). We then train a logistic regression model on these features using the dataset. Cross-validation is used for tuning the threshold for classification. In contrast to our model, most of the methods used for this task were largely based on extensive use of feature engineering, or a combination of feature engineering with semantic spaces. Table 2 shows the performance of our model compared to the top four models in the SemEval 2015 competition, and also a model that was trained using ParagraphVec. Our model (Tweet2Vec) outperforms all these models, without resorting to extensive task-specific feature engineering."]}
{"question_id": "ff2b58c90784eda6dddd8a92028e6432442c1093", "predicted_answer": "", "predicted_evidence": ["The average accuracy across the 84 domains for each method is found in Table 1. On average our method significantly out-performed all the baselines, with the average improvement in accuracy across OMICS tasks between SEM-HMM and each baseline being statistically significant at a .01 level across all pairs and on sizes of INLINEFORM0 and INLINEFORM1 using one-sided paired t-tests. For INLINEFORM2 improvement was not statistically greater than zero. We see that the results improve with batch size INLINEFORM3 until INLINEFORM4 for SEM-HMM and BMM+EM, but they decrease with batch size for BMM without EM. Both of the methods which use EM depend on statistics to be robust and hence need a larger INLINEFORM5 value to be accurate. However for BMM, a smaller INLINEFORM6 size means it reconciles a couple of documents with the current model in each iteration which ultimately helps guide the structure search. The accuracy for \u201cSEM-HMM Approx.\u201d is close to the exact version at each batch level, while only taking half the time on average."]}
{"question_id": "5e4eac0b0a73d465d74568c21819acaec557b700", "predicted_answer": "", "predicted_evidence": ["The average accuracy across the 84 domains for each method is found in Table 1. On average our method significantly out-performed all the baselines, with the average improvement in accuracy across OMICS tasks between SEM-HMM and each baseline being statistically significant at a .01 level across all pairs and on sizes of INLINEFORM0 and INLINEFORM1 using one-sided paired t-tests. For INLINEFORM2 improvement was not statistically greater than zero. We see that the results improve with batch size INLINEFORM3 until INLINEFORM4 for SEM-HMM and BMM+EM, but they decrease with batch size for BMM without EM. Both of the methods which use EM depend on statistics to be robust and hence need a larger INLINEFORM5 value to be accurate. However for BMM, a smaller INLINEFORM6 size means it reconciles a couple of documents with the current model in each iteration which ultimately helps guide the structure search. The accuracy for \u201cSEM-HMM Approx.\u201d is close to the exact version at each batch level, while only taking half the time on average."]}
{"question_id": "bc6ad5964f444cf414b661a4b942dafb7640c564", "predicted_answer": "", "predicted_evidence": ["The average accuracy across the 84 domains for each method is found in Table 1. On average our method significantly out-performed all the baselines, with the average improvement in accuracy across OMICS tasks between SEM-HMM and each baseline being statistically significant at a .01 level across all pairs and on sizes of INLINEFORM0 and INLINEFORM1 using one-sided paired t-tests. For INLINEFORM2 improvement was not statistically greater than zero. We see that the results improve with batch size INLINEFORM3 until INLINEFORM4 for SEM-HMM and BMM+EM, but they decrease with batch size for BMM without EM. Both of the methods which use EM depend on statistics to be robust and hence need a larger INLINEFORM5 value to be accurate. However for BMM, a smaller INLINEFORM6 size means it reconciles a couple of documents with the current model in each iteration which ultimately helps guide the structure search. The accuracy for \u201cSEM-HMM Approx.\u201d is close to the exact version at each batch level, while only taking half the time on average."]}
{"question_id": "380e71848d4b0d1e983d504b1249119612f00bcb", "predicted_answer": "", "predicted_evidence": ["In this paper, we experiment with multiple classifiers such as Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks(DNNs). The feature spaces for these classifiers are in turn defined by task-specific embeddings learned using three deep learning architectures: FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs). As baselines, we compare with feature spaces comprising of char n-grams BIBREF0 , TF-IDF vectors, and Bag of Words vectors (BoWV)."]}
{"question_id": "21c89ee0281f093b209533453196306b9699b552", "predicted_answer": "", "predicted_evidence": ["Baseline Methods: As baselines, we experiment with three broad representations. (1) Char n-grams: It is the state-of-the-art method BIBREF0 which uses character n-grams for hate speech detection. (2) TF-IDF: TF-IDF are typical features used for text classification. (3) BoWV: Bag of Words Vector approach uses the average of the word (GloVe) embeddings to represent a sentence. We experiment with multiple classifiers for both the TF-IDF and the BoWV approaches."]}
{"question_id": "5096aaea2d0f4bea4c12e14f4f7735e1aea1bfa6", "predicted_answer": "", "predicted_evidence": ["We use `adam' for CNN and LSTM, and `RMS-Prop' for FastText as our optimizer. We perform training in batches of size 128 for CNN & LSTM and 64 for FastText. More details on the experimental setup can be found from our publicly available source code."]}
{"question_id": "452e2d7d7d9e1bb4914903479cd7caff9f6fae42", "predicted_answer": "", "predicted_evidence": ["To verify the task-specific nature of the embeddings, we show top few similar words for a few chosen words in Table TABREF7 using the original GloVe embeddings and also embeddings learned using DNNs. The similar words obtained using deep neural network learned embeddings clearly show the \u201chatred\u201d towards the target words, which is in general not visible at all in similar words obtained using GloVe."]}
{"question_id": "cdb211be0340bb18ba5a9ee988e9df0e2ba8b793", "predicted_answer": "", "predicted_evidence": ["Twitter is an online social networking platform. Users post 140-character messages, which appear in their followers' timelines. Because follower ties can be asymmetric, Twitter serves multiple purposes: celebrities share messages with millions of followers, while lower-degree users treat Twitter as a more intimate social network for mutual communication BIBREF13 . In this paper, we use a large-scale Twitter data set, acquired via an agreement between Microsoft and Twitter. This data set contains all public messages posted between June 2013 and June 2014 by several million users, augmented with social network and geolocation metadata. We excluded retweets, which are explicitly marked with metadata, and focused on messages that were posted in English from within the United States."]}
{"question_id": "4cb2e80da73ae36de372190b4c1c490b72977ef8", "predicted_answer": "", "predicted_evidence": ["We quantify tie strength in terms of embeddedness. Specifically, we use the normalized mutual friends metric introduced by Adamic and Adar BIBREF39 : "]}
{"question_id": "a064337bafca8cf01e222950ea97ebc184c47bc0", "predicted_answer": "", "predicted_evidence": ["In contrast, adding in F4 (local) only improved goodness of fit for three words: asl, jawn, and lls. We therefore conclude that support for hypothesis H2\u2014that the linguistic influence exerted across geographically local ties is greater than the linguistic influence across than across other ties\u2014is limited at best."]}
{"question_id": "993d5bef2bf1c0cd537342ef76d4b952f0588b83", "predicted_answer": "", "predicted_evidence": ["The influence toward geographically distinctive linguistic markers is greater when exerted across geographically local ties than across other ties."]}
{"question_id": "a8e5e10d13b3f21dd11e8eb58e30cc25efc56e93", "predicted_answer": "", "predicted_evidence": ["The main problem we address in this section is generating terminological ontologies in an unsupervised fashion. The fundamental concept of hrLDA is as follows. When people construct a document, they start with selecting several topics. Then, they choose some noun phrases as subjects for each topic. Next, for each subject they come up with relation triplets to describe this subject or its relationships with other subjects. Finally, they connect the subject phrases and relation triplets to sentences via reasonable grammar. The main topic is normally described with the most important relation triplets. Sentences in one paragraph, especially adjacent sentences, are likely to express the same topic."]}
{"question_id": "949a2bc34176e47a4d895bcc3223f2a960f15a81", "predicted_answer": "", "predicted_evidence": ["To summarize this process more succinctly: we build the topic hierarchies with rLDA in a divisive way (see Figure FIGREF35 ). We start with the collection of extracted noun phrases and split them using rLDA and ACRP. Then, we apply the procedure recursively until each noun phrase is selected as a topic label. After every rLDA assignment, each inner node only contains the topic label (top phrase), and the rest of the phrases are divided into nodes at the next level using ACRP and rLDA. Hence, we build a topic tree with each node as a topic label (noun phrase), and each topic is composed of its topic labels and the topic labels of the topic's descendants. In the end, we finalize our terminological ontology by linking the extracted relation triplets with the topic labels as subjects."]}
{"question_id": "70abb108c3170e81f8725ddc1a3f2357be5a4959", "predicted_answer": "", "predicted_evidence": ["We use KB-LDA, phrase_hLDA, and LDA+GSHL as our baseline methods, and compare ontologies extracted from hrLDA, KB-LDA, phrase_hLDA, and LDA+GSHL with DBpedia ontologies. We use precision, recall and F-measure for this ontology evaluation. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule. Table TABREF61 shows the evaluation results of ontologies extracted from Wikipedia articles pertaining to European Capital Cities (Corpus E), Office Buildings in Chicago (Corpus O) and Birds of the United States (Corpus B) using hrLDA, KB-LDA, phrase_hLDA (tree depth INLINEFORM0 = 3), and LDA+GSHL in contrast to these gold ontologies belonging to DBpedia. The three corpora used in this evaluation were collected from Wikipedia abstracts, the same text source of DBpedia. The seeds of hrLDA and the root concepts of LDA+GSHL are capital, building, and bird. For both KB-LDA and phrase_hLDA we kept the top five tokens in each topic as each node of their topic trees is a distribution/list of phrases. hrLDA achieves the highest precision and F-measure scores in the three experiments compared to the other models. KB-LDA performs better than phrase_hLDA and LDA+GSHL, and phrase_hLDA performs similarly to LDA+GSHL. In general, hrLDA works well especially when the pre-knowledge already exists inside the corpora. Consider the following two statements taken from the corpus on Birds of the United States as an example. In order to use two short documents \u201cThe Acadian flycatcher is a small insect-eating bird.\" and \u201cThe Pacific loon is a medium-sized member of the loon.\" to infer that the Acadian flycatcher and the Pacific loon are both related to topic bird, the pre-knowledge that \u201cthe loon is a species of bird\" is required for hrLDA. This example explains why the accuracy of extracting ontologies from this kind of corpus is low."]}
{"question_id": "ce504a7ee2c1f068ef4dde8d435245b4e77bb0b5", "predicted_answer": "", "predicted_evidence": ["We start with the root node ( INLINEFORM0 ) and apply rLDA to all the documents in a corpus."]}
{"question_id": "468eb961215a554ace8088fa9097a7ad239f2d71", "predicted_answer": "", "predicted_evidence": ["A possible future work is to use a weighted combination of multiple metrics for source domain selection. These similarity metrics may be used to extract suitable data or features for efficient CDSA. Similarity metrics may also be used as features to predict the CDSA performance in terms of accuracy degradation."]}
{"question_id": "57d07d2b509c5860880583efe2ed4c5620a96747", "predicted_answer": "", "predicted_evidence": ["Asymmetric Metrics - The metrics which are 2-way in nature i.e., $(D_1,D_2)$ and $(D_2,D_1)$ have different similarity values viz. Entropy Change, Doc2Vec embeddings, and FastText sentence embeddings. These metrics offer additional advantage as they can help decide which domain to train from and which domain to test on amongst $D_1$ and $D_2$."]}
{"question_id": "d126d5d6b7cfaacd58494f1879547be9e91d1364", "predicted_answer": "", "predicted_evidence": ["Asymmetric Metrics - The metrics which are 2-way in nature i.e., $(D_1,D_2)$ and $(D_2,D_1)$ have different similarity values viz. Entropy Change, Doc2Vec embeddings, and FastText sentence embeddings. These metrics offer additional advantage as they can help decide which domain to train from and which domain to test on amongst $D_1$ and $D_2$."]}
{"question_id": "7dca806426058d59f4a9a4873e9219d65aea0987", "predicted_answer": "", "predicted_evidence": ["Based on CDSA results, we create a recommendation chart that prescribes domains that are the best as the source or target domain, for each of the domains."]}
{"question_id": "800fcd8b08d36c5276f9e5e1013208d41b46de59", "predicted_answer": "", "predicted_evidence": ["Sentiment Lexicon Features"]}
{"question_id": "cdbbba22e62bc9402aea74ac5960503f59e984ff", "predicted_answer": "", "predicted_evidence": ["For each task, our team submitted three sets of predictions. The submissions differed in the sets of features and parameters used to train the classification models (Table TABREF32 )."]}
{"question_id": "301a453abaa3bc15976817fefce7a41f3b779907", "predicted_answer": "", "predicted_evidence": ["The official evaluation metric was the F-score for class 1 (ADR): INLINEFORM0 "]}
{"question_id": "f3673f6375f065014e8e4bb8c7adf54c1c7d7862", "predicted_answer": "", "predicted_evidence": ["Below we describe in detail the two tasks we participated in, Task 1 and Task 2."]}
{"question_id": "0bd3bea892c34a3820e98c4a42cdeda03753146b", "predicted_answer": "", "predicted_evidence": ["Domain-Specific Features"]}
{"question_id": "8cf5abf0126f19253930478b02f0839af28e4093", "predicted_answer": "", "predicted_evidence": ["Sentiment Lexicon Features"]}
{"question_id": "d211a37830c59aeab4970fdb2e03d9b7368b421c", "predicted_answer": "", "predicted_evidence": ["The following surface-form features were used:"]}
{"question_id": "c3ce95658eea1e62193570955f105839de3d7e2d", "predicted_answer": "", "predicted_evidence": ["uses the same structure as the BERT with randomly initialized parameters."]}
{"question_id": "389cc454ac97609e9d0f2b2fe70bf43218dd8ba7", "predicted_answer": "", "predicted_evidence": ["We automatically construct a query-focused summarization dataset (named as WikiRef) using Wikipedia and corresponding reference web pages. In the following sections, we will first elaborate the creation process. Then we will analyze the queries, documents and summaries quantitatively and qualitatively."]}
{"question_id": "2c4db4398ecff7e4c1c335a2cb3864bfdc31df1a", "predicted_answer": "", "predicted_evidence": ["Where, $p(\\tilde{y}_{t})=p(\\tilde{y}_{t} | \\tilde{y}_{1}, \\tilde{y}_{2}, ..., \\tilde{y}_{t-1}, x)$ is the sampling probability and $V$ is the size of the vocabulary. It is similar to the exploration-exploitation trade-off. $\\alpha $ is the regularization coefficient that explicitly controls this trade-off: a higher $\\alpha $ corresponds to more exploration, and a lower $\\alpha $ corresponds to more exploitation. We have found that all TensorFlow based open-source implementations of self-critic models use a function (tf.py_func) that runs only on CPU and it is very slow. To the best of our knowledge, ours is the first GPU based implementation."]}
{"question_id": "4738158f92b5b520ceba6207e8029ae082786dbe", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF38 below shows the self-critical model. All the examples shown in Tables TABREF39-TABREF44 are chosen as per the shortest article lengths available due to space constraints."]}
{"question_id": "4dadde7c61230553ef14065edd8c1c7e41b9c329", "predicted_answer": "", "predicted_evidence": ["The factoring of lemma and Part-of-Speech (PoS) tag of surface words, are observed BIBREF22 to increase the performance of NMT models in terms of BLEU score drastically. This is due to the improvement of the vocabulary coverage and better generalization. We have added a pre-processing step by incorporating the lemma and PoS tag to every word of the dataset and training the supervised model on the factored data. The process of extracting the lemma and the PoS tags has been described in BIBREF22. Please refer to the appendix for an example of factoring."]}
{"question_id": "014830892d93e3c01cb659ad31c90de4518d48f3", "predicted_answer": "", "predicted_evidence": ["where $\\gamma _1$, $\\gamma _2$, and $\\gamma _3$ correspond to the weights of $\\mathcal {L}_{rl}$, $\\mathcal {L}_{ml}$, and $\\mathcal {L}_{sp}$, respectively. In our experiments, we use the same vocabulary for both the encoder and decoder. Our vocabulary consists of the top 50,000 frequent words from the training data. We use the development dataset for hyper-parameter tuning. Pre-trained GloVe embeddings BIBREF34 of dimension 300 are used in the document encoding step. The hidden dimension of all the LSTM cells is set to 512. Answer tagging features and supporting facts position features are embedded to 3-dimensional vectors. The dropout BIBREF35 probability $p$ is set to $0.3$. The beam size is set to 4 for beam search. We initialize the model parameters randomly using a Gaussian distribution with Xavier scheme BIBREF36. We first pre-train the network by minimizing only the maximum likelihood (ML) loss. Next, we initialize our model with the pre-trained ML weights and train the network with the mixed-objective learning function. The following values of hyperparameters are found to be optimal: (i) $\\gamma _1=0.99$, $\\gamma _2=0.01$, $\\gamma _3=0.1$, (ii) $d_1=300$, $d_2=d_3=3$, (iii) $\\alpha =0.9, \\beta = 10$, $h=5000$. Adam BIBREF37 optimizer is used to train the model with (i) $ \\beta _{1} = 0.9 $, (ii) $ \\beta _{2} = 0.999 $, and (iii) $ \\epsilon =10^{-8} $. For MTL-QG training, the initial learning rate is set to $0.01$. For our proposed model training the learning rate is set to $0.00001$. We also apply gradient clipping BIBREF38 with range $ [-5, 5] $."]}
{"question_id": "ae7c5cf9c2c121097eb00d389cfd7cc2a5a7d577", "predicted_answer": "", "predicted_evidence": ["A candidate sentence $S_i$ contains the $n_i$ number of words. In a given document list $L$, we have $K$ candidate sentences such that $\\sum _{i=1}^{i=K} n_i = N$. We generate the supporting fact encoding $sf_i \\in \\mathbb {R}^{n_i \\times d_3}$ for the candidate sentence $S_i$ as follows:"]}
{"question_id": "af948ea91136c700957b438d927f58d9b051c97c", "predicted_answer": "", "predicted_evidence": ["We also measure the multi-hopping in terms of SF coverage and reported the results in Table TABREF26 and Table TABREF27. We achieve skyline performance of $80.41$ F1 value on the ground-truth questions of the test dataset of HotPotQA."]}
{"question_id": "a913aa14d4e05cc9d658bf6697fe5b2652589b1b", "predicted_answer": "", "predicted_evidence": ["We predict the partial labels from one of the parsing abstractions as main tasks. The partial labels from the other parsing paradigm are used as auxiliary tasks. The loss is computed as INLINEFORM0 = INLINEFORM1 , where INLINEFORM2 is an auxiliary loss and INLINEFORM3 its specific weighting factor. Figure FIGREF17 shows the architecture used in this and the following multi-paradigm model."]}
{"question_id": "b065a3f598560fdeba447f0a100dd6c963586268", "predicted_answer": "", "predicted_evidence": ["Table TABREF22 shows the speeds (sentences/second) on a single core of a CPU. The d-mtl setup comes at almost no added computational cost, so the very good speed-accuracy tradeoff already provided by the single-task models is improved."]}
{"question_id": "9d963d385bd495a7e193f8a498d64c1612e6c20c", "predicted_answer": "", "predicted_evidence": ["Table TABREF30 shows model hyperparameters."]}
{"question_id": "179bc57b7b5231ea6ad3e93993a6935dda679fa2", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF30 shows recall, precision, F INLINEFORM0 (average of MUC, B INLINEFORM1 , CEAF INLINEFORM2 ), on the development set when training with INLINEFORM3 and INLINEFORM4 . As expected, higher values of INLINEFORM5 yield lower precisions but higher recalls. In contrast, F INLINEFORM6 increases until reaching the highest point when INLINEFORM7 for INLINEFORM8 ( INLINEFORM9 for INLINEFORM10 ), it then decreases gradually."]}
{"question_id": "a59e86a15405c8a11890db072b99fda3173e5ab2", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF30 shows recall, precision, F INLINEFORM0 (average of MUC, B INLINEFORM1 , CEAF INLINEFORM2 ), on the development set when training with INLINEFORM3 and INLINEFORM4 . As expected, higher values of INLINEFORM5 yield lower precisions but higher recalls. In contrast, F INLINEFORM6 increases until reaching the highest point when INLINEFORM7 for INLINEFORM8 ( INLINEFORM9 for INLINEFORM10 ), it then decreases gradually."]}
{"question_id": "9489b0ecb643c1fc95c001c65d4e9771315989aa", "predicted_answer": "", "predicted_evidence": ["We use five most popular metrics,"]}
{"question_id": "b210c3e48c15cdc8c47cf6f4b6eb1c29a1933654", "predicted_answer": "", "predicted_evidence": ["A justification for this approach is that in scenarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. In the French-English to Uzbek-English example, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The English embeddings should be kept but the Uzbek embeddings should be modified during training of the child model. Freezing certain portions of the parent model and fine tuning others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameters. We also experiment with ordinary L2 regularization, but find it does not significantly improve over the parameter freezing described above."]}
{"question_id": "00341a46a67d31d36e6dc54d5297626319584891", "predicted_answer": "", "predicted_evidence": ["The fact that the two models start from and converge to very different points, yet have similar training set performances, indicates that our architecture and training algorithm are able to reach a good minimum of the training objective regardless of the initialization. However, the training objective seems to have a large basin of models with similar performance and not all of them generalize well to the development set. The transfer model, starting with and staying close to a point known to perform well on a related task, is guided to a final point in the weight space that generalizes to the development set much better."]}
{"question_id": "d0dc6729b689561370b6700b892c9de8871bb44d", "predicted_answer": "", "predicted_evidence": ["In Figure 3 we plot learning curves for both a transfer and a non-transfer model on training and development sets. We see that the final training set perplexities for both the transfer and non-transfer model are very similar, but the development set perplexity for the transfer model is much better."]}
{"question_id": "17fd6deb9e10707f9d1b70165dedb045e1889aac", "predicted_answer": "", "predicted_evidence": ["2. Rank existing query structures. To find an appropriate query structure for the input question, we rank existing query structures ( INLINEFORM0 ) by using a scoring function, see Section SECREF20 ."]}
{"question_id": "c4a3f270e942803dab9b40e5e871a2e8886ce444", "predicted_answer": "", "predicted_evidence": ["To classify various queries with similar query intentions and narrow the search space for query generation, we introduce the notion of query structures. A query structure is a set of structurally-equivalent queries. Let INLINEFORM0 and INLINEFORM1 denote two queries. INLINEFORM2 is structurally-equivalent to INLINEFORM3 , denoted by INLINEFORM4 , if and only if there exist two bijections INLINEFORM5 and INLINEFORM6 such that:"]}
{"question_id": "1faccdc78bbd99320c160ac386012720a0552119", "predicted_answer": "", "predicted_evidence": ["We also test the performance of our approach only using the EARL linking results. The performance dropped dramatically in comparison to the first two rows. The main reason is that, for 82.8% of the questions, EARL provided partially correct results. If we consider the remaining questions, our system again have 73.2% and 84.8% of correctly-generated queries in top-1 and top-5 output, respectively."]}
{"question_id": "804466848f4fa1c552f0d971dce226cd18b9edda", "predicted_answer": "", "predicted_evidence": ["We simulated the real KBQA environment by considering noisy entity/relation linking results. We firstly mixed the correct linking result for each mention with the top-5 candidates generated from EARL BIBREF6 , which is a joint entity/relation linking system with state-of-the-art performance on LC-QuAD. The result is shown in the second row of Table TABREF42 . Although the precision for first output declined 11.4%, in 85% cases we still can generate correct answer in top-5. This is because SubQG ranked query structures first and considered linking results in the last step. Many error linking results can be filtered out by the empty query check or domain/range check."]}
{"question_id": "8d683d2e1f46626ceab60ee4ab833b50b346c29e", "predicted_answer": "", "predicted_evidence": ["The results on QALD-5 dataset is not as high as the result on LC-QuAD. This is because QALD-5 contains 11% of very difficult questions, requiring complex filtering conditions such as Regex and numerical comparison. These questions are currently beyond our approach's ability. Also, the size of training data is significant smaller."]}
{"question_id": "5ae005917efc17a505ba1ba5e996c4266d6c74b6", "predicted_answer": "", "predicted_evidence": ["When comparing models on our test set (Table TABREF19 ), we see that given the same training set, SubGram significantly outperforms Skip-gram model (22.4% vs. 9.7%). The performance of Skip-gram trained on the much larger dataset is higher (43.5%) and it would be interesting to see the SubGram model, if we could get access to such training data. Note however, that the Rule-based baseline is significantly better on both test sets."]}
{"question_id": "72c04eb3fc323c720f7f8da75c70f09a35abf3e6", "predicted_answer": "", "predicted_evidence": ["past-tense: Remove ing and add ed at the end of the verb."]}
{"question_id": "0715d510359eb4c851cf063c8b3a0c61b8a8edc0", "predicted_answer": "", "predicted_evidence": ["In this paper, we begin by discussing how this task relates to the current state of text summarization and similar tasks in Section SECREF2 . We then introduce the novel dataset and provide details on the level of abstraction, compression, and readability in Section SECREF3 . Next, we provide results and analysis on the performance of extractive summarization baselines on our data in Section SECREF5 . Finally, we discuss the potential for unsupervised systems in this genre in Section SECREF6 ."]}
{"question_id": "4e106b03cc2f54373e73d5922e97f7e5e9bf03e4", "predicted_answer": "", "predicted_evidence": ["Furthermore, as shown in Figure FIGREF15 , the dataset is very compressive, with a mean compression rate of 0.31 (std 0.23). The original texts have a mean of 3.6 (std 3.8) sentences per document and a mean of 105.6 (std 147.8) words per document. The reference summaries have a mean of 1.2 (std 0.6) sentences per document, and a mean of 17.2 (std 11.8) words per document."]}
{"question_id": "f8edc911f9e16559506f3f4a6bda74cde5301a9a", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 is the Kullback-Leibler divergence. Then, the posterior distribution of the parameters given the target data INLINEFORM1 can be estimated by optimizing the evidence of the target data INLINEFORM2 : DISPLAYFORM0 "]}
{"question_id": "8c288120139615532838f21094bba62a77f92617", "predicted_answer": "", "predicted_evidence": ["To evaluate our work we measured how the discovered units compared to the forced aligned phones in term of segmentation and information. The accuracy of the segmentation was measured in term of Precision, Recall and F-score. If a unit boundary occurs at the same time (+/- 10ms) of an actual phone boundary it is considered as a true positive, otherwise it is considered to be a false positive. If no match is found with a true phone boundary, this is considered to be a false negative. The consistency of the units was evaluated in term of normalized mutual information (NMI - see BIBREF1 , BIBREF3 , BIBREF5 for details) which measures the statistical dependency between the units and the forced aligned phones. A NMI of 0 % means that the units are completely independent of the phones whereas a NMI of 100 % indicates that the actual phones could be retrieved without error given the sequence of discovered units."]}
{"question_id": "a464052fd11af1d2d99e407c11791269533d43d1", "predicted_answer": "", "predicted_evidence": ["Note that when the model is trained with an uninformative prior the loss function is the as in Eq. EQREF13 but with INLINEFORM0 instead of the INLINEFORM1 . For the case of the uninformative prior, the Variational Bayes Inference was initialized as described in BIBREF1 . In the informative prior case, we initialized the algorithm by setting INLINEFORM2 ."]}
{"question_id": "5f6c1513cbda9ae711bc38df08fe72e3d3028af2", "predicted_answer": "", "predicted_evidence": ["We used the Mboshi5K corpus BIBREF13 as a test set for all the experiments reported here. Mboshi (Bantu C25) is a typical Bantu language spoken in Congo-Brazzaville. It is one of the languages documented by the BULB (Breaking the Unwritten Language Barrier) project BIBREF14 . This speech dataset was collected following a real language documentation scenario, using Lig_Aikuma, a mobile app specifically dedicated to fieldwork language documentation, which works both on android powered smartphones and tablets BIBREF15 . The corpus is multilingual (5130 Mboshi speech utterances aligned to French text) and contains linguists' transcriptions in Mboshi (in the form of a non-standard graphemic form close to the language phonology). It is also enriched with automatic forced-alignment between speech and transcriptions. The dataset is made available to the research community. More details on this corpus can be found in BIBREF13 ."]}
{"question_id": "130d73400698e2b3c6860b07f2e957e3ff022d48", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF33 shows the results of the second test of indirect bias, and reports the accuracy of a classifier trained to reclassify previously gender biased words on the Wikipedia embeddings (Gigaword patterns similarly). These results reinforce the finding of the clustering experiment: once again, nCDS outperforms all other methods significantly on both corpora ($p<0.01$), although it should be noted that the successful reclassification rate remains relatively high (e.g. 88.9% on Wikipedia)."]}
{"question_id": "7e9aec2bdf4256c6249cad9887c168d395b35270", "predicted_answer": "", "predicted_evidence": ["We perform an empirical comparison of these bias mitigation techniques on two corpora, the Annotated English Gigaword BIBREF8 and Wikipedia. Wikipedia is of particular interest, since though its Neutral Point of View (NPOV) policy predicates that all content should be presented without bias, women are nonetheless less likely to be deemed \u201cnotable\u201d than men of equal stature BIBREF9, and there are differences in the choice of language used to describe them BIBREF10, BIBREF11. We use the annotation native to the Annotated English Gigaword, and process Wikipedia with CoreNLP (statistical coreference; bidirectional tagger). Embeddings are created using Word2Vec. We use the original complex lexical input (gender-word pairs and the like) for each algorithm as we assume that this benefits each algorithm most. [author=simone,color=blue!40,size=,fancyline,caption=,]I am not 100% sure of which \"expansion\" you are talking about here. The classifier Bolucbasi use maybe?[author=rowan,color=green!40,size=,fancyline,caption=,]yup - clarified Expanding the set of gender-specific words for WED (following BIBREF1, using a linear classifier) on Gigaword resulted in 2141 such words, 7146 for Wikipedia."]}
{"question_id": "1acf06105f6c1930f869347ef88160f55cbf382b", "predicted_answer": "", "predicted_evidence": ["We fixedly associate pairs of names for swapping, thus expanding BIBREF5's short list of gender pairs vastly. Clearly both name frequency and the degree of gender-specificity are relevant to this bipartite matching. If only frequency were considered, a more gender-neutral name (e.g. Taylor) could be paired with a very gender-specific name (e.g. John), which would negate the gender intervention in many cases (namely whenever a male occurrence of Taylor is transformed into John, which would also result in incorrect pronouns, if present). If, on the other hand, only the degree of gender-specificity were considered, we would see frequent names (like James) being paired with far less frequent names (like Sybil), which would distort the overall frequency distribution of names. This might also result in the retention of a gender signal: for instance, swapping a highly frequent male name with a rare female name might simply make the rare female name behave as a new link between masculine contexts (instead of the original male name), as it rarely appears in female contexts."]}
{"question_id": "9ce90f4132b34a328fa49a63e897f376a3ad3ca8", "predicted_answer": "", "predicted_evidence": ["Table TABREF35 reports the SimLex-999 Spearman rank-order correlation coefficients $r_s$ (all are significant, $p<0.01$). Surprisingly, the WED40 and 70 methods outperform the unmitigated embedding, although the difference in result is small (0.386 and 0.395 vs. 0.385 on Gigaword, 0.371 and 0.367 vs. 0.368 on Wikipedia). nWED70, on the other hand, performs worse than the unmitigated embedding (0.384 vs. 0.385 on Gigaword, 0.367 vs. 0.368 on Wikipedia). CDA and CDS methods do not match the quality of the unmitigated space, but once again the difference is small. [author=simone,color=blue!40,size=,fancyline,caption=,]Second Part of Reaction to Reviewer 4.It should be noted that since SimLex-999 was produced by human raters, it will reflect the human biases these methods were designed to remove, so worse performance might result from successful bias mitigation."]}
{"question_id": "3138f916e253abed643d3399aa8a4555b2bd8c0f", "predicted_answer": "", "predicted_evidence": ["Table TABREF27 presents the $d$ scores and WEAT one-tailed $p$-values, which indicate whether the difference in samples means between targets $X$ and $Y$ and attributes $A$ and $B$ is significant. We also compute a two-tailed $p$-value to determine whether the difference between the various sets is significant."]}
{"question_id": "810e6d09813486a64e87ef6c1fb9b1e205871632", "predicted_answer": "", "predicted_evidence": ["In contrast, our model aims to force the decoder to learn a better language model. Suppose that if a few words' speech features are masked, the E2E model has to predict the token based on other signals, such as tokens that have generated or other unmasked speech features. In this way, we might alleviate the over-fitting issue that generating words only considering its corresponding speech features while ignoring other useful features. We believe our model is more effective when the input is noisy, because a model may generate correct tokens without considering previous generated tokens in a noise-free setting but it has to consider other signals when inputs are noisy, which is confirmed in our experiment."]}
{"question_id": "ab8b0e6912a7ca22cf39afdac5531371cda66514", "predicted_answer": "", "predicted_evidence": ["End-to-end (E2E) acoustic models, particularly with the attention-based encoder-decoder framework BIBREF0, have achieved a competitive recognition accuracy in a wide range of speech datasets BIBREF1. This model directly learns the mapping from the input acoustic signals to the output transcriptions without decomposing the problems into several different modules such as lexicon modeling, acoustic modeling and language modeling as in the conventional hybrid architecture. While this kind of E2E approach significantly simplifies the speech recognition pipeline, the weakness is that it is difficult to tune the strength of each component. One particular problem from our observations is that the attention based E2E model tends to make grammatical errors, which indicates that the language modeling power of the model is weak, possibly due to the small amount of training data, or the mismatch between the training and evaluation data. However, due to the jointly model approach in the attention model, it is unclear how to improve the strength of the language modeling power, i.e., attributing more weights to the previous output tokens in the decoder, or to improve the strength of the acoustic modeling power, i.e., attributing more weights to the context vector from the encoder."]}
{"question_id": "89373db8ced1fe420eae0093b2736f06b565616e", "predicted_answer": "", "predicted_evidence": ["The second dataset we use contains user reviews about household devices purchased at mall.cz BIBREF7 . The reviews are evaluative in nature (users apprising items they bought) and were categorized as negative or positive only. Some minor problems they carry are the grammatical or typing errors that frequently appear in their texts BIBREF11 . In Table 1 we present some rounded statistics about the two datasets. As we can see, Mall product reviews are slightly longer (13 vs. 10 tokens) than Czech Facebook posts. We also see that the number of data samples in each sentiment category are unbalanced in both cases. A few samples of Mall reviews are illustrated in Figure 2 ."]}
{"question_id": "74a17eb3bf1d4f36e2db1459a342c529b9785f6e", "predicted_answer": "", "predicted_evidence": ["In this section, we describe the experiments to evaluate our proposed methods."]}
{"question_id": "4b6745982aa64fbafe09f7c88c8d54d520b3f687", "predicted_answer": "", "predicted_evidence": ["1) Which words are worthy to recommend at each decoding step?"]}
{"question_id": "6656a9472499331f4eda45182ea697a4d63e943c", "predicted_answer": "", "predicted_evidence": ["In this section, we describe the experiments to evaluate our proposed methods."]}
{"question_id": "430ad71a0fd715a038f3c0fe8d7510e9730fba23", "predicted_answer": "", "predicted_evidence": ["2) Baseline: It is the baseline attention-based NMT system BIBREF23 , BIBREF24 ."]}
{"question_id": "b79ff0a50bf9f361c5e5fed68525283856662076", "predicted_answer": "", "predicted_evidence": ["In the future, we plan to design more effective methods to calculate accurate bonus values."]}
{"question_id": "d66c31f24f582c499309a435ec3c688dc3a41313", "predicted_answer": "", "predicted_evidence": ["In this section we describe our metrics, training procedure, and the results, including the impact of our method in production."]}
{"question_id": "c47312f2ca834ee75fa9bfbf912ea04239064117", "predicted_answer": "", "predicted_evidence": ["In this section we describe our metrics, training procedure, and the results, including the impact of our method in production."]}
{"question_id": "5499440674f0e4a9d6912b9ac29fa1f7b7cd5253", "predicted_answer": "", "predicted_evidence": ["The only difference between the four models is which sentences from each document are passed to the classifier for training and testing. The intuition is that a classifier utilizing the correct sentences should outperform both other models."]}
{"question_id": "de313b5061fc22e8ffef1706445728de298eae31", "predicted_answer": "", "predicted_evidence": ["Extracting data elements such as study descriptors from publication full texts is an essential step in a number of tasks including systematic review preparation BIBREF0 , construction of reference databases BIBREF1 , and knowledge discovery BIBREF2 . These tasks typically involve domain experts identifying relevant literature pertaining to a specific research question or a topic being investigated, identifying passages in the retrieved articles that discuss the sought after information, and extracting structured data from these passages. The extracted data is then analyzed, for example to assess adherence to existing guidelines BIBREF1 . Figure FIGREF2 shows an example text excerpt with information relevant to a specific task (assessment of adherence to existing guidelines BIBREF1 ) highlighted."]}
{"question_id": "47b7bc232af7bf93338bd3926345e23e9e80c0c1", "predicted_answer": "", "predicted_evidence": ["The only difference between the four models is which sentences from each document are passed to the classifier for training and testing. The intuition is that a classifier utilizing the correct sentences should outperform both other models."]}
{"question_id": "0b5c599195973c563c4b1a0fe5d8fc77204d71a0", "predicted_answer": "", "predicted_evidence": ["There are two main contributions of this work. We present an unsupervised method that employs representation learning to identify text segments from publication full text which are relevant to/contain specific sought after information (such as number of dose groups). In addition, we explore a new dataset which hasn't been previously used in the field of information extraction."]}
{"question_id": "1397b1c51f722a4ee2b6c64dc9fc6afc8bd3e880", "predicted_answer": "", "predicted_evidence": ["Extracting data elements such as study descriptors from publication full texts is an essential step in a number of tasks including systematic review preparation BIBREF0 , construction of reference databases BIBREF1 , and knowledge discovery BIBREF2 . These tasks typically involve domain experts identifying relevant literature pertaining to a specific research question or a topic being investigated, identifying passages in the retrieved articles that discuss the sought after information, and extracting structured data from these passages. The extracted data is then analyzed, for example to assess adherence to existing guidelines BIBREF1 . Figure FIGREF2 shows an example text excerpt with information relevant to a specific task (assessment of adherence to existing guidelines BIBREF1 ) highlighted."]}
{"question_id": "230f127e83ac62dd65fccf6b1a4960cf0f7316c7", "predicted_answer": "", "predicted_evidence": ["Raise awareness of how a judicial choice of optimizer with a good learning rate policy can help improve performance;"]}
{"question_id": "75c221920bee14a6153bd5f4c1179591b2f48d59", "predicted_answer": "", "predicted_evidence": ["Raise awareness of how a judicial choice of optimizer with a good learning rate policy can help improve performance;"]}
{"question_id": "4eb42c5d56d695030dd47ea7f6d65164924c4017", "predicted_answer": "", "predicted_evidence": ["We collect the set of audio samples $\\mathbb {X}_{\\text{init}}=\\lbrace \\mathbf {x}_{\\text{init}}^{i}\\rbrace _{i=1}^{N_{\\text{init}}}$, with $N_{\\text{init}}=12000$ and their corresponding metadata (e.g. tags that indicate their content, and a short textual description), from the online platform Freesound BIBREF8. $\\mathbf {x}_{\\text{init}}$ was obtained by randomly sampling audio files from Freesound fulfilling the following criteria: lossless file type, audio quality at least 44.1 kHz and 16-bit, duration $10\\text{ s}\\le d({\\mathbf {x}_{\\text{init}}^{i}})\\le 300$ s (where $d(\\mathbf {x})$ is the duration of $\\mathbf {x}$), a textual description which first sentence does not have spelling errors according to US and UK English dictionaries (as an indication of the correctness of the metadata, e.g. tags), and not having tags that indicate music, sound effects, or speech. As tags indicating speech files we consider those like \u201cspeech\u201d, \u201cspeak\u201d, and \u201cwoman\u201d. We normalize $\\mathbf {x}^{i}_{\\text{init}}$ to the range $[-1, 1]$, trim the silence (60 dB below the maximum amplitude) from the beginning and end, and resample to 44.1 kHz. Finally, we keep samples that are longer than 15 s as a result of the processing. This results in $\\mathbb {X}^{\\prime }_{\\text{init}}=\\lbrace \\mathbf {x}_{\\text{init}}^{j}\\rbrace _{j=1}^{N^{\\prime }_{\\text{init}}},\\,N^{\\prime }_{\\text{init}}=9000$."]}
{"question_id": "eff9192e05d23e9a67d10be0c89a7ab2b873995b", "predicted_answer": "", "predicted_evidence": ["We use AMT and a novel three-step based framework BIBREF0 for crowdsourcing the annotation of $\\mathbb {X}_{\\text{sam}}$, acquiring the set of captions $\\mathbb {C}_{\\text{sam}}^{z}=\\lbrace c_{\\text{sam}}^{z,u}\\rbrace _{u=1}^{N_{\\text{cp}}}$ for each $\\mathbf {x}_{\\text{sam}}^{z}$, where $c_{\\text{sam}}^{z,u}$ is an eight to 20 words long caption for $\\mathbf {x}_{\\text{sam}}^{z}$. In a nutshell, each audio sample $\\mathbf {x}_{\\text{sam}}^{z}$ gets annotated by $N_{\\text{cp}}$ different annotators in the first step of the framework. The annotators have access only to $\\mathbf {x}_{\\text{sam}}^{z}$ and not any other information. In the second step, different annotators are instructed to correct any grammatical errors, typos, and/or rephrase the captions. This process results in $2\\times N_{\\text{cp}}$ captions per $\\mathbf {x}_{\\text{sam}}^{z}$. Finally, three (again different) annotators have access to $\\mathbf {x}_{\\text{sam}}^{z}$ and its $2\\times N_{\\text{cp}}$ captions, and score each caption in terms of the accuracy of the description and fluency of English, using a scale from 1 to 4 (the higher the better). The captions for each $\\mathbf {x}_{\\text{sam}}^{z}$ are sorted (first according to accuracy of description and then according to fluency), and two groups are formed: the top $N_{\\text{cp}}$ and the bottom $N_{\\text{cp}}$ captions. The top $N_{\\text{cp}}$ captions are selected as $\\mathbb {C}_{\\text{sam}}^{z}$. We manually sanitize further $\\mathbb {C}_{\\text{sam}}^{z}$, e.g. by replacing \u201cit's\u201d with \u201cit is\u201d or \u201cits\u201d, making consistent hyphenation and compound words (e.g. \u201cnonstop\u201d, \u201cnon-stop\u201d, and \u201cnon stop\u201d), removing words or rephrasing captions pertaining to the content of speech (e.g. \u201cFrench\u201d, \u201cforeign\u201d), and removing/replacing named entities (e.g. \u201cWindex\u201d)."]}
{"question_id": "87523fb927354ddc8ad1357a81f766b7ea95f53c", "predicted_answer": "", "predicted_evidence": ["Finally, we observe that some captions include transcription of speech. To remove it, we employ extra annotators (not from AMT) which had access only at the captions. We instruct the annotators to remove the transcribed speech and rephrase the caption. If the result is less than eight words, we check the bottom $N_{\\text{cp}}$ captions for that audio sample. If they include a caption that has been rated with at least 3 by all the annotators for both accuracy and fluency, and does not contain transcribed speech, we use that caption. Otherwise, we remove completely the audio sample. This process yields the final set of audio samples and captions, $\\mathbb {X}=\\lbrace \\mathbf {x}^{o}\\rbrace _{o=1}^{N}$ and $\\mathbb {C}^{\\prime }=\\lbrace \\mathbb {C}^{\\prime o}\\rbrace _{o=1}^{N}$, respectively, with $\\mathbb {C}^{\\prime o}=\\lbrace c^{\\prime o,u}\\rbrace _{u=1}^{N_{\\text{cp}}}$ and $N=4981$."]}
{"question_id": "9e9aa8af4b49e2e1e8cd9995293a7982ea1aba0e", "predicted_answer": "", "predicted_evidence": ["In Table TABREF13 are the scores of the employed metrics for the evaluation and testing splits."]}
{"question_id": "1fa9b6300401530738995f14a37e074c48bc9fd8", "predicted_answer": "", "predicted_evidence": ["Score distribution Due to the caption pairs are generated from different images, strong bias towards low scores is expected (see Figure FIGREF3 ). We measured the score distribution in the two subsets separately and jointly, and see that the two subsets follow same distribution. As expected, the most frequent score is 0 (Table TABREF2 ), but the dataset still shows wide range of similarity values, with enough variability."]}
{"question_id": "9d98975ab0b75640b2c83e29e1438c76a959fbde", "predicted_answer": "", "predicted_evidence": ["Score distribution Due to the caption pairs are generated from different images, strong bias towards low scores is expected (see Figure FIGREF3 ). We measured the score distribution in the two subsets separately and jointly, and see that the two subsets follow same distribution. As expected, the most frequent score is 0 (Table TABREF2 ), but the dataset still shows wide range of similarity values, with enough variability."]}
{"question_id": "cc8bcea4052bf92f249dda276acc5fd16cac6fb4", "predicted_answer": "", "predicted_evidence": ["In addition we show that the dataset allows to explore two hypothesis: H1) whether the image representations alone are able to predict caption similarity; H2) whether a combination of image and text representations allow to improve the text-only results on this similarity task."]}
{"question_id": "35f48b8f73728fbdeb271b170804190b5448485a", "predicted_answer": "", "predicted_evidence": ["Subset 2014 This subset is derived from the Image Descriptions dataset which is a subset of the PASCAL VOC-2008 dataset BIBREF16 . PASCAL VOC-2008 dataset consists of 1,000 images and has been used by a number of image description systems. In total, we obtained 374 pairs (out of 750 in the original file)."]}
{"question_id": "16edc21a6abc89ee2280dccf1c867c2ac4552524", "predicted_answer": "", "predicted_evidence": ["Results Table TABREF4 shows the results of the single and combined models. Among single models, as expected, dam obtains the highest Pearson correlation ( INLINEFORM0 ). Interestingly, the results show that images alone are valid to predict caption similarity (0.61 INLINEFORM1 ). Results also show that image and sentence representations are complementary, with the best results for a combination of DAM and RESNET50 representations. These results confirm our hypotheses, and more generally, show indications that in systems that work with text describing the real world, the representation of the real world helps to better understand the text and do better inferences."]}
{"question_id": "3b8da74f5b359009d188cec02adfe4b9d46a768f", "predicted_answer": "", "predicted_evidence": ["The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model."]}
{"question_id": "6bce04570d4745dcfaca5cba64075242308b65cf", "predicted_answer": "", "predicted_evidence": ["The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model."]}
{"question_id": "37e6ce5cfc9d311e760dad8967d5085446125408", "predicted_answer": "", "predicted_evidence": ["RoBERTa is similar to BERT, except that it changes the masking strategy and removes the NSP taskBIBREF9."]}
{"question_id": "6683008e0a8c4583058d38e185e2e2e18ac6cf50", "predicted_answer": "", "predicted_evidence": ["The BiGRU+CRF model was used as the baseline model. Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model."]}
{"question_id": "7bd24920163a4801b34d0a50aed957ba8efed0ab", "predicted_answer": "", "predicted_evidence": ["We use three popular datasets in ABSA task: Restaurant reviews and Laptop reviews from SemEval 2014 Task 4 BIBREF5, and ACL 14 Twitter dataset BIBREF6."]}
{"question_id": "df01e98095ba8765d9ab0d40c9e8ef34b64d3700", "predicted_answer": "", "predicted_evidence": ["The SNLI dataset is quite large, so we simply take the best-performing model on the development set for testing."]}
{"question_id": "a7a433de17d0ee4dd7442d7df7de17e508baf169", "predicted_answer": "", "predicted_evidence": ["Main contributions of this paper can be summarized as follows:"]}
{"question_id": "abfa3daaa984dfe51289054f4fb062ce93f31d19", "predicted_answer": "", "predicted_evidence": ["The gains seem to be small, but the improvements of the method are straightforwardly reasonable and the flexibility of our strategies makes it easier to apply to a variety of other tasks."]}
{"question_id": "1702985a3528e876bb19b8e223399729d778b4e4", "predicted_answer": "", "predicted_evidence": ["The quality of sentiment labels generated by our updated VADER lexicon is better compared to the labels generated by the original VADER English lexicon.TABREF4.Sentiment labels by human annotators was able to capture nuances that the rule based sentiment labelling could not capture.More work can be done to increase the number of instances in the dataset."]}
{"question_id": "f44a9ed166a655df1d54683c91935ab5e566a04f", "predicted_answer": "", "predicted_evidence": ["We acknowledge Kessiena Rita David,Patrick Ehizokhale Oseghale and Peter Chimaobi Onuoha for using their mastery of Nigerian Pidgin to translate and label the datasets."]}
{"question_id": "0ba1514fb193c52a15c31ffdcd5c3ddbb2bb2c40", "predicted_answer": "", "predicted_evidence": ["This study uses the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores for about 14,000 Nigerian Pidgin tweets. The updated VADER lexicon (updated with 300 Pidgin tokens and their sentiment scores) performed better than the original VADER lexicon. The labelled sentiments from the updated VADER were then compared with sentiment labels by expert Pidgin English speakers."]}
{"question_id": "d14118b18ee94dafe170439291e20cb19ab7a43c", "predicted_answer": "", "predicted_evidence": ["This study uses the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores for about 14,000 Nigerian Pidgin tweets. The updated VADER lexicon (updated with 300 Pidgin tokens and their sentiment scores) performed better than the original VADER lexicon. The labelled sentiments from the updated VADER were then compared with sentiment labels by expert Pidgin English speakers."]}
{"question_id": "d922eaa5aa135c1ae211827c6a599b4d69214563", "predicted_answer": "", "predicted_evidence": ["The results indicates the impact of contextual information using different embeddings, which are different in feature representation. Results of class happy without contextual features has %44.16 by GRU-att-ELMo model, and %49.38 by GRU-att-ELMo+F."]}
{"question_id": "ff668c7e890064756cdd2f9621e1cedb91eef1d0", "predicted_answer": "", "predicted_evidence": ["The results indicates the impact of contextual information using different embeddings, which are different in feature representation. Results of class happy without contextual features has %44.16 by GRU-att-ELMo model, and %49.38 by GRU-att-ELMo+F."]}
{"question_id": "d3cfbe497a30b750a8de3ea7f2cecf4753a4e1f9", "predicted_answer": "", "predicted_evidence": ["Using different word embedding or end to end models where word representation learned from local context create different results in emotion detection. We noted that pre-trained word embeddings need to be tuned with local context during our experiments or it causes the model to not converge. We experimented with different word embedding methods such as word2vec, GloVe BIBREF7 , fasttext BIBREF8 , and ELMo. Among these methods fasttext and ELMo create better results."]}
{"question_id": "73d87f6ead32653a518fbe8cdebd81b4a3ffcac0", "predicted_answer": "", "predicted_evidence": ["Data-free and data-bound shrinking can be interpreted as setting the expected difference between network outputs before and after a removal operation to zero under different assumptions."]}
{"question_id": "fda47c68fd5f7b44bd539f83ded5882b96c36dd7", "predicted_answer": "", "predicted_evidence": ["Junczys-Dowmunt et al. averaging2,averaging1 reported gains from averaging the weight matrices of multiple checkpoints of the same training run. However, our attempts to replicate their approach were not successful. Averaging might work well when the behaviour of corresponding units is similar across networks, but that cannot be guaranteed when networks are trained independently."]}
{"question_id": "643645e02ffe8fde45918615ec92013a035d1b92", "predicted_answer": "", "predicted_evidence": ["Our data-bound algorithm uses a data-bound version of the neuron selection criterion in Eq. EQREF8 which operates on the activity matrix INLINEFORM0 . We search for the pair INLINEFORM1 according the following term and remove neuron INLINEFORM2 . DISPLAYFORM0 "]}
{"question_id": "a994cc18046912a8c9328dc572f4e4310736c0e2", "predicted_answer": "", "predicted_evidence": ["The top systems in recent machine translation evaluation campaigns on various language pairs use ensembles of a number of NMT systems BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . Ensembling BIBREF7 , BIBREF8 of neural networks is a simple yet very effective technique to improve the accuracy of NMT. The decoder makes use of INLINEFORM0 NMT networks which are either trained independently BIBREF9 , BIBREF2 , BIBREF3 , BIBREF4 or share some amount of training iterations BIBREF10 , BIBREF1 , BIBREF5 , BIBREF6 . The ensemble decoder computes predictions from each of the individual models which are then combined using the arithmetic average BIBREF9 or the geometric average BIBREF5 ."]}
{"question_id": "9baca9bdb8e7d5a750f8cbe3282beb371347c164", "predicted_answer": "", "predicted_evidence": ["To obtain meaningful linguistic data we pre-processed the incoming tweet streams in several ways. As our central question here deals with language semantics of individuals, re-tweets do not bring any additional information to our study, thus we removed them by default. We also removed any expressions considered to be semantically meaningless like URLs, emoticons, mentions of other users (denoted by the @ symbol) and hashtags (denoted by the # symbol) to simplify later post-processing. In addition, as a last step of textual pre-processing, we downcased and stripped the punctuation from the text of every tweet."]}
{"question_id": "2cb20bae085b67e357ab1e18ebafeac4bbde5b4a", "predicted_answer": "", "predicted_evidence": ["The precise inference of SES would contribute to overcome several scientific challenges and could potentially have several commercial applications BIBREF7 . Further, robust SES inference would provide unique opportunities to gain deeper insights on socioeconomic inequalities BIBREF8 , social stratification BIBREF2 , and on the driving mechanisms of network evolution, such as status homophily or social segregation."]}
{"question_id": "892ee7c2765b3764312c3c2b6f4538322efbed4e", "predicted_answer": "", "predicted_evidence": ["Earlier studies BIBREF9 , BIBREF11 , BIBREF12 demonstrated that annotated occupation information can be effectively used to derive precise income for individuals and infer therefore their SES. However, these methods required a somewhat selective set of Twitter users as well as an expensive annotation process by hiring premium annotators e.g. from Amazon Mechanical Turk. Our goal here was to obtain the occupations for a general set of Twitter users without the involvement of annotators, but by collecting data from parallel online services."]}
{"question_id": "c68946ae2e548ec8517c7902585c032b3f3876e6", "predicted_answer": "", "predicted_evidence": ["The quantification and inference of SES of individuals is a long lasting question in the social sciences. It is a rather difficult problem as it may depend on a combination of individual characteristics and environmental variables BIBREF0 . Some of these features can be easier to assess like income, gender, or age whereas others, relying to some degree on self-definition and sometimes entangled with privacy issues, are harder to assign like ethnicity, occupation, education level or home location. Furthermore, individual SES correlates with other individual or network attributes, as users tend to build social links with others of similar SES, a phenomenon known as status homophily BIBREF1 , arguably driving the observed stratification of society BIBREF2 . At the same time, shared social environment, similar education level, and social influence have been shown to jointly lead socioeconomic groups to exhibit stereotypical behavioral patterns, such as shared political opinion BIBREF3 or similar linguistic patterns BIBREF4 . Although these features are entangled and causal relation between them is far from understood, they appear as correlations in the data."]}
{"question_id": "7557f2c3424ae70e2a79c51f9752adc99a9bdd39", "predicted_answer": "", "predicted_evidence": ["We provide in Section SECREF2 an overview of the related literature to contextualize the novelty of our work. In Section SECREF3 we provide a detailed description of the data collection and combination methods. In Section SECREF4 we introduce the features extracted to solve the SES inference problem, with results summarized in Section SECREF5 . Finally, in Section SECREF6 and SECREF7 we conclude our paper with a brief discussion of the limitations and perspectives of our methods."]}
{"question_id": "b03249984c26baffb67e7736458b320148675900", "predicted_answer": "", "predicted_evidence": ["For each socioeconomic dataset, we trained our models by using 75% of the available data for training and the remaining 25% for testing. During the training phase, the training data undergoes a INLINEFORM0 -fold inner cross-validation, with INLINEFORM1 , where all splits are computed in a stratified manner to get the same ratio of lower to higher SES users. The four first blocks were used for inner training and the remainder for inner testing. This was repeated ten times for each model so that in the end, each model's performance on the validation set was averaged over 50 samples. For each model, the parameters were fine-tuned by training 500 different models over the aforementioned splits. The selected one was that which gave the best performance on average, which was then applied to the held-out test set. This is then repeated through a 5-fold outer cross-validation."]}
{"question_id": "9595fdf7b51251679cd39bc4f6befc81f09c853c", "predicted_answer": "", "predicted_evidence": [" INLINEFORM0 Annotated home locations: The remote sensing annotation was done by experts and their evaluation was based on visual inspection and biased by some unavoidable subjectivity. Although their annotations were cross-referenced and found to be consistent, they still contained biases, like over-representative middle classes, which somewhat undermined the prediction task based on this dataset."]}
{"question_id": "08c0d4db14773cbed8a63e69381a2265e85f8765", "predicted_answer": "", "predicted_evidence": ["As a second method to estimate SES, we took a sample of Twitter users who mentioned their LinkedIn BIBREF29 profile url in their tweets or Twitter profile. Using these pointers we collected professional profile descriptions from LinkedIn by relying on an automatic crawler mainly used in Search Engine Optimization (SEO) tasks BIBREF30 . We obtained INLINEFORM0 Twitter/LinkedIn users all associated with their job title, professional skills and profile description. Apart from the advantage of working with structured data, professional information extracted from LinkedIn is significantly more reliable than Twitter's due to the high degree of social scrutiny to which each profile is exposed BIBREF31 ."]}
{"question_id": "5e29f16d7302f24ab93b7707d115f4265a0d14b0", "predicted_answer": "", "predicted_evidence": ["Table TABREF7 shows that best results are achieved by adding only those samples for which two back-translations agree with one another. This may represent the best trade-off between reliability of the label and the amount of additional data. The setting where the data from all languages is added performs badly despite the large number of samples, because this method contains different labels for the same argument pairs, for all those instances where the back-translations don't yield the same label, introducing noise into the system. The size of the extra data used in BIBREF0 is about 10 times larger than our 2-votes data, as they relied on additional training data (which we could not use in this experiment, as there is no pairing with translations into other languages) and exploited also intra-sentential instances. While we don't match the performance of BIBREF0 on the PDTB-Lin test set, the high quality translation data shows better generalisability by outperforming all other settings in the cross-validation (which is based on 16 test instances, while the PDTB-Lin test set contains less than 800 instances and hence exhibits more variability in general)."]}
{"question_id": "26844cec57df6ff0f02245ea862af316b89edffe", "predicted_answer": "", "predicted_evidence": ["The difficulty in classifying implicit discourse relations stems from the lack of strong indicative cues. Early work has already shown that implicit relations cannot be learned from explicit ones BIBREF9 , making human-annotated relations the currently only source for training relation classification."]}
{"question_id": "d1d59bca40b8b308c0a35fed1b4b7826c85bc9f8", "predicted_answer": "", "predicted_evidence": ["After parsing the back-translations of French, German and Czech, we can compare whether they contain explicit relations which connect the same relational arguments. The analysis of this subset then allows us to identify those instances which could be labeled with high confidence."]}
{"question_id": "4d824b49728649432371ecb08f66ba44e50569e0", "predicted_answer": "", "predicted_evidence": ["ts_tick_rel (TS symbol, change in num. value),"]}
{"question_id": "02a5acb484bda77ef32a13f5d93d336472cf8cd4", "predicted_answer": "", "predicted_evidence": ["We compute a consistency score INLINEFORM0 for the candidate extraction, measuring if the extracted relation is consistent with (noisy) supervision INLINEFORM1 (e.g. an existing database)."]}
{"question_id": "863d8d32a1605402e11f0bf63968a14bcfd15337", "predicted_answer": "", "predicted_evidence": ["The document is parsed using a potentially constraint-based parser, which outputs a set of candidate extractions. Each candidate extraction consists of the character offsets of all extracted constituent entities, as well as a representation of the extracted relation. It may additionally contain auxilliary information that the parser may have generated, such as part of speech tags."]}
{"question_id": "d4b84f48460517bc0a6d4e0c38f6853c58081166", "predicted_answer": "", "predicted_evidence": ["To do so, first we took the geolocated Twitter users in France and partitioned them into nine socioeconomic classes using their inferred income $S_\\mathrm {inc}^u$ . Partitioning was done first by sorting users by their $S^u_\\mathrm {inc}$ income to calculate their $C(S^u_\\mathrm {inc})$ cumulative income distribution function. We defined socioeconomic classes by segmenting $C(S^u_\\mathrm {inc})$ such that the sum of income is the same for each classes (for an illustration of our method see Fig. 6 a in the Appendix). We constructed a social network by considering mutual mention links between these users (as introduced in Section \"Data Description\" ). Taking the assigned socioeconomic classes of connected individuals, we confirmed the effects of status homophily in the Twitter mention network by computing the connection matrix of socioeconomic groups normalized by the equivalent matrix of corresponding configuration model networks, which conserved all network properties except structural correlations (as explained in the Appendix). The diagonal component in Fig. 6 matrix indicated that users of similar socioeconomic classes were better connected, while people from classes far apart were less connected than one would expect by chance from the reference model with users connected randomly."]}
{"question_id": "90756bdcd812b7ecc1c5df2298aa7561fd2eb02c", "predicted_answer": "", "predicted_evidence": ["Via a detailed multidimensional correlation study we concluded that (a) socioeconomic indicators and linguistic variables are significantly correlated. i.e. people with higher socioeconomic status are more prone to use more standard variants of language and a larger vocabulary set, while people on the other end of the socioeconomic spectrum tend to use more non-standard terms and, on average, a smaller vocabulary set; (b) Spatial position was also found to be a key feature of standard language use as, overall, people from the North tended to use more non-standard terms and a smaller vocabulary set compared to people from the South; a more fine-grained analysis reveals that the spatial variability of language is determined to a greater extent locally by the socioeconomic status; (c) In terms of temporal activity, standard language was more likely to be used during the daytime while non-standard variants were predominant during the night. We explained this temporal variability by the turnover of population with different socioeconomic status active during night and day; Finally (d) we showed that the social network and status homophily mattered in terms of linguistic similarity between peers, as connected users with the same socioeconomic status appeared to be the most similar, while disconnected people were found to be the most dissimilar in terms of their individual use of the aforementioned linguistic markers."]}
{"question_id": "028d0d9b7a71133e51a14a32cd09dea1e2f39f05", "predicted_answer": "", "predicted_evidence": ["The strong tendency of communication systems to vary, diversify and evolve seems to contradict their basic function: allowing mutual intelligibility within large communities over time. Language variation is not counter adaptive. Rather, subtle differences in the way others speak provide critical cues helping children and adults to organize the social world BIBREF3 . Linguistic variability contributes to the construction of social identity, definition of boundaries between social groups and the production of social norms and hierarchies."]}
{"question_id": "cfc73e0c82cf1630b923681c450a541a964688b9", "predicted_answer": "", "predicted_evidence": ["Many studies have overcome this limitation by taking advantage of the geolocation feature allowing Twitter users to include in their posts the location from which they were tweeted. Based on this metadata, studies have been able to assign home location to geolocated users with varying degrees of accuracy BIBREF15 . Subsequent work has also been devoted to assigning to each user some indicator that might characterize their socioeconomic status based on their estimated home location. These indicators are generally extracted from other datasets used to complete the Twitter one, namely census data BIBREF16 , BIBREF12 , BIBREF17 or real estate online services as Zillow.com BIBREF18 . Other approaches have also relied on sources of socioeconomic information such as the UK Standard Occupation Classification (SOC) hierarchy, to assign socioeconomic status to users with occupation mentions BIBREF19 . Despite the relative success of these methods, their common limitation is to provide observations and predictions based on a carefully hand-picked small set of users, letting alone the problem of socioeconomic status inference on larger and more heterogeneous populations. Our work stands out from this well-established line of research by expanding the definition of socioeconomic status to include several demographic features as well as by pinpointing potential home location to individual users with an unprecedented accuracy. Identifying socioeconomic status and the network effects of homophily BIBREF20 is an open question BIBREF21 . However, recent results already showed that status homophily, i.e. the tendency of people of similar socioeconomic status are better connected among themselves, induce structural correlations which are pivotal to understand the stratified structure of society BIBREF22 . While we verify the presence of status homophily in the Twitter social network, we detect further sociolinguistic correlations between language, location, socioeconomic status, and time, which may inform novel methods to infer socioeconomic status for a broader set of people using common information available on Twitter."]}
{"question_id": "3746aaa1a81d9c725bc7a4a67086634c11998d39", "predicted_answer": "", "predicted_evidence": ["Using these engines and/or external data collection, HoME can facilitate tasks such as:"]}
{"question_id": "143409d16125790c8db9ed38590a0796e0b2b2e2", "predicted_answer": "", "predicted_evidence": ["Here, we consider how this global structure in the learned embeddings is related to a linearity in the training objective. In particular, linear functions have the property that INLINEFORM0 , imposing a systematic relation between the predictions we make for INLINEFORM1 , INLINEFORM2 and INLINEFORM3 . In fact, we could think of this as a form of translational symmetry where adding INLINEFORM4 to the input has the same effect on the output throughout the space."]}
{"question_id": "8ba582939823faae6822a27448ea011ab6b90ed7", "predicted_answer": "", "predicted_evidence": ["We suggest that NLP will benefit from incorporating more global structure into its models. Existing background knowledge is one possible source for such additional structure BIBREF17 , BIBREF18 . But it will also be necessary to uncover novel global relations, following the example of the other natural sciences."]}
{"question_id": "65c7a2b734dab51c4c81f722527424ff33b023f8", "predicted_answer": "", "predicted_evidence": ["There are two broad set of approaches that have been explored in the literature for translation between related languages that leverage lexical similarity between source and target languages."]}
{"question_id": "11ef46187a5bf15e89d63220fdeaecbeb92d818e", "predicted_answer": "", "predicted_evidence": ["Our major observations are described below (based on BLEU scores):"]}
{"question_id": "45aab23790161cbc55f78e16fdf5678a3f5b4b92", "predicted_answer": "", "predicted_evidence": ["Both OS and BPE units are variable length units which provide longer and more relevant context for translation compared to character n-grams. In contrast to orthographic syllables, the BPE units are highly frequent character sequences reflecting the underlying statistical properties of the text. Some of the character sequences discovered by the BPE algorithm may be different linguistic units like syllables, morphemes and affixes. Moreover, BPE can be applied to text in any writing system."]}
{"question_id": "bf5e80f1ab4eae2254b4f4d7651969a3cf945fb4", "predicted_answer": "", "predicted_evidence": ["This section describes the results of various experiments and analyses them. A comparison of BPE with other units across languages and writing systems, choice of number of merge operations and effect of domain change and training data size are studied. We also report initial results with a joint bilingual BPE model."]}
{"question_id": "0a70af6ba334dfd3574991b1dd06f54fc6a700f2", "predicted_answer": "", "predicted_evidence": ["Our main research questions are therefore, RQ1) are there semantic and linguistic differences between fake news and satire stories that can help to tell them apart?; and RQ2) can these semantic and linguistic differences contribute to the understanding of nuances between fake news and satire beyond differences in the language being used?"]}
{"question_id": "98b97d24f31e9c535997e9b6cb126eb99fc72a90", "predicted_answer": "", "predicted_evidence": ["With regard to research question RQ2 on the understanding of semantic and linguistic nuances between fake news and satire - a key advantage of studying the coherence metrics is explainability. While the pre-trained model of BERT gives the best result, it is not easily interpretable. The coherence metrics allow us to study the differences between fake news and satire in a straightforward manner."]}
{"question_id": "71b07d08fb6ac8732aa4060ae94ec7c0657bb1db", "predicted_answer": "", "predicted_evidence": ["With regard to research question RQ2 on the understanding of semantic and linguistic nuances between fake news and satire - a key advantage of studying the coherence metrics is explainability. While the pre-trained model of BERT gives the best result, it is not easily interpretable. The coherence metrics allow us to study the differences between fake news and satire in a straightforward manner."]}
{"question_id": "812c974311747f74c3aad23999bfef50539953c8", "predicted_answer": "", "predicted_evidence": ["With regard to research question RQ2 on the understanding of semantic and linguistic nuances between fake news and satire - a key advantage of studying the coherence metrics is explainability. While the pre-trained model of BERT gives the best result, it is not easily interpretable. The coherence metrics allow us to study the differences between fake news and satire in a straightforward manner."]}
{"question_id": "180c7bea8caf05ca97d9962b90eb454be4176425", "predicted_answer": "", "predicted_evidence": ["First, we consider the semantic representation with BERT. Our experiments included multiple pre-trained models of BERT with different sizes and cases sensitivity, among which the large uncased model, bert_uncased_L-24_H-1024_A-16, gave the best results. We use the recommended settings of hyper-parameters in BERT's Github repository and use the fake news and satire data to fine-tune the model. Furthermore, we tested separate models based on the headline and body text of a story, and in combination. Results are shown in Table TABREF6. The models based on the headline and text body give a similar F1 score. However, while the headline model performs poorly on precision, perhaps due to the short text, the model based on the text body performs poorly on recall. The model based on the full text of headline and body gives the best performance."]}
{"question_id": "95083d486769b9b5e8c57fe2ef1b452fc3ea5012", "predicted_answer": "", "predicted_evidence": ["Experiments show that RNNGs are effective for both language modeling and parsing (\u00a7 SECREF6 ). Our generative model obtains (i) the best-known parsing results using a single supervised generative model and (ii) better perplexities in language modeling than state-of-the-art sequential LSTM language models. Surprisingly\u2014although in line with previous parsing results showing the effectiveness of generative models BIBREF7 , BIBREF14 \u2014parsing with the generative model obtains significantly better results than parsing with the discriminative model."]}
{"question_id": "4c7ec282697f4f6646eb1c19f46bbaf8670b0de6", "predicted_answer": "", "predicted_evidence": ["(2) A large-scale Baidu Baike corpus is introduced for entity recognition pre-training, which is of weekly supervised learning since there is no actual named entity label."]}
{"question_id": "07104dd36a0e7fdd2c211ad710de9a605495b697", "predicted_answer": "", "predicted_evidence": ["(1) BERT BIBREF14 is introduced as a feature extraction layer in place of BiLSTM. We also optimize the pre-training process of BERT by introducing a semantic-enhanced task."]}
{"question_id": "3e88fcc94d0f451e87b65658751834f6103b2030", "predicted_answer": "", "predicted_evidence": ["(3) Soft label embedding is proposed to effectively transmit information between entity recognition and relation extraction."]}
{"question_id": "c8cf20afd75eb583aef70fcb508c4f7e37f234e1", "predicted_answer": "", "predicted_evidence": ["The U.S. Bureau of Labor Statistics data summarized in Table TABREF3 contains statistics about the percentage of women participation in each occupation category. This data is also available for each individual occupation, which allows us to compute the frequency of women participation for each 12-quantile. We carried the same computation in the context of frequencies of translated female pronouns, and the resulting histograms are plotted side-by-side in Figure FIGREF29 . The data shows us that Google Translate outputs fail to follow the real-world distribution of female workers across a comprehensive set of job positions. The distribution of translated female pronouns is consistently inversely distributed, with female pronouns accumulating in the first 12-quantile. By contrast, BLS data shows that female participation peaks in the fourth 12-quantile and remains significant throughout the next ones."]}
{"question_id": "3567241b3fafef281d213f49f241071f1c60a303", "predicted_answer": "", "predicted_evidence": ["We can also visualize male, female, and gender neutral histograms side by side, in which context is useful to compare the dissimilar distributions of translated STEM and Healthcare occupations (Figures FIGREF16 and FIGREF17 respectively). The number of translated female pronouns among languages is not normally distributed for any of the individual categories in Table TABREF3 , but Healthcare is in many ways the most balanced category, which can be seen in comparison with STEM \u2013 in which male defaults are second to most prominent."]}
{"question_id": "d5d48b812576470edbf978fc18c00bd24930a7b7", "predicted_answer": "", "predicted_evidence": ["While it is possible to construct gender neutral sentences in two of the languages omitted in our experiments (namely Korean and Nepali), we have chosen to omit them for the following reasons:"]}
{"question_id": "643527e94e8eed1e2229915fcf8cd74d769173fc", "predicted_answer": "", "predicted_evidence": ["SLA modeling is actually the word level classification task, so we use area under the ROC curve (AUC) BIBREF32 and $F_{1}$ score BIBREF33 as evaluation metric."]}
{"question_id": "bfd55ae9630a08a9e287074fff3691dfbffc3258", "predicted_answer": "", "predicted_evidence": ["LR Here, we use the official baseline provided by Duolingo BIBREF29. It is a simple logistic regression using all the meta information and context information provided by datasets."]}
{"question_id": "3a06d40a4bf5ba6e26d9138434e9139a014deb40", "predicted_answer": "", "predicted_evidence": ["We first verify the advantages of our method in cases where the training data of the whole language-learning dataset is insufficient."]}
{"question_id": "641fe5dc93611411582e6a4a0ea2d5773eaf0310", "predicted_answer": "", "predicted_evidence": ["Lexical glue: Sentences that lexically link two concepts, such as \u201cto add means to increase\u201d, or \u201cheating means adding heat\u201d. This is an artificial category in our corpus, brought about by the need for explanation graphs to be explicitly lexically linked."]}
{"question_id": "7d34cdd9cb1c988e218ce0fd59ba6a3b5de2024a", "predicted_answer": "", "predicted_evidence": ["Finally, we examine the growth of the tablestore as it relates to the number of questions in the corpus. Figure 6 shows a monte-carlo simulation of the number of unique tablestore rows required to author explanations for specific corpus sizes. This relationship is strongly correlated (R=0.99) with an exponential proportional decrease. For this elementary science corpus, this asymptotes at approximately 6,000 unique table rows, and 10,000 questions, providing an estimate of the upper-bound of knowledge required in this domain, and the number of unique questions that can be generated within the scope of the elementary science curriculum."]}
{"question_id": "83db51da819adf6faeb950fe04b4df942a887fb5", "predicted_answer": "", "predicted_evidence": ["To examine the efficacy of each model, our methodology consisted of constructing three sets of data:"]}
{"question_id": "7e7471bc24970c6f23baff570be385fd3534926c", "predicted_answer": "", "predicted_evidence": ["In section SECREF2 we outline the nature of the data we have collected, a precise definition of an alert and how we processed the data for the neural network. In section SECREF3 we outline the definition of the models we evaluate and how they are defined. In section SECREF4 we outline our methodology in determining which models perform best given representative sensitivities of the engine. We attempt to give an approximation of the importance of each feature of the final model."]}
{"question_id": "ec5e84a1d1b12f7185183d165cbb5eae66d9833e", "predicted_answer": "", "predicted_evidence": ["Since the programs inception, we have greatly expanded our collection of training data, which is summarized below in Table TABREF3 . While we have accumulated over 1.11 million essay responses, which include many types of essays over a range of essay topics, student age ranges, styles of writing as well as a multitude of types of alerts, we find that many of them are mapped to the same set of words after applying our preprocessing steps. When we disregard duplicate responses after preprocessing, our training sample consists of only 866,137 unique responses."]}
{"question_id": "7f958017cbb08962c80e625c2fd7a1e2375f27a3", "predicted_answer": "", "predicted_evidence": ["To examine the efficacy of each model, our methodology consisted of constructing three sets of data:"]}
{"question_id": "4130651509403becc468bdbe973e63d3716beade", "predicted_answer": "", "predicted_evidence": ["In section SECREF2 we outline the nature of the data we have collected, a precise definition of an alert and how we processed the data for the neural network. In section SECREF3 we outline the definition of the models we evaluate and how they are defined. In section SECREF4 we outline our methodology in determining which models perform best given representative sensitivities of the engine. We attempt to give an approximation of the importance of each feature of the final model."]}
{"question_id": "6edef748370e63357a57610b5784204c9715c0b4", "predicted_answer": "", "predicted_evidence": ["To examine the efficacy of each model, our methodology consisted of constructing three sets of data:"]}
{"question_id": "6b302280522c350c4d1527d8c6ebc5b470f9314c", "predicted_answer": "", "predicted_evidence": ["This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses. Further study is required to approximate what percentage are Tier A and Tier B."]}
{"question_id": "7da138ec43a88ea75374c40e8491f7975db29480", "predicted_answer": "", "predicted_evidence": ["This method also lends itself to a method of approximating the number of alerts in a typical population. we use any engine produced to score a set of responses, which we call the threshold data, which consisted of a representative sample of 200,014 responses. Using these scores and given a percentage of responses we wish to flag for review, we produce a threshold value in which scores above this threshold level are considered alerts and those below are normal responses. This threshold data was scored using our best engine and the 200 responses that looked most like alerts were sent to be evaluated by our hand-scorers and while only 14 were found to be true alerts. Using the effectiveness of the model used, this suggests between 15 and 17 alerts may be in the entire threshold data set. We aggregated the estimates at various levels of sensitivity in combination with the efficacy of our best model to estimate that the rate of alerts is approximately 77 to 90 alerts per million responses. Further study is required to approximate what percentage are Tier A and Tier B."]}
{"question_id": "d5d4504f419862275a532b8e53d0ece16e0ae8d1", "predicted_answer": "", "predicted_evidence": ["Since a multimodal attribute extractor needs to be able to return values for attributes which occur in images as well as text, we cannot treat the problem as a labeling problem as is done in the existing approaches to attribute extraction. We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ). In general, the products at query time will not be in $a$2 , and we do not assume any fixed ontology for products, attributes, or values. We evaluate the performance on this task as the accuracy of the predicted value with the observed value, however since there may be multiple correct values, we also include hits@ $a$3 evaluation."]}
{"question_id": "f1e70b63c45ab0fc35dc63de089c802543e30c8f", "predicted_answer": "", "predicted_evidence": ["Motivated by this goal, we introduce the task of multimodal attribute extraction. Provided contextual information about an entity, in the form of any of the modes described above, along with an attribute query, the goal is to extract the corresponding value for that attribute. While attribute extraction on the domain of text has been well-studied BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , to our knowledge this is the first time attribute extraction using a combination of multiple modes of data has been considered. This introduces additional challenges to the problem, since a multimodal attribute extractor needs to be able to return values provided any kind of evidence, whereas modern attribute extractors treat attribute extraction as a tagging problem and thus only work when attributes occur as a substring of text."]}
{"question_id": "39d20b396f12f0432770c15b80dc0d740202f98d", "predicted_answer": "", "predicted_evidence": ["Since a multimodal attribute extractor needs to be able to return values for attributes which occur in images as well as text, we cannot treat the problem as a labeling problem as is done in the existing approaches to attribute extraction. We instead define the problem as following: Given a product $i$ and a query attribute $a$ , we need to extract a corresponding value $v$ from the evidence provided for $i$ , namely, a textual description of it ( $D_i$ ) and a collection of images ( $I_i$ ). For example, in Figure 1 , we observe the image and the description of a product, and examples of some attributes and values of interest. For training, for a set of product items $\\mathcal {I}$ , we are given, for each item $i \\in \\mathcal {I}$ , its textual description $D_i$ and the images $I_i$ , and a set $a$0 comprised of attribute-value pairs (i.e. $a$1 ). In general, the products at query time will not be in $a$2 , and we do not assume any fixed ontology for products, attributes, or values. We evaluate the performance on this task as the accuracy of the predicted value with the observed value, however since there may be multiple correct values, we also include hits@ $a$3 evaluation."]}
{"question_id": "4e0df856b39055a9ba801cc9c8e56d5b069bda11", "predicted_answer": "", "predicted_evidence": ["Motivated by this goal, we introduce the task of multimodal attribute extraction. Provided contextual information about an entity, in the form of any of the modes described above, along with an attribute query, the goal is to extract the corresponding value for that attribute. While attribute extraction on the domain of text has been well-studied BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , to our knowledge this is the first time attribute extraction using a combination of multiple modes of data has been considered. This introduces additional challenges to the problem, since a multimodal attribute extractor needs to be able to return values provided any kind of evidence, whereas modern attribute extractors treat attribute extraction as a tagging problem and thus only work when attributes occur as a substring of text."]}
{"question_id": "bbc6d0402cae16084261f8558cebb4aa6d5b1ea5", "predicted_answer": "", "predicted_evidence": ["Motivated by this goal, we introduce the task of multimodal attribute extraction. Provided contextual information about an entity, in the form of any of the modes described above, along with an attribute query, the goal is to extract the corresponding value for that attribute. While attribute extraction on the domain of text has been well-studied BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , to our knowledge this is the first time attribute extraction using a combination of multiple modes of data has been considered. This introduces additional challenges to the problem, since a multimodal attribute extractor needs to be able to return values provided any kind of evidence, whereas modern attribute extractors treat attribute extraction as a tagging problem and thus only work when attributes occur as a substring of text."]}
{"question_id": "a7e03d24549961b38e15b5386d9df267900ef4c8", "predicted_answer": "", "predicted_evidence": ["Given the large collections of unstructured and semi-structured data available on the web, there is a crucial need to enable quick and efficient access to the knowledge content within them. Traditionally, the field of information extraction has focused on extracting such knowledge from unstructured text documents, such as job postings, scientific papers, news articles, and emails. However, the content on the web increasingly contains more varied types of data, including semi-structured web pages, tables that do not adhere to any schema, photographs, videos, and audio. Given a query by a user, the appropriate information may appear in any of these different modes, and thus there's a crucial need for methods to construct knowledge bases from different types of data, and more importantly, combine the evidence in order to extract the correct answer."]}
{"question_id": "036c400424357457e42b22df477b7c3cdc2eefe9", "predicted_answer": "", "predicted_evidence": ["Given the large collections of unstructured and semi-structured data available on the web, there is a crucial need to enable quick and efficient access to the knowledge content within them. Traditionally, the field of information extraction has focused on extracting such knowledge from unstructured text documents, such as job postings, scientific papers, news articles, and emails. However, the content on the web increasingly contains more varied types of data, including semi-structured web pages, tables that do not adhere to any schema, photographs, videos, and audio. Given a query by a user, the appropriate information may appear in any of these different modes, and thus there's a crucial need for methods to construct knowledge bases from different types of data, and more importantly, combine the evidence in order to extract the correct answer."]}
{"question_id": "63eda2af88c35a507fbbfda0ec1082f58091883a", "predicted_answer": "", "predicted_evidence": ["Given the large collections of unstructured and semi-structured data available on the web, there is a crucial need to enable quick and efficient access to the knowledge content within them. Traditionally, the field of information extraction has focused on extracting such knowledge from unstructured text documents, such as job postings, scientific papers, news articles, and emails. However, the content on the web increasingly contains more varied types of data, including semi-structured web pages, tables that do not adhere to any schema, photographs, videos, and audio. Given a query by a user, the appropriate information may appear in any of these different modes, and thus there's a crucial need for methods to construct knowledge bases from different types of data, and more importantly, combine the evidence in order to extract the correct answer."]}
{"question_id": "fe6181ab0aecf5bc8c3def843f82e530347d918b", "predicted_answer": "", "predicted_evidence": ["In the other type of evaluation, we measure the relative improvement of a model against the Baseline model; Three professional raters are shown the input image and two captions (anonymized and randomly shuffled with respect to their left/right position) side-by-side. One of the captions is from a candidate model and the other always from Baseline. We ask for relative judgments on three dimensions \u2013 Informativeness, Correctness and Fluency, using their corresponding questions shown in Table TABREF32. Each of these dimensions allows a 5-way choice, shown below together with their corresponding scores:"]}
{"question_id": "0b1b8e1b583242e5be9b7be73160630a0d4a96b2", "predicted_answer": "", "predicted_evidence": ["In the experiments, we use Conceptual Captions BIBREF0, a large-scale captioning dataset that consists of images crawled from the Internet, with captions derived from corresponding Alt-text labels on the webpages. The training and validation splits have approximately 3.3M and 16K samples, respectively."]}
{"question_id": "830f9f9499b06fb4ac3ce2f2cf035127b4f0ec63", "predicted_answer": "", "predicted_evidence": ["We train Baseline using the Adam optimizer BIBREF34 on the training split of the Conceptual dataset for 3M iterations with the batch size of 4,096 and the learning rate of $3.2\\times 10^{-5}$. The learning rate is warmed up for 20 epochs and exponentially decayed by a factor of 0.95 every 25 epochs. Baseline$+(t)$ are obtained by fine-tuning Baseline on the merged dataset for 1M iterations, with the learning rate of $3.2\\times 10^{-7}$ and the same decaying factor. For OnPG, because its memory footprint is increased significantly due to the additional parameters for the rating estimator, we reduce the batch size for training this model by a 0.25 factor; the value of $b$ in Eq. (DISPLAY_FORM12) is set to the moving average of the rating estimates. During OffPG training, for each batch, we sample half of the examples from the Conceptual dataset and the other half from Caption-Quality dataset; $b$ is set to the average of the ratings in the dataset."]}
{"question_id": "a606bffed3bfeebd1b66125be580f908244e5d92", "predicted_answer": "", "predicted_evidence": ["Each model is evaluated by the average rating scores from 3 distinct raters. As a result, we obtain 3 values for each model in the range $[-1, 1]$, where a negative score means a performance degradation in the given dimension with respect to Baseline. For every human evaluation, we report confidence intervals based on bootstrap resampling BIBREF35."]}
{"question_id": "f8fe4049bea86d0518d1881f32049e60526d0f34", "predicted_answer": "", "predicted_evidence": ["In the analysis view, the user provides two tag sets INLINEFORM0 and INLINEFORM1 and two tag type sets INLINEFORM2 and INLINEFORM3 as input. The tag type difference view shows the text annotated in three panes: (i) the common tag types INLINEFORM4 , (ii) the tag types in INLINEFORM5 but not in INLINEFORM6 , and (iii) the tag types in INLINEFORM7 and not in INLINEFORM8 . Similarly, the tag difference view shows INLINEFORM9 , INLINEFORM10 and INLINEFORM11 in addition to precision, recall and F-measure values. The user selects a predicate to compute the metrics from the following predicates: (1) \u201cIntersection\u201d: a tag from INLINEFORM12 intersects in text with a tag in INLINEFORM13 , (2) \u201cExact\u201d: a tag from INLINEFORM14 exactly matches a tag in INLINEFORM15 , (3) \u201cA includes B\u201d: a tag from INLINEFORM16 contains a tag from INLINEFORM17 , and (4) \u201cB includes A\u201d: a tag from INLINEFORM18 contains a tag from INLINEFORM19 ."]}
{"question_id": "a9eb8039431e2cb885cfcf96eb58c0675b36b3bd", "predicted_answer": "", "predicted_evidence": ["The methodology to automatically generate our dataset is presented in Section SECREF3. Data preprocessing and linking, along with details on the generated dataset, are given in Section SECREF4. Section SECREF5 presents a baseline using deep neural networks."]}
{"question_id": "998fa38634000f2d7b52d16518b9e18e898ce933", "predicted_answer": "", "predicted_evidence": ["A predicted entity will be discarded entirely if it conflicts with an annotated one, since we aim to maximize the entities tagged using human-constructed resources as knowledge base."]}
{"question_id": "a82686c054b96f214521e468b17f0435e6cdf7cf", "predicted_answer": "", "predicted_evidence": ["SESAME consists of 87,769,158 tokens in total. The count and proportion of each entity tag (not a named entity, organization, person, location) is given in TABREF45."]}
{"question_id": "80d425258d027e3ca3750375d170debb9d92fbc6", "predicted_answer": "", "predicted_evidence": ["Knowledge sharing platforms such as Quora and Zhihu emerge as very convenient tools for acquiring knowledge. These question and answer (Q&A) platforms are newly emerged communities about knowledge acquisition, experience sharing and social networks services (SNS)."]}
{"question_id": "2ae66798333b905172e2c0954e9808662ab7f221", "predicted_answer": "", "predicted_evidence": ["The rating scores are within a range of INLINEFORM0 . We calculate min, Q1, median, Q3, max, mean, and mode about review count (see Table TABREF8 ). Because the number of received review may greatly influence the reliability of the review score. From Table TABREF8 we can see that many responses on Zhihu Live receive no review at all, which are useless for quality evaluation."]}
{"question_id": "9d80ad8cf4d5941a32d33273dc5678195ad1e0d2", "predicted_answer": "", "predicted_evidence": ["The authors thank Andre Martins for his advice regarding the word-level QE task."]}
{"question_id": "bd817a520a62ddd77e65e74e5a7e9006cdfb19b3", "predicted_answer": "", "predicted_evidence": ["One bi-directional gated recurrent unit (BiGRU; BIBREF10 ) layer with hidden size 200, where the forward and backward hidden states are concatenated and further normalized by layer normalization BIBREF11 ."]}
{"question_id": "c635295c2b77aaab28faecca3b5767b0c4ab3728", "predicted_answer": "", "predicted_evidence": ["For English-Czech, English-German (SMT), and English-German (NMT), removing POS tags makes the model more biased towards predicting \u201cOK\u201d tags, which leads to higher F1-OK scores and lower F1-BAD scores."]}
{"question_id": "7f8fc3c7d59aba80a3e7c839db6892a1fc329210", "predicted_answer": "", "predicted_evidence": ["As an example, the sentence \u201cparis hilton was once the toast of the town\u201d can show the potential of the proposed approach. The token \u201cparis\u201d with a LOC bias (0.6) and \u201chilton\u201d (global brand of hotels and resorts) with indicators leading to LOC (0.7) or ORG (0.1, less likely though). Furthermore, \u201ctown\u201d being correctly biased to LOC (0.7). The algorithm also suggests that the compound \u201cparis hilton\u201d is more likely to be a PER instead (0.7) and updates (correctly) the previous predictions. As a downside in this example, the algorithm misclassified \u201ctoast\u201d as LOC. However, in this same example, Stanford NER annotates (mistakenly) only \u201cparis\u201d as LOC. It is worth noting also the ability of the algorithm to take advantage of search engine capabilities. When searching for \u201cmiCRs0ft\u201d, the returned values strongly indicate a bias for ORG, as expected ( INLINEFORM0 = 0.2, INLINEFORM1 = 0.8, INLINEFORM2 = 0.0, INLINEFORM3 = 6, INLINEFORM4 = -56, INLINEFORM5 = 0.0, INLINEFORM6 = 0.5, INLINEFORM7 = 0.0, INLINEFORM8 = 5). More local organisations are also recognized correctly, such as \u201ckaufland\u201d (German supermarket), which returns the following metadata: INLINEFORM9 = 0.2, INLINEFORM10 = 0.4, INLINEFORM11 = 0.0, INLINEFORM12 = 2, INLINEFORM13 = -50, INLINEFORM14 = 0.1, INLINEFORM15 = 0.4, INLINEFORM16 = 0.0, INLINEFORM17 = 3."]}
{"question_id": "2d92ae6b36567e7edb6afdd72f97b06ac144fbdf", "predicted_answer": "", "predicted_evidence": ["Computer Vision (CV): Detecting Objects: Function Description (D.1): given a set of images INLINEFORM0 , the basic idea behind this component is to detect a specific object (denoted by a class INLINEFORM1 ) in each image. Thus, we query the web for a given term INLINEFORM2 and then extract the features from each image and try to detect a specific object (e.g., logos for ORG) for the top INLINEFORM3 images retrieved as source candidates. The mapping between objects and NER classes is detailed in tab:tbempirical."]}
{"question_id": "a5df7361ae37b9512fb57cb93efbece9ded8cab1", "predicted_answer": "", "predicted_evidence": ["As an example, the sentence \u201cparis hilton was once the toast of the town\u201d can show the potential of the proposed approach. The token \u201cparis\u201d with a LOC bias (0.6) and \u201chilton\u201d (global brand of hotels and resorts) with indicators leading to LOC (0.7) or ORG (0.1, less likely though). Furthermore, \u201ctown\u201d being correctly biased to LOC (0.7). The algorithm also suggests that the compound \u201cparis hilton\u201d is more likely to be a PER instead (0.7) and updates (correctly) the previous predictions. As a downside in this example, the algorithm misclassified \u201ctoast\u201d as LOC. However, in this same example, Stanford NER annotates (mistakenly) only \u201cparis\u201d as LOC. It is worth noting also the ability of the algorithm to take advantage of search engine capabilities. When searching for \u201cmiCRs0ft\u201d, the returned values strongly indicate a bias for ORG, as expected ( INLINEFORM0 = 0.2, INLINEFORM1 = 0.8, INLINEFORM2 = 0.0, INLINEFORM3 = 6, INLINEFORM4 = -56, INLINEFORM5 = 0.0, INLINEFORM6 = 0.5, INLINEFORM7 = 0.0, INLINEFORM8 = 5). More local organisations are also recognized correctly, such as \u201ckaufland\u201d (German supermarket), which returns the following metadata: INLINEFORM9 = 0.2, INLINEFORM10 = 0.4, INLINEFORM11 = 0.0, INLINEFORM12 = 2, INLINEFORM13 = -50, INLINEFORM14 = 0.1, INLINEFORM15 = 0.4, INLINEFORM16 = 0.0, INLINEFORM17 = 3."]}
{"question_id": "915e4d0b3cb03789a20380ead961d473cb95bfc3", "predicted_answer": "", "predicted_evidence": ["In the first step (A), we simply apply POS Tagging and Shallow Parsing to filter out tokens except for those tagged as INLINEFORM0 or INLINEFORM1 and their INLINEFORM2 (local context). Afterwards, we use the search engine (B) to query and cache (C) the top INLINEFORM3 texts and images associated to each term INLINEFORM4 , where INLINEFORM5 is the set resulting of the pre-processing step (A) for each (partial or complete) sentence INLINEFORM6 . This resulting data (composed of excerpts of texts and images from web pages) is used to predict a possible class for a given term. These outcomes are then used in the first two levels (D.1 and D.2) of our approach: the Computer Vision and Text Analytics components, respectively, which we introduce as follows:"]}
{"question_id": "c01a8b42fd27b0a3bec717ededd98b6d085a0f5c", "predicted_answer": "", "predicted_evidence": ["Computer Vision (CV): Detecting Objects: Function Description (D.1): given a set of images INLINEFORM0 , the basic idea behind this component is to detect a specific object (denoted by a class INLINEFORM1 ) in each image. Thus, we query the web for a given term INLINEFORM2 and then extract the features from each image and try to detect a specific object (e.g., logos for ORG) for the top INLINEFORM3 images retrieved as source candidates. The mapping between objects and NER classes is detailed in tab:tbempirical."]}
{"question_id": "8e113fd9661bc8af97e30c75a20712f01fc4520a", "predicted_answer": "", "predicted_evidence": ["In our experiments we compared our model with several seven different classifiers under different settings. For the ELMo system we used the mean-pooling of all contextualized word representations, i.e. character-based embedding representations and the output of the two layer LSTM resulting with a 1024 dimensional vector, and passed it through two deep dense ReLu activated layers with 256 and 64 units. Similarly, USE embeddings are trained with a Transformer encoder and output 512 dimensional vector for each sample, which is also passed through through two deep dense ReLu activated layers with 256 and 64 units. Both ELMo and USE embeddings retrieved from TensorFlow Hub. NBSVM system was modified according to BIBREF93 and trained with a ${10^{-3}}$ leaning rate for 5 epochs with Adam optimizer BIBREF100. FastText system was implemented by utilizing pre-trained embeddings BIBREF94 passed through a global max-pooling and a 64 unit fully connected layer. System was trained with Adam optimizer with learning rate ${0.1}$ for 3 epochs. XLnet model implemented using the base-cased model with 12 layers, 768 hidden units and 12 attention heads. Model trained with learning rate ${4 \\times 10^{-5}}$ using ${10^{-5}}$ weight decay for 3 epochs. We exploited both cased and uncased BERT-base models containing 12 layers, 768 hidden units and 12 attention heads. We trained models for 3 epochs with learning rate ${2 \\times 10^{-5}}$ using ${10^{-5}}$ weight decay. We trained RoBERTa model following the setting of BERT model. RoBERTa, XLnet and BERT models implemented using pytorch-transformers library ."]}
{"question_id": "35e0e6f89b010f34cfb69309b85db524a419c862", "predicted_answer": "", "predicted_evidence": ["We may identify three common FL expression forms namely, irony, sarcasm and metaphor. In this paper, figurative expressions, and especially ironic or sarcastic ones, are considered as a way of indirect denial. From this point of view, the interpretation and ultimately identification of the indirect meaning involved in a passage does not entail the cancellation of the indirectly rejected message and its replacement with the intentionally implied message (as advocated in BIBREF26, BIBREF27). On the contrary ironic/sarcastic expressions presupposes the processing of both the indirectly rejected and the implied message so that the difference between them can be identified. This view differs from the assumption that irony and sarcasm involve only one interpretation BIBREF28, BIBREF29. Holding that irony activates both grammatical / explicit as well as ironic / involved notions provides that irony will be more difficult to grasp than a non-ironic use of the same expression."]}
{"question_id": "992e67f706c728bc0e534f974c1656da10e7a724", "predicted_answer": "", "predicted_evidence": ["Many approaches attempt to create datasets using social media API\u2019s to automatically collect data rather than exploiting their system on benchmark datasets, with proven quality. To this end, it is impossible to be compared and evaluated BIBREF35, BIBREF37, BIBREF36."]}
{"question_id": "61e96abdc924c34c6b82a587168ea3d14fe792d1", "predicted_answer": "", "predicted_evidence": ["We may identify three common FL expression forms namely, irony, sarcasm and metaphor. In this paper, figurative expressions, and especially ironic or sarcastic ones, are considered as a way of indirect denial. From this point of view, the interpretation and ultimately identification of the indirect meaning involved in a passage does not entail the cancellation of the indirectly rejected message and its replacement with the intentionally implied message (as advocated in BIBREF26, BIBREF27). On the contrary ironic/sarcastic expressions presupposes the processing of both the indirectly rejected and the implied message so that the difference between them can be identified. This view differs from the assumption that irony and sarcasm involve only one interpretation BIBREF28, BIBREF29. Holding that irony activates both grammatical / explicit as well as ironic / involved notions provides that irony will be more difficult to grasp than a non-ironic use of the same expression."]}
{"question_id": "ee8a77cddbe492c686f5af3923ad09d401a741b5", "predicted_answer": "", "predicted_evidence": ["We may identify three common FL expression forms namely, irony, sarcasm and metaphor. In this paper, figurative expressions, and especially ironic or sarcastic ones, are considered as a way of indirect denial. From this point of view, the interpretation and ultimately identification of the indirect meaning involved in a passage does not entail the cancellation of the indirectly rejected message and its replacement with the intentionally implied message (as advocated in BIBREF26, BIBREF27). On the contrary ironic/sarcastic expressions presupposes the processing of both the indirectly rejected and the implied message so that the difference between them can be identified. This view differs from the assumption that irony and sarcasm involve only one interpretation BIBREF28, BIBREF29. Holding that irony activates both grammatical / explicit as well as ironic / involved notions provides that irony will be more difficult to grasp than a non-ironic use of the same expression."]}
{"question_id": "552b1c813f25bf39ace6cd5eefa56f4e4dd70c84", "predicted_answer": "", "predicted_evidence": ["For our experiments, we excluded submissions that did not have an image associated with them and solely used submission image and title data. We performed 2-way, 3-way, and 5-way classification for each of the three types of inputs: image only, text only, and multimodal (text and image)."]}
{"question_id": "1100e442e00c9914538a32aca7af994ce42e1b66", "predicted_answer": "", "predicted_evidence": ["A variety of datasets for fake news detection have been published in recent years. These are listed in Table TABREF1, along with their specific characteristics. When comparing these datasets, a few trends can be seen. Most of the datasets are small in size, which can be ineffective for current machine learning models that require large quantities of training data. Only four contain over half a million samples, with CREDBANK and FakeNewsCorpus being the largest with millions of samples BIBREF2. In addition, many of the datasets separate their data into a small number of classes, such as fake vs. true. However, fake news can be categorized into many different types BIBREF3. Datasets such as NELA-GT-2018, LIAR, and FakeNewsCorpus provide more fine-grained labels BIBREF4, BIBREF5. While some datasets include data from a variety of categories BIBREF6, BIBREF7, many contain data from specific areas, such as politics and celebrity gossip BIBREF8, BIBREF9, BIBREF10, BIBREF11. These data samples may contain limited styles of writing due to this categorization. Finally, most of the existing fake news datasets collect only text data, which is not the only mode that fake news can appear in. Datasets such as image-verification-corpus, Image Manipulation, BUZZFEEDNEWS, and BUZZFACE can be utilized for fake image detection, but contain small sample sizesBIBREF12, BIBREF13, BIBREF14. It can be seen from the table that compared to other existing datasets, Fakeddit contains a large quantity of data, while also annotating for three different types of classification labels (2-way, 3-way, and 5-way) and comparing both text and image data."]}
{"question_id": "82b93ecd2397e417e1e80f93b7cf49c7bd9aeec3", "predicted_answer": "", "predicted_evidence": ["User embeddings have also been used in: conversational agents BIBREF9 ; sentiment analysis BIBREF10 ; retweet prediction BIBREF11 ; predicting which topics a user is likely to tweet about, the accounts a user may want to follow, and the age, gender, political affiliation of Twitter users BIBREF12 ."]}
{"question_id": "2973fe3f5b4bf70ada02ac4a9087dd156cc3016e", "predicted_answer": "", "predicted_evidence": ["As shown in Table TABREF17 , BM25 performs better than TFIDF and CENTROID. CENTROID maps each query and document to a vector by taking a centroid of word embedding vectors, and the cosine similarity between two vectors is used for scoring and ranking documents. As mentioned earlier, this approach is not effective when multiple topics exist in a document. From the table, the embedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007, respectively. However, CENTROID provides scores lower than BM25 and SEM approaches."]}
{"question_id": "42269ed04e986ec5dc4164bf57ef306aec4a1ae1", "predicted_answer": "", "predicted_evidence": ["First, following Kusner et al. Kusner2015, documents are represented by normalized bag-of-words (BOW) vectors, i.e. if a word INLINEFORM0 appears INLINEFORM1 times in a document, the weight is DISPLAYFORM0 "]}
{"question_id": "31a3ec8d550054465e55a26b0136f4d50d72d354", "predicted_answer": "", "predicted_evidence": ["As shown in Table TABREF17 , BM25 performs better than TFIDF and CENTROID. CENTROID maps each query and document to a vector by taking a centroid of word embedding vectors, and the cosine similarity between two vectors is used for scoring and ranking documents. As mentioned earlier, this approach is not effective when multiple topics exist in a document. From the table, the embedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007, respectively. However, CENTROID provides scores lower than BM25 and SEM approaches."]}
{"question_id": "a7e1b13cc42bfe78d37b9c943de6288e5f00f01b", "predicted_answer": "", "predicted_evidence": ["The Word Mover's Distance makes use of word importance and the relatedness of words as we now describe."]}
{"question_id": "49cd18448101da146c3187a44412628f8c722d7b", "predicted_answer": "", "predicted_evidence": ["Our training data for this part was created by taking a random sample from Twitter and having it manually annotated on a 5-label basis to produce fully sentiment-labeled parse-trees, much like the Stanford sentiment treebank. The sample contains twenty thousand tweets with sentiment distribution as following:"]}
{"question_id": "e9260f6419c35cbd74143f658dbde887ef263886", "predicted_answer": "", "predicted_evidence": ["The five models output is concatenated and used as input for the various tasks, as described in SECREF27 ."]}
{"question_id": "2834a340116026d5995e537d474a47d6a74c3745", "predicted_answer": "", "predicted_evidence": ["We started with the training data passing our pipeline. We calculated the mean distribution for each entity on the training and testing datasets. We trained a logistic regression from a 5-label to a binary distribution and predicted a positive probability for each entity in the test set. This was used as a prior distribution for each entity, modeled as a Beta distribution. We then trained a logistic regression where the input is a concatenation of the 5-labels with the positive component of the probability distribution of the entity's sentiment and the output is a binary prediction for each tweet. Then we chose the label\u2014using the mean positive probability as a threshold. These predictions are submitted as task B. We obtained a macro-averaged recall score of INLINEFORM0 and accuracy of INLINEFORM1 ."]}
{"question_id": "bd53399be8ff59060792da4c8e42a7fc1e6cbd85", "predicted_answer": "", "predicted_evidence": ["We used ROUGE scores (F1), including ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L), as the evaluation metrics BIBREF12. ROUGE scores were calculated using the files2rouge toolkit."]}
{"question_id": "a7313c29b154e84b571322532f5cab08e9d49e51", "predicted_answer": "", "predicted_evidence": ["The decoder consists of $M$ layer decoder blocks. The inputs of the decoder are the output of the encoder $H_e^M$ and the output of the previous step of the decoder $\\lbrace y_1,...,y_{t-1} \\rbrace $. The output through the $M$ layer Transformer decoder blocks is defined as"]}
{"question_id": "cfe21b979a6c851bdafb2e414622f61e62b1d98c", "predicted_answer": "", "predicted_evidence": ["The encoder consists of $M$ layer encoder blocks. The input of the encoder is $X = \\lbrace x_i, x_2, ... x_L \\rbrace $. The output through the $M$ layer encoder blocks is defined as"]}
{"question_id": "3e3d123960e40bcb1618e11999bd2031ccc1d155", "predicted_answer": "", "predicted_evidence": ["The third type uses both the shared encoder and the extractor (\u00a7SECREF39). These models consist of the extractor, shared encoder, and decoder and also follow two steps: first, blackthe extractor extracts the important tokens from the source text, and second, blackthe shared encoder uses them as an input of the seq-to-seq model."]}
{"question_id": "2e37eb2a2a9ad80391e57acb53616eab048ab640", "predicted_answer": "", "predicted_evidence": ["This model combines the CIT and SA, so we also train two saliency models. The SA model is trained in an unsupervised way, the same as the CIT + SE model. The attention score $a_i^t \\in \\mathbb {R}^{L+K}$ is weighted by $S \\in \\mathbb {R}^{L+K}$ with Eq. (DISPLAY_FORM32). The loss function of the extractor is $L_\\mathrm {ext} = L_\\mathrm {sal}$, and that of the seq-to-seq model is $L_\\mathrm {abs} = L_\\mathrm {sum}$."]}
{"question_id": "54002c15493d4082d352a66fb9465d65bfe9ddca", "predicted_answer": "", "predicted_evidence": ["Fusion is a key research problem in multimodal studies, which integrates information extracted from different unimodal data into one compact multimodal representation. There is a clear connection between fusion and multimodal representation. We classify an approach into the fusion category if its focus is the architectures for integrating unimodal representations for particular a task."]}
{"question_id": "7caeb5ef6f2985b2cf383cd01765d247c936605f", "predicted_answer": "", "predicted_evidence": ["Removing SC and BN simplifies the model structure and speeds up both training and inference. Removing BN also solves the sequence level training problem discussed in Section SECREF3. More importantly, we always observe as good or better accuracy (WER) with the proposed simplified model structure."]}
{"question_id": "1fcd25e9a63a53451cac9ad2b8a1b529aff44a97", "predicted_answer": "", "predicted_evidence": ["Table TABREF21 compares WER of different model topologies for en_US. The training data contains 300 hours of speech and the testing data has 7 hours of speech. From Table TABREF21, we have the following observations:"]}
{"question_id": "049415676f8323f4af16d349f36fbcaafd7367ae", "predicted_answer": "", "predicted_evidence": ["We take a hypothesis reranking approach, which is widely used in large-scale domain classification for higher scalability BIBREF13, BIBREF4. Within the approach, a shortlister, which is a light-weighted domain classifier, suggests the most promising $k$ domains as the hypotheses. We train the shortlister along with the added pseudo labels, leveraging negative system responses, and self-distillation, which are described in Section SECREF3. Then a hypothesis reranker selects the final prediction from the $k$ hypotheses enriched with additional input features, which is described in Section SECREF4."]}
{"question_id": "fee498457774d9617068890ff29528e9fa05a2ac", "predicted_answer": "", "predicted_evidence": ["In this section, we show training and evaluation sets, and experiment results."]}
{"question_id": "c626637ed14dee3049b87171ddf326115e59d9ee", "predicted_answer": "", "predicted_evidence": ["Domain classification is a task that predicts the most relevant domain given an input utterance BIBREF0. It is becoming more challenging since recent conversational interaction systems such as Amazon Alexa, Google Assistant, and Microsoft Cortana support more than thousands of domains developed by external developers BIBREF3, BIBREF2, BIBREF4. As they are independently and rapidly developed without a centralized ontology, multiple domains have overlapped capabilities that can process the same utterances. For example, \u201cmake an elephant sound\u201d can be processed by AnimalSounds, AnimalNoises, and ZooKeeper domains."]}
{"question_id": "b160bfb341f24ae42a268aa18641237a4b3a6457", "predicted_answer": "", "predicted_evidence": ["Domains predicted with the highest confidences for $r$ times consecutively so that consistent top predictions are used as pseudo labels."]}
{"question_id": "c0120d339fcdb3833884622e532e7513d1b2c7dd", "predicted_answer": "", "predicted_evidence": ["The composition of the entire dataset and data created by augmenting the original data is shown in Table 3. We ensured the ratio between the utterance types is balanced so that common utterances which were not statistically well-represented in the corpus had enough training samples. Additionally, we increased the absolute count of utterances for wh-questions where our approach can be proven most effective. As a result, the class imbalance which was problematic for at the initial point, has been partially resolved."]}
{"question_id": "f52c9744a371104eb2677c181a7004f7a77d9dd3", "predicted_answer": "", "predicted_evidence": ["(3) $\\rightarrow $ the information about treadstone"]}
{"question_id": "867b1bb1e6a38de525be7757d49928a132d0dbd8", "predicted_answer": "", "predicted_evidence": ["The composition of the entire dataset and data created by augmenting the original data is shown in Table 3. We ensured the ratio between the utterance types is balanced so that common utterances which were not statistically well-represented in the corpus had enough training samples. Additionally, we increased the absolute count of utterances for wh-questions where our approach can be proven most effective. As a result, the class imbalance which was problematic for at the initial point, has been partially resolved."]}
{"question_id": "6167618e0c53964f3a706758bdf5e807bc5d7760", "predicted_answer": "", "predicted_evidence": ["The emergence of deep-learning architectures have led to the development of the VQA systems. We discuss the state-of-the-art methods with an overview in Table TABREF6."]}
{"question_id": "78a0c25b83cdeaeaf0a4781f502105a514b2af0e", "predicted_answer": "", "predicted_evidence": ["Pythia v1.0 BIBREF27: Pythia v1.0 is the award winning architecture for VQA Challenge 2018. The architecture is similar to Teney et al. BIBREF13 with reduced computations with element-wise multiplication, use of GloVe vectors BIBREF22, and ensemble of 30 models."]}
{"question_id": "08202b800a946b8283c2684e23b51c0ec1e8b2ac", "predicted_answer": "", "predicted_evidence": ["Vanilla VQA BIBREF0: Considered as a benchmark for deep learning methods, the vanilla VQA model uses CNN for feature extraction and LSTM or Recurrent networks for language processing. These features are combined using element-wise operations to a common feature, which is used to classify to one of the answers as shown in Fig. FIGREF4."]}
{"question_id": "00aea97f69290b496ed11eb45a201ad28d741460", "predicted_answer": "", "predicted_evidence": ["Pythia v1.0 BIBREF27: Pythia v1.0 is the award winning architecture for VQA Challenge 2018. The architecture is similar to Teney et al. BIBREF13 with reduced computations with element-wise multiplication, use of GloVe vectors BIBREF22, and ensemble of 30 models."]}
{"question_id": "4e1293592e41646a6f5f0cb00c75ee8de14eb668", "predicted_answer": "", "predicted_evidence": ["Visual7W: The Visual7W dataset BIBREF8 is also based on the MS-COCO dataset. It contains 47,300 COCO images with 327,939 question-answer pairs. The dataset also consists of 1,311,756 multiple choice questions and answers with 561,459 groundings. The dataset mainly deals with seven forms of questions (from where it derives its name): What, Where, When, Who, Why, How, and Which. It is majorly formed by two types of questions. The \u2018telling\u2019 questions are the ones which are text-based, giving a sort of description. The \u2018pointing\u2019 questions are the ones that begin with \u2018Which,\u2019 and have to be correctly identified by the bounding boxes among the group of plausible answers."]}
{"question_id": "15aeda407ae3912419fd89211cdb98989d9cde58", "predicted_answer": "", "predicted_evidence": ["Given the pretrained language representations, we construct episodes to compute the gradients and update our model in each training iteration. The training episode is formed by randomly selecting a subset of classes from the training set, then choosing a subset of examples within each selected class to act as the support set INLINEFORM0 with a subset of the remaining examples to serve as the query set INLINEFORM1 . Training with such episodes is achieved by feeding the support set INLINEFORM2 to the model and updating its parameters to minimize the loss in the query set INLINEFORM3 . We call this strategy as episode-based meta training. The adaptation of meta-learning using the MAML framework with pretrained language representations is summarized in Algorithm SECREF4 , called P-MAML. The use of episodes makes the training procedure more faithful to the test environment, thereby improving generalization. It is worth noting that there are exponentially many possible meta tasks to train the model with, thus making it difficult to overfit."]}
{"question_id": "c8b2fb9e0d5fb9014a25b88d559d93b6dceffbc0", "predicted_answer": "", "predicted_evidence": ["Existing approaches for few-shot learning are still plagued by problems, including imposed strong priors BIBREF5 , complex gradient transfer between tasks BIBREF6 , and fine-tuning of the target problem BIBREF7 . The approaches proposed by BIBREF8 and BIBREF9 , which combine non-parametric methods and metric learning, may provide possible solutions to these problems. The non-parametric methods allow novel examples to be rapidly assimilated without suffering from the effects of catastrophic overfitting. Such non-parametric models only need to learn the representation of the samples and the metric measure."]}
{"question_id": "c24f7c030010ad11e71ef4912fd79093503f3a8d", "predicted_answer": "", "predicted_evidence": ["The goal of MAML is to optimize the model parameters INLINEFORM0 such that the model can learn to adapt to new tasks with parameters via a few gradient steps on the training examples of the new tasks. The model is improved by considering how the test errors on the unseen test data from INLINEFORM1 change with respect to the parameters. The meta-objective across tasks is optimized using stochastic gradient descent (SGD). The model parameters INLINEFORM2 are updated as follows:"]}
{"question_id": "1d7b99646a1bc05beec633d7a3beb083ad1e8734", "predicted_answer": "", "predicted_evidence": ["For our big model, we used 6 encoder and decoder layers, $d_x = 1024$ , $d_z = 64$ , 16 attention heads, 4096 feed forward inner-layer dimensions, and $P_{dropout} = 0.3$ for EN-DE and $P_{dropout} = 0.1$ for EN-FR. When using relative position encodings, we used $k = 8$ , and used unique edge representations per layer. We trained for 300,000 steps on 8 P100 GPUs, and averaged the last 20 checkpoints, saved at 10 minute intervals."]}
{"question_id": "4d887ce7dc43528098e7a3d9cd13c6c36f158c53", "predicted_answer": "", "predicted_evidence": ["For our base model, we used 6 encoder and decoder layers, $d_x = 512$ , $d_z = 64$ , 8 attention heads, 1024 feed forward inner-layer dimensions, and $P_{dropout} = 0.1$ . When using relative position encodings, we used clipping distance $k = 16$ , and used unique edge representations per layer and head. We trained for 100,000 steps on 8 K40 GPUs, and did not use checkpoint averaging."]}
{"question_id": "d48b5e4a7cf1f96c5b939ba9b46350887c5e5268", "predicted_answer": "", "predicted_evidence": ["We also evaluated the impact of ablating each of the two relative position representations defined in section \"Conclusions\" , $a^V_{ij}$ in eq. ( 6 ) and $a^K_{ij}$ in eq. ( 7 ). Including relative position representations solely when determining compatibility between elements may be sufficient, but further work is needed to determine whether this is true for other tasks. The results are shown in Table 3 ."]}
{"question_id": "de344aeb089affebd15a8c370ae9ab5734e99203", "predicted_answer": "", "predicted_evidence": ["The past decade witnessed rapid growth and widespread usage of social media platforms by generating a significant amount of user-generated text. The user-generated texts contain high information content in the form of news, expression, or knowledge. Automatically mining information from user-generated data is unraveling a new field of research in Natural Language Processing (NLP) and has been a difficult task due to unstructured and noisy nature. In spite of the existing challenges, much research has been conducted on user-generated data in the field of information extraction, sentiment analysis, event extraction, user profiling and many more."]}
{"question_id": "84327a0a9321bf266e22d155dfa94828784595ce", "predicted_answer": "", "predicted_evidence": ["Overall: The macro average precision, recall, and f-score are calculated for all submitted runs."]}
{"question_id": "c2037887945abbdf959389dc839a86bc82594505", "predicted_answer": "", "predicted_evidence": ["This subsection describes the details of systems submitted for the shared task. Six teams have submitted their system details and those are described below in order of decreasing f-score."]}
{"question_id": "e9a0a69eacd554141f56b60ab2d1912cc33f526a", "predicted_answer": "", "predicted_evidence": ["The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances. Then, similar evaluation techniques are applied to the baseline system and the results are presented in Table TABREF29 ."]}
{"question_id": "5b2839bef513e5d441f0bb8352807f673f4b2070", "predicted_answer": "", "predicted_evidence": ["The precision, recall and f-score are calculated using the sklearn package of scikit-learn BIBREF15 . The macro average f-score is used to rank the submitted systems, because it independently calculates the metric for each classes and then takes the average hence treating all classes equally. Two different types of evaluation are considered and these are described below."]}
{"question_id": "2abf916bc03222d3b2a3d66851d87921ff35c0d2", "predicted_answer": "", "predicted_evidence": ["This subsection describes the details of systems submitted for the shared task. Six teams have submitted their system details and those are described below in order of decreasing f-score."]}
{"question_id": "a222dc5d804a7b453a0f7fbc1d6c1b165a3ccdd6", "predicted_answer": "", "predicted_evidence": ["In this paper, we present an open-source holistic NLG framework for the SW, named LD2NL, which facilitates the verbalization of the three key languages of the SW, i.e., RDF, OWL, and SPARQL into NL. Our framework is based on a bottom-up paradigm for verbalizing SW data. Additionally, LD2NL builds upon SPARQL2NL as it is open-source and the paradigm it follows can be reused and ported to RDF and OWL. Thus, LD2NL is capable of generating either a single sentence or a summary of a given resource, rule, or query. To validate our framework, we evaluated LD2NL using experts 66 in NLP and SW as well as 20 non-experts who were lay users or non-users of SW. The results suggest that LD2NL generates texts which can be easily understood by humans. The version of LD2NL used in this paper, all experimental results will be publicly available."]}
{"question_id": "7de0b2df60d3161dd581ed7915837d460020bc11", "predicted_answer": "", "predicted_evidence": ["Considering the mentioned problem requirements, we believe Extended Named Entities Hierarchy BIBREF8, containing 200 fine-grained categories tailored for Wikipedia articles, is the best fitting tag set."]}
{"question_id": "0a3a7e412682ce951329c37b06343d2114acad9d", "predicted_answer": "", "predicted_evidence": ["We have performed the evaluation in a 10 fold cross validation manner in each fold of which 80% of the data has been used for training, 10% for validation and model selection, and 10% for testing. In addition, classes with a frequency less than 20 in the dataset have been ignored in the train/test procedure."]}
{"question_id": "74cc0300e22f60232812019011a09df92bbec803", "predicted_answer": "", "predicted_evidence": ["The upper section of Table TABREF11 shows the Precision and Recall results for the patterns learned during bootstrapping. The Iter 0 row shows the performance of the patterns learned only from the original, annotated training data. The remaining rows show the results for the patterns learned from the unannotated texts during bootstrapping, added cumulatively. We show the results after each iteration of bootstrapping."]}
{"question_id": "865811dcf63a1dd3f22c62ec39ffbca4b182de31", "predicted_answer": "", "predicted_evidence": ["Table TABREF14 shows the number of patterns learned from the annotated data (Iter 0) and the number of new patterns added after each bootstrapping iteration. The first iteration dramatically increases the set of patterns, and more patterns are steadily added throughout the rest of bootstrapping process."]}
{"question_id": "9e378361b6462034aaf752adf04595ef56370b86", "predicted_answer": "", "predicted_evidence": ["Table TABREF14 shows the number of patterns learned from the annotated data (Iter 0) and the number of new patterns added after each bootstrapping iteration. The first iteration dramatically increases the set of patterns, and more patterns are steadily added throughout the rest of bootstrapping process."]}
{"question_id": "667dce60255d8ab959869eaf8671312df8c0004b", "predicted_answer": "", "predicted_evidence": ["The feeling responses may seem to lack argumentative merit, but previous work on argumentation describes situations in which such arguments can be effective, such as the use of emotive arguments to draw attention away from the facts, or to frame a discussion in a particular way BIBREF18 , BIBREF19 . Furthermore, work on persuasion suggest that feeling based arguments can be more persuasive in particular circumstances, such as when the hearer shares a basis for social identity with the source (speaker) BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 . However none of this work has documented the linguistic patterns that characterize the differences in these argument types, which is a necessary first step to their automatic recognition or classification. Thus the goal of this paper is to use computational methods for pattern-learning on conversational arguments to catalog linguistic expressions and stylistic properties that distinguish Factual from Emotional arguments in these on-line debate forums."]}
{"question_id": "d5e716c1386b6485e63075e980f80d44564d0aa2", "predicted_answer": "", "predicted_evidence": ["From the learned patterns, we derive some characteristic syntactic forms associated with the fact and feel that we use to discriminate between the classes. We observe distinctions between the way that different arguments are expressed, with respect to the technical and more opinionated terminologies used, which we analyze on the basis of grammatical forms and more direct syntactic patterns, such as the use of different prepositional phrases. Overall, we demonstrate how the learned patterns can be used to more precisely gather similarly-styled argument responses from a pool of unannotated responses, carrying the characteristics of factual and emotional argumentation style."]}
{"question_id": "1fd31fdfff93d65f36e93f6919f6976f5f172197", "predicted_answer": "", "predicted_evidence": ["The IAC includes 10,003 Quote-Response (Q-R) pairs with annotations for factual vs. feeling argument style, across a range of topics. Figure FIGREF4 shows the wording of the survey question used to collect the annotations. Fact vs. Feeling was measured as a scalar ranging from -5 to +5, because previous work suggested that taking the means of scalar annotations reduces noise in Mechanical Turk annotations BIBREF26 . Each of the pairs was annotated by 5-7 annotators. For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class. We did not use the responses with scores between -1 and 1 because they had a very weak Fact/Feeling assessment, which could be attributed to responses either containing aspects of both factual and feeling expression, or neither. The resulting set contains 3,466 fact and 2,382 feeling posts. We randomly partitioned the fact/feel responses into three subsets: a training set with 70% of the data (2,426 fact and 1,667 feeling posts), a development (tuning) set with 20% of the data (693 fact and 476 feeling posts), and a test set with 10% of the data (347 fact and 239 feeling posts). For the bootstrapping method, we also used 11,560 responses from the unannotated data."]}
{"question_id": "d2d9c7177728987d9e8b0c44549bbe03c8c00ef2", "predicted_answer": "", "predicted_evidence": ["Lack of benchmarks: Due to the low discoverability and the lack of research in the field, there are no publicly available benchmarks or leader boards to new compare machine translation techniques to."]}
{"question_id": "6657ece018b1455035421b822ea2d7961557c645", "predicted_answer": "", "predicted_evidence": ["We trained translation models for two established NMT architectures for each language, namely, ConvS2S and Transformer. As the purpose of this work is to provide a baseline benchmark, we have not performed significant hyperparameter optimization, and have left that as future work."]}
{"question_id": "175cddfd0bcd77b7327b62f99e57d8ea93f8d8ba", "predicted_answer": "", "predicted_evidence": ["Lack of benchmarks: Due to the low discoverability and the lack of research in the field, there are no publicly available benchmarks or leader boards to new compare machine translation techniques to."]}
{"question_id": "f0afc116809b70528226d37190e8e79e1e9cd11e", "predicted_answer": "", "predicted_evidence": ["Reproducibility: The data and code of existing research are rarely shared, which means researchers cannot reproduce the results properly. Examples of papers that do not publicly provide their data and code are described in Section SECREF4 ."]}
{"question_id": "3588988f2230f3329d7523fbb881b20bf177280d", "predicted_answer": "", "predicted_evidence": ["The new form of the ontology was produced automatically, using patterns that searched the definition strings for relation names (e.g., results_in), sentence breaks, and words introducing secondary clauses (e.g., \u201cthat\u201d, \u201cwhich\u201d). Some sentences of the original definition strings that did not include declared relation names (e.g., \u201cThe virus affects...and humans\u201d in the `Definition' string of Rift Valley Fever) were discarded, because they could not be automatically converted to appropriate owl statements."]}
{"question_id": "78f8dad0f1acf024f69b3218b2d204b8019bb0d2", "predicted_answer": "", "predicted_evidence": ["Table TABREF22 reports the maximum F1 scores achieved by each method on the Satisfaction test set. For the model uncertainty approach, we tested two variants: (a) predict a mistake when the confidence in the top rated response is below some threshold INLINEFORM0 , and (b) predict a mistake when the gap between the top two rated responses is below the threshold INLINEFORM1 . We used the best-performing standalone Dialogue model (one trained on the full 131k training examples) for assessing uncertainty and tuned the thresholds to achieve maximum F1 score. For the user satisfaction approach, we trained our dialogue agent on just the Satisfaction task. Finally, we also report the performance of a regular-expression-based method which we used during development, based on common ways of expressing dissatisfaction that we observed in our pilot studies, see Appendix SECREF12 for details."]}
{"question_id": "73a5783cad4ed468a8dbb31b5de2c618ce351ad1", "predicted_answer": "", "predicted_evidence": ["Throughout this section, we use the ranking metric hits@X/Y, or the fraction of the time that the correct candidate response was ranked in the top X out of Y available candidates; accuracy is another name for hits@1/Y. Statistical significance for improvement over baselines is assessed with a two-sample one-tailed T-test."]}
{"question_id": "1128a600a813116cba9a2cf99d8568ae340f327a", "predicted_answer": "", "predicted_evidence": ["We first conduct our experiment on classification tasks."]}
{"question_id": "d64fa192a7e9918c6a22d819abad581af0644c7d", "predicted_answer": "", "predicted_evidence": ["The meta network can be considered as off-the-shelf knowledge and then be used for unseen new tasks."]}
{"question_id": "788f70a39c87abf534f4a9ee519f6e5dbf2543c2", "predicted_answer": "", "predicted_evidence": ["With the meta network, our model can use quite a few parameters to achieve the state-of-the-art performances."]}
{"question_id": "3d1ad8a4aaa2653d0095bafba74738bd20795acf", "predicted_answer": "", "predicted_evidence": ["We use three data sets related to the hate speech."]}
{"question_id": "ec54ae2f4811196fcaafa45e76130239e69995f9", "predicted_answer": "", "predicted_evidence": ["Our main contributions are:"]}
{"question_id": "5102dc911913e9ca0311253e44fd31c73eed0a57", "predicted_answer": "", "predicted_evidence": ["In the first set of experiments, we represented the text with word embeddings (sparse TF-IDF BIBREF31 or dense word2vec BIBREF32, and ELMo BIBREF33). We utilise the gensim library BIBREF34 for word2vec model, the scikit-learn for TFIDF, and the ELMo pretrained model from TensorFlow Hub. We compared different classification models using these word embeddings. The results are presented in Table TABREF32."]}
{"question_id": "5752c8d333afc1e6c666b18d1477c8f669b7a602", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF45 shows the performances of datasets in Multi-Domain scenario with different INLINEFORM0 . Compared to INLINEFORM1 , our model can achieve considerable improvements when INLINEFORM2 as more samples combinations are available. However, there are no more salient gains as INLINEFORM3 gets larger and potential noises from other tasks may lead to performance degradations. For a trade-off between efficiency and effectiveness, we determine INLINEFORM4 as the optimal value for our experiments."]}
{"question_id": "fcdafaea5b1c9edee305b81f6865efc8b8dc50d3", "predicted_answer": "", "predicted_evidence": ["As Table TABREF35 shows, we select five benchmark datasets for text classification and design three experiment scenarios to evaluate the performances of our model."]}
{"question_id": "91d4fd5796c13005fe306bcd895caaed7fa77030", "predicted_answer": "", "predicted_evidence": ["Most previous multi-task learning models BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 belongs to Type-I or Type-II. The total number of input samples is INLINEFORM0 , where INLINEFORM1 are the sample numbers of each task."]}
{"question_id": "27d7a30e42921e77cfffafac5cb0d16ce5a7df99", "predicted_answer": "", "predicted_evidence": ["Recently neural network based models have obtained substantial interests in many natural language processing tasks for their capabilities to represent variable-length text sequences as fix-length vectors, for example, Neural Bag-of-Words (NBOW), Recurrent Neural Networks (RNN), Recursive Neural Networks (RecNN) and Convolutional Neural Network (CNN). Most of them first map sequences of words, n-grams or other semantic units into embedding representations with a pre-trained lookup table, then fuse these vectors with different architectures of neural networks, and finally utilize a softmax layer to predict categorical distribution for specific classification tasks. For recurrent neural network, input vectors are absorbed one by one in a recurrent way, which makes RNN particularly suitable for natural language processing tasks."]}
{"question_id": "7561bd3b8ba7829b3a01ff07f9f3e93a7b8869cc", "predicted_answer": "", "predicted_evidence": ["Table TABREF19 contains the results. LEAD-5 achieves less than 20 for ROUGE-L as well as ROUGE-1 and less than $3.5$ for ROUGE-2. Taking only 3 sentences leads to even worse results: below 13 and 3 respectively. Unlike in other datasets, these results are significantly outperformed by all other extractive models but surprisingly, abstractive models perform worse on average. This demonstrates the difficulty of the task in GameWikiSum compared to nallapati2016abstractive and graff2003."]}
{"question_id": "a3ba21341f0cb79d068d24de33b23c36fa646752", "predicted_answer": "", "predicted_evidence": ["To ensure consistent results across all comparative experiments, extractive models generate summaries of the same length as reference summaries. In realistic scenarios, summary lengths are not pre-defined and can be adjusted to produce different types of summaries (e.g., short, medium or long). We do not explicitly constrain the output length for abstractive models, as each summary is auto-regressively generated."]}
{"question_id": "96295e1fe8713417d2b4632438a95d23831fbbdc", "predicted_answer": "", "predicted_evidence": ["Journalists are paid to write complete reviews for various types of entertainment products, describing different aspects thoroughly. Reviewed aspects in video games include the gameplay, richness, and diversity of dialogues, or the soundtrack. Compared to usual reviews written by users, these are assumed to be of higher-quality and longer."]}
{"question_id": "5bfbc9ca7fd41be9627f6ef587bb7e21c7983be0", "predicted_answer": "", "predicted_evidence": ["https://www-nlpir.nist.gov/projects/duc/guidelines.html http://www.nist.gov/tac/"]}
{"question_id": "5181527e6a61a9a192db5f8064e56ec263c42661", "predicted_answer": "", "predicted_evidence": ["On the one hand, the system is able to answer complex out-of-context questions such as \u201cWhat are the capitals of the countries of the Iberian Peninsula?\", by correctly answering the list of capitals: \u201cAndorra la Vella, Gibraltar, Lisbon, Madrid\"."]}
{"question_id": "334aa5540c207768931a0fe78aa4981a895ba37c", "predicted_answer": "", "predicted_evidence": ["The Search QA system uses an internal knowledge base, which finely indexes data using Elasticsearch. It is powered by Wikidata and enriched by Wikipedia, especially to calculate a Page-Rank BIBREF10 on each entity. This QA system first determines the potential named entities in the question (i.e. subjects, predicates, and types of subjects). Second, it constructs a correlation matrix by looking for the triplets in Wikidata that link these entities. This matrix is filtered according the coverage of the question and the relevance of each entity in order to find the best answer."]}
{"question_id": "b8bbdc3987bb456739544426c6037c78ede01b77", "predicted_answer": "", "predicted_evidence": ["In the remainder of this section, we explain the components of our system."]}
{"question_id": "fea9b4d136156f23a88e5c7841874a467f2ba86d", "predicted_answer": "", "predicted_evidence": ["We further investigate the difficulty of the task for the encoder and decoder by comparing their convergence speed. Encoder (decoder) that needs more training time indicates the task that encoder (decoder) handles is more difficult."]}
{"question_id": "4e59808a7f73ac499b9838d3c0ce814196a02473", "predicted_answer": "", "predicted_evidence": ["The decoder is more sensitive to the input noise than the encoder. We randomly add different level of noise to the input of the encoder and decoder respectively during inference, and find that adding noise to the input of the decoder leads to better accuracy drop than that of the encoder."]}
{"question_id": "7ef7a5867060f91eac8ad857c186e51b767c734b", "predicted_answer": "", "predicted_evidence": ["The decoder is more sensitive to the input noise than the encoder. We randomly add different level of noise to the input of the encoder and decoder respectively during inference, and find that adding noise to the input of the decoder leads to better accuracy drop than that of the encoder."]}
{"question_id": "0b10cfa61595b21bf3ff13b4df0fe1c17bbbf4e9", "predicted_answer": "", "predicted_evidence": ["The training is to minimize loss function $L_{rc}$, denoted as Equation (DISPLAY_FORM34), where $R^{\\prime }$ indicates the real relation type."]}
{"question_id": "67104a5111bf8ea626532581f20b33b851b5abc1", "predicted_answer": "", "predicted_evidence": ["The two hyperparameters $K$ and $MASK^{rc}$ in the model will be further studied in Section SECREF47. Within a fixed number of epochs, we select the model corresponding to the best relation performance on development dataset."]}
{"question_id": "1d40d177c5e410cef1142ec9a5fab9204db22ae1", "predicted_answer": "", "predicted_evidence": ["In this section, we compare the proposed model with NER, RC and joint models. Dataset description and evaluation metrics are first introduced in the following contents, followed by the experimental settings and results."]}
{"question_id": "344238de7208902f7b3a46819cc6d83cc37448a0", "predicted_answer": "", "predicted_evidence": ["Dataset descriptions. The earliest dataset published in this domain was compiled by Spertus smokey. It consisted of INLINEFORM0 private messages written in English from the web-masters of controversial web resources such as NewtWatch. These messages were marked as flame (containing insults or abuse; INLINEFORM1 ), maybe flame ( INLINEFORM2 ), or okay ( INLINEFORM3 ). We refer to this dataset as data-smokey. Yin et al. Yin09detectionof constructed three English datasets and annotated them for harassment, which they defined as \u201csystematic efforts by a user to belittle the contributions of other users\". The samples were taken from three social media platforms: Kongregate ( INLINEFORM4 posts; INLINEFORM5 harassment), Slashdot ( INLINEFORM6 posts; INLINEFORM7 harassment), and MySpace ( INLINEFORM8 posts; INLINEFORM9 harassment). We refer to the three datasets as data-harass. Several datasets have been compiled using samples taken from portals of Yahoo!, specifically the News and Finance portals. Djuric et al. djuric created a dataset of INLINEFORM10 user comments in English from the Yahoo! Finance website that were editorially labeled as either hate speech ( INLINEFORM11 ) or clean (data-yahoo-fin-dj). Nobata et al. nobata produced four more datasets with comments from Yahoo! News and Yahoo! Finance, each labeled abusive or clean: 1) data-yahoo-fin-a: INLINEFORM12 comments, 7.0% abusive; 2) data-yahoo-news-a: INLINEFORM13 comments, 16.4% abusive; 3) data-yahoo-fin-b: INLINEFORM14 comments, 3.4% abusive; and 4) data-yahoo-news-b: INLINEFORM15 comments, 9.7% abusive."]}
{"question_id": "56bbca3fe24c2e9384cc57f55f35f7f5ad5c5716", "predicted_answer": "", "predicted_evidence": ["In this section, we review the approaches to abuse detection that utilize or rely solely on neural networks. We also include methods that use embeddings generated from a neural architecture within an otherwise non-neural framework."]}
{"question_id": "4c40fa01f626def0b69d1cb7bf9181b574ff6382", "predicted_answer": "", "predicted_evidence": ["In what follows, we review several commonly-used datasets manually annotated for abuse."]}
{"question_id": "71b29ab3ddcdd11dcc63b0bb55e75914c07a2217", "predicted_answer": "", "predicted_evidence": ["Remarks. In their study, Ross et al. ross stressed the difficulty in reliably annotating abuse, which stems from multiple factors, such as the lack of \u201cstandard\u201d definitions for the myriad types of abuse, differences in annotators' cultural background and experiences, and ambiguity in the annotation guidelines. That said, Waseem et al. W17-3012 and Nobata et al. nobata observed that annotators with prior expertise provide good-quality annotations with high levels of agreement. We note that most datasets contain discrete labels only; abuse detection systems trained on them would be deprived of the notion of severity, which is vital in real-world settings. Also, most datasets cover few types of abuse only. Salminen et al. salminen2018anatomy suggest fine-grained annotation schemes for deeper understanding of abuse; they propose 29 categories that include both types of abuse and their targets (e.g., humiliation, religion)."]}
{"question_id": "22225ba18a6efe74b1315cc08405011d5431498e", "predicted_answer": "", "predicted_evidence": ["We are grateful to Nikolaos Tsileponis (University of Manchester) and Mahmoud El-Haj (Lancaster University) for access to headlines in the corpus of financial news articles collected from Factiva. This research was supported at Lancaster University by an EPSRC PhD studentship."]}
{"question_id": "bd3562d2b3c162e9d27404d56b77e15f707d8b0f", "predicted_answer": "", "predicted_evidence": ["The main evaluation over the test data is based on the best performing SVR and the two BLSTM models once trained on all of the training data. The result table TABREF28 shows three columns based on the three evaluation metrics that the organisers have used. Metric 1 is the original metric, weighted cosine similarity (the metric used to evaluate the final version of the results, where we were ranked 5th; metric provided on the task website). This was then changed after the evaluation deadline to equation EQREF25 (which we term metric 2; this is what the first version of the results were actually based on, where we were ranked 4th), which then changed by the organisers to their equation as presented in BIBREF18 (which we term metric 3 and what the second version of the results were based on, where we were ranked 5th)."]}
{"question_id": "9c529bd3f7565b2178a79aae01c98c90f9d372ad", "predicted_answer": "", "predicted_evidence": ["We additionally trained a word2vec BIBREF10 word embedding model on a set of 189,206 financial articles containing 161,877,425 tokens, that were manually downloaded from Factiva. The articles stem from a range of sources including the Financial Times and relate to companies from the United States only. We trained the model on domain specific data as it has been shown many times that the financial domain can contain very different language."]}
{"question_id": "cf82251a6a5a77e29627560eb7c05c3eddc20825", "predicted_answer": "", "predicted_evidence": ["In lines 7-9 of Table TABREF40 we consider lattice-rescoring the baseline output, using three models debiased on the handcrafted data."]}
{"question_id": "b1fe6a39b474933038b44b6d45e5ca32af7c3e36", "predicted_answer": "", "predicted_evidence": ["Finally, we wish to reduce gender bias without reducing translation performance. We report BLEU BIBREF22 on separate, general test sets for each language pair. WinoMT is designed to work without target language references, and so it is not possible to measure translation performance on this set by measures such as BLEU."]}
{"question_id": "919681faa9731057b3fae5052b7da598abd3e04b", "predicted_answer": "", "predicted_evidence": ["$\\theta ^{B}_{j}$ are the converged parameters of the original biased model, and $\\theta ^{DB}_j$ are the current debiased model parameters. $F_j=\\mathbb {E} \\big [ \\nabla ^2 L(\\theta ^{B}_j)\\big ] $, a Fisher information estimate over samples from the biased data under the biased model. We apply EWC when performance on the original validation set drops, selecting hyperparameter $\\lambda $ via validation set BLEU."]}
{"question_id": "2749fb1725a2c4bdba5848e2fc424a43e7c4be51", "predicted_answer": "", "predicted_evidence": ["This lets us compare four related sets for gender debiasing adaptation, as illustrated in Figure FIGREF11:"]}
{"question_id": "7239c02a0dcc0c3c9d9cddb5e895bcf9cfcefee6", "predicted_answer": "", "predicted_evidence": ["The following models rely on (freely-available) data that has more structure than raw text."]}
{"question_id": "9dcc10a4a325d4c9cb3bb8134831ee470be47e93", "predicted_answer": "", "predicted_evidence": ["While these consistency scores are a promising sign, they could also be symptomatic of a set of evaluations that are all limited in the same way. The inter-rater agreement is only reported for one of the 8 evaluations considered (MPQA, INLINEFORM0 BIBREF34 ), and for MR, SUBJ and TREC, each item is only rated by one or two annotators to maximise coverage. Table TABREF15 illustrates why this may be an issue for the unsupervised evaluations; the notion of sentential 'relatedness' seems very subjective. It should be emphasised, however, that the tasks considered in this study are all frequently used for evaluation, and, to our knowledge, there are no existing benchmarks that overcome these limitations."]}
{"question_id": "31236a876277c6e1c80891a3293c105a1b1be008", "predicted_answer": "", "predicted_evidence": ["Differences in resource requirements As shown in Table TABREF14 , different models require different resources to train and use. This can limit their possible applications. For instance, while it was easy to make an online demo for fast querying of near neighbours in the CBOW and FastSent spaces, it was not practical for other models owing to memory footprint, encoding time and representation dimension."]}
{"question_id": "19ebfba9aa5a9596b09a0cfb084ff8ebf24a3b91", "predicted_answer": "", "predicted_evidence": ["Differences between supervised and unsupervised performance Many of the best performing models on the supervised evaluations do not perform well in the unsupervised setting. In the SkipThought, S(D)AE and NMT models, the cost is computed based on a non-linear decoding of the internal sentence representations, so, as also observed by BIBREF40 , the informative geometry of the representation space may not be reflected in a simple cosine distance. The log-linear models generally perform better in this unsupervised setting."]}
{"question_id": "2288f567d2f5cfbfc5097d8eddf9abd238ffbe25", "predicted_answer": "", "predicted_evidence": ["It is not surprised to find that Baseline_1 shows the poorest performance, which it just considers the probability information, ignores the contextual link. The perfor-mance of Baseline_2 is better than that of \u201cBaseline_1\u201d. This can be mainly credited to the ability of abundant lexical and syntax features. Our parser shows better per-formance than Baselin_2 because the most of features we use are textual type fea-tures, which are convenient for the maximum entropy model. Though the textual type features can turn into numeric type according to hashcode of string, it is incon-venient for Support Vector Machine because the hashcode of string is not continu-ous. According the performance of the parser, we find that the connective identifying can achieve higher precision and recall rate. In addition, the precision and recall rate of identifying Arg2 is higher than that of identifying Arg1 because Arg2 has stronger syntax link with connective compared to Arg1. The sense has three layers: class, type and subtype."]}
{"question_id": "caebea05935cae1f5d88749a2fc748e62976eab7", "predicted_answer": "", "predicted_evidence": ["In our experiments, we make use of the Section 02-21 in the PDTB as training set, Section 22 as testing set. All of components adopt maximum entropy model. In order to evaluate the performance of the discourse parser, we compare it with other approaches: (1) Baseline_1, which applies the probability information. The connective identifier predicts the connective according the frequency of the connec-tive in the train set. The arguments identifier takes the immediately previous sentence in which the connective appears as Arg1 and the text span after the connective but in the same sentence with connective as Arg2. The non-explicit identifier labels the ad-jacent sentences according to the frequency of the non-explicit relation. (2) Base-line_2, which is the parser using the Support Vector Maching as the train and predic-tion model with numeric type feature from the hashcode of the textual type feature."]}
{"question_id": "e381f1811774806be109f9b05896a2a3c5e1ef43", "predicted_answer": "", "predicted_evidence": ["Automated deriving discourse relation from free text is a challenging but im-portant problem. The shallow discourse parsing is very useful in the text summariza-tion BIBREF0 , opinion analysis BIBREF1 and natural language generation. Shallow discourse parser is the system of parsing raw text into a set of discourse relations between two adjacent or non-adjacent text spans. Discourse relation is composed of a discourse connective, two arguments of the discourse connective and the sense of the discourse connective. Discourse connective signals the explicit dis-course relation, but in non-explicit discourse relation, a discourse connective is omit-ted. Two arguments of the discourse connective, Arg1 and Arg2, which are the two adjacent or non-adjacent text spans connecting in the discourse relation. The sense of the discourse connective characterizes the nature of the discourse relations. The following discourse relation annotation is taken from the document in the PDTB. Arg1 is shown in italicized, and Arg2 is shown in bold. The discourse connective is underlined."]}
{"question_id": "9eec16e560f9ccafd7ba6f1e0db742b330b42ba9", "predicted_answer": "", "predicted_evidence": ["It is not surprised to find that Baseline_1 shows the poorest performance, which it just considers the probability information, ignores the contextual link. The perfor-mance of Baseline_2 is better than that of \u201cBaseline_1\u201d. This can be mainly credited to the ability of abundant lexical and syntax features. Our parser shows better per-formance than Baselin_2 because the most of features we use are textual type fea-tures, which are convenient for the maximum entropy model. Though the textual type features can turn into numeric type according to hashcode of string, it is incon-venient for Support Vector Machine because the hashcode of string is not continu-ous. According the performance of the parser, we find that the connective identifying can achieve higher precision and recall rate. In addition, the precision and recall rate of identifying Arg2 is higher than that of identifying Arg1 because Arg2 has stronger syntax link with connective compared to Arg1. The sense has three layers: class, type and subtype."]}
{"question_id": "d788076c0d19781ff3f6525bd9c05b0ef0ecd0f1", "predicted_answer": "", "predicted_evidence": ["On this step, we adopt the head-based thinking BIBREF12 , which turns the problem of identifying arguments of discourse connective into identifying the head and end of the arguments. First, we need to extract the candidates of arguments. To reduce the Arg1 candidates space, we only consider words with appropriate part-of-speech (all verbs, common nouns, adjectives) and within 10 \u201dsteps\u201d between word and connec-tive as candidates, where a step is either a sentence boundary or a dependency link. Only words in the same sentence with the connective are considered for Arg2 candi-dates. Second, we need to choose the best candidate as the head of Arg1 and Arg2. In the end, we need to obtain the arguments span according head and end of argu-ments on the constituent tree. The table 2 shows the feature we use. The table 3 shows the procedure of the arguments identifier."]}
{"question_id": "ec70c7c560e08cff2820bad93f5216bc0a469f5a", "predicted_answer": "", "predicted_evidence": ["Concretely, we take turns choosing one domain and use its training data to train the basic model. Then, we use the testing data of the remaining domains to evaluate the model with the automatic metric ROUGE BIBREF25"]}
{"question_id": "940a16e9db8be5b5f4e67d9c7622b3df99ac10a5", "predicted_answer": "", "predicted_evidence": ["Concretely, we take turns choosing one domain and use its training data to train the basic model. Then, we use the testing data of the remaining domains to evaluate the model with the automatic metric ROUGE BIBREF25"]}
{"question_id": "0b1cc6c0de286eb724b1fd18dbc93e67ab89a236", "predicted_answer": "", "predicted_evidence": ["Analysis: To address the multi-domain learning task and the adaptation to new domains, Model$^{II}_{BERT}$, Model$^{III}_{Tag}$, Model$^{IV}_{Meta}$ take different angles. Specifically, Model$^{II}_{BERT}$ utilizes a large-scale pre-trained model while Model$^{III}_{Tag}$ proposes to introduce domain type information explicitly. Lastly, Model$^{IV}_{Meta}$ is designed to update parameters more consistently, by adjusting the gradient direction of the main domain A with the auxiliary domain B during training. This mechanism indeed purifies the shared feature space via filtering out the domain-specific features which only benefit A."]}
{"question_id": "1c2d4dc1e842b962c6407d6436f3dc73dd44ce55", "predicted_answer": "", "predicted_evidence": ["We briefly outline connections and differences to the following related lines of research."]}
{"question_id": "654306d26ca1d9e77f4cdbeb92b3802aa9961da1", "predicted_answer": "", "predicted_evidence": ["Comparing M-BERT and FinBERT, we find that the language-specific models outperform the multilingual models across the full range of training data sizes for both datasets. For news, the four BERT variants have broadly similar learning curves, with the absolute advantage for FinBERT models ranging from 3% points for 1K examples to just over 1% point for 100K examples, and relative reductions in error from 20% to 13%. For online discussion, the differences are much more pronounced, with M-BERT models performing closer to the FastText baseline than to FinBERT. Here the language-specific BERT outperforms the multilingual by over 20% points for the smallest training data and maintains a 5% point absolute advantage even with 100,000 training examples, halving the error rate of the multilingual model for the smallest training set and maintaining an over 20% relative reduction for the largest."]}
{"question_id": "5a7d1ae6796e09299522ebda7bfcfad312d6d128", "predicted_answer": "", "predicted_evidence": ["In brief, the tasks can be roughly categorized into 3 different groups: surface, syntactic and semantic information."]}
{"question_id": "bd191d95806cee4cf80295e9ce1cd227aba100ab", "predicted_answer": "", "predicted_evidence": ["To facilitate analysis and comparison, we downsample both corpora to create balanced datasets with 10000 training examples as well as 1000 development and 1000 test examples of each class. To reflect generalization performance to new documents, both resources were split chronologically, drawing the training set from the oldest texts, the test set from the newest, and the development set from texts published between the two. To assess classifier performance across a range of training dataset sizes, we further downsampled the training sets to create versions with 100, 316, 1000, and 3162 examples of each class ($10^2, 10^{2.5}, \\ldots $). Finally, we truncated each document to a maximum of 256 basic tokens to minimize any advantage the language-specific model might have due to its more compact representation of Finnish."]}
{"question_id": "a9cae57f494deb0245b40217d699e9a22db0ea6e", "predicted_answer": "", "predicted_evidence": ["To prove the generalization of our classifiers, we use two of the TV series as training data and the rest as testing set. We compare them with classifiers trained without the replacement of generic tags like role_i or actor_j. So 3 sets of experiments are performed, and each are trained on top of Bayes, Logistic Regression and SVM. Average accuracies among them are reported as the performance measure for the sake of space limit. The results are shown in Table TABREF42 . \u201c1\u201d, \u201c2\u201d and \u201c3\u201d represent the TV series \u201cThe Journey of Flower\u201d, \u201cNirvana in Fire\u201d and \u201cGood Time\u201d respectively. In each cell, the left value represents accuracy of classifier without replacement of generic tags and winners are bolded."]}
{"question_id": "0a736e0e3305a50d771dfc059c7d94b8bd27032e", "predicted_answer": "", "predicted_evidence": ["As shown in Figure FIGREF32 , it is easy for us to determine the feature size for each classifier. Also it's obvious that test accuracies of classifiers for plot, actor/actress, analysis, and thumb up or down, didn't increase much with adding more words. Therefore, the top 1000 words with respect to these classes are fixed as the final feature words. While for the rest of classifiers, they achieved top testing performances at the size of about 4000. Based on these findings, we use different feature sizes in our final classifiers."]}
{"question_id": "283d358606341c399e369f2ba7952cd955326f73", "predicted_answer": "", "predicted_evidence": ["Based on INLINEFORM0 and DRC discussed in section 3.4, we can sort the importance of each word term. With different feature size, we can train the eight generic classifiers and get their performances on both training and testing set. Here we use SVM as the classifier to compare feature size's influence. Our results suggest that it performs best among the three. The results are shown in Figure FIGREF32 . The red squares represent the training accuracy, while the blue triangles are testing accuracies."]}
{"question_id": "818c85ee26f10622c42ae7bcd0dfbdf84df3a5e0", "predicted_answer": "", "predicted_evidence": ["As our final goal is to learn a generic classifier, which is agnostic to TV series but can predict review's category reasonably, we did experiments following our procedures of building the classifier as discussed in section 1."]}
{"question_id": "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c", "predicted_answer": "", "predicted_evidence": ["Predicting human behavior, however, is challenging due to its huge variability and the way the variables interact with each other. Running Randomized Control Trials (RCT) to decouple each variable is not always feasible and also expensive. It is possible to collect a large amount of observational data due to the advent of content sharing platforms such as YouTube, Massive Open Online Courses (MOOC), or ted.com. However, the uncontrolled variables in the observational dataset always keep a possibility of incorporating the effects of the \u201cdata bias\u201d into the prediction model. Recently, the problems of using biased datasets are becoming apparent. BIBREF3 showed that the error rates in the commercial face-detectors for the dark-skinned females are 43 times higher than the light-skinned males due to the bias in the training dataset. The unfortunate incident of Google's photo app tagging African-American people as \u201cGorilla\u201d BIBREF4 also highlights the severity of this issue."]}
{"question_id": "bba70f3cf4ca1e0bb8c4821e3339c655cdf515d6", "predicted_answer": "", "predicted_evidence": ["Predicting human behavior, however, is challenging due to its huge variability and the way the variables interact with each other. Running Randomized Control Trials (RCT) to decouple each variable is not always feasible and also expensive. It is possible to collect a large amount of observational data due to the advent of content sharing platforms such as YouTube, Massive Open Online Courses (MOOC), or ted.com. However, the uncontrolled variables in the observational dataset always keep a possibility of incorporating the effects of the \u201cdata bias\u201d into the prediction model. Recently, the problems of using biased datasets are becoming apparent. BIBREF3 showed that the error rates in the commercial face-detectors for the dark-skinned females are 43 times higher than the light-skinned males due to the bias in the training dataset. The unfortunate incident of Google's photo app tagging African-American people as \u201cGorilla\u201d BIBREF4 also highlights the severity of this issue."]}
{"question_id": "c5f9894397b1a0bf6479f5fd9ee7ef3e38cfd607", "predicted_answer": "", "predicted_evidence": ["Predicting human behavior, however, is challenging due to its huge variability and the way the variables interact with each other. Running Randomized Control Trials (RCT) to decouple each variable is not always feasible and also expensive. It is possible to collect a large amount of observational data due to the advent of content sharing platforms such as YouTube, Massive Open Online Courses (MOOC), or ted.com. However, the uncontrolled variables in the observational dataset always keep a possibility of incorporating the effects of the \u201cdata bias\u201d into the prediction model. Recently, the problems of using biased datasets are becoming apparent. BIBREF3 showed that the error rates in the commercial face-detectors for the dark-skinned females are 43 times higher than the light-skinned males due to the bias in the training dataset. The unfortunate incident of Google's photo app tagging African-American people as \u201cGorilla\u201d BIBREF4 also highlights the severity of this issue."]}
{"question_id": "9f8c0e02a7a8e9ee69f4c1757817cde85c7944bd", "predicted_answer": "", "predicted_evidence": ["We use two neural network architectures in the prediction task. In the first architecture, we use LSTM BIBREF7 for a sequential input of the words within the sentences of the transcripts. In the second architecture, we use TreeLSTM BIBREF8 to represent the input sentences in the form of a dependency tree. Our experiments show that the dependency tree-based model can predict the TED talk ratings with slightly higher performance (average F-score 0.77) than the word sequence model (average F-score 0.76). To the best of our knowledge, this is the best performance in the literature on predicting the TED talk ratings. We compare the performances of these two models with a baseline of classical machine learning techniques using hand-engineered features. We find that the neural networks largely outperform the classical methods. We believe this gain in performance is achieved by the networks' ability to capture better the natural relationship of the words (as compared to the hand engineered feature selection approach in the baseline methods) and the correlations among different rating labels."]}
{"question_id": "6cbbedb34da50286f44a0f3f6312346e876e2be5", "predicted_answer": "", "predicted_evidence": ["We address the data bias issue as much as possible by carefully analyzing the relationships of different variables in the data generating process. We use a Causal Diagram BIBREF5 , BIBREF6 to analyze and remove the effects of the data bias (e.g., the speakers' reputations, popularity gained by publicity, etc.) in our prediction model. In order to make the prediction model less biased to the speakers' race and gender, we confine our analysis to the transcripts only. Besides, we normalize the ratings to remove the effects of the unwanted variables such as the speakers' reputations, publicity, contemporary hot topics, etc."]}
{"question_id": "173060673cb15910cc310058bbb9750614abda52", "predicted_answer": "", "predicted_evidence": ["We address the data bias issue as much as possible by carefully analyzing the relationships of different variables in the data generating process. We use a Causal Diagram BIBREF5 , BIBREF6 to analyze and remove the effects of the data bias (e.g., the speakers' reputations, popularity gained by publicity, etc.) in our prediction model. In order to make the prediction model less biased to the speakers' race and gender, we confine our analysis to the transcripts only. Besides, we normalize the ratings to remove the effects of the unwanted variables such as the speakers' reputations, publicity, contemporary hot topics, etc."]}
{"question_id": "98c8ed9019e43839ffb53a714bc37fbb1c28fe2c", "predicted_answer": "", "predicted_evidence": ["We address the data bias issue as much as possible by carefully analyzing the relationships of different variables in the data generating process. We use a Causal Diagram BIBREF5 , BIBREF6 to analyze and remove the effects of the data bias (e.g., the speakers' reputations, popularity gained by publicity, etc.) in our prediction model. In order to make the prediction model less biased to the speakers' race and gender, we confine our analysis to the transcripts only. Besides, we normalize the ratings to remove the effects of the unwanted variables such as the speakers' reputations, publicity, contemporary hot topics, etc."]}
{"question_id": "50c441a9cc7345a0fa408d1ce2e13f194c1e82a8", "predicted_answer": "", "predicted_evidence": ["To assess the quality of generations, we conducted a MTurk human evaluation. We recruited a total of 15 participants and each participant was asked to evaluate 25 randomly sampled outputs from the test set on three metrics:"]}
{"question_id": "2895a3fc63f6f403445c11043460584e949fb16c", "predicted_answer": "", "predicted_evidence": ["Contributions Generality and specificity obviously contradict to each other. How to find a good trade-off between them is the main challenge in this paper. We will use minimum description length (MDL) as the basic framework to reconcile the two objectives. More specifically, our contribution in this paper can be summarized as follows:"]}
{"question_id": "1e7e3f0f760cd628f698b73d82c0f946707855ca", "predicted_answer": "", "predicted_evidence": ["Verb Phrase Data The pattern assignment uses the phrase distribution INLINEFORM0 . To do this, we use the \u201cEnglish All\u201d dataset in Google Syntactic N-Grams. The dataset contains counted syntactic ngrams extracted from the English portion of the Google Books corpus. It contains 22,230 different verbs (without stemming), and 147,056 verb phrases. For a fixed verb, we compute the probability of phrase INLINEFORM1 by: DISPLAYFORM0 "]}
{"question_id": "64632981279c7aa16ffc1a44ffc31f4520f5559e", "predicted_answer": "", "predicted_evidence": [", where INLINEFORM0 is the frequency of INLINEFORM1 in the corpus, and the denominator sums over all phrases of this verb."]}
{"question_id": "deed225dfa94120fafcc522d4bfd9ea57085ef8d", "predicted_answer": "", "predicted_evidence": ["Some of these results make intuitive sense. For example, the closer the day of week is to Friday and Saturday, the more positive the sentiment, with a drop on Sunday. As with spatial, the average sentiment of all the hours, days and months lean more towards the positive side."]}
{"question_id": "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd", "predicted_answer": "", "predicted_evidence": ["We collected a total of 18 million, geo-tagged, English-language tweets over three years, from January 1st, 2012 to January 1st, 2015, evenly divided across all 36 months, using Historical PowerTrack for Twitter provided by GNIP. We created geolocation bounding boxes for each of the 50 states which were used to collect our dataset. All 18 million tweets originated from one of the 50 states and are tagged as such. Moreover, all tweets contained one of the six emoticons in Table TABREF7 and were labelled as either positive or negative based on the emoticon. Out of the 18 million tweets, INLINEFORM0 million ( INLINEFORM1 ) were labelled as positive and INLINEFORM2 million ( INLINEFORM3 ) were labelled as negative. The 18 million tweets came from INLINEFORM4 distinct users."]}
{"question_id": "9aabcba3d44ee7d0bbf6a2c019ab9e0f02fab244", "predicted_answer": "", "predicted_evidence": ["This is our purely linguistic baseline model."]}
{"question_id": "242c626e89bca648b65af135caaa7ceae74e9720", "predicted_answer": "", "predicted_evidence": ["The last contextual variable we looked at was authorial. People have different baseline attitudes, some are optimistic and positive, some are pessimistic and negative, and some are in between. This difference in personalities can manifest itself in the sentiment of tweets. We attempted to capture this difference by looking at the history of tweets made by users. The 18 million labelled tweets in our dataset come from INLINEFORM0 authors."]}
{"question_id": "bba677d1a1fe38a41f61274648b386bdb44f1851", "predicted_answer": "", "predicted_evidence": ["Some of these results make intuitive sense. For example, the closer the day of week is to Friday and Saturday, the more positive the sentiment, with a drop on Sunday. As with spatial, the average sentiment of all the hours, days and months lean more towards the positive side."]}
{"question_id": "b6c2a391c4a94eaa768150f151040bb67872c0bf", "predicted_answer": "", "predicted_evidence": ["Some of these results make intuitive sense. For example, the closer the day of week is to Friday and Saturday, the more positive the sentiment, with a drop on Sunday. As with spatial, the average sentiment of all the hours, days and months lean more towards the positive side."]}
{"question_id": "06d5de706348dbe8c29bfacb68ce65a2c55d0391", "predicted_answer": "", "predicted_evidence": ["Bigram frequencies are often calculated using the approximation "]}
{"question_id": "6014c2219d29bae17279625716e7c2a1f8a2bd05", "predicted_answer": "", "predicted_evidence": ["In a much cited paper, Church and Hanks BIBREF0 use ` $=$ ' in place of ` $\\approx $ ' because the approximation is so good. Indeed, this approximation will only cause errors for the very few words which occur near the beginning or the end of the text. Take for example the text appearing above - the bigram (doggies, *) does not occur once, but the approximation says it does."]}
{"question_id": "9be9354eeb2bb1827eeb1e23a20cfdca59fb349a", "predicted_answer": "", "predicted_evidence": ["For a new text analytics application requiring feature engineering, it starts with estimating its semantic proximity (from the perspective of a NLP data scientist) with existing applications with known features. Based upon these proximity estimates as well as expected relevance of features for existing applications, system would recommend features for the new application in a ranked order. Furthermore, if user's selections are not aligned with system's recommendations, system gradually adapts its recommendation so that eventually it can achieve alignment with user preferences."]}
{"question_id": "5d5c25d68988fa5effe546507c66997785070573", "predicted_answer": "", "predicted_evidence": ["Table TABREF21 below depicts classes of features selected by authors of these works (as described in the corresponding references above) to highlight the point that despite domain differences, these applications share similar sets of features. Since authors of these works did not cite each other, it is possible that that these features might have been identified independently. This, in turn, supports the hypothesis that if adequate details of any one or two of these applications are fed to a system described in this work, which is designed to estimate semantic similarities across applications, system can automatically suggest potential features for consideration for the remaining applications to start with without requiring manual knowledge of the semantically related applications."]}
{"question_id": "ca595151735444b5b30a003ee7f3a7eb36917208", "predicted_answer": "", "predicted_evidence": ["Next, let us consider scenarios when features are specified as elements of a language. Let us refer to this language as NLP Feature Specification Language (nlpFSpL) such that a program in nlpFSpL would specify which features should be used by the underlying ML based solution to achieve goals of the target application. Given a corpus of unstructured natural language text data and a specifications in the nlpFSpL, an interpreter can be implemented as feature extraction system (FExSys) to automatically generate feature matrix which can be directly used by underlying ML technique."]}
{"question_id": "a2edd0454026811223b8f31512bdae91159677be", "predicted_answer": "", "predicted_evidence": ["In this paper, we aim to present an approach towards automating NLP feature engineering. We start with an outline of a language for expressing NLP features abstracting over the feature extraction process, which often implicitly captures intent of the NLP data scientist to extract specific features from given input text. We next discuss a method to enable automated reuse of features across semantically related applications when a corpus of feature specifications for related applications is available. Proposed language and system would help achieving reduction in manual effort towards design and extraction of features, would ensure standardization in feature specification process, and could enable effective reuse of features across similar and/or related applications."]}
{"question_id": "3b4077776f4e828f0d1687d0ce8018c9bce4fdc6", "predicted_answer": "", "predicted_evidence": ["* Equal contribution. Listing order is random."]}
{"question_id": "d1a88fe6655c742421da93cf88b5c541c09866d6", "predicted_answer": "", "predicted_evidence": ["We present the exact numbers on all languages to allow future papers to compare to our results in tab:dev and tab:test."]}
{"question_id": "184382af8f58031c6e357dbee32c90ec95288cb3", "predicted_answer": "", "predicted_evidence": ["This section contains the results obtained for all three tasks: PD detection with the PPD corpus, OSA detection with the PSD corpus and PD detection with the SPD corpus. Results are reported in terms of average Precision, Recall and F1 Score. The values highlighted in Tables TABREF19, TABREF21 and TABREF23 represent the best results, both at the speaker and segment levels."]}
{"question_id": "97abc2e7b39869f660986b91fc68be4ba196805c", "predicted_answer": "", "predicted_evidence": ["Table TABREF23 presents the results achieved for the classification of SPD corpus. This experiment was designed to assess the suitability of x-vectors trained in one language and being applied to disease classification in a different language. Our results show that KB features outperform both speaker representations. This is most likely caused by the language mismatch between the Spanish PD corpus and the European Portuguese training corpus. Nonetheless, it should be noted that, as in the previous task, x-vectors are able to surpass i-vectors in an out-of-domain corpus."]}
{"question_id": "9ec0527bda2c302f4e82949cc0ae7f7769b7bfb8", "predicted_answer": "", "predicted_evidence": ["Table TABREF23 presents the results achieved for the classification of SPD corpus. This experiment was designed to assess the suitability of x-vectors trained in one language and being applied to disease classification in a different language. Our results show that KB features outperform both speaker representations. This is most likely caused by the language mismatch between the Spanish PD corpus and the European Portuguese training corpus. Nonetheless, it should be noted that, as in the previous task, x-vectors are able to surpass i-vectors in an out-of-domain corpus."]}
{"question_id": "330fe3815f74037a9be93a4c16610c736a2a27b3", "predicted_answer": "", "predicted_evidence": ["This section contains the results obtained for all three tasks: PD detection with the PPD corpus, OSA detection with the PSD corpus and PD detection with the SPD corpus. Results are reported in terms of average Precision, Recall and F1 Score. The values highlighted in Tables TABREF19, TABREF21 and TABREF23 represent the best results, both at the speaker and segment levels."]}
{"question_id": "7546125f43eec5b09a3368c95019cb2bf1478255", "predicted_answer": "", "predicted_evidence": ["The majority of the English text available worldwide is generated by non-native speakers BIBREF0 . Such texts introduce a variety of challenges, most notably grammatical errors, and are of paramount importance for the scientific study of language acquisition as well as for NLP. Despite the ubiquity of non-native English, there is currently no publicly available syntactic treebank for English as a Second Language (ESL)."]}
{"question_id": "e96b0d64c8d9fdd90235c499bf1ec562d2cbb8b2", "predicted_answer": "", "predicted_evidence": ["2 necessaryiest ADJ JJS 3 amod"]}
{"question_id": "576a3ed6e4faa4c3893db632e97a52ac6e864aac", "predicted_answer": "", "predicted_evidence": ["#SENT=...<ns type=\"IJ\" ua=true> <i>interestings</i><c>interesting</c></ns> things... *1.5cm*1.3cm*1.1cm*1.1cm..."]}
{"question_id": "73c535a7b46f0c2408ea2b1da0a878b376a2bca5", "predicted_answer": "", "predicted_evidence": ["We utilize our two step review process to estimate agreement rates between annotators. We measure agreement as the fraction of annotation tokens approved by the editor. Table TABREF15 presents the agreement between annotators and reviewers, as well as the agreement between reviewers and the judges. Agreement measurements are provided for both the original the corrected versions of the dataset."]}
{"question_id": "620b6c410a055295d137511d3c99207a47c03b5e", "predicted_answer": "", "predicted_evidence": ["We also compare with models from previous work, listed below:"]}
{"question_id": "e459760879f662b2205cbdc0f5396dbfe41323ae", "predicted_answer": "", "predicted_evidence": ["Results for the review headline generation task are also shown in Table TABREF47. The table shows less promising results, where the best model, CHIM-encoder, achieves a decrease of 0.88 points in perplexity from the random encodings. Although this still means that some information has been transferred, one may argue that the gain is too small to be considered significant. However, it has been well perceived, that using only the user and product attributes to generate text is unreasonable, since we expect the model to generate coherent texts using only two vectors. This impossibility is also reported by BIBREF8 where they also used sentiment information, and BIBREF33 where they additionally used learned aspects and a short version of the text to be able to generate well-formed texts. Nevertheless, the results in this experiment agree to the results above regarding injecting attributes to the attention mechanism; bias-attention performs worse than the random baseline, and CHIM-attention performs the worst among CHIM-based models."]}
{"question_id": "1c3a20dceec2a86fb61e70fab97a9fb549b5c54c", "predicted_answer": "", "predicted_evidence": ["A more intuitive way of representing attributes is through the weight matrix $W$. Specifically, given the attribute embeddings $u$ and $p$, we linearly transform their concatenation into a vector $w^{\\prime }$ of size $D_1*D_2$ where $D_1$ and $D_2$ are the dimensions of $W$. We then reshape $w^{\\prime }$ into $W^{\\prime }$ to get the same shape as $W$ and replace $W$ with $W^{\\prime }$:"]}
{"question_id": "9686f3ff011bc6e3913c329c6a5671932c27e63e", "predicted_answer": "", "predicted_evidence": ["The purpose of this modification is two-fold. First, this model explicitly opens more possibilities for language-independent representation to occur, because every sentence is compressed into a consistent number of states. Second, we can observe the balance between language-independent and language-dependent information in the encoder; if zero-shot performance is minimally affected, then the encoder is in general able to capture language-independent information, and this restricted encoder retains this information."]}
{"question_id": "1f053f338df6d238cb163af1a0b1b073e749ed8a", "predicted_answer": "", "predicted_evidence": ["We evaluated the quality of the extracted parallel sentence pairs, by performing machine translation experiments on the augmented parallel corpus."]}
{"question_id": "fb06ed5cf9f04ff2039298af33384ca71ddbb461", "predicted_answer": "", "predicted_evidence": ["Both neural and statistical machine translation approaches are highly reliant on the availability of large amounts of data and are known to perform poorly in low resource settings. Recent crowd-sourcing efforts and workshops on machine translation have resulted in small amounts of parallel texts for building viable machine translation systems for low-resource pairs BIBREF0 . But, they have been shown to suffer from low accuracy (incorrect translation) and low coverage (high out-of-vocabulary rates), due to insufficient training data. In this project, we try to address the high OOV rates in low-resource machine translation systems by leveraging the increasing amount of multilingual content available on the Internet for enriching the bilingual lexicon."]}
{"question_id": "754d7475b8bf50499ed77328b4b0eeedf9cb2623", "predicted_answer": "", "predicted_evidence": ["As the dataset for training the machine translation systems, we used high precision sentences extracted with greedy decoding, by ranking the sentence-pairs on their translation probabilities. Phrase-Based SMT systems were trained using Moses BIBREF14 . We used the grow-diag-final-and heuristic for extracting phrases, lexicalised reordering and Batch MIRA BIBREF15 for tuning (the default parameters on Moses). We trained 5-gram language models with Kneser-Ney smoothing using KenLM BIBREF16 . With these parameters, we trained SMT systems for en\u2013ta and en\u2013hi language pairs, with and without the use of extracted parallel sentence pairs."]}
{"question_id": "1d10e069b4304fabfbed69acf409f0a311bdc441", "predicted_answer": "", "predicted_evidence": ["For training neural machine translation models, we used the TensorFlow BIBREF17 implementation of OpenNMT BIBREF18 with attention-based transformer architecture BIBREF19 . The BLEU scores for the NMT models were higher than for SMT models, for both en\u2013ta and en\u2013hi pairs, as can be seen in Table TABREF23 ."]}
{"question_id": "718c0232b1f15ddb73d40c3afbd6c5c0d0354566", "predicted_answer": "", "predicted_evidence": ["For the statistical machine translation and neural machine translation evaluation we use the BLEU score BIBREF13 as an evaluation metric, computed using the multi-bleu script from Moses BIBREF14 ."]}
{"question_id": "7cf44877dae8873139aede381fb9908dd0c546c4", "predicted_answer": "", "predicted_evidence": ["We hope that researchers will find Nematus an accessible and well documented toolkit to support their research. The toolkit is by no means limited to research, and has been used to train MT systems that are currently in production BIBREF21 ."]}
{"question_id": "86de8de906e30bb2224a2f70f6e5cf5e5ad4be72", "predicted_answer": "", "predicted_evidence": ["Nematus has its roots in the dl4mt-tutorial. We found the codebase of the tutorial to be compact, simple and easy to extend, while also producing high translation quality. These characteristics make it a good starting point for research in NMT. Nematus has been extended to include new functionality based on recent research, and has been used to build top-performing systems to last year's shared translation tasks at WMT BIBREF2 and IWSLT BIBREF3 ."]}
{"question_id": "361f330d3232681f1a13c6d59abb6c18246e7b35", "predicted_answer": "", "predicted_evidence": ["However, their neural-based ZP prediction model still produce low accuracies on predicting ZPs, which is 66% in F1 score. This is a key problem for the pipeline framework, since numerous errors would be propagated to the subsequent translation process."]}
{"question_id": "f7d61648ae4bd46c603a271185c3adfac5fc5114", "predicted_answer": "", "predicted_evidence": ["In pro-drop languages such as Chinese and Japanese, ZPs occur much more frequently compared to non-pro-drop languages such as English BIBREF8 . UTF8gbsn As seen in Table 1 , the subject pronoun (\u201c\u6211\u201d) and the object pronoun (\u201c\u5b83\u201d) are omitted in Chinese sentences (\u201cInp.\u201d) while these pronouns are all compulsory in their English translations (\u201cRef.\u201d). This is not a problem for human beings since we can easily recall these missing pronoun from the context. Taking the second sentence for example, the pronoun \u201c\u5b83\u201d is an anaphoric ZP that refers to the antecedent (\u201c\u86cb\u7cd5\u201d) in previous sentence, while the non-anaphoric pronoun \u201c\u6211\u201d can still be inferred from the whole sentence. The first example also indicates the necessity of intra-sentential information for ZP prediction."]}
{"question_id": "c9a323c152c5d9bc2d244e0ed10afbdb0f93062a", "predicted_answer": "", "predicted_evidence": ["In pro-drop languages such as Chinese and Japanese, ZPs occur much more frequently compared to non-pro-drop languages such as English BIBREF8 . UTF8gbsn As seen in Table 1 , the subject pronoun (\u201c\u6211\u201d) and the object pronoun (\u201c\u5b83\u201d) are omitted in Chinese sentences (\u201cInp.\u201d) while these pronouns are all compulsory in their English translations (\u201cRef.\u201d). This is not a problem for human beings since we can easily recall these missing pronoun from the context. Taking the second sentence for example, the pronoun \u201c\u5b83\u201d is an anaphoric ZP that refers to the antecedent (\u201c\u86cb\u7cd5\u201d) in previous sentence, while the non-anaphoric pronoun \u201c\u6211\u201d can still be inferred from the whole sentence. The first example also indicates the necessity of intra-sentential information for ZP prediction."]}
{"question_id": "d6a815d24c46557827d8aca65d3ffd008ac1bc07", "predicted_answer": "", "predicted_evidence": ["For our QA system we used the DBpedia repository and the Subtitles corpus. Due to the nature of training a sequence-to-sequence neural model, questions and answers need to be aligned. The statistics on the used data are shown in Table TABREF5 ."]}
{"question_id": "23252644c04a043f630a855b563666dd57179d98", "predicted_answer": "", "predicted_evidence": ["Each caption must contain at least ten Vietnamese words."]}
{"question_id": "2f75b0498cf6a1fc35f1fb1cac44fc2fbd3d7878", "predicted_answer": "", "predicted_evidence": ["Overall, we can see that English set only out-performed Vietnamese ones in BLEU-1 metric, rather, the Vietnamese sets performing well basing on BLEU-2 to BLEU-4, especially CIDEr scores. On the other hand, when UIT-ViIC is compared with the dataset having captions translated by Google, the evaluation results and the output examples suggest that Google Translation service is able to perform acceptablly even though most translated captions are not perfectly natural and linguistically friendly. As a results, we proved that manually written captions for Vietnamese dataset is currently prefered."]}
{"question_id": "0d3193d17c0a4edc8fa9854f279c2a1b878e8b29", "predicted_answer": "", "predicted_evidence": ["The two following tables, Table TABREF36 and Table TABREF36, summarize experimental results of Pytorch-tutorial, NIC - Show and Tell models. The two models are trained with three mentioned datasets, which are English-sportball, GT-sportball, UIT-ViIC. After training, 924 images from validation subset for each dataset are used to validate the our models."]}
{"question_id": "b424ad7f9214076b963a0077d7345d7bb5a7a205", "predicted_answer": "", "predicted_evidence": ["We summarize in Table TABREF8 an incomplete list of published Image Captioning datasets, in English and in other languages. Several image caption datasets for English have been constructed, the representative examples are Flickr3k BIBREF5, BIBREF6; Flickr 30k BIBREF7 \u2013 an extending of Flickr3k and Microsoft COCO (Microsoft Common in Objects in Context) BIBREF8."]}
{"question_id": "0dfe43985dea45d93ae2504cccca15ae1e207ccf", "predicted_answer": "", "predicted_evidence": ["Secondly, we introduce our annotation tool for dataset construction, which is also published to help annotators conveniently create captions."]}
{"question_id": "8276671a4d4d1fbc097cd4a4b7f5e7fadd7b9833", "predicted_answer": "", "predicted_evidence": ["Overall, CNN is first used for extracting image features for encoder part. The image features which are presented in vectors will be used as layers for decoding. For decoder part, RNN - LSTM are used to embed the vectors to target sentences using words/tokens provided in vocabulary."]}
{"question_id": "79885526713cc16eb734c88ff1169ae802cad589", "predicted_answer": "", "predicted_evidence": ["Secondly, we introduce our annotation tool for dataset construction, which is also published to help annotators conveniently create captions."]}
{"question_id": "0871827cfeceed4ee78ce7407aaf6e85dd1f9c25", "predicted_answer": "", "predicted_evidence": ["We evaluate our model on RACE dataset BIBREF6 , which consists of two subsets: RACE-M and RACE-H. RACE-M comes from middle school examinations while RACE-H comes from high school examinations. RACE is the combination of the two."]}
{"question_id": "240058371e91c6b9509c0398cbe900855b46c328", "predicted_answer": "", "predicted_evidence": ["We evaluate our model on RACE dataset BIBREF6 , which consists of two subsets: RACE-M and RACE-H. RACE-M comes from middle school examinations while RACE-H comes from high school examinations. RACE is the combination of the two."]}
{"question_id": "c7d3bccee59ab683e6bf047579bc6eab9de9d973", "predicted_answer": "", "predicted_evidence": ["We believe that Deep Learning techniques potentially offer improved handling of unknown words, long distance dependencies in text, and non\u2013linear relationships among words and concepts. Moving forward we intend to explore a variety of these ideas and describe those briefly below."]}
{"question_id": "376c6c74f008bb79a0dd9f073ac7de38870e80ad", "predicted_answer": "", "predicted_evidence": ["We began this research by participating in SemEval-2017 Task 6 #HashtagWars: Learning a Sense of Humor BIBREF7 . This included two subtasks : Pairwise Comparison (Subtask A) and Semi-ranking (Subtask B). Pairwise comparison asks a system to choose the funnier of two tweets. Semi-ranking requires that each of the tweets associated with a particular hashtag be assigned to one of the following categories : top most funny tweet, next nine most funny tweets, and all remaining tweets."]}
{"question_id": "c59d67930edd3d369bd51a619849facdd0770644", "predicted_answer": "", "predicted_evidence": ["We used traditional Ngram language models as our first approach for two reasons : First, Ngram language models can learn a certain style of humor by using examples of that as the training data for the model. Second, they assign a probability to each input they are given, making it possible to rank statements relative to each other. Thus, Ngram language models make relative rankings of humorous statements based on a particular style of humor, thereby accounting for the continuous and subjective nature of humor."]}
{"question_id": "9d6b2672b11d49c37a6bfb06172d39742d48aef4", "predicted_answer": "", "predicted_evidence": ["These results show that models trained on the news data have a significant advantage over the tweets model, and that bigram models performed slightly better than trigrams. We submitted trigram models trained on news and tweets to the official evaluation of SemEval-2017 Task 6. The trigram language models trained on the news data placed fourth in Subtask A and first in Subtask B."]}
{"question_id": "b0e894536857cb249bd75188c3ca5a04e49ff0b6", "predicted_answer": "", "predicted_evidence": ["To investigate the capacities of different neural network architectures, we need to first define what it means for a neural network to accept a language. There are a variety of ways to formalize language acceptance, and changes to this definition lead to dramatically different characterizations."]}
{"question_id": "94c22f72665dfac3e6e72e40f2ffbc8c99bf849c", "predicted_answer": "", "predicted_evidence": ["thm:lstmupperbound constitutes a very tight upper bound on the expressiveness of LSTM computation. Asymptotically, LSTMs are not powerful enough to model even the deterministic context-free language INLINEFORM0 ."]}
{"question_id": "ce8d8de78a21a3ba280b658ac898f73d0b52bf1b", "predicted_answer": "", "predicted_evidence": ["Other attempts to increase diversity focus on selecting diverse responses after the model is trained. li2015diversity introduce a modification of beam search. Beam search attempts to find the highest probability response to a given input by producing a tree of possible responses and \"pruning\" branches that have the lowest probability. The top K highest probability responses are returned, of which the highest is selected as the output response. li2015diversity observe that beam search tends to select certain families of responses that temporarily have higher probability. To combat this, a discount factor of probabilities is added to responses that come from the same parent response candidate. This encourages selecting responses that are different from one another when searching for the highest probability target."]}
{"question_id": "e069fa1eecd711a573c0d5c83a3493f5f04b1d8a", "predicted_answer": "", "predicted_evidence": ["The best performing model was the NTM-LM architecture. While the model received the best performance in perplexity, it demonstrated only a one-point improvement over the existing language model architecture. While in state-of-the-art comparisons a one point difference can be significant, it does indicate that the proposed NTM addition to the language model only contributed a small improvement. It is possible that the additional NTM module was too difficult to train, or that the NTM module injected noise into the input of the GRU such that training became difficult. It is still surprising that the NTM was not put to better use, for performance gains. It is possible the model has not been appropriately tuned."]}
{"question_id": "8db11d9166474a0e98b99ac7f81d1f14539d79ec", "predicted_answer": "", "predicted_evidence": ["See Table TABREF3 for details on model and baseline perplexity. To begin, it is worth noting that all of the above architectures were trained in a similar environment, with the exception of HRED, which was trained using an existing Github implementation implementation. Overall, the NTM-LM architecture performed the best of all model architectures, whereas the sequence-to-sequence architecture performed the worst. The proposed NTM-LM outperformed the DNTM-S architecture."]}
{"question_id": "fa5f5f58f6277a1e433f80c9a92a5629d6d9a271", "predicted_answer": "", "predicted_evidence": ["To compare with dataset baselines in multiple dimensions and test the model's performance, we use the overall Bilingual Evaluation Understudy (BLEU) BIBREF22 to evaluate the imaginators' generation performance. As for arbitrator, we use accuracy score of the classification to evaluate. Accuracy in our experiments is the correct ratio in all samples."]}
{"question_id": "3b9da1af1550e01d2e6ba2b9edf55a289f5fa8e2", "predicted_answer": "", "predicted_evidence": ["From Table TABREF30, we can see that not only our BERT based model get the best results in both datasets, the other two models also significantly beat the corresponding baselines. Even the TextCNNs based model can beat all baselines in both datasets."]}
{"question_id": "f88f45ef563ea9e40c5767ab2eaa77f4700f95f8", "predicted_answer": "", "predicted_evidence": ["From Table TABREF30, we can see that not only our BERT based model get the best results in both datasets, the other two models also significantly beat the corresponding baselines. Even the TextCNNs based model can beat all baselines in both datasets."]}
{"question_id": "99e99f2c25706085cd4de4d55afe0ac43213d7c8", "predicted_answer": "", "predicted_evidence": ["Because the task we concentrate on is different from traditional ones, to make the datasets fit our problems and real life, we modify the datasets with the following steps:"]}
{"question_id": "da10e3cefbbd7ec73eabc6c93d338239ce84709e", "predicted_answer": "", "predicted_evidence": ["While it is not currently possible to make definitive judgements as to which dataset most closely captures \u201ctruth\u201d, another point more deeply discussed in Chapter 5, it is interesting to note that the statistical signal contained within the two datasets, as evidenced by the correlations and broad trends, is not largely different."]}
{"question_id": "00c443f8d32d6baf7c7cea8f4ca9fa749532ccfd", "predicted_answer": "", "predicted_evidence": ["Additionally, the ICEWS project makes use of the BBN ACCENT coder. Since ACCENT is a propriety software produce developed by BBN, not much currently exists in the way of public description on how the coder works from an algorithmic perspective. Previous work by BBN on the SERIF coder does have a public description, however, and it is likely that ACCENT shares something with the SERIF coder. BIBREF20 notes that SERIF works at both the sentence- and document-level to code events. At a high level, the coder makes use of a syntactic parse, and other linguistic information, to generate text graphs with candidate who-did-what-to-whom relationships. The sentence-level information is aggregated up to a document-level in an attempt to provide the most accurate event codings. The next section provides a comparison between the type of data the ICEWS coding procedure produces, and the data the Phoenix pipeline produces."]}
{"question_id": "6e3e9818551fc2f8450bbf09b0fe82ac2506bc7a", "predicted_answer": "", "predicted_evidence": ["Discussion 1.12. This result shows that simple RNNs with arbitrary precision are at least as computationally powerful as PDAs."]}
{"question_id": "0b5a505c1fca92258b9e83f53bb8cfeb81cb655a", "predicted_answer": "", "predicted_evidence": ["Discussion 2.15. We \u201ccheated\" a little bit by allowing INLINEFORM0 edge weights and by having INLINEFORM1 where INLINEFORM2 wasn't quite linear. However, INLINEFORM3 edge weights make sense in the context of allowing infinite precision, and simple nonlinear functions over the hidden nodes are often used in practice, like the common softmax activation function."]}
{"question_id": "2b32cf05c5e736f764ceecc08477e20ab9f2f5d7", "predicted_answer": "", "predicted_evidence": ["Although in Fig. FIGREF26, the curve fitted is parabolic, in the interval between -0.2 and 0, the score is almost linear (and strongly monotone decreasing) giving a good indication that at least -0.2 should be a good threshold to produce a higher F-1 score without any loss."]}
{"question_id": "014a3aa07686ee18a86c977bf0701db082e8480b", "predicted_answer": "", "predicted_evidence": ["We also experimented with other different approaches. The results of the first two were left out (they did not perform better), for the sake of conciseness."]}
{"question_id": "6e6d64e2cb7734599890fff3f10c18479756d540", "predicted_answer": "", "predicted_evidence": ["In Table TABREF27, the results of the one-vs-all approach regarding the true negative, false positives, false negatives and true positives for the different threshold 0, -0.25 and LCA are shown. Applying other threshold than 0 caused the number of true positives to increase without much hurting the number of true negatives. In fact, the number of false positives and false negatives became much more similar for -0.25 and LCA than for 0. This results in the score of recall and precision being also similar, in a way that the micro F-1 is increased. Also, the threshold -0.25 resulted that the number of false positive is greater than the number of false negatives, than for example -0.2. LCA produced similar results, but was more conservative having a lower false positive and higher true negative and false negative score."]}
{"question_id": "8675d39f1647958faab7fa40cdaab207d4fe5a29", "predicted_answer": "", "predicted_evidence": ["The experiments with alternative approaches, such as Flair, meta-classifier and semi-supervised learning yielded discouraging results, so we will concentrate in the SVM-TF-IDF methods. Especially, semi-supervised proved in other setups very valuable, here it worsened the prediction quality, so we could assume the same \"distribution\" of samples were in the training and development set (and so we concluded in the test set)."]}
{"question_id": "14fdc8087f2a62baea9d50c4aa3a3f8310b38d17", "predicted_answer": "", "predicted_evidence": ["Based on the distributions in Fig. FIGREF19, the test data was split. Two cases were considered: (a) same enhancement for training and test data (matched case, Table TABREF20), and (b) unprocessed training data and enhanced test data (mismatched case, Table TABREF21). As expected, the WER increases monotonically as the amount of overlap increases in both scenarios, and the recognition accuracy improves as the enhancement method becomes stronger."]}
{"question_id": "3d2b5359259cd3518f361d760bacc49d84c40d82", "predicted_answer": "", "predicted_evidence": ["For the single array track, the proposed system without RNN LM rescoring achieves 16% (11%) relative WER reduction on the DEV (EVAL) set when compared with System8 in BIBREF12 (row one in Table TABREF14). RNN LM rescoring further helps improve the proposed system performance."]}
{"question_id": "26a321e242e58ea5f2ceaf37f26566dd0d0a0da1", "predicted_answer": "", "predicted_evidence": ["Depending on the number of available arrays for CHiME-5, two flavours of GSS enhancement were used in this work. In the single array track, all 4 channels of the array are used as input ($M = 4$), and the system is referred to as GSS1. In the multi array track, all six arrays are stacked to form a 24 channels super-array ($M = 24$), and this system is denoted as GSS6. The baseline time synchronization provided by the challenge organizers was sufficient to align the data for GSS6."]}
{"question_id": "6920fd470e6a99c859971828e20276a1b9912280", "predicted_answer": "", "predicted_evidence": ["Finally, cleaning up the training set not only boosted the recognition performance, but managed to do so using a fraction of the training data in BIBREF12, as shown in Table TABREF15. This translates to significantly faster and cheaper training of acoustic models, which is a major advantage in practice."]}
{"question_id": "f741d32b92630328df30f674af16fbbefcad3f93", "predicted_answer": "", "predicted_evidence": ["The Reuters Corpus Volume 2 BIBREF2 , in short RCV2, is a multilingual corpus with a collection of 487,000 news stories. Each news story was manually classified into four hierarchical groups: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social) and MCAT (Markets). Topic codes were assigned to capture the major subject of the news story. The entire corpus covers thirteen languages, i.e. Dutch, French, German, Chinese, Japanese, Russian, Portuguese, Spanish, Latin American Spanish, Italian, Danish, Norwegian, and Swedish, written by local reporters in each language. The news stories are not parallel. Single-label stories, i.e. those labeled with only one topic out of the four top categories, are often used for evaluations. However, the class distributions vary significantly across all the thirteen languages (see Table 1 ). Therefore, using random samples to extract evaluation corpora may lead to very imbalanced test sets, i.e. undesired and misleading variability among the languages when the main focus is to evaluate cross-lingual transfer."]}
{"question_id": "fe7f7bcf37ca964b4dc9e9c7ebf35286e1ee042b", "predicted_answer": "", "predicted_evidence": ["Split the data into train, development and test corpus: for each languages, we provide training data of different sizes (1k, 2k, 5k and 10k stories), a development (1k) and a test corpus (4k);"]}
{"question_id": "d9354c0bb32ec037ff2aacfed58d57887a713163", "predicted_answer": "", "predicted_evidence": ["We use five feature templates: context words, distance between entities, presence of punctuation, dependency paths, and negated keyword."]}
{"question_id": "c035a011b737b0a10deeafc3abe6a282b389d48b", "predicted_answer": "", "predicted_evidence": ["We use five feature templates: context words, distance between entities, presence of punctuation, dependency paths, and negated keyword."]}
{"question_id": "d3fb0d84d763cb38f400b7de3daaa59ed2a1b0ab", "predicted_answer": "", "predicted_evidence": ["Prior work has made predictions about contests such as NFL games BIBREF0 and elections using tweet volumes BIBREF1 or sentiment analysis BIBREF2 , BIBREF3 . Many such indirect signals have been shown useful for prediction, however their utility varies across domains. In this paper we explore whether the \u201cwisdom of crowds\" BIBREF4 , as measured by users' explicit predictions, can predict outcomes of future events. We show how it is possible to accurately forecast winners, by aggregating many individual predictions that assert an outcome. Our approach requires no historical data about outcomes for training and can directly be adapted to a broad range of contests."]}
{"question_id": "6da1320fa25b2b6768358d3233a5ecf99cc73db5", "predicted_answer": "", "predicted_evidence": ["Next, we also studied if there are topics that are more polarizing than others and how different topics impact classification performance. We identified polarizing topics, i.e, topics that invoke opposite sentiments across two classes (ideologies) by using the following equation. DISPLAYFORM0 "]}
{"question_id": "351f7b254e80348221e0654478663a5e53d3fe65", "predicted_answer": "", "predicted_evidence": ["For an unseen (test) document INLINEFORM0 , we first compute the TSM INLINEFORM1 , and assign it the label corresponding to the label whose TSM is most proximal to INLINEFORM2 . DISPLAYFORM0 "]}
{"question_id": "d323f0d65b57b30ae85fb9f24298927a3d1216e9", "predicted_answer": "", "predicted_evidence": ["We now outline a simple classification model that uses summaries of TSMs. Given a labeled training set of documents, we would like to find the prototypical TSM corresponding to each label. This can be done by identifying the matrix that minimizes the cumulative deviation from those corresponding to the documents with the label. DISPLAYFORM0 "]}
{"question_id": "05118578b46e9d93052e8a760019ca735d6513ab", "predicted_answer": "", "predicted_evidence": ["The rest of the paper is organized as follows. Section 2 presents a brief review of related work. Section 3 discusses the architecture of our AC-BLSTM and our semi-supervised framework. Section 4 presents the experiments result with comparison analysis. Section 5 concludes the paper."]}
{"question_id": "31b9337fdfbbc33fc456552ad8c355d836d690ff", "predicted_answer": "", "predicted_evidence": ["We used standard train/test splits for those datasets that had them. Otherwise, we performed 10-fold cross validation. We repeated each experiment 10 times and report the mean accuracy. Results of our models against other methods are listed in table TABREF16 . To the best of our knowledge, AC-BLSTM achieves the best results on five tasks."]}
{"question_id": "389ff1927ba9fc8bac50959fc09f30c2143cc14e", "predicted_answer": "", "predicted_evidence": ["We describe next each of the three main categories of factors examined in the model. An example of these features is given in Table TABREF7 ."]}
{"question_id": "b968bd264995cd03d7aaad1baba1838c585ec909", "predicted_answer": "", "predicted_evidence": ["As we saw in Figure FIGREF1 , embeddings are sometimes surprisingly unstable. To understand the factors behind the (in)stability of word embeddings, we build a regression model that aims to predict the stability of a word given: (1) properties related to the word itself; (2) properties of the data used to train the embeddings; and (3) properties of the algorithm used to construct these embeddings. Using this regression model, we draw observations about factors that play a role in the stability of word embeddings."]}
{"question_id": "afcd1806b931a97c0679f873a71b825e668f2b75", "predicted_answer": "", "predicted_evidence": ["The goodness of fit of a regression model is measured using the coefficient of determination INLINEFORM0 . This measures how much variance in the dependent variable INLINEFORM1 is captured by the independent variables INLINEFORM2 . A model that always predicts the expected value of INLINEFORM3 , regardless of the input features, will receive an INLINEFORM4 score of 0. The highest possible INLINEFORM5 score is 1, and the INLINEFORM6 score can be negative."]}
{"question_id": "01c8c3836467a4399cc37e86244b5bdc5dda2401", "predicted_answer": "", "predicted_evidence": ["In addition to word and data properties, we encode features about the embedding algorithms. These include the different algorithms being used, as well as the different parameter settings of these algorithms. Here, we consider three embedding algorithms, word2vec, GloVe, and PPMI. The choice of algorithm is represented in our feature vector as a bag-of-words."]}
{"question_id": "568466c62dd73a025bfd9643417cdb7a611f23a1", "predicted_answer": "", "predicted_evidence": ["The results in the tables show that increasing the size of the candidate pool is beneficial. We see that most scores are better (marked in bold) than the model fine-tuned with only authentic data. However, the performance is also dependent on the domain. When comparing BIO and NEWS subtables we see that the models adapted for the latter domain tend to achieve better performances as most of the scores are statistically significant improvements."]}
{"question_id": "3a19dc6999aeb936d8a1c4509ebd5bfcda50f0f1", "predicted_answer": "", "predicted_evidence": ["The work presented in this paper is based on two main concepts: the generation of synthetic sentences, and the selection of sentences from a set $S$ of candidates."]}
{"question_id": "338a3758dccfa438a52d173fbe23a165ef74a0f0", "predicted_answer": "", "predicted_evidence": ["Test sets: We evaluate the models with two test sets in different domains:"]}
{"question_id": "2686e8d51caff9a19684e0c9984bcb5a1937d08d", "predicted_answer": "", "predicted_evidence": ["For German INLINEFORM0 English, the parser annotates the German input with morphological features. Different word types have different sets of features \u2013 for instance, nouns have case, number and gender, while verbs have person, number, tense and aspect \u2013 and features may be underspecified. We treat the concatenation of all morphological features of a word, using a special symbol for underspecified features, as a string, and treat each such string as a separate feature value."]}
{"question_id": "df623717255ea2c9e0f846859d8a9ef51dc1102b", "predicted_answer": "", "predicted_evidence": ["The decoder is a recurrent neural network that predicts a target sequence INLINEFORM0 . Each word INLINEFORM1 is predicted based on a recurrent hidden state INLINEFORM2 , the previously predicted word INLINEFORM3 , and a context vector INLINEFORM4 . INLINEFORM5 is computed as a weighted sum of the annotations INLINEFORM6 . The weight of each annotation INLINEFORM7 is computed through an alignment model INLINEFORM8 , which models the probability that INLINEFORM9 is aligned to INLINEFORM10 . The alignment model is a single-layer feedforward neural network that is learned jointly with the rest of the network through backpropagation."]}
{"question_id": "ac482ab8a5c113db7c1e5f106a5070db66e7ba37", "predicted_answer": "", "predicted_evidence": ["Semantic tagging BIBREF4 , BIBREF7 is the task of assigning language-neutral semantic categories to words. It is designed to overcome a lack of semantic information syntax-oriented part-of-speech tagsets, such as the Penn Treebank tagset BIBREF8 , usually have. Such tagsets exclude important semantic distinctions, such as negation and modals, types of quantification, named entity types, and the contribution of verbs to tense, aspect, or event."]}
{"question_id": "24897f57e3b0550be1212c0d9ebfcf83bad4164e", "predicted_answer": "", "predicted_evidence": ["The fact that NLI is a sentence-level task, while semantic tags are word-level annotations presents a difficulty in measuring the effect of semantic tags on the systems' performance, as there is no one-to-one correspondence between a correct label and a particular semantic tag. We therefore employ the following method in order to assess the contribution of semantic tags. Given the performance ranking of all our systems \u2014 $FSN < ST < PSN < LWS$ \u2014 we make a pairwise comparison between the output of a superior system $S_{sup}$ and an inferior system $S_{inf}$ . This involves taking the pairs of sentences that every $S_{sup}$ classifies correctly, but some $S_{inf}$ does not. Given that FSN is the worst performing system and, as such, has no `worse' system for comparison, we are left with six sets of sentences: ST-FSN, PSN-FSN, PSN-ST, LWS-PSN, LWS-ST, and LWS-FSN. To gain insight as to where a given system $S_{sup}$ performs better than a given $S_{inf}$ , we then sort each comparison sentence set by the frequency of semtags predicted therein, which are normalized by dividing by their frequency in the full SNLI test set."]}
{"question_id": "d576af4321fe71ced9e521df1f3fe1eb90d2df2d", "predicted_answer": "", "predicted_evidence": ["(6) Co-attention-based Method (Coatt): a variation of our model by replacing interactive attention with another co-attention model BIBREF22."]}
{"question_id": "fd651d19046966ca65d4bcf6f6ae9c66cdf13777", "predicted_answer": "", "predicted_evidence": ["We call it the back-translation objective. Therefore, our final objective consists of content fidelity objective, style preservation objective and back-translation objective."]}
{"question_id": "08b77c52676167af72581079adf1ca2b994ce251", "predicted_answer": "", "predicted_evidence": ["The third objective is used for training our system in a true text manipulation setting. We can regard this as an application of the back-translation algorithm in document-scale text content manipulation. Subsection \"Back-translation Objective\" will give more details."]}
{"question_id": "89fa14a04008c93907fa13375f9e70b655d96209", "predicted_answer": "", "predicted_evidence": ["Next, we compute the suitable records of the table in light of each word of the reference."]}
{"question_id": "ff36168caf48161db7039e3bd4732cef31d4de99", "predicted_answer": "", "predicted_evidence": ["Figure.FIGREF20 shows the accuracy of the four Support vector machine and Random forest models trained on the original human labeled data and on the data labeled by our method. The accuracies are hit ratios that compute the number of correctly classified sentences over the number of all sentences in the test data. For example, if a model classified 85 sentences correctly out of 100 test sentences, then the accuracy is 0.85. In order to accurately compute the Ground truth hit ratio, we used the ground truth messages in the chatbot. The messages are the sentences that are to be shown to the chatbot users in response to the classification for a particular user query as below."]}
{"question_id": "556782bb96f8fc07d14865f122362ebcc79134ec", "predicted_answer": "", "predicted_evidence": ["We propose a new approach of building text classification models using a network community detection algorithm with unlabeled text data, and show that the network community detection is indeed useful in labeling text data by clustering the text data into multiple distinctive groups, and also in improving the classification accuracy. This study follows below steps (see Figure.FIGREF7), and uses Python packages such as NLTK, NetworkX and SKlearn."]}
{"question_id": "cb58605a7c230043bd0d6e8d5b068f8b533f45fe", "predicted_answer": "", "predicted_evidence": ["Text data is a great source of knowledge for building many useful recommendation systems, search engines as well as conversational intelligence systems. However, it is often found to be a difficult and time consuming task to structure the unstructured text data especially when it comes to labeling the text data for training text classification models. Data labeling, typically done by humans, is prone to make misslabeled data entries, and hard to track whether the data is correctly labeled or not. This human labeling practice indeed impacts on the quality of the trained models in solving classificastion problems."]}
{"question_id": "7969b8d80e12aa3ebb89b5622bc564f44e98329f", "predicted_answer": "", "predicted_evidence": ["The Class-split happens when a human labeled class is devided into multiple communities as the sentence network is clustered based on the semantic similarity. This actually can help improve the text classification based systems to work more sophisticatedly as the data set gets more detailed subclasses to design the systems with. Although, it is indeed a helpful phenomena, we would like to minimize the number of subclasses created by the community detection algorithm simply because we want to avoid having too many subclasses that would add more complexity in designing any applications using the community data. On the other hand, the Class-merge happens when multiple human labeled classes are merged into one giant community. This Class-merge phenomena also helps improve the original data set by detecting either misslabeled or ambiguous data entries. We will discuss more details in the following subsection. Nonetheless, we also want to minimize the number of classes merged into the one giant community, because when too many classes are merged into one class, it simply implies that the sentence network is not correctly clustered. For example, as shown in Figure.FIGREF15 red lines, 12 different human labeled classes that do not share any similar intents are merged into COMMUNITY_7. If we trained a text classification model on this data, we would have lost the specifically designed purposes of the 12 different classes, expecting COMMUNITY_7 to deal with all the 12 different types of sentences. This would dramatically degrade the performance of the text classification models."]}
{"question_id": "a554cd1ba2a8d1348a898e0cb4b4c16cc8998257", "predicted_answer": "", "predicted_evidence": ["To enable a fair comparison, we limit the number of articles for each dataset to 20,000 and the size of the vocabulary to the 18,000 most common words. Datasets are split into 60%, 20%, and 20% for training, validation, and testing. We want to see if there are correlations showing stereotypes across different nations. Does the biased correlations learned by an encoder transfer to the decoder considering word sequences from different countries?"]}
{"question_id": "3cc9a820c4a2cd2ff61da920c41ed09f3c0135be", "predicted_answer": "", "predicted_evidence": ["As originally introduced by BIBREF1, we compute the bias score of a word $x$ considering its word embedding $h^{fair}(x)$ and two gender indicators (words man and woman). For example, the bias score of scientist is:"]}
{"question_id": "95ef89dc29ff291bdbe48cb956329a6a06d36db8", "predicted_answer": "", "predicted_evidence": ["As illustrated in Figure FIGREF3, the memory $M$ consists of arrays $K$ and $V$ that store addressable keys (latent representations of the input) and values (class labels), respectively as in BIBREF0. To support our technique, we extend this definition with an array $G$ that stores the gender associated to each word, e.g., actor is male, actress is female, and scientist is no-gender. The final form of the memory module is as follows:"]}
{"question_id": "79258cea30cd6c0662df4bb712bf667589498a1f", "predicted_answer": "", "predicted_evidence": ["Table TABREF22 presents the ranking of the systems with respect to their F1-score as well as the precision and recall scores."]}
{"question_id": "8e5ce0d2635e7bdec4ba1b8d695cd06790c8cdaa", "predicted_answer": "", "predicted_evidence": ["In the framework of the challenge, we were required to first identify the entities occurring in the dataset and, then, annotate them with of the 13 possible types. Table TABREF12 provides a description for each type of entity that we made available both to the annotators and to the participants of the challenge."]}
{"question_id": "4e568134c896c4616bc7ab4924686d8d59b57ea1", "predicted_answer": "", "predicted_evidence": ["We measure the inter-annotator agreement between the annotators based on the Cohen's Kappa (cf. Table TABREF15 ) calculated on the first 200 tweets of the training set. According to BIBREF1 our score for Cohen's Kappa (0,70) indicates a strong agreement."]}
{"question_id": "55612e92791296baf18013d2c8dd0474f35af770", "predicted_answer": "", "predicted_evidence": ["In the framework of the challenge, we were required to first identify the entities occurring in the dataset and, then, annotate them with of the 13 possible types. Table TABREF12 provides a description for each type of entity that we made available both to the annotators and to the participants of the challenge."]}
{"question_id": "2f23bd86a9e27dcd88007c9058ddfce78a1a377b", "predicted_answer": "", "predicted_evidence": ["The CAp 2017 challenge concerns the problem of NER for tweets written in French. A significant milestone while organizing the challenge was the creation of a suitable benchmark. While one may be able to find Twitter datasets for NER in English, to the best of our knowledge, this is the first resource for Twitter NER in French. Following this observation, our expectations for developing the novel benchmark are twofold: first, we hope that it will further stimulate the research efforts for French NER with a focus on in user-generated text social media. Second, as its size is comparable with datasets previously released for English NER we expect it to become a reference dataset for the community."]}
{"question_id": "e0b8a2649e384bbdb17472f8da2c3df4134b1e57", "predicted_answer": "", "predicted_evidence": ["In this section we describe the steps taken during the organisation of the challenge. We begin by introducing the general guidelines for participation and then proceed to the description of the dataset."]}
{"question_id": "3f8a42eb0e904ce84c3fded2103f674e9cbc893d", "predicted_answer": "", "predicted_evidence": ["For each target domain, we split the examples into 70%/15%/15% training/development/test partitions. We present some statistics for the data sets in Table TABREF2."]}
{"question_id": "521a3e7300567f6e8e4c531f223dbc9fc306c393", "predicted_answer": "", "predicted_evidence": ["To reduce the forgetting of source domain knowledge, we introduce auxiliary penalty terms to regularise the fine-tuning process. We favour this approach as it does not require storing data samples from the source domain. In general, there are two types of penalty: selective and non-selective. The former penalises the model when certain parameters diverge significantly from the source model, while the latter uses a pre-defined distance function to measure the change of all parameters."]}
{"question_id": "863b3f29f8c59f224b4cbdb5f1097b45a25f1d88", "predicted_answer": "", "predicted_evidence": ["To learn model parameters, we minimize the difference between the true ratings and the predicted ratings, as measured by the mean squared error"]}
{"question_id": "e4cbfabf4509ae0f476f950c1079714a9cd3814e", "predicted_answer": "", "predicted_evidence": ["Another vital challenge is how to reliably represent each review. Importantly, sentences are not equally useful within each review. For example, in Fig. FIGREF1, the second sentence in $u$'s review 1, \u201cI take these in the morning and after every workout.\u201d conveys little regarding $u$'s concerns for Vitamin C, and thus is less pertinent than other sentences in the same review. Since including irrelevant sentences can introduce noise and may harm the final embedding quality, it is crucial to aggregate only useful sentences to represent each review."]}
{"question_id": "7a84fed904acc1e0380deb6e5a2e1daacfb5907a", "predicted_answer": "", "predicted_evidence": ["The parameters of the compared methods are selected based on their performance on the validation set. Specifically, for FM, the dimensionality of the factorized parameters is 10. For SVD, PMF, and NMF, the number of factors is set to 50. DeepCoNN uses 100 convolutional kernels with window size 3. D-ATT uses 200 filters and window size 5 for local attention; 100 filters and window sizes [2, 3, 4] for global attention. MPCN uses 3 pointers, and hidden dimensionality of 300 for inferring affinity matrix. HUITA uses 200 filters in the word-level CNN with window size 3, and 100 filters in the sentence-level CNN with window size 3."]}
{"question_id": "16b816925567deb734049416c149747118e13963", "predicted_answer": "", "predicted_evidence": ["Evaluation. To evaluate the performance of the model, we utilized the official script of the SemEval contest for AE. These results are reported as F1 scores. For ASC, to be consistent with BERT-PT, we utilized their script whose results are reported in Accuracy and Macro-F1 (MF1) measures. Macro-F1 is the average of F1 score for each class and it is used to deal with the issue of unbalanced classes."]}
{"question_id": "9b536f4428206ef7afabc4ff0a2ebcbabd68b985", "predicted_answer": "", "predicted_evidence": ["Adversarial Examples. Adversarial examples are created to attack a neural network to make erroneous predictions. There are two main types of adversarial attacks which are called white-box and black-box. White-box attacks BIBREF32 have access to the model parameters, while black-box attacks BIBREF33 work only on the input and output. In this work, we utilize a white-box method working on the embedding level. In order to create adversarial examples, we utilize the formula used by BIBREF16, where the perturbations are created using gradient of the loss function. Assuming $p(y|x;\\theta )$ is the probability of label $y$ given the input $x$ and the model parameters $\\theta $, in order to find the adversarial examples the following minimization problem should be solved:"]}
{"question_id": "9d04fc997689f44e5c9a551b8571a60b621d35c2", "predicted_answer": "", "predicted_evidence": ["Implementation details. We performed all our experiments on a GPU (GeForce RTX 2070) with 8 GB of memory. Except for the code specific to our model, we adapted the codebase utilized by BERT-PT. To carry out the ablation study of BERT-PT model, batches of 32 were specified. However, to perform the experiments for our proposed model, we reduced the batch size to 16 in order for the GPU to be able to store our model. For optimization, the Adam optimizer with a learning rate of $3e-5$ was used. From SemEval's training data, 150 examples were chosen for the validation and the remaining was used for training the model."]}
{"question_id": "8a0e1a298716698a305153c524bf03d18969b1c6", "predicted_answer": "", "predicted_evidence": ["BERT Encoder. BERT encoder is constructed by making use of Transformer blocks from the Transformer model. For $\\mathbf {BERT_{BASE}}$, these blocks are used in 12 layers, each of which consists of 12 multi-head attention blocks. In order to make the model aware of both previous and future contexts, BERT uses the Masked Language Model (MLM) where $15\\%$ of the input sentence is masked for prediction."]}
{"question_id": "538430077b1820011c609c8ae147389b960932c8", "predicted_answer": "", "predicted_evidence": ["Aspect Extraction. Given a collection of review sentences, the goal is to extract all the terms, such as waiter, food, and price in the case of restaurants, which point to aspects of a larger entity BIBREF30. In order to perform this task, it is usually modeled as a sequence labeling task, where each word of the input is labeled as one of the three letters in {B, I, O}. Label `B' stands for Beginning of the aspect terms, `I' for Inside (aspect terms' continuation), and `O' for Outside or non-aspect terms. The reason for Inside label is that sometimes aspects can contain two or more words and the system has to return all of them as the aspect. In order for a sequence ($s$) of $n$ words to be fed into the BERT architecture, they are represented as"]}
{"question_id": "97055ab0227ed6ac7a8eba558b94f01867bb9562", "predicted_answer": "", "predicted_evidence": ["We evaluated our approach in terms of both subjective and objective evaluation."]}
{"question_id": "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8", "predicted_answer": "", "predicted_evidence": ["Notice that, automatic metrics were computed on the entire test set, whereas subjective evaluation was based on 79 randomly chosen test samples due to the limitation of human resources available."]}
{"question_id": "bfcbb47f3c54ee1a459183e04e4c5a41ac9ae83b", "predicted_answer": "", "predicted_evidence": ["Specifically, we use an RNN with gated recurrent units (GRUs) for sequence modeling. Let $\\mathbf {x}_t$ be the word embeddings of the time step $t$ and $\\mathbf {h}_{t-1}$ be the previous hidden state of RNN. We have "]}
{"question_id": "fe1a74449847755cd7a46647cc9d384abfee789e", "predicted_answer": "", "predicted_evidence": ["Data and code. We make the data collected via Unfun.me, as well as our code for analyzing it, publicly available online BIBREF9 ."]}
{"question_id": "425d17465ff91019eb87c28ff3942f781ba1bbcb", "predicted_answer": "", "predicted_evidence": ["Data and code. We make the data collected via Unfun.me, as well as our code for analyzing it, publicly available online BIBREF9 ."]}
{"question_id": "08561f6ba578ce8f8d284abf90f5b24eb1f804d3", "predicted_answer": "", "predicted_evidence": ["Data and code. We make the data collected via Unfun.me, as well as our code for analyzing it, publicly available online BIBREF9 ."]}
{"question_id": "ec2045e0da92989642a5b5f2b1130c8bd765bcc5", "predicted_answer": "", "predicted_evidence": ["Data and code. We make the data collected via Unfun.me, as well as our code for analyzing it, publicly available online BIBREF9 ."]}
{"question_id": "25f699c7a33e77bd552782fb3886b9df9d02abb2", "predicted_answer": "", "predicted_evidence": ["Data preprocessing, parallel text preparation and training hyper-parameters are the same as in BIBREF3. Experiments included evaluations of the effect of the various texts, notably for JW300, which is a disproportionately large contributor to the dataset. We also evaluated models trained with pre-trained FastText embeddings to understand the boost in performance possible with word embeddings BIBREF6, BIBREF7. Our training hardware configuration was an AWS EC2 p3.2xlarge instance with OpenNMT-py BIBREF8."]}
{"question_id": "3e4e415e346a313f5a7c3764fe0f51c11f51b071", "predicted_answer": "", "predicted_evidence": ["Table TABREF6 presents the micro F1 scores from different models. Note that we use a corpus with 0.8 billion words and vocabulary of 100,000 words when training the language model, comparing with BIBREF4 using 100 billion words and vocabulary of 1,000,000 words. The context abstraction using the language model is the most crucial step. The sizes of the training corpus and vocabulary significantly affect the performance of this process, and consequently the final WSD results. However, BIBREF4 did not publish the 100 billion words corpus used for training their LSTM language model."]}
{"question_id": "d622564b250cffbb9ebbe6636326b15ec3c622d9", "predicted_answer": "", "predicted_evidence": ["Additionally, the bottleneck of the LSTM approach is the training speed. The training process of the LSTM model by BIBREF9 took approximately 4.5 months even after applying optimization of trimming sentences, while the training process of our FOFE-based model took around 3 days to produce the claimed results."]}
{"question_id": "4367617c0b8c9f33051016e8d4fbb44831c54d0f", "predicted_answer": "", "predicted_evidence": ["Additionally, the bottleneck of the LSTM approach is the training speed. The training process of the LSTM model by BIBREF9 took approximately 4.5 months even after applying optimization of trimming sentences, while the training process of our FOFE-based model took around 3 days to produce the claimed results."]}
{"question_id": "2c60628d54f2492e0cbf0fb8bacd8e54117f0c18", "predicted_answer": "", "predicted_evidence": ["For example, word INLINEFORM0 has two senses INLINEFORM1 for INLINEFORM2 occurring in the training corpus, and each sense has INLINEFORM3 instances. The pseudo language model converts all the instances into context embeddings INLINEFORM4 for INLINEFORM5 , and these embeddings are used as training data to build a classifier for INLINEFORM6 . The classifier can then be used to predict the sense of an instance of INLINEFORM7 by taking the predicting context embedding INLINEFORM8 ."]}
{"question_id": "77a331d4d909d92fab9552b429adde5379b2ae69", "predicted_answer": "", "predicted_evidence": ["Next we evaluate our proposed GRURNTN and LSTMRNTN models against baselines GRURNN and LSTMRNN with two different tasks and datasets."]}
{"question_id": "516b691ef192f136bb037c12c3c9365ef5a6604c", "predicted_answer": "", "predicted_evidence": ["$$r_t &=& \\sigma (x_t W_{xr} + h_{t-1} W_{hr} + b_r)\\\\\nz_t &=& \\sigma (x_t W_{xz} + h_{t-1} W_{hz} + b_r)\\\\\n\\tilde{h_t} &=& f(x_t W_{xh} + (r_t \\odot h_{t-1}) W_{hh} + b_h)\\\\\nh_t &=& (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h_t}$$   (Eq. 9) "]}
{"question_id": "c53b036eff430a9d0449fb50b8d2dc9d2679d9fe", "predicted_answer": "", "predicted_evidence": ["Table 1 shows the PTB test set PPL among our baseline models, proposed models, and several published results. Both our proposed models outperformed their baseline models. GRURNTN reduced the perplexity from 97.78 to 87.38 (10.4 absolute / 10.63% relative PPL) over the baseline GRURNN and LSTMRNTN reduced the perplexity from 108.26 to 96.97 (11.29 absolute / 10.42% relative PPL) over the baseline LSTMRNN. Overall, LSTMRNTN improved the LSTMRNN model and its performance closely resembles the baseline GRURNN. However, GRURNTN outperformed all the baseline models as well as the other models by a large margin."]}
{"question_id": "5da9e2eef741bd7efccec8e441b8e52e906b2d2d", "predicted_answer": "", "predicted_evidence": ["The interface allows the user to obtain an overview of the range of opinion that is exhibited towards a topic of interest by various news outlets. The user can quickly collect evidence by skimming articles that fall on different parts of this opinion spectrum using the provided excerpts or peruse any of the original articles by following the available links."]}
{"question_id": "77bc886478925c8e9fb369b1ba5d05c42b3cd79a", "predicted_answer": "", "predicted_evidence": ["When these two inputs are provided, the application retrieves a predefined number of news articles (up to 50) that match the first input, and analyzes their stance towards the target (the second input) using the stance detection model. The stance detection model is exposed as a web service and returns for each article-target entity pair a stance label (i.e. one of `in favour', `against' or `neutral') along with a probability."]}
{"question_id": "f15bc40960bd3f81bc791f43ab5c94c52378692d", "predicted_answer": "", "predicted_evidence": ["The demo then visualizes the collected news articles as a 2D scatter plot with each (x,y) coordinate representing a single news article from a particular outlet that matched the user query. The x-axis shows the stance of the article in the range INLINEFORM0 . The y-axis displays the prominence of the news outlet that published the article in the range INLINEFORM1 , measured by its Alexa ranking. A table displays the provided information in a complementary format, listing the news outlets of the articles, the stance labels, confidence scores, and prominence rankings. Excerpts of the articles can be scanned by hovering over the news outlets in the table and the original articles can be read by clicking on the source."]}
{"question_id": "80d6b9123a10358f57f259b8996a792cac08cb88", "predicted_answer": "", "predicted_evidence": ["Other approaches used large text corpora for trying to find connections and relatedness by making statistics over the words in the texts. This of course only works for people appearing in the texts and we will discuss this in section SECREF2 . All these methods do not cover the changes of relations of the persons over time, that may change over the years. Therefore the measure should have a time parameter, which can be set to the desired time we are investigating."]}
{"question_id": "5181aefb8a7272b4c83a1f7cb61f864ead6a1f1f", "predicted_answer": "", "predicted_evidence": ["The method can be used for other named entities such as organizations or cities but we expect not as much variation over time periods as with persons. And similarities between different types of entities would we interesting. So as the relation of a person to a city may chance over time."]}
{"question_id": "f010f9aa4ba1b4360a78c00aa0747d7730a61805", "predicted_answer": "", "predicted_evidence": ["We collected datasets of news articles in English and German language from the news agency Reuters (Table TABREF13 ). After a data cleaning step, which was deleting meta information like author and editor name from the article, title, body and date were stored in a local database and imported to a Pandas data frame BIBREF12 . The English corpus has a dictionary of length 106.848, the German version has a dictionary of length 163.788."]}
{"question_id": "1e582319df1739dcd07ba0ba39e8f70187fba049", "predicted_answer": "", "predicted_evidence": ["Number of Speakers. Numerous speakers create complex dialogs and increased candidate addressee, thus the task becomes more challenging. In Figure FIGREF27 (Upper), we investigate how ADR accuracy changes with the number of speakers in the context of length 15, corresponding to the rows with T=15 in Table TABREF23 . Recent+TF-IDF always chooses the most recent speaker and the accuracy drops dramatically as the number of speakers increases. Direct-Recent+TF-IDF shows better performance, and Dynamic-RNNis marginally better. SI-RNN is much more robust and remains above 70% accuracy across all bins. The advantage is more obvious for bins with more speakers."]}
{"question_id": "aaf2445e78348dba66d7208b7430d25364e11e46", "predicted_answer": "", "predicted_evidence": ["Overall Result. As shown in Table TABREF23 , SI-RNN significantly improves upon the previous state-of-the-art. In particular, addressee selection (ADR) benefits most, with different number of candidate responses (denoted as RES-CAND): around 12% in RES-CAND INLINEFORM0 and more than 10% in RES-CAND INLINEFORM1 . Response selection (RES) is also improved, suggesting role-sensitive GRUs and joint selection are helpful for response selection as well. The improvement is more obvious with more candidate responses (2% in RES-CAND INLINEFORM2 and 4% in RES-CAND INLINEFORM3 ). These together result in significantly better accuracy on the ADR-RES metric as well."]}
{"question_id": "d98148f65d893101fa9e18aaf549058712485436", "predicted_answer": "", "predicted_evidence": ["We thank the members of the UMichigan-IBM Sapphire Project and all the reviewers for their helpful feedback. This material is based in part upon work supported by IBM under contract 4915012629. Any opinions, findings, conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of IBM."]}
{"question_id": "34e9e54fa79e89ecacac35f97b33ef3ca3a00f85", "predicted_answer": "", "predicted_evidence": ["We propose a novel quadripartite analysis of the BLI models, in which we independently control for four different variables: (i) word form frequency, (ii) morphology, (iii) lexeme frequency and (iv) lexeme. We provide detailed descriptions for each of those conditions in the following sections. For each condition, we analyzed all 40 language pairs for each of our selected models\u2014a total of 480 experiments. In the body of the paper we only present a small representative subset of our results."]}
{"question_id": "6e63db22a2a34c20ad341eb33f3422f40d0001d3", "predicted_answer": "", "predicted_evidence": ["We focus on pairs of genetically-related languages for which we can cleanly map one morphological inflection onto another. We selected 5 languages from the Slavic family: Polish, Czech, Russian, Slovak and Ukrainian, and 5 Romance languages: French, Spanish, Italian, Portuguese and Catalan. Table TABREF5 presents an example extract from our resource; every source\u2013target pair is followed by their corresponding lemmata and a shared tag."]}
{"question_id": "58259f2e22363aab20c448e5dd7b6f432556b32d", "predicted_answer": "", "predicted_evidence": ["The explainability of AI algorithms is related to how context is processed, computationally, based on the machine's perceptual capabilities and on the external knowledge resources that are available."]}
{"question_id": "b9e0b1940805a5056f71c66d176cc87829e314d4", "predicted_answer": "", "predicted_evidence": ["Endowing machines with this sense-making capability has been one of the long-standing goals of Artificial Intelligence (AI) practice and research, both in industry and academia. Data-driven and knowledge-driven methods are two classical techniques in the pursuit of such machine sense-making capability. Sense-making is not only a key for improving machine autonomy, but is a precondition for enabling seamless interaction with humans. Humans communicate effectively with each other, thanks to their shared mental models of the physical world and social context BIBREF2. These models foster reciprocal trust by making contextual knowledge transparent; they are also crucial for explaining how decision-making unfolds. In a similar fashion, we can assert that `explainable AI' is a byproduct or an affordance of computational context understanding and is predicated on the extent to which humans can introspect the decision processes that enable machine sense-making BIBREF3."]}
{"question_id": "b54525a0057aa82b73773fa4dacfd115d8f86f1c", "predicted_answer": "", "predicted_evidence": ["Along this direction, the remainder of this chapter explores two concrete scenarios of context understanding, realized by neuro-symbolic architectures|i.e., hybrid AI frameworks that instruct machine perception (based on deep neural networks) with knowledge graphs. These examples were chosen to illustrate the general applicability of neuro-symbolism and its relevance to contemporary research problems."]}
{"question_id": "f264612db9096caf938bd8ee4085848143b34f81", "predicted_answer": "", "predicted_evidence": ["Matching profiles across social networks is a hard task for humans. It is a task on par with detecting plagiarism, something a non-trained person (or sometimes even a trained person) cannot easily accomplish. (Hence the need for the development of the field of stylometry in early Renaissance.) Be that as it may, we wanted to evaluate our model against humans to make sure that it is indeed outperforming them."]}
{"question_id": "da0a2195bbf6736119ff32493898d2aadffcbcb8", "predicted_answer": "", "predicted_evidence": ["Other than the obvious technical goal, the purpose of this paper is to shed light on the relative ease with which seemingly innocuous information can be used to track users across social networks, even when signing up on different services using completely different account and profile information (such as name and birthday). This paper is as much of a technical contribution, as it is a warning to users who increasingly share a large part of their private lives on these services."]}
{"question_id": "f5513f9314b9d7b41518f98c6bc6d42b8555258d", "predicted_answer": "", "predicted_evidence": ["Stylometry is defined as, \"the statistical analysis of variations in literary style between one writer or genre and another\". It is a centuries-old practice, dating back the early Renaissance. It is most often used to attribute authorship to disputed or anonymous documents. Stylometry techniques have also successfully been applied to other, non-linguistic fields, such as paintings and music. The main principles of stylometry were compiled and laid out by the philosopher Wincenty Lutos\u0142awski in 1890 in his work \"Principes de stylom\u00e9trie\" BIBREF0 ."]}
{"question_id": "d97843afec733410d2c580b4ec98ebca5abf2631", "predicted_answer": "", "predicted_evidence": ["The main goal of the proposed system is to obtain a characterization of a certain entity regarding both mentioned topics and sentiment throughout time, i.e. obtain a classification for each entity/day combination."]}
{"question_id": "813a8156f9ed8ead53dda60ef54601f6ca8076e9", "predicted_answer": "", "predicted_evidence": ["The main goal of the proposed system is to obtain a characterization of a certain entity regarding both mentioned topics and sentiment throughout time, i.e. obtain a classification for each entity/day combination."]}
{"question_id": "dd807195d10c492da2b0da8b2c56b8f7b75db20e", "predicted_answer": "", "predicted_evidence": ["Before actually analyzing the text in the tweets, we apply the following operations:"]}
{"question_id": "aa287673534fc05d8126c8e3486ca28821827034", "predicted_answer": "", "predicted_evidence": ["Before actually analyzing the text in the tweets, we apply the following operations:"]}
{"question_id": "8b8adb1d5a1824c8995b3eba668745c44f61c9c6", "predicted_answer": "", "predicted_evidence": ["The recent increase in digitally available language corpora made it possible to extend the traditional linguistic tools to a vast amount of often user-generated texts. Understanding how these corpora differ from traditional texts is crucial in developing computational methods for web search, information retrieval or machine translation BIBREF0 . The amount of these texts enables the analysis of language on a previously unprecedented scale BIBREF1 , BIBREF2 , BIBREF3 , including the dynamics, geography and time scale of language change BIBREF4 , BIBREF5 , social media cursing habits BIBREF6 , BIBREF7 , BIBREF8 or dialectal variations BIBREF9 ."]}
{"question_id": "88d1bd21b53b8be4f9d3cb26ecc3cbcacffcd63e", "predicted_answer": "", "predicted_evidence": ["We use the following form for Zipf's law that is proposed in BIBREF48 , and that fits the probability distribution of the word frequencies apart from the very rare words: INLINEFORM0 "]}
{"question_id": "74cef0205e0f31d0ab28d0e4d96c1e8ef62d4cce", "predicted_answer": "", "predicted_evidence": ["where INLINEFORM0 denotes a quantity (economic output, number of patents, crime rate etc.) related to the city, INLINEFORM1 is a multiplication factor, and INLINEFORM2 is the size of the city in terms of its population, and INLINEFORM3 denotes a scaling exponent, that captures the dynamics of the change of the quantity INLINEFORM4 with city population INLINEFORM5 . INLINEFORM6 describes a linear relationship, where the quantity INLINEFORM7 is linearly proportional to the population, which is usually associated with individual human needs such as jobs, housing or water consumption. The case INLINEFORM8 is called superlinear scaling, and it means that larger cities exhibit disproportionately more of the quantity INLINEFORM9 than smaller cities. This type of scaling is usually related to larger cities being disproportionately the centers of innovation and wealth. The opposite case is when INLINEFORM10 , that is called sublinear scaling, and is usually related to infrastructural quantities such as road network length, where urban agglomeration effects create more efficiency. BIBREF26 "]}
{"question_id": "200c37060d037dee33f3b7c8b1a2aaa58376566e", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF18 shows the vocabulary size as a function of the metropolitan area population, and the power-law fit. It shows that in contrary to the previous aggregate metrics, the vocabulary size grows very sublinearly ( INLINEFORM0 ) with the city size. This relationship can also be translated to the dependency on the total word count, which would give a INLINEFORM1 , another sublinear scaling."]}
{"question_id": "415014a5bcd83df52c9307ad16fab1f03d80f705", "predicted_answer": "", "predicted_evidence": ["We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features."]}
{"question_id": "b79c85fa84712d3028cb5be2af873c634e51140e", "predicted_answer": "", "predicted_evidence": ["Using Searle's speech act taxonomy BIBREF3 , we established a list of six speech act categories that are commonly seen on Twitter: Assertion, Recommendation Expression, Question, Request, and Miscellaneous. Table TABREF1 shows an example tweet for each of these categories."]}
{"question_id": "dc473819b196c0ea922773e173a6b283fa778791", "predicted_answer": "", "predicted_evidence": ["We trained four different classifiers on our 3,313 binary features using the following methods: naive bayes (NB), decision tree (DT), logistic regression (LR), SVM, and a baseline max classifier BL. We trained classifiers across three granularities: Twitter-wide, Type-specific, and Topic-specific. All of our classifiers are evaluated using 20-fold cross validation. Table TABREF9 shows the performance of our five classifiers trained and evaluated on all of the data. We report the F1 score for each class. As shown in Table TABREF9 , the logistic regression was the performing classifier with a weighted average F1 score of INLINEFORM0 . Thus we picked logistic regression as our classier and the rest of the results reported will be for LR only. Table TABREF10 shows the average performance of the LR classifier for Twitter-wide, type and topic specific classifiers."]}
{"question_id": "9207f19e65422bdf28f20e270ede6c725a38e5f9", "predicted_answer": "", "predicted_evidence": ["Finally, we compared the performance of our classifier (called TweetAct) to a logistic regression classifier trained on features proposed by, as far as we know, the only other supervised Twitter speech act classifier by Zhang et al. (called Zhang). Table TABREF12 shows the results. Not only did our classifier outperform the Zhang classifier for every class, both the semantic and syntactic classifiers (see Table TABREF11 ) also generally outperformed the Zhang classifier."]}
{"question_id": "8ddf78dbdc6ac964a7102ae84df18582841f2e3c", "predicted_answer": "", "predicted_evidence": ["We selected two topics for each of the three topic types described in the last section for a total of six topics (see Figure FIGREF2 for list of topics). We collected a few thousand tweets from the Twitter public API for each of these topics using topic-specific queries (e.g., #fergusonriots, #redsox, etc). We then asked three undergraduate annotators to independently annotate each of the tweets with one of the speech act categories described earlier. The kappa for the annotators was INLINEFORM0 . For training, we used the label that the majority of annotators agreed upon (7,563 total tweets)."]}
{"question_id": "079e654c97508c521c07ab4d24cdaaede5602c61", "predicted_answer": "", "predicted_evidence": ["Twitter-specific Characters: There are certain Twitter-specific characters that can signal speech acts. These characters are #, @, and RT.The position of these characters is also important to consider since Twitter-specific characters used in the initial position of a tweet is more predictive than in other positions. Therefore, we have three additional binary features indicating whether these symbols appear in the initial position."]}
{"question_id": "7efbd9adbc403de4be6b1fb1999dd5bed9d6262c", "predicted_answer": "", "predicted_evidence": ["We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features."]}
{"question_id": "95bbd91badbfe979899cca6655afc945ea8a6926", "predicted_answer": "", "predicted_evidence": ["We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features."]}
{"question_id": "76ae794ced3b5ae565f361451813f2f3bc85b214", "predicted_answer": "", "predicted_evidence": ["We trained four different classifiers on our 3,313 binary features using the following methods: naive bayes (NB), decision tree (DT), logistic regression (LR), SVM, and a baseline max classifier BL. We trained classifiers across three granularities: Twitter-wide, Type-specific, and Topic-specific. All of our classifiers are evaluated using 20-fold cross validation. Table TABREF9 shows the performance of our five classifiers trained and evaluated on all of the data. We report the F1 score for each class. As shown in Table TABREF9 , the logistic regression was the performing classifier with a weighted average F1 score of INLINEFORM0 . Thus we picked logistic regression as our classier and the rest of the results reported will be for LR only. Table TABREF10 shows the average performance of the LR classifier for Twitter-wide, type and topic specific classifiers."]}
{"question_id": "2a9c7243744b42f1e9fed9ff2ab17c6f156b1ba4", "predicted_answer": "", "predicted_evidence": ["Models H-1,2,3 used TDNN-LSTM BIBREF25 , also at 1-fold, 3-fold and 5-fold, with the same features as Model G."]}
{"question_id": "f8f64da7172e72e684f0e024a19411b43629ff55", "predicted_answer": "", "predicted_evidence": ["These songs were manually segmented into fragments with duration ranging from 10 to 35 sec primarily at the end of the verses. Then we randomly divided the vocal data by the singer and split it into training and testing sets. We got a total of 640 fragments in the training set and 97 fragments in the testing set. The singers in the two sets do not overlap. The details of the vocal data are listed in Table. TABREF3 ."]}
{"question_id": "8da8c4651979a4b1d1d3008c1f77bc7e9397183b", "predicted_answer": "", "predicted_evidence": ["There are two special token tags (__eou__ and __eot__) on ubuntu dialogue corpus. __eot__ tag is used to denote the end of a user's turn within the context and __eou__ tag is used to denote of a user utterance without a change of turn. Table TABREF42 shows the performance with/without two special tags."]}
{"question_id": "8cf52ba480d372fc15024b3db704952f10fdca27", "predicted_answer": "", "predicted_evidence": ["Used the fixed pre-trained GloVe vectors ."]}
{"question_id": "d8ae36ae1b4d3af5b59ebd24efe94796101c1c12", "predicted_answer": "", "predicted_evidence": ["Douban conversation corpus BIBREF11 which are constructed from Douban group (a popular social networking service in China) is also used in experiments. Response candidates on the test set are collected by Lucene retrieval model, other than negative sampling without human judgment on Ubuntu Dialogue Corpus. That is, the last turn of each Douban dialogue with additional keywords extracted from the context on the test set was used as query to retrieve 10 response candidates from the Lucene index set (Details are referred to section 4 in BIBREF11 ). For the performance measurement on test set, we ignored samples with all negative responses or all positive responses. As a result, 6,670 context-response pairs were left on the test set. Some statistics of Douban conversation corpus are shown below:"]}
{"question_id": "2bd702174e915d97884d1571539fb1b5b0b7123a", "predicted_answer": "", "predicted_evidence": ["Many pre-trained word embedding vectors on general large text-corpus are available. For domain-specific tasks, out-of-vocabulary may become an issue. Here we propose algorithm SECREF12 to combine pre-trained word vectors with those word2vec BIBREF9 generated on the training set. Here the pre-trainined word vectors can be from known methods such as GloVe BIBREF28 , word2vec BIBREF9 and FastText BIBREF7 ."]}
{"question_id": "0c247a04f235a4375dd3b0fd0ce8d0ec72ef2256", "predicted_answer": "", "predicted_evidence": ["We conduct experiments across various settings and datasets. We report macro-averaged scores in all the settings."]}
{"question_id": "66dfcdab1db6a8fcdf392157a478b4cca0d87961", "predicted_answer": "", "predicted_evidence": ["We conduct experiments across various settings and datasets. We report macro-averaged scores in all the settings."]}
{"question_id": "7ef34b4996ada33a4965f164a8f96e20af7470c0", "predicted_answer": "", "predicted_evidence": ["We conduct experiments across various settings and datasets. We report macro-averaged scores in all the settings."]}
{"question_id": "6e80386b33fbfba8bc1ab811a597d844ae67c578", "predicted_answer": "", "predicted_evidence": ["We conduct experiments across various settings and datasets. We report macro-averaged scores in all the settings."]}
{"question_id": "1c182b4805b336bd6e1a3f43dc84b07db3908d4a", "predicted_answer": "", "predicted_evidence": ["We use SLN: Satirical and Legitimate News Database BIBREF2, RPN: Random Political News Dataset BIBREF10 and LUN: Labeled Unreliable News Dataset BIBREF0 for our experiments. Table TABREF4 shows the statistics. Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines,"]}
{"question_id": "f71b95001dce46ee35cdbd8d177676de19ca2611", "predicted_answer": "", "predicted_evidence": ["A comparative analysis of how various deep learning models perform across different input representations and how various regularization techniques help with the generalization of our models."]}
{"question_id": "5aa6556ffd7142933f820a015f1294d38e8cd96c", "predicted_answer": "", "predicted_evidence": ["In this study, we seek to answer the following research questions:"]}
{"question_id": "10edfb9428b8a4652274c13962917662fdf84f8a", "predicted_answer": "", "predicted_evidence": ["The results for all of our models on both the ground-truth and augmented datasets are given in Table TABREF22."]}
{"question_id": "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763", "predicted_answer": "", "predicted_evidence": ["The results for all of our models on both the ground-truth and augmented datasets are given in Table TABREF22."]}
{"question_id": "0b5a7ccf09810ff5a86162d502697d16b3536249", "predicted_answer": "", "predicted_evidence": ["How does SEPT performance comparing to the existing single task system?"]}
{"question_id": "8f00859f74fc77832fa7d38c22f23f74ba13a07e", "predicted_answer": "", "predicted_evidence": ["How does SEPT performance comparing to the existing single task system?"]}
{"question_id": "bda21bfb2dd74085cbc355c70dab5984ef41dba7", "predicted_answer": "", "predicted_evidence": ["Table TABREF8 shows statistics for our final dataset of videos labeled with actions, and Figure 2 shows a sample video and transcript, with annotations."]}
{"question_id": "c2497552cf26671f6634b02814e63bb94ec7b273", "predicted_answer": "", "predicted_evidence": ["For our experiments, we use the first eight YouTube channels from our dataset as train data, the ninth channel as validation data and the last channel as test data. Statistics for this split are shown in Table TABREF10 ."]}
{"question_id": "441a2b80e82266c2cc2b306c0069f2b564813fed", "predicted_answer": "", "predicted_evidence": ["Moreover, the addition of extra information improves the results for both modalities. Specifically, the addition of context is found to bring improvements. The use of POS is also found to be generally helpful."]}
{"question_id": "e462efb58c71f186cd6b315a2d861cbb7171f65b", "predicted_answer": "", "predicted_evidence": ["To help detect spam, we identify and reject the turkers that assign the same label for every action in all five miniclips that they annotate. Additionally, each HIT contains a ground truth miniclip that has been pre-labeled by two reliable annotators. Each ground truth miniclip has more than four actions with labels that were agreed upon by both reliable annotators. We compute accuracy between a turker's answers and the ground truth annotations; if this accuracy is less than 20%, we reject the HIT as spam."]}
{"question_id": "84f9952814d6995bc99bbb3abb372d90ef2f28b4", "predicted_answer": "", "predicted_evidence": ["A distinctive aspect of this work is that we label actions in videos based on the language that accompanies the video. This has the potential to create a large repository of visual depictions of actions, with minimal human intervention, covering a wide spectrum of actions that typically occur in everyday life."]}
{"question_id": "5364fe5f256f1263a939e0a199c3708727ad856a", "predicted_answer": "", "predicted_evidence": ["Segment Videos into Miniclips. The length of our collected videos varies from two minutes to twenty minutes. To ease the annotation process, we split each video into miniclips (short video sequences of maximum one minute). Miniclips are split to minimize the chance that the same action is shown across multiple miniclips. This is done automatically, based on the transcript timestamp of each action. Because YouTube transcripts have timing information, we are able to line up each action with its corresponding frames in the video. We sometimes notice a gap of several seconds between the time an action occurs in the transcript and the time it is shown in the video. To address this misalignment, we first map the actions to the miniclips using the time information from the transcript. We then expand the miniclip by 15 seconds before the first action and 15 seconds after the last action. This increases the chance that all actions will be captured in the miniclip."]}
{"question_id": "e500948fa01c74e5cb3e6774f66aaa9ad4b3e435", "predicted_answer": "", "predicted_evidence": ["We are grateful to the crowd of experts that performed the hard work of precisely annotating problems. Most of them chose to remain anonymous. The others were, in alphabetical order: Rasmus Blank, Robin Cooper, Matthew Gotham, Julian Hough and Aarne Talman."]}
{"question_id": "b8b79a6123716cb9fabf751b31dff424235a2ee2", "predicted_answer": "", "predicted_evidence": ["We have additionally tagged each missing hypothesis according to the following classification:"]}
{"question_id": "00f507053c47e55d7e72bebdbd8a75b3ca88cf85", "predicted_answer": "", "predicted_evidence": ["We learn our response generation model by minimizing the negative log likelihood of INLINEFORM0 DISPLAYFORM0 "]}
{"question_id": "e14e3e0944ec3290d1985e9a3da82a7df17575cd", "predicted_answer": "", "predicted_evidence": ["Regarding human side-by-side evaluation, we can find that Edit-Default and Edit-N-rerank are slightly better than Retrieval-default and Retrieval-rerank (The winning examples are more than the losing examples), indicating that the post-editing is able to improve the response quality. Ed-Default is worse than Ens-Default, but Ed-N-Rerank is better than Ens-Rerank. This is mainly because the editing model regards the prototype response as the source language, so it is highly depends on the quality of prototype response."]}
{"question_id": "f637bba86cfb94ca8ac4b058faf839c257d5eaa0", "predicted_answer": "", "predicted_evidence": ["The decoder takes INLINEFORM0 as an input and generates a response by a GRU language model with attention. The hidden state of the decoder is acquired by DISPLAYFORM0 "]}
{"question_id": "0b5bf00d2788c534c4c6c007b72290c48be21e16", "predicted_answer": "", "predicted_evidence": ["Regarding human side-by-side evaluation, we can find that Edit-Default and Edit-N-rerank are slightly better than Retrieval-default and Retrieval-rerank (The winning examples are more than the losing examples), indicating that the post-editing is able to improve the response quality. Ed-Default is worse than Ens-Default, but Ed-N-Rerank is better than Ens-Rerank. This is mainly because the editing model regards the prototype response as the source language, so it is highly depends on the quality of prototype response."]}
{"question_id": "86c867b393db0ec4ad09abb48cc1353cac47ea4c", "predicted_answer": "", "predicted_evidence": ["A good prototype selector INLINEFORM0 plays an important role in the prototype-then-edit paradigm. We use different strategies to select prototypes for training and testing. In testing, as we described above, we retrieve a context-response pair INLINEFORM1 from a pre-defined index for context INLINEFORM2 according to the similarity of INLINEFORM3 and INLINEFORM4 . Here, we employ Lucene to construct the index and use its inline algorithm to compute the context similarity."]}
{"question_id": "8f6b11413a19fe4639b3fba88fc6b3678286fa0c", "predicted_answer": "", "predicted_evidence": ["In this section, we provide details of the specifications used in our experiments."]}
{"question_id": "141f23e87c10c2d54d559881e641c983e3ec8ef3", "predicted_answer": "", "predicted_evidence": ["Hyperparameters used in the baseline model are shown in Table TABREF25."]}
{"question_id": "45e6532ac06a59cb6a90624513242b06d7391501", "predicted_answer": "", "predicted_evidence": ["With the top-$k$ selection, the high attention scores are selected through an explicit way. This is different from dropout which randomly abandons the scores. Such explicit selection can not only guarantee the preservation of important components, but also simplify the model since $k$ is usually a small number such as 8, detailed analysis can be found in SECREF28. The next step after top-$k$ selection is normalization:"]}
{"question_id": "a98ae529b47362f917a398015c8525af3646abf0", "predicted_answer": "", "predicted_evidence": ["Evaluation\uff1a We use case-sensitive tokenized BLEU score BIBREF55 for the evaluation of WMT14 En-De, and we use case-insensitive BLEU for that of IWSLT 2015 En-Vi and IWSLT 2014 De-En following BIBREF8. Same as BIBREF0, compound splitting is used for WMT 14 En-De. For WMT 14 En-De and IWSLT 2014 De-En, we save checkpoints every epoch and average last 10 checkpoints every 5 epochs, We select the averaged checkpoint with best valid BLEU and report its BLEU score on the test set. For IWSLT 2015 En-Vi, we save checkpoints every 600 seconds and average last 20 checkpoints."]}
{"question_id": "58df55002fbcba76b9aeb2181d78378b8c01a827", "predicted_answer": "", "predicted_evidence": ["The time complexity for GLAD, GCE and our model (G-SAT) is shown in Figure FIGREF36. All models are executed with batch size of 50, under the same environment and hardware (single GeForce GTX 1080Ti GPU). As the pre-processing and post-processing of each model can vary based on the implementation and the approach, we report only the time complexity for the model execution after it is loaded and ready to be executed. GLAD requires 1.78 seconds for training for each batch of data, and 0.84 seconds to predict for a batch. Since GCE does not require a separate encoder for each slot, as in GLAD, it reduces the training time to 1.16 seconds, and the prediction time to 0.52 seconds. Our approach has a significant advantage in the execution time, requiring only 0.06 seconds for training and 0.03 for prediction of each batch. We notice that, while the time complexity of GLAD and GCE reported in BIBREF9 coincide with our results for training, results on the test data differ considerably. In fact, the time complexity for GCE reported in BIBREF9 was 1.92 seconds, while in our experiment we found that GCE instead processes 1.92 batches/second leading to 0.52 seconds/batch."]}
{"question_id": "7a60f29e28063f50c2a7afd1c2a7668fb615cd53", "predicted_answer": "", "predicted_evidence": ["further experiments show that the proposed model is highly robust when either pre-trained embeddings are used or when they are not used, in this case outperforming state-of-art systems."]}
{"question_id": "6371c6863fe9a14bf67560e754ce531d70de10ab", "predicted_answer": "", "predicted_evidence": ["where $_{\\mathrm {BERT}}^{\\mathrm {seed_1}}$ denotes the set of data points which are predicted correctly by BERT$_{\\small \\textsc {BASE}}$ with seed 1, and similarly for the rest. Table TABREF18 shows the average performance for each model trained with four different random seeds and the number of data points predicted correctly by all of them. Finally, we get 440 data points from the testing set $_{\\mathrm {TEST}}$ and we denote this subset as EASY set $_{\\mathrm {EASY}}$ and the other as HARD set $_{\\mathrm {HARD}}$."]}
{"question_id": "28a8a1542b45f67674a2f1d54fff7a1e45bfad66", "predicted_answer": "", "predicted_evidence": ["As mentioned earlier, biases prevalently exist in human-annotated datasets BIBREF16, BIBREF17, BIBREF18, BIBREF42, which are often exploited by models to perform well without truly understanding the text. Therefore, it is necessary to find out the biased data points in ReClor in order to evaluate models in a more comprehensive manner BIBREF43. To this end, we feed the five strong baseline models (GPT, GPT-2, BERT$_{\\small \\textsc {BASE}}$, XLNet$_{\\small \\textsc {BASE}}$ and RoBERTa$_{\\small \\textsc {BASE}}$) with ONLY THE ANSWER OPTIONS for each problem. In other words, we purposely remove the context and question in the inputs. In this way, we are able to identify those problems that can be answered correctly by merely exploiting the biases in answer options without knowing the relevant context and question. However, the setting of this task is a multiple-choice question with 4 probable options, and even a chance baseline could have 25% probability to get it right. To eliminate the effect of random guess, we set four different random seeds for each model and pick the data points that are predicted correctly in all four cases to form the EASY set. Then, the data points which are predicted correctly by the models at random could be nearly eliminated, since any data point only has a probability of $(25\\%)^{4}=0.39\\%$ to be guessed right consecutively for four times. Then we unite the sets of data points that are consistently predicted right by each model, because intuitively different models may learn different biases of the dataset. The above process is formulated as the following expression,"]}
{"question_id": "539f5c27e1a2d240e52b711d0a50a3a6ddfa5cb2", "predicted_answer": "", "predicted_evidence": ["Imbalanced dataset is a challenge for the task because the top patterns are taking too much attention so that most weights might be determined by the easier ones. We have tried different methods to deal with this problem. The first method is data expansion using oversampling. Attempts include duplicating the text with low pattern proportion, replacing first few characters with paddings in the window, randomly changing digits, and shifting the context window. The other method is to add loss control in the model as mentioned in SECREF2. The loss function helps the model to focus on harder cases in different classes and therefore reduce the impact of the imbalanced data. The experimental results are in SECREF11."]}
{"question_id": "aa7c5386aedfb13a361a2629b67cb54277e208d2", "predicted_answer": "", "predicted_evidence": ["Overall, w2v model has a better performance than BERT. A possible reason is that the model with BERT overfits the training data. The result also shows that data expansion does not give us better accuracy even though we find the model becomes more robust and has better performance on the lower proportioned patterns. This is because it changes the pattern distribution and the performance on the top proportioned patterns decreases a little, resulting in a large number of misclassifications. This is a tradeoff between a robust and a high-accuracy model, and we choose Model 1 for the following test since our golden set uses accuracy as the metric."]}
{"question_id": "9b3371dcd855f1d3342edb212efa39dfc9142ae3", "predicted_answer": "", "predicted_evidence": ["The rule-based TN model can handle the TN task alone and is the baseline in our experiments. It has the same idea as in BIBREF8 but has a more complicated system of rules with priorities. The model contains 45 different groups and about 300 patterns as sub-groups, each of which uses a keyword with regular expressions to match the preceding and following texts. Each pattern also has a priority value. During normalization, each sentence is fed as input and the NSW will be matched by the regular expressions. The model tries to match patterns with longer context and slowly decrease the context length until a match is found. If there are multiple pattern matches with the same length, the one with a higher priority will be chosen for the NSW. The model has been developed on abundant test data and bad cases. The advantage of the rule-based system is the flexibility, since one can simply add more special cases when they appear, such as new units. However, improving the performance of this system on more general cases becomes a bottleneck. For example, in a report of a football game, it cannot transform \u201c1-3\u201d to score if there are no keywords like \u201cscore\u201d or \u201cgame\u201d close to it."]}
{"question_id": "b02a6f59270b8c55fa4df3751bcb66fca2371451", "predicted_answer": "", "predicted_evidence": ["The training data is split into 36 different classes, each of which has its own NSW-SFW transformation. The distribution of the dataset is the same with the NSW in our internal news corpus and is imbalanced, which is one of the challenges for our neural model. The approaches to deal with the imbalanced dataset are discussed in the next section."]}
{"question_id": "3a3c372b6d73995adbdfa26103c85b32d071ff10", "predicted_answer": "", "predicted_evidence": ["Multi-head self-attention was proposed by GoogleBIBREF12 in the model of transformer. The model uses self-attention in the encoder and decoder and encoder-decoder attention in between. Motivated by the structure of transformer, multi-head self-attention is adopted in our neural model and the structure is shown in Fig. FIGREF4. Compared with other modules like LSTM and GRU, self-attention can efficiently extract the information of the NSW with all context in parallel and is fast to train. The core part of the neural model is similar to the encoder of a transformer. The inputs of the model are the sentences with their manually labeled NSW. We take a 30-character context window around each NSW and send it to the embedding layer. Padding is used when the window exceeds the sentence range. After 8 heads of self-attention, the model outputs a vector with dimension of the number of patterns. Finally, the highest masked softmax probability is chosen as the classified pattern group. The mask uses a regular expression to check if the NSW contain symbols and filters illegal ones such as classifying \u201c12:00\u201d as pure number, which is like a bi-class classification before softmax is applied."]}
{"question_id": "952fe4fbf4e0bcfcf44fab2dbd3ed85dd961eff3", "predicted_answer": "", "predicted_evidence": ["The following subsection includes examples of the above name variant categories in the Turkish tweet dataset analyzed, in addition to statistical information indicating the share of each category in the overall dataset."]}
{"question_id": "1dc5bf9dca7de2ba21db10e9056d3906267ef5d5", "predicted_answer": "", "predicted_evidence": ["The following subsection includes examples of the above name variant categories in the Turkish tweet dataset analyzed, in addition to statistical information indicating the share of each category in the overall dataset."]}
{"question_id": "8faec509406d33444bd620afc829adc9eae97644", "predicted_answer": "", "predicted_evidence": ["The following subsection includes examples of the above name variant categories in the Turkish tweet dataset analyzed, in addition to statistical information indicating the share of each category in the overall dataset."]}
{"question_id": "e3c2b6fcf77a7b1c76add2e6e1420d07c29996ea", "predicted_answer": "", "predicted_evidence": ["Results of our experiments are shown in Table 1. We find that while word-level knowledge distillation (Word-KD) does improve upon the baseline, sequence-level knowledge distillation (Seq-KD) does better on English INLINEFORM0 German and performs similarly on Thai INLINEFORM1 English. Combining them (Seq-KD INLINEFORM2 Word-KD) results in further gains for the INLINEFORM3 and INLINEFORM4 models (although not for the INLINEFORM5 model), indicating that these methods provide orthogonal means of transferring knowledge from the teacher to the student: Word-KD is transferring knowledge at the the local (i.e. word) level while Seq-KD is transferring knowledge at the global (i.e. sequence) level."]}
{"question_id": "ee2c2fb01d67f4c58855bf23186cbd45cecbfa56", "predicted_answer": "", "predicted_evidence": ["We therefore focus next on reducing the memory footprint of the student models further through weight pruning. Weight pruning for NMT was recently investigated by See2016, who found that up to INLINEFORM0 of the parameters in a large NMT model can be pruned with little loss in performance. We take our best English INLINEFORM1 German student model ( INLINEFORM2 Seq-KD INLINEFORM3 Seq-Inter) and prune INLINEFORM4 of the parameters by removing the weights with the lowest absolute values. We then retrain the pruned model on Seq-KD data with a learning rate of INLINEFORM5 and fine-tune towards Seq-Inter data with a learning rate of INLINEFORM6 . As observed by See2016, retraining proved to be crucial. The results are shown in Table 3."]}
{"question_id": "f77d7cddef3e021d70e16b9e16cecfd4b8ee80d3", "predicted_answer": "", "predicted_evidence": ["and train on both observed ( INLINEFORM0 ) and teacher-generated ( INLINEFORM1 ) data. However, this process is non-ideal for two reasons: (1) unlike for standard knowledge distribution, it doubles the size of the training data, and (2) it requires training on both the teacher-generated sequence and the true sequence, conditioned on the same source input. The latter concern is particularly problematic since we observe that INLINEFORM2 and INLINEFORM3 are often quite different."]}
{"question_id": "a0197894ee94b01766fa2051f50f84e16b5c9370", "predicted_answer": "", "predicted_evidence": ["Finally, although past work has shown that models with lower perplexity generally tend to have higher BLEU, our results indicate that this is not necessarily the case. The perplexity of the baseline INLINEFORM0 English INLINEFORM1 German model is INLINEFORM2 while the perplexity of the corresponding Seq-KD model is INLINEFORM3 , despite the fact that Seq-KD model does significantly better for both greedy ( INLINEFORM4 BLEU) and beam search ( INLINEFORM5 BLEU) decoding."]}
{"question_id": "55bafa0f7394163f4afd1d73340aac94c2d9f36c", "predicted_answer": "", "predicted_evidence": ["From Table 1 , we can see that the joint EL & RE gives a performance boost of 3% (from 44.1 to 47.1). We also analyze the impact of joint inference on the individual components of EL & RE."]}
{"question_id": "cbb4eba59434d596749408be5b923efda7560890", "predicted_answer": "", "predicted_evidence": ["Our work also intersects with relation extraction methods. While these methods aim to predict a relation between two entities in order to populate KBs BIBREF46 , BIBREF47 , BIBREF48 , we work with sentence level relation extraction for question answering. krishnamurthy2012weakly and fader2014open adopt open relation extraction methods for QA but they require hand-coded grammar for parsing queries. Closest to our extraction method is yao-jacana-freebase-acl2014 and yao-scratch-qa-naacl2015 who also uses sentence level relation extraction for QA. Unlike them, we can predict multiple relations per question, and our MCCNN architecture is more robust to unseen contexts compared to their logistic regression models."]}
{"question_id": "1d9d7c96c5e826ac06741eb40e89fca6b4b022bd", "predicted_answer": "", "predicted_evidence": ["Our refinement model is inspired by the intuition of how people refine their answers. If you ask someone: who did shaq first play for, and give them four candidate answers (Los Angeles Lakers, Boston Celtics, Orlando Magic and Miami Heat), as well as access to Wikipedia, that person might first determine that the question is about Shaquille O'Neal, then go to O'Neal's Wikipedia page, and search for the sentences that contain the candidate answers as evidence. By analyzing these sentences, one can figure out whether a candidate answer is correct or not."]}
{"question_id": "d1d37dec9053d465c8b6f0470e06316bccf344b3", "predicted_answer": "", "predicted_evidence": ["Table 3 presents the results on the impact of individual and joint channels on the end QA performance. When using a single-channel network, we tune the parameters of only one channel while switching off the other channel. As seen, the sentential features are found to be more important than syntactic features. We attribute this to the short and noisy nature of WebQuestions questions due to which syntactic parser wrongly parses or the shortest dependency path does not contain sufficient information to predict a relation. By using both the channels, we see further improvements than using any one of the channels."]}
{"question_id": "90eeb1b27f84c83ffcc8a88bc914a947c01a0c8b", "predicted_answer": "", "predicted_evidence": ["We use the pipelined EL and RE along with inference on Wikipedia as described in sec:refine."]}
{"question_id": "e057fa254ea7a4335de22fd97a0f08814b88aea4", "predicted_answer": "", "predicted_evidence": [" where DM is a pretrained distributed memory model BIBREF12 which converts the document-level context into a distributed representation. INLINEFORM0 and INLINEFORM1 are weight matrices."]}
{"question_id": "134a66580c363287ec079f353ead8f770ac6d17b", "predicted_answer": "", "predicted_evidence": ["Fine-grained entity typing is considered a multi-label classification problem: Each entity INLINEFORM0 in the text INLINEFORM1 is assigned a set of types INLINEFORM2 drawn from the fine-grained type set INLINEFORM3 . The goal of this task is to predict, given entity INLINEFORM4 and its context INLINEFORM5 , the assignment of types to the entity. This assignment can be represented by a binary vector INLINEFORM6 where INLINEFORM7 is the size of INLINEFORM8 . INLINEFORM9 iff the entity is assigned type INLINEFORM10 ."]}
{"question_id": "610fc593638c5e9809ea9839912d0b282541d42d", "predicted_answer": "", "predicted_evidence": ["tab:cases shows examples illustrating the benefits brought by our proposed approach. Example A illustrates that sentence-level context sometimes is not informative enough, and attention, though already placed on the head verbs, can be misleading. Including document-level context (i.e., \u201cCanada's declining crude output\u201d in this case) helps preclude wrong predictions (i.e., /other/health and /other/health/treatment). Example B shows that the semantic patterns learnt by our attention mechanism help make the correct prediction. As we observe in tab:ontonotes and tab:figer, adding hand-crafted features to our approach does not improve the results. One possible explanation is that hand-crafted features are mostly about syntactic-head or topic information, and such information are already covered by our attention mechanism and document-level contexts as shown in tab:cases. Compared to hand-crafted features that heavily rely on system or human annotations, attention mechanism requires significantly less supervision, and document-level or paragraph-level contexts are much easier to get."]}
{"question_id": "ab895ed198374f598e13d6d61df88142019d13b8", "predicted_answer": "", "predicted_evidence": ["To better understand the phenomena present in Quoref , we manually analyzed a random sample of 100 paragraph-question pairs. The following are some empirical observations."]}
{"question_id": "8795bb1f874e5f3337710d8c3d5be49e672ab43a", "predicted_answer": "", "predicted_evidence": ["We crowdsourced questions about these paragraphs on Mechanical Turk. We asked workers to find two or more co-referring spans in the paragraph, and to write questions such that answering them would require the knowledge that those spans are coreferential. We did not ask them to explicitly mark the co-referring spans. Workers were asked to write questions for a random sample of paragraphs from our pool, and we showed them examples of good and bad questions in the instructions (see Appendix ). For each question, the workers were also required to select one or more spans in the corresponding paragraph as the answer, and these spans are not required to be same as the coreferential spans that triggered the questions. We used an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0 as an adversary running in the background that attempted to answer the questions written by workers in real time, and the workers were able to submit their questions only if their answer did not match the adversary's prediction. Appendix further details the logistics of the crowdsourcing tasks. Some basic statistics of the resulting dataset can be seen in Table ."]}
{"question_id": "c30b0d6b23f0f01573eea315176c5ffe4e0c6b5c", "predicted_answer": "", "predicted_evidence": ["Figure FIGREF27 highlights the cross influence of each of the four parameters on their four associated attributes. Parameters are successively set to ratios of 0.25 (yellow), 0.50 (blue), 0.75 (violet) and 1.00 (red); the ground truth is displayed in green. Plots located on the diagonal show that most parameters have an effect their respective attributes (NbChars affects compression ratio, LevSim controls Levenshtein similarity...), although not with the same level of effectiveness."]}
{"question_id": "311f9971d61b91c7d76bba1ad6f038390977a8be", "predicted_answer": "", "predicted_evidence": ["On the other hand SARI compares the predicted simplification with both the source and the target references. It is an average of F1 scores for three $n$-gram operations: additions, keeps and deletions. For each operation, these scores are then averaged for all $n$-gram orders (from 1 to 4) to get the overall F1 score."]}
{"question_id": "23cbf6ab365c1eb760b565d8ba51fb3f06257d62", "predicted_answer": "", "predicted_evidence": ["ACCESS scores best on SARI (41.87), a significant improvement over previous state of the art (40.45), and third to best FKGL (7.22). The second and third models in terms of SARI, DMASS+DCSS (40.45) and SBMT+PPDB+SARI (39.96), both use the external resource Simple PPDB BIBREF36 that was extracted from 1000 times more data than what we used for training. Our FKGL is also better (lower) than these methods. The Hybrid model scores best on FKGL (4.56) i.e. they generated the simplest (and shortest) sentences, but it was done at the expense of SARI (31.40)."]}
{"question_id": "6ec267f66a1c5f996519aed8aa0befb5e5aec205", "predicted_answer": "", "predicted_evidence": ["Table TABREF13 shows the results obtained for the baseline and the CNNs trained for each language individually. Similar accuracies are obtained between the baseline and the CNN model for Spanish language, which also exhibit the highest accuracy among the three languages. Note that the highest accuracy for German language was obtained with the baseline model. Conversely, for Czech language the CNN produces the highest accuracy. Note also that for the three languages, the results are unbalanced towards one of the two classes according to the specificity and sensitivity values. The difference in the results obtained among the three languages can be explained considering the information provided in Table TABREF5. For the patients in the Spanish language, the average MDS-UPDRS-III score is higher compared with the German and Czech patients, i.e, there are patients with higher disease severity in the Spanish data compared to German and Czech patients."]}
{"question_id": "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e", "predicted_answer": "", "predicted_evidence": ["Table TABREF13 shows the results obtained for the baseline and the CNNs trained for each language individually. Similar accuracies are obtained between the baseline and the CNN model for Spanish language, which also exhibit the highest accuracy among the three languages. Note that the highest accuracy for German language was obtained with the baseline model. Conversely, for Czech language the CNN produces the highest accuracy. Note also that for the three languages, the results are unbalanced towards one of the two classes according to the specificity and sensitivity values. The difference in the results obtained among the three languages can be explained considering the information provided in Table TABREF5. For the patients in the Spanish language, the average MDS-UPDRS-III score is higher compared with the German and Czech patients, i.e, there are patients with higher disease severity in the Spanish data compared to German and Czech patients."]}
{"question_id": "a9d5f83f4b32c52105f2ae1c570f1c590ac52487", "predicted_answer": "", "predicted_evidence": ["To evaluate our proposed method, we conduct experiments on two benchmark datasets."]}
{"question_id": "288f0c003cad82b3db5e7231c189c0108ae7423e", "predicted_answer": "", "predicted_evidence": ["After determining the stances of people's reactions, another challenge is how we can utilize public stances to predict rumor veracity accurately. We observe that the temporal dynamics of public stances can indicate rumor veracity. Figure FIGREF2 illustrates the stance distributions of tweets discussing $true$ rumors, $false$ rumors, and $unverified$ rumors, respectively. As we can see, $supporting$ stance dominates the inception phase of spreading. However, as time goes by, the proportion of $denying$ tweets towards $false$ rumors increases quite significantly. Meanwhile, the proportion of $querying$ tweets towards $unverified$ rumors also shows an upward trend. Based on this observation, we propose to model the temporal dynamics of stance evolution with a recurrent neural network (RNN), capturing the crucial signals containing in stance features for effective veracity prediction."]}
{"question_id": "562a995dfc8d95777aa2a3c6353ee5cd4a9aeb08", "predicted_answer": "", "predicted_evidence": ["We conduct additional experiments to further demonstrate the effectiveness of our model."]}
{"question_id": "71e1f06daf6310609d00850340e64a846fbe2dfb", "predicted_answer": "", "predicted_evidence": ["To test the inference speed, we ran experiments on 105k samples from QNLI training set BIBREF20. Inference is performed on a single Titan RTX GPU with batch size set to 128, maximum sequence length set to 128, and FP16 activated. The inference time for the embedding layer is negligible compared to the Transformer layers. Results in Table TABREF26 show that the proposed Patient-KD approach achieves an almost linear speedup, 1.94 and 3.73 times for BERT$_6$ and BERT$_3$, respectively."]}
{"question_id": "ebb4db9c24aa36db9954dd65ea079a798df80558", "predicted_answer": "", "predicted_evidence": ["Finally, when comparing Setting #3 vs. #4, where for setting #4 we use Patient-KD-Skip instead of vanilla KD, we observe a performance gain on almost all the tasks, which indicates Patient-KD is a generic approach independent of the selection of the teacher model (BERT$_{12}$ or BERT$_{24}$)."]}
{"question_id": "7a212a34e9dbb0ba52c40471842b2e0e3e14f276", "predicted_answer": "", "predicted_evidence": ["Besides direct fine-tuning, we further implement a vanilla KD method on all the tasks by optimizing the objective function in Eqn. (DISPLAY_FORM10). We set the temperature $T$ as {5, 10, 20}, $\\alpha = \\lbrace 0.2, 0.5, 0.7 \\rbrace $, and perform grid search over $T$, $\\alpha $ and learning rate, to select the model with the best validation accuracy. For our proposed Patient-KD approach, we conduct additional search over $\\beta $ from $\\lbrace 10, 100, 500, 1000\\rbrace $ on all the tasks. Since there are so many hyper-parameters to learn for Patient KD, we fix $\\alpha $ and $T$ to the values used in the model with the best performance from the vanilla KD experiments, and only search over $\\beta $ and learning rate."]}
{"question_id": "ed15a593d64a5ba58f63c021ae9fd8f50051a667", "predicted_answer": "", "predicted_evidence": ["One full training iteration consisting of an insertion phase followed by a deletion phase can be represented by the following steps:"]}
{"question_id": "e86fb784011de5fda6ff8ccbe4ee4deadd7ee7d6", "predicted_answer": "", "predicted_evidence": ["We generate 100k examples to train on, and evaluate on 1000 held-out examples. We train our models for 200k steps, batch size of 32 and perform no model selection. The table below shows that the deletion model again increases the BLEU score over just the insertion model, by around 2 BLEU points."]}
{"question_id": "d206f2cbcc3d2a6bd0ccaa3b57fece396159f609", "predicted_answer": "", "predicted_evidence": ["Our final corpus consists of 622 protocols annotated by a team of 10 annotators. Corpus statistics are provided in Table TABREF5 and TABREF6 . In the first phase of annotation, we worked with a subset of 4 annotators including one linguist and one biologist to develop the annotation guideline for 6 iterations. For each iteration, we asked all 4 annotators to annotate the same 10 protocols and measured their inter-annotator agreement, which in turn helped in determining the validity of the refined guidelines. The average time to annotate a single protocol of 40 sentences was approximately 33 minutes, across all annotators."]}
{"question_id": "633e2210c740b4558b1eea3f041b3ae8e0813293", "predicted_answer": "", "predicted_evidence": ["In this paper, we described our effort to annotate wet lab protocols with actions and their semantic arguments. We presented an annotation scheme that is both biologically and linguistically motivated and demonstrated that non-experts can effectively annotate lab protocols. Additionally, we empirically demonstrated the utility of our corpus for developing machine learning approaches to shallow semantic parsing of instructions. Our annotated corpus of protocols is available for use by the research community."]}
{"question_id": "bb7c80ab28c2aebfdd0bd90b22a55dbdf3a8ed5b", "predicted_answer": "", "predicted_evidence": ["To emit a character the speller uses the attention mechanism to find a set of relevant activations of the listener INLINEFORM0 and summarize them into a context INLINEFORM1 . The history of previously emitted characters is encapsulated in a recurrent state INLINEFORM2 : DISPLAYFORM0 "]}
{"question_id": "6c4e1a1ccc0c5c48115864a6928385c248f4d8ad", "predicted_answer": "", "predicted_evidence": ["Understanding and preventing limitations specific to seq2seq models is crucial for their successful development. Discriminative training allows seq2seq models to focus on the most informative features. However, it also increases the risk of overfitting to those few distinguishing characteristics. We have observed that seq2seq models often yield very sharp predictions, and only a few hypotheses need to be considered to find the most likely transcription of a given utterance. However, high confidence reduces the diversity of transcripts obtained using beam search."]}
{"question_id": "55bde89fc5822572f794614df3130d23537f7cf2", "predicted_answer": "", "predicted_evidence": ["As a summary, all the experiments on different tasks show that training the Pre-LN Transformer does not rely on the learning rate warm-up stage and can be trained much faster than the Post-LN Transformer."]}
{"question_id": "523bc4e3482e1c9a8e0cb92cfe51eea92c20e8fd", "predicted_answer": "", "predicted_evidence": ["Our contributions are summarized as follows:"]}
{"question_id": "6073be8b88f0378cd0c4ffcad87e1327bc98b991", "predicted_answer": "", "predicted_evidence": ["$\\bullet $ We investigate two Transformer variants, the Post-LN Transformer and the Pre-LN Transformer, using mean field theory. By studying the gradients at initialization, we provide evidence to show why the learning rate warm-up stage is essential in training the Post-LN Transformer."]}
{"question_id": "f3b4e52ba962a0004064132d123fd9b78d9e12e2", "predicted_answer": "", "predicted_evidence": ["We conduct multiple experiments to evaluate the effectiveness of our system in many ways."]}
{"question_id": "ea6edf45f094586caf4684463287254d44b00e95", "predicted_answer": "", "predicted_evidence": ["Effectiveness on Translation Quality. As shown in Table TABREF49 , there is a great deal of difference between the sub-sentence and the baseline model. On an average the sub-sentence shows weaker performance by a 3.08 drop in BLEU score (40.39 INLINEFORM0 37.31). Similarly, the wait-k model also brings an obvious decrease in translation quality, even with the best wait-15 policy, its performance is still worse than the baseline system, with a 2.15 drop, averagely, in BLEU (40.39 INLINEFORM1 38.24). For a machine translation product, a large degradation in translation quality will largely affect the use experience even if it has low latency."]}
{"question_id": "ba406e07c33a9161e29c75d292c82a15503beae5", "predicted_answer": "", "predicted_evidence": ["Effectiveness on Translation Quality. As shown in Table TABREF49 , there is a great deal of difference between the sub-sentence and the baseline model. On an average the sub-sentence shows weaker performance by a 3.08 drop in BLEU score (40.39 INLINEFORM0 37.31). Similarly, the wait-k model also brings an obvious decrease in translation quality, even with the best wait-15 policy, its performance is still worse than the baseline system, with a 2.15 drop, averagely, in BLEU (40.39 INLINEFORM1 38.24). For a machine translation product, a large degradation in translation quality will largely affect the use experience even if it has low latency."]}
{"question_id": "3d662fb442d5fc332194770aac835f401c2148d9", "predicted_answer": "", "predicted_evidence": ["To ensure there are no sentences written in non-English languages, we keep questions that contain 80% or more of valid English characters, including punctuation."]}
{"question_id": "2280ed1e2b3e99921e2bca21231af43b58ca04f0", "predicted_answer": "", "predicted_evidence": ["We also benchmark other methods involving different training datasets and models. All the methods in this subsection use transformer models."]}
{"question_id": "961a97149127e1123c94fbf7e2021eb1aa580ecb", "predicted_answer": "", "predicted_evidence": ["The Cohen's Kappa inter-rater reliability scores BIBREF27 are 0.83, 0.77, and 0.89 respectively for the question quality annotations, and 0.86 for question semantic equivalence. These values show good inter-rater agreement on the annotations of the qualities and semantic equivalences of the MQR question pairs."]}
{"question_id": "1e4f45c956dfb40fadb8e10d4c1bfafa8968be4d", "predicted_answer": "", "predicted_evidence": ["The ill-formed and well-formed questions are shuffled so the annotators do not have any prior knowledge or bias regarding these questions during annotation. We randomly sample 300 questions from the shuffled DEVTEST questions, among which 145 examples are well-formed and 155 are ill-formed. Two annotators produce a judgment for each of the three aspects for all 300 questions."]}
{"question_id": "627ce8a1db08a732d5a8f7e1f8a72e3de89847e6", "predicted_answer": "", "predicted_evidence": ["The most common categories in DEV and TEST are \u201cdiy\u201d(295), \u201caskubuntu\u201d(288), \u201cmath\u201d(250), \u201cgaming\u201d(189), and \u201cphysics\u201d(140). The least common categories are mostly \u201cMeta Stack Exchange\u201d websites where people ask questions regarding the policies of posting questions on Stack Exchange sites. The most common categories in TRAIN are \u201caskubuntu\u201d(6237), \u201cmath\u201d(5933), \u201cgaming\u201d(3938), \u201cdiy\u201d(2791), and \u201c2604\u201d(scifi)."]}
{"question_id": "80bb07e553449bde9ac0ff35fcc718d7c161f2d4", "predicted_answer": "", "predicted_evidence": ["Code, data, trained models and result translations are available here - https://github.com/orevaoghene/pidgin-baseline"]}
{"question_id": "c8f8ecac23a991bceb8387e68b3b3f2a5d8cf029", "predicted_answer": "", "predicted_evidence": ["Future works include establishing qualitative metrics and the use of pre-trained models to bolster these translation models."]}
{"question_id": "28847b20ca63dc56f2545e6f6ec3082d9dbe1b3f", "predicted_answer": "", "predicted_evidence": ["Taking a look at the results from the word-level tokenization Pidgin to English models, the supervised model outperforms the unsupervised model, achieving a BLEU score of 24.67 in comparison to the BLEU score of 7.93 achieved by the unsupervised model. The supervised model trained with byte pair encoding tokenization achieved a BLEU score of 13.00. One thing that is worthy of note is that word-level tokenization methods seem to perform better on Pidgin to English translation models, in comparison to English to Pidgin translation models."]}
{"question_id": "2d5d0b0c54105717bf48559b914fefd0c94964a6", "predicted_answer": "", "predicted_evidence": ["For the word-level tokenization English to Pidgin models, the supervised model outperforms the unsupervised model, achieving a BLEU score of 17.73 in comparison to the BLEU score of 5.18 achieved by the unsupervised model. The supervised model trained with byte pair encoding tokenization outperforms both word-level tokenization models, achieving a BLEU score of 24.29."]}
{"question_id": "dd81f58c782169886235c48b8f9a08e0954dd3ae", "predicted_answer": "", "predicted_evidence": ["Future works include establishing qualitative metrics and the use of pre-trained models to bolster these translation models."]}
{"question_id": "c138a45301713c1a9f6edafeef338ba2f99220ce", "predicted_answer": "", "predicted_evidence": ["Then we see the group of contextual features Metadata with MAP=.256, and P@50=.370, followed by two sentence-level features: length and named entities, with MAP of .254 and .236, and P@50 of .340 and .280, respectively."]}
{"question_id": "56d788af4694c1cd1eebee0b83c585836d1f5f99", "predicted_answer": "", "predicted_evidence": ["Another interesting question is whether we should use our generic system or we should retrain with respect to the target medium. Table TABREF31 shows the results for such a comparison, and it further compares to CB Platform. We can see that for all nine media, our model outperforms CB Platform in terms of MAP and P@50; this is also true for the other measures in most cases."]}
{"question_id": "34b434825f0ca3225dc8914f9da865d2b4674f08", "predicted_answer": "", "predicted_evidence": ["At the bottom of the table we find position, a general contextual feature with MAP of .212 and P@50 of .230, followed by discourse and topics."]}
{"question_id": "61a2599acfbd3d75de58e97ecdba2d9cf0978324", "predicted_answer": "", "predicted_evidence": ["In this section, we present some in-depth analysis and further discussion."]}
{"question_id": "cf58d25bfa2561a359fdd7b6b20aef0b41dc634e", "predicted_answer": "", "predicted_evidence": ["New dataset: We build a new dataset of manually-annotated claims, extracted from the 2016 US presidential and vice-presidential debates, which we gathered from nine reputable sources such as CNN, NPR, and PolitiFact, and which we release to the research community."]}
{"question_id": "e86b9633dc691976dd00ed57d1675e1460f7167b", "predicted_answer": "", "predicted_evidence": ["We use a bert classifier to implement this classification model."]}
{"question_id": "b0edb9023f35a5a02eb8fb968e880e36233e66b3", "predicted_answer": "", "predicted_evidence": ["We combine the above two methods. On the one hand, we use the retrieve based method to sort KB relationships and entities, and on the other hand, we use the most related relationship and entity to generate the sparql statement to query the final answer."]}
{"question_id": "8c872236e4475d5d0969fb90d2df94589c7ab1c4", "predicted_answer": "", "predicted_evidence": ["It is conceivable that text representations could be context-sensitive. For example, the hidden states of a character language model have been used as a kind of nonsymbolic text representation BIBREF16 , BIBREF17 , BIBREF18 and these states are context-sensitive. However, such models will in general be a second level of representation; e.g., the hidden states of a character language model generally use character embeddings as the first level of representation. Conversely, position embeddings can also be the basis for a context-sensitive second-level text representation. We have to start somewhere when we represent text. Position embeddings are motivated by the desire to provide a representation that can be computed easily and quickly (i.e., without taking context into account), but that on the other hand is much richer than the symbolic alphabet."]}
{"question_id": "f6ba0a5cfd5b35219efe5e52b0a5b86ae85c5abd", "predicted_answer": "", "predicted_evidence": ["Processing text vs. speech vs. images. gillick16 write: \u201cIt is worth noting that noise is often added ... to images ... and speech where the added noise does not fundamentally alter the input, but rather blurs it. [bytes allow us to achieve] something like blurring with text.\u201d It is not clear to what extent blurring on the byte level is useful; e.g., if we blur the bytes of the word \u201cuniversity\u201d individually, then it is unlikely that the noise generated is helpful in, say, providing good training examples in parts of the space that would otherwise be unexplored. In contrast, the text representation we have introduced in this paper can be blurred in a way that is analogous to images and speech. Each embedding of a position is a vector that can be smoothly changed in every direction. We have showed that the similarity in this space gives rise to natural variation."]}
{"question_id": "b21f61c0f95fefdb1bdb90d51cbba4655cd59896", "predicted_answer": "", "predicted_evidence": ["It is conceivable that text representations could be context-sensitive. For example, the hidden states of a character language model have been used as a kind of nonsymbolic text representation BIBREF16 , BIBREF17 , BIBREF18 and these states are context-sensitive. However, such models will in general be a second level of representation; e.g., the hidden states of a character language model generally use character embeddings as the first level of representation. Conversely, position embeddings can also be the basis for a context-sensitive second-level text representation. We have to start somewhere when we represent text. Position embeddings are motivated by the desire to provide a representation that can be computed easily and quickly (i.e., without taking context into account), but that on the other hand is much richer than the symbolic alphabet."]}
{"question_id": "0dbb5309d2be97f6eda29d7ae220aa16cafbabb7", "predicted_answer": "", "predicted_evidence": ["End-to-end models are also used for entity typing based on the character sequence of the entity's name BIBREF113 ."]}
{"question_id": "c27b885b1e38542244f52056abf288b2389b9fc6", "predicted_answer": "", "predicted_evidence": ["In order to provide demographic annotations at scale, there exist two feasible methods: crowdsourcing and model-driven annotations. In the case of large-scale image datasets, crowdsourcing quickly becomes prohibitively expensive; ImageNet, for example, employed 49k AMT workers during its collection BIBREF14 . Model-driven annotations use supervised learning methods to create models that can predict annotations, but this approach comes with its own meta-problem; as the goal of this work is to identify demographic representation in data, we must analyze the annotation models for their performance on intersectional groups to determine if they themselves exhibit bias."]}
{"question_id": "1ce6c09cf886df41a3d3c52ce82f370c5a30334a", "predicted_answer": "", "predicted_evidence": ["We recognize that a binary representation of gender does not adequately capture the complexities of gender or represent transgender identities. In this work, we express gender as a continuous value between 0 and 1. When thresholding at 0.5, we use the sex labels of `male' and `female' to define gender classes, as training datasets and evaluation benchmarks use this binary label system. We again follow Merler et al. BIBREF18 and employ a DEX model to annotate the gender of an individual. When tested on APPA-REAL, with enhanced annotations provided by BIBREF21 , the model achieves an accuracy of 91.00%, however its errors are not evenly distributed, as shown in Table TABREF3 . The model errs more on younger and older age groups and on those with a female gender label."]}
{"question_id": "5429add4f166a3a66bec2ba22232821d2cbafd62", "predicted_answer": "", "predicted_evidence": ["In order to provide demographic annotations at scale, there exist two feasible methods: crowdsourcing and model-driven annotations. In the case of large-scale image datasets, crowdsourcing quickly becomes prohibitively expensive; ImageNet, for example, employed 49k AMT workers during its collection BIBREF14 . Model-driven annotations use supervised learning methods to create models that can predict annotations, but this approach comes with its own meta-problem; as the goal of this work is to identify demographic representation in data, we must analyze the annotation models for their performance on intersectional groups to determine if they themselves exhibit bias."]}
{"question_id": "d3d6a4a721b8bc9776f62759b8d9be1a19c6b0d2", "predicted_answer": "", "predicted_evidence": ["The training of the double-attention NMT model is similar to the conventional attention-based NMT model, though the log likelihood function now depends on two input sequences INLINEFORM0 and INLINEFORM1 . This is written as follows: DISPLAYFORM0 "]}
{"question_id": "cc8f495cac0af12054c746a5b796e989ff0e5d5f", "predicted_answer": "", "predicted_evidence": ["The remaining of the paper is structured as follows: the next section reviews some related work, and Section 3 briefly describes the attention-based NMT model. Section 4 introduces the double-attention NMT model, and Section 5 presents the experiments. Finally, the paper ends up with a conclusion."]}
{"question_id": "64c45fdb536ae294cf06716ac20d08b5fdb7944d", "predicted_answer": "", "predicted_evidence": ["We reproduced the attention-based NMT system proposed by Bahdanau et al. BIBREF4 . The implementation was based on Tensorflow. We compared our implementation with a public implementation using Theano, and got a comparable performance on the same data sets with the same parameter settings."]}
{"question_id": "bab4ae97afd598a11d1fc7c05c6fdb98c30cafe0", "predicted_answer": "", "predicted_evidence": ["To evaluate and monitor the annotation quality of different annotators, our Multi-Annotator Analysis (MAA) toolkit imports all the annotated files and gives the analysis results in a matrix. As shown in Figure FIGREF16 , the matrix gives the F1-scores in full level (consider both boundary and label accuracy) and boundary level (ignore the label correctness, only care about the boundary accuracy) of all annotator pairs."]}
{"question_id": "f5913e37039b9517a323ec700b712e898316161b", "predicted_answer": "", "predicted_evidence": ["\u2022 INLINEFORM0 Overall statistics: it shows the specific precision, recall and F1-score of two files in all labels. It also gives the three accuracy indexes on overall full level and boundary level in the end."]}
{"question_id": "a064d01d45a33814947161ff208abb88d4353b26", "predicted_answer": "", "predicted_evidence": ["\u2022 INLINEFORM0 Comprehensive: it integrates useful toolkits to give the statistical index of analyzing multi-user annotation results and generate detailed content comparison for annotation pairs."]}
{"question_id": "3d5b4aa1ce99903b1fcd257c1e394f7990431d13", "predicted_answer": "", "predicted_evidence": ["We used three ontologies: (i) the Wine Ontology, one of the most commonly used examples of owl ontologies; (ii) the m-piro ontology, which describes a collection of museum exhibits, was originally developed in the m-piro project BIBREF27 , was later ported to owl, and accompanies Naturalowl BIBREF11 ; and (iii) the Disease Ontology, which describes diseases, including their symptoms, causes etc."]}
{"question_id": "8d3f79620592d040f9f055b4fce0f73cc45aab63", "predicted_answer": "", "predicted_evidence": ["In this section, we first give a brief introduction to the RACE dataset, and then explain the rationale behind choosing RACE as the testbed in our study."]}
{"question_id": "65e30c842e4c140a6cb8b2f9498fcc6223ed49c0", "predicted_answer": "", "predicted_evidence": ["We would like to thank Panupong Pasupat for helpful discussions on the pruning algorithm, and for providing us with the unpruned logical form candidates. We would like to thank Pradeep Dasigi for helping us train the KDG model. "]}
{"question_id": "65e26b15e087bedb6e8782d91596b35e7454b16b", "predicted_answer": "", "predicted_evidence": ["The basic idea in binary paragraph vector models is to introduce a sigmoid nonlinearity before the softmax that models the conditional probability of words given the context. If we then enforce binary or near-binary activations in this nonlinearity, the probability distribution over words will be conditioned on a bit vector context, rather than real-valued representation. The inference in the model proceeds like in Paragraph Vector, except the document code is constructed from the sigmoid activations. After rounding, this code can be seen as a distributed binary representation of the document."]}
{"question_id": "a8f189fad8b72f8b2b4d2da4ed8475d31642d9e7", "predicted_answer": "", "predicted_evidence": ["The basic idea in binary paragraph vector models is to introduce a sigmoid nonlinearity before the softmax that models the conditional probability of words given the context. If we then enforce binary or near-binary activations in this nonlinearity, the probability distribution over words will be conditioned on a bit vector context, rather than real-valued representation. The inference in the model proceeds like in Paragraph Vector, except the document code is constructed from the sigmoid activations. After rounding, this code can be seen as a distributed binary representation of the document."]}
{"question_id": "eafea4a24d103fdecf8f347c7d84daff6ef828a3", "predicted_answer": "", "predicted_evidence": ["We use AdaGrad BIBREF17 for training and inference in all experiments reported in this work. During training we employ dropout BIBREF18 in the embedding layer. To facilitate models with large vocabularies, we approximate the gradients with respect to the softmax logits using the method described by BIBREF9 . Binary PV-DM networks use the same number of dimensions for document codes and word embeddings."]}
{"question_id": "e099a37db801718ab341ac9a380a146c7452fd21", "predicted_answer": "", "predicted_evidence": ["Binary codes have also been applied to cross-modal retrieval where text is one of the modalities. Specifically, BIBREF4 incorporated tag information that often accompany text documents, while BIBREF5 employed siamese neural networks to learn single binary representation for text and image data."]}
{"question_id": "ead7704a64447dccd504951618d3be463eba86bf", "predicted_answer": "", "predicted_evidence": ["The data set for the coding of death certificates is called the C\u00e9piDC corpus. Three CSV files (AlignedCauses) were provided by task organizers containing annotated death certificates for different periods : 2006 to 2012, 2013 and 2014. This training set contained 125383 death certificates. Each certificate contains one or more lines of text (medical causes that led to death) and some metadata. Each CSV file contains a \"Raw Text\" column entered by a physician, a \"Standard Text\" column entered by a human coder that supports the selection of an ICD-10 code in the last column. Table TABREF2 presents an excerpt of these files. Zero to multiples ICD-10 codes can be assigned to each line of a death certificate."]}
{"question_id": "8476d0bf5962f4ed619a7b87415ebe28c38ce296", "predicted_answer": "", "predicted_evidence": ["We also plan to combine machine learning techniques with a dictionary-based approach. Our system can already detect and replace typos and abbreviations to help machine learning techniques increase their performance."]}
{"question_id": "bbfe7e131ed776c85f2359b748db1325386c1af5", "predicted_answer": "", "predicted_evidence": ["The first dictionary contained 42439 terms and 3,539 ICD-10 codes (run2) and the second one 148448 terms and 6,392 ICD-10 codes (run1)."]}
{"question_id": "b6dae03d56dff0db8ad2a1bff9c7dd3f87551cd1", "predicted_answer": "", "predicted_evidence": ["Expressen"]}
{"question_id": "f93bad406e004014618dd64f6c604b1a9ee6a371", "predicted_answer": "", "predicted_evidence": ["As can be seen in Table 3 , there is a marked difference when conditioning on issues versus using regular document \u2014 i.e. cosine \u2014 similarity. Furthermore, we observe that conditioned similarity seems to align left wing media with left wing parties, nativist media with the Swedish Democrats, but not align right wing media with right wing parties. This effect can be made more apparent by grouping the parties into blocs and fitting a simple additive model for the similarities along all dimensions (i.e. Media, Issues, and Bloc), as a way to normalize for general Media, Issue, and Bloc similarity. The results of this normalization, i.e. the residuals, can be observed in Table 4 . From this one can see a small trend where left wing media is similar to left wing parties, nativist media being similar to the Swedish Democrats, and both left wing media and right wing media being dissimilar to the Swedish Democrats."]}
{"question_id": "c5ea4da3c760ba89194ad807bc1ef60e1761429f", "predicted_answer": "", "predicted_evidence": ["(1) Are the sentiment classes ordered?"]}
{"question_id": "4a093a9af4903a59057a4372ac1b01603467ca58", "predicted_answer": "", "predicted_evidence": ["The main hypothesis of this paper is that the inter-annotator agreement approximates an upper bound for a classifier performance. In Fig FIGREF8 we observe three such cases where the classifier performance, in the range 0.4\u20130.6, approaches its limit: Polish, Slovenian, and DJIA30. There are also three cases where there still appears a gap between the classifier performance and the inter-annotator agreement: English, Facebook(it), and Environment. In order to confirm the hypothesis, we analyze the evolution of the classifiers performance through time and check if the performance is still improving or was the plateau already reached. This is not always possible: There are datasets where only one annotator was engaged and for which there is no inter-annotator agreement (Russian, Swedish, Hungarian, Slovak, and Portuguese). For them we can only draw analogies with the multiply annotated datasets and speculate about the conclusions."]}
{"question_id": "f4e16b185b506713ff99acc4dbd9ec3208e4997b", "predicted_answer": "", "predicted_evidence": ["(5) What are acceptable levels of annotators agreement? On the basis of the 17 datasets analyzed, we propose the following rule-of-thumb: for self-agreement, INLINEFORM0 INLINEFORM1 , and for the inter-annotator agreement, INLINEFORM2 INLINEFORM3 ."]}
{"question_id": "4683812cba21c92319be68c03260b5a8175bbb6e", "predicted_answer": "", "predicted_evidence": ["A standard statistical method for testing the significant differences between multiple classifiers BIBREF43 is the well-known ANOVA and its non-parametric counterpart, the Friedman test BIBREF14 . The Friedman test ranks the classifiers for each dataset separately. The best performing classifier is assigned rank 1, the second best rank 2, etc. When there are ties, average ranks are assigned. The Friedman test then compares the average ranks of the classifiers. The null hypothesis is that all the classifiers are equivalent and so their ranks should be equal. If the null hypothesis is rejected, one proceeds with a post-hoc test."]}
{"question_id": "c25014b7e57bb2949138d64d49f356d69838bc25", "predicted_answer": "", "predicted_evidence": ["Each of our three models is evaluated by internal cross-validation using the metrics described above; however, the conversational AI system as a whole is validated using external metrics: agent handoff rate and booking completion rate. The agent handoff rate is the proportion of conversations that involve a customer support agent; the booking completion rate is the proportion of conversations that lead to a completed hotel booking. Both are updated on a daily basis."]}
{"question_id": "25a8d432bf94af1662837877bc6c284e2fc3fbe2", "predicted_answer": "", "predicted_evidence": ["Our NER model instead identifies hotel and location names, for example:"]}
{"question_id": "be632f0246c2e5f049d12e796812f496e083c33e", "predicted_answer": "", "predicted_evidence": ["Second, we employ professional annotators to create training data for each of our models, using a custom-built interface. A pool of relevant messages is selected from past user conversations; each message is annotated once and checked again by a different annotator to minimize errors. We use the PyBossa framework to manage the annotation processes."]}
{"question_id": "415b42ef6ff92553d04bd44ed0cbf6b3d6c83e51", "predicted_answer": "", "predicted_evidence": ["Each of our three models is evaluated by internal cross-validation using the metrics described above; however, the conversational AI system as a whole is validated using external metrics: agent handoff rate and booking completion rate. The agent handoff rate is the proportion of conversations that involve a customer support agent; the booking completion rate is the proportion of conversations that lead to a completed hotel booking. Both are updated on a daily basis."]}
{"question_id": "9da181ac8f2600eb19364c1b1e3cdeb569811a11", "predicted_answer": "", "predicted_evidence": ["Second, we employ professional annotators to create training data for each of our models, using a custom-built interface. A pool of relevant messages is selected from past user conversations; each message is annotated once and checked again by a different annotator to minimize errors. We use the PyBossa framework to manage the annotation processes."]}
{"question_id": "67f1b8a9f72e62cd74ec42e9631ef763a9b098c7", "predicted_answer": "", "predicted_evidence": ["The intent model processes each incoming user message and classifies it as one of several intents. The most common intents are thanks, cancel, stop, search, and unknown (described in Table TABREF12); these intents were chosen for automation based on volume, ease of classification, and business impact. The result of the intent model is used to determine the bot's response, what further processing is necessary (in the case of search intent), and whether to direct the conversation to a human agent (in the case of unknown intent)."]}
{"question_id": "9a6bf1d481e6896eef9f8fed835d9d29658ede36", "predicted_answer": "", "predicted_evidence": ["We see similar observations for the second baseline (PG$+$Cov): recall is generally improved at the expense of precision. In terms of F1, the gap between the baseline and our models is a little closer, and M1-latent and M2-latent are the two best performers."]}
{"question_id": "4999da863ecbd40378505bfb1f4e395061a3f559", "predicted_answer": "", "predicted_evidence": ["We once again use the methods of incorporation presented in Section SECREF18. As the classification model uses convolution networks, only Method-1 is directly applicable."]}
{"question_id": "3098793595252039f363ee1150d4ea956f2504b8", "predicted_answer": "", "predicted_evidence": ["We train the models for approximately 240,000-270,000 iterations (13 epochs). When we include the coverage mechanism (second baseline), we train for an additional 3,000\u20133,500 iterations using the coverage penalty, following see2017get."]}
{"question_id": "99f898eb91538cb82bc9a00892d54ae2a740961e", "predicted_answer": "", "predicted_evidence": ["The contribution of our work is summarised as:"]}
{"question_id": "cf68906b7d96ca0c13952a6597d1f23e5184c304", "predicted_answer": "", "predicted_evidence": ["2. Decoding the subsequent 30 words, which was adopted from the skip-thought training code, gave reasonably good performance. More words for decoding didn't give us a significant performance gain, and took longer to train."]}
{"question_id": "3e5162e6399c7d03ecc7007efd21d06c04cf2843", "predicted_answer": "", "predicted_evidence": ["Research Objectives nolistsep"]}
{"question_id": "bd255aadf099854541d06997f83a0e478f526120", "predicted_answer": "", "predicted_evidence": ["What do you know about the ParityBOT?"]}
{"question_id": "a9ff35f77615b3a4e7fd7b3a53d0b288a46f06ce", "predicted_answer": "", "predicted_evidence": ["Did you encounter it? Tell me about how you first encountered it? Did it provide any value to you during your campaign? How? Do you think this is a useful tool? Why or why not? Did it mitigate the barrier of online harassment during your time as a politician?"]}
{"question_id": "69a46a227269c3aac9bf9d7c3d698c787642f806", "predicted_answer": "", "predicted_evidence": ["In this section, we outline the technical details of ParityBot. The system consists of: 1) a Twitter listener that collects and classifies tweets directed at a known list of women candidates, and 2) a responder that sends out positivitweets when hateful tweets are detected."]}
{"question_id": "ebe6b8ec141172f7fea66f0a896b3124276d4884", "predicted_answer": "", "predicted_evidence": ["We would like to run ParityBOT in more jurisdictions to expand the potential impact and feedback possibilities. In future iterations, the system might better match positive tweets to the specific type of negative tweet the bot is responding to. Qualitative analysis helps to support the interventions we explore in this paper. To that end, we plan to survey more women candidates to better understand how a tool like this impacts them. Additionally, we look forward to talking to more women interested in politics to better understand whether a tool like this would impact their decision to run for office. We would like to expand our hateful tweet classification validation study to include larger, more recent abusive tweet datasets BIBREF19, BIBREF20. We are also exploring plans to extend ParityBOT to invite dialogue: for example, asking people to actively engage with ParityBOT and analyse reply and comment tweet text using natural language-based discourse analysis methods."]}
{"question_id": "946d7c877d363f549f84e9500c852dce70ae5d36", "predicted_answer": "", "predicted_evidence": ["Later, two-layer bidirectional GRU, with the output size of $d$ for each direction, is used to fully fuse the information contained in the preliminary representation and the additional useful information included in the complementary representation. We concatenate the outputs of the GPUs in two dimensions together, and we hence obtain the final contextualized representation $F$ of input text:"]}
{"question_id": "26e32f24fe0c31ef25de78935daa479534b9dd58", "predicted_answer": "", "predicted_evidence": ["HIdden Representation Extractor (HIRE) dynamically learns a complementary representation which contains the information that the final layer's output fails to capture. We put 2-layer bidirectional GRU beside the encoder to summarize the output of each layer into a single vector which will be used to compute the contribution score."]}
{"question_id": "22375aac4cbafd252436b756bdf492a05f97eed8", "predicted_answer": "", "predicted_evidence": ["However, the experiment result (Table TABREF17 ) shows that almost the same perplexity was achieved by reversing the order of words. This indicates that the same amount statistical information, but not exactly the same statistical information, for a word in a word sequence can be obtained from its following context as from its previous context, at least for English."]}
{"question_id": "d2f91303cec132750a416192f67c8ac1d3cf6fc0", "predicted_answer": "", "predicted_evidence": ["Like word classes, caching is also a common used optimization technique in LM. The cache language models are based on the assumption that the word in recent history are more likely to appear again. In cache language model, the conditional probability of a word is calculated by interpolating the output of standard language model and the probability evaluated by caching, like: INLINEFORM0 "]}
{"question_id": "9f065e787a0d40bb4550be1e0d64796925459005", "predicted_answer": "", "predicted_evidence": ["The performance of neural network language models is usually measured using perplexity (PPL) which can be defined as: INLINEFORM0 "]}
{"question_id": "e6f5444b7c08d79d4349e35d5298a63bb30e7004", "predicted_answer": "", "predicted_evidence": ["Various architectures of neural network language models are described and a number of improvement techniques are evaluated in this paper, but there are still something more should be included, like gate recurrent unit (GRU) RNNLM, dropout strategy for addressing overfitting, character level neural network language model and ect. In addition, the experiments in this paper are all performed on Brown Corpus which is a small corpus, and different results may be obtained when the size of corpus becomes larger. Therefore, all the experiments in this paper should be repeated on a much larger corpus."]}
{"question_id": "59f41306383dd6e201bded0f1c7c959ec4f61c5a", "predicted_answer": "", "predicted_evidence": ["We present our results in [tab:elmo]Table tab:elmo."]}
{"question_id": "b3432f52af0b95929e6723dd1f01ce029d90a268", "predicted_answer": "", "predicted_evidence": ["We present our results in [tab:elmo]Table tab:elmo."]}
{"question_id": "6b1a6517b343fdb79f246955091ff25e440b9511", "predicted_answer": "", "predicted_evidence": ["We consider several evaluation metrics to estimate the quality and diversity of the generations."]}
{"question_id": "5f25b57a1765682331e90a46c592a4cea9e3a336", "predicted_answer": "", "predicted_evidence": ["where $\\mathcal {R}$ includes all face position trajectories detected by the face tracking module within the input period. We call a face position trajectory a tracklet. The joint posterior probability on the right hand side (RHS) can be factorized as"]}
{"question_id": "2ba2ff6c21a16bd295b07af1ef635b3b4c5bd17e", "predicted_answer": "", "predicted_evidence": ["The meetings were recorded in various conference rooms. The recording device was placed at a random position on a table in each room. We had meeting attendees sign up for the data collection program and go through audio and video enrollment steps. For each attendee, we obtained approximately a voice recording of 20 to 30 seconds and 10 or fewer close-up photos from different angles. A total of 26 meetings were recorded for the evaluation purpose. Each meeting had a different number of attendees, ranging from 2 to 11. The total number of unique participants were 62. No constraint was imposed on seating arrangements."]}
{"question_id": "74acaa205a5998af4ad7edbed66837a6f2b5c58b", "predicted_answer": "", "predicted_evidence": ["KDMN-NoKG: baseline version of our model. No external knowledge involved in this model. Other parameters are set the same as full model."]}
{"question_id": "cfcf94b81589e7da215b4f743a3f8de92a6dda7a", "predicted_answer": "", "predicted_evidence": ["In this section, we report the quantitative evaluation along with representative samples of our method, compared with our ablative models and the state-of-the-art method for both the conventional (close-domain) VQA task and open-domain VQA."]}
{"question_id": "d147117ef24217c43252d917d45dff6e66ff807c", "predicted_answer": "", "predicted_evidence": ["KDMN: our full model. External knowledge triples are incorporated in Dynamic Memory Network."]}
{"question_id": "1a2b69dfa81dfeadd67b133229476086f2cc74a8", "predicted_answer": "", "predicted_evidence": ["KDMN: our full model. External knowledge triples are incorporated in Dynamic Memory Network."]}
{"question_id": "6d6a9b855ec70f170b854baab6d8f7e94d3b5614", "predicted_answer": "", "predicted_evidence": ["This method corresponds to the bottom-left part of Figure FIGREF1 (in green). It consists in extracting certain features from the content of each considered message, and to train a Support Vector Machine (SVM) classifier to distinguish abusive (Abuse class) and non-abusive (Non-abuse class) messages BIBREF14 . These features are quite standard in Natural Language Processing (NLP), so we only describe them briefly here."]}
{"question_id": "870358f28a520cb4f01e7f5f780d599dfec510b4", "predicted_answer": "", "predicted_evidence": ["Besides a better understanding of the dataset and classification process, one interesting use of the TF is that they can allow decreasing the computational cost of the classification. In our case, this is true for all methods: we can retain 97% of the performance while using only a handful of features instead of hundreds. For instance, with the Late Fusion TF, we need only 3% of the total Late Fusion runtime."]}
{"question_id": "98aa86ee948096d6fe16c02c1e49920da00e32d4", "predicted_answer": "", "predicted_evidence": ["We now want to identify the most discriminative features for all three fusion strategies. We apply an iterative method based on the Sklearn toolkit, which allows us to fit a linear kernel SVM to the dataset and provide a ranking of the input features reflecting their importance in the classification process. Using this ranking, we identify the least discriminant feature, remove it from the dataset, and train a new model with the remaining features. The impact of this deletion is measured by the performance difference, in terms of INLINEFORM0 -measure. We reiterate this process until only one feature remains. We call Top Features (TF) the minimal subset of features allowing to reach INLINEFORM1 of the original performance (when considering the complete feature set)."]}
{"question_id": "c463136ba9a312a096034c872b5c74b9d58cef95", "predicted_answer": "", "predicted_evidence": ["The Top Features obtained for each method are listed in Table TABREF12 . The last 4 columns precise which variants of the graph-based features are concerned. Indeed, as explained in Section SECREF3 , most of these topological measures can handle/ignore edge weights and/or edge directions, can be vertex- or graph-focused, and can be computed for each of the three types of networks (Before, After and Full)."]}
{"question_id": "81e101b2c803257492d67a00e8a1d9a07cbab136", "predicted_answer": "", "predicted_evidence": ["The supervision is described under each task, e.g., there are three (conflicting) sources for the Intent task. A task requires labels at the appropriate granularity (singleton, sequence, or set) and type (multiclass or bitvector). The labels are tagged by the source that produced them: these labels may be incomplete and even contradictory. Overton models the sources of these labels, which may come human annotators, or from engineer-defined heuristics such as data augmentation or heuristic labelers. Overton learns the accuracy of these sources using ideas from the Snorkel project BIBREF1. In particular, it estimates the accuracy of these sources and then uses these accuracies to compute a probability that each training point is correct BIBREF9. Overton incorporates this information into the loss function for a task; this also allows Overton to automatically handle common issues like rebalancing classes."]}
{"question_id": "b942d94e4187e4fdc706cfdf92e3a869fc294911", "predicted_answer": "", "predicted_evidence": ["Overton takes as input a schema whose design goal is to support rich applications from modeling to automatic deployment. In more detail, the schema has two elements: (1) data payloads similar to a relational schema, which describe the input data, and (2) model tasks, which describe the tasks that need to be accomplished. The schema defines the input, output, and coarse-grained data flow of a deep learning model. Informally, the schema defines what the model computes but not how the model computes it: Overton does not prescribe architectural details of the underlying model (e.g., Overton is free to embed sentences using an LSTM or a Transformer) or hyperparameters, like hidden state size. Additionally, sources of supervision are described as data\u2013not in the schema\u2013so they are free to rapidly evolve."]}
{"question_id": "8ffae517bc0efa453b7e316d41bd9f1b6679b158", "predicted_answer": "", "predicted_evidence": ["We briefly cover some of the design decisions in Overton."]}
{"question_id": "0fd2854dd8d8191f00c8d12483b5a81a04de859f", "predicted_answer": "", "predicted_evidence": ["Overton takes as input a schema whose design goal is to support rich applications from modeling to automatic deployment. In more detail, the schema has two elements: (1) data payloads similar to a relational schema, which describe the input data, and (2) model tasks, which describe the tasks that need to be accomplished. The schema defines the input, output, and coarse-grained data flow of a deep learning model. Informally, the schema defines what the model computes but not how the model computes it: Overton does not prescribe architectural details of the underlying model (e.g., Overton is free to embed sentences using an LSTM or a Transformer) or hyperparameters, like hidden state size. Additionally, sources of supervision are described as data\u2013not in the schema\u2013so they are free to rapidly evolve."]}
{"question_id": "742d5e182b57bfa5f589fde645717ed0ac3f49c2", "predicted_answer": "", "predicted_evidence": ["We also evaluate the models using mean reciprocal rank (MRR). When calculating the F INLINEFORM0 -based evaluation measure we decode the model by taking the single highest-scoring value for each slot. However, this does not necessarily reflect the quality of the overall value ranking produced by the model. For this reason we include MRR, defined as: DISPLAYFORM0 "]}
{"question_id": "726c5c1b6951287f4bae22978f9a91d22d9bef61", "predicted_answer": "", "predicted_evidence": ["The representation and scoring components of our architecture, with an additional slot for predicting a null value. The INLINEFORM0 scores are used when constructing the loss and during decoding. These scores can also be aggregated in a max/sum manner after decoding, but such aggregation is not incorporated during training."]}
{"question_id": "dfdd309e56b71589b25412ba85b0a5d79a467ceb", "predicted_answer": "", "predicted_evidence": ["Table TABREF54 shows the accuracy of the model on each slot type. The model is struggles with predicting the Injuries and Survivors slots. The nature of news media leads these slots to be discussed less frequently, with their mentions often embedded more deeply in the document, or expressed textually. For instance, it is common to express INLINEFORM0 =Survivors, INLINEFORM1 as \u201cno survivors\u201d, but it is impossible to predict a 0 value in this case, under the current problem definition."]}
{"question_id": "7ae95716977d39d96e871e552c35ca0753115229", "predicted_answer": "", "predicted_evidence": ["Table TABREF54 shows the accuracy of the model on each slot type. The model is struggles with predicting the Injuries and Survivors slots. The nature of news media leads these slots to be discussed less frequently, with their mentions often embedded more deeply in the document, or expressed textually. For instance, it is common to express INLINEFORM0 =Survivors, INLINEFORM1 as \u201cno survivors\u201d, but it is impossible to predict a 0 value in this case, under the current problem definition."]}
{"question_id": "ff3e93b9b5f08775ebd1a7408d7f0ed2f6942dde", "predicted_answer": "", "predicted_evidence": ["We use a pre-trained n-gram language model to score the phrase translation candidates by providing the relative likelihood estimation $P(t)$, so that the translation of a source phrase is derived from: $arg max_{t} P(t|s)=arg max_{t} P(s|t)P(t)$."]}
{"question_id": "59a3d4cdd1c3797962bf8d72c226c847e06e1d44", "predicted_answer": "", "predicted_evidence": ["In the pre-processing, we use the special tokens <NUMBER> and <DATE> to replace numbers that express a specific quantity and date respectively. Therefore, in the post-processing, we need to restore those numbers. We simply detect the pattern <NUMBER> and <DATE> in the original source sentences and then replace the special tokens in the translated sentences with the corresponding numbers detected in the source sentences. In order to make the replacement more accurate, we will detect more complicated patterns like <NUMBER> / <NUMBER> in the original source sentences. If the translated sentences also have the pattern, we replace this pattern <NUMBER> / <NUMBER> with the corresponding numbers in the original source sentences."]}
{"question_id": "49474a3047fa3f35e1bcd63991e6f15e012ac10b", "predicted_answer": "", "predicted_evidence": ["We propose a method to combine word and subword (BPE) pre-trained input representations aligned using MUSE BIBREF0 as an NMT training initialization on a morphologically-rich language pair such as German and Czech."]}
{"question_id": "63279ecb2ba4e51c1225e63b81cb021abc10d0d1", "predicted_answer": "", "predicted_evidence": ["where $v^*(x)$ denotes sentences in the target language translated from source language sentences $S$, $u^*(y)$ similarly denotes sentences in the source language translated from the target language sentences $T$ and $P_{t \\rightarrow s}$, and $P_{s \\rightarrow t}$ denote the translation direction from target to source and from source to target respectively."]}
{"question_id": "f1a50f88898556ecdba8e9cac13ae54c11835945", "predicted_answer": "", "predicted_evidence": ["Examples of QuaRTz questions $Q_{i}$ are shown in Table TABREF3, along with a sentence $K_{i}$ expressing the relevant qualitative relationship. The QuaRTz task is to answer the questions given a small (400 sentence) corpus $K$ of general qualitative relationship sentences. Questions are crowdsourced, and the sentences $K_{i}$ were collected from a larger corpus, described shortly."]}
{"question_id": "ef6304512652ba56bd13dbe282a5ce1d41a4f171", "predicted_answer": "", "predicted_evidence": ["The results are shown in Table TABREF10, and provide insights into both the data and the models:"]}
{"question_id": "72dbdd11b655b25b2b254e39689a7d912f334b71", "predicted_answer": "", "predicted_evidence": ["Second, crowdworkers were shown a seed sentence $K_i$, and asked to annotate the two properties being compared using the template below, illustrated using $K_2$ from Table TABREF3:"]}
{"question_id": "9b6339e24f58b576143d2adf599cfc4a31fd3b0c", "predicted_answer": "", "predicted_evidence": ["Even after carefully preparing before launching any labeling task, the labelers encountered novel situations that were not considered while developing the ontologies and the guidelines. This led to further refinements of the tasks including changing or adding a new tag. As a result, the portions of the data had to re-labeled, which was expensive. Analysis of the existing labels were used to guide these decisions, such as ignoring tags that appeared too infrequently, focusing on tags where there was a high level of disagreement, and looking at the distribution of labeled text for each tag."]}
{"question_id": "55e3daecaf8030ed627e037992402dd0a7dd67ff", "predicted_answer": "", "predicted_evidence": ["In this section, we first provide a high level description of our proposed framework. We then discuss each module component and the training methods in detail."]}
{"question_id": "5522a9eeb06221722052e3c38f9b0d0dbe7c13e6", "predicted_answer": "", "predicted_evidence": [" where INLINEFORM0 in the agent's network is a multilayer perceptron (MLP) with a single hidden layer and a INLINEFORM1 activation function over all possible system actions."]}
{"question_id": "30870a962cf88ac8c8e6b7b795936fd62214f507", "predicted_answer": "", "predicted_evidence": ["Figure 2 shows the design of the user simulator. User simulator is given a randomly sampled goal at the beginning of the conversation. Similar to the design of the dialog agent, state of the user simulator is maintained in the state of an LSTM. At the INLINEFORM0 th turn of a dialog, the user simulator takes in (1) the goal encoding INLINEFORM1 , (2) the previous user output encoding INLINEFORM2 , (3) the current turn agent input encoding INLINEFORM3 , and updates its internal state conditioning on the previous user state INLINEFORM4 . On the output side, the user simulator firstly emits a user action INLINEFORM5 based on the updated state INLINEFORM6 . Conditioning on this emitted user action and the user dialog state INLINEFORM7 , a set of slot values are emitted. The user action and slot values are then passed to an NLG module to generate the final user utterance."]}
{"question_id": "7ece07a84635269bb19796497847e4517d1e3e61", "predicted_answer": "", "predicted_evidence": ["During model training, we use softmax policy for both the dialog agent and the user simulator to encourage exploration. Softmax policy samples action from the action probability distribution calculated by the INLINEFORM0 in the system action output. During evaluation, we apply greedy policy to the dialog agent, and still apply softmax policy to the user simulator. This is to increase randomness and diversity in the user simulator behavior, which is closer to the realistic dialog system evaluation settings with human users. This also prevents the two agents from fully cooperating with each other and exploiting the game."]}
{"question_id": "f94cea545f745994800c1fb4654d64d1384f2c26", "predicted_answer": "", "predicted_evidence": ["The General Embedding Model : It analyzes images and returns numerical vectors that represent the input images in a 1024-dimensional space. The vector representation is computed by using Clarifai\u2019s \u2018General\u2019 model. The vectors of visually similar images will be close to each other in the 1024-dimensional space. This is used to eliminate multiple similar images of the same food item."]}
{"question_id": "f3b851c9063192c86a3cc33b2328c02efa41b668", "predicted_answer": "", "predicted_evidence": ["Thus from these charts, we see that the user likes to eat Italian and Mexican food on most occasions. This is also in sync with the rudimentary method that we had used earlier."]}
{"question_id": "54b25223ab32bf8d9205eaa8a570e99c683f0077", "predicted_answer": "", "predicted_evidence": ["We have carried out a number of experiments with both baselines. The scores reported are an average of the BLEU scores (in percentage points, or pp) BIBREF46 over the test sets of 5 independently trained models. Table TABREF44 shows the results over the en-fr dataset. In this case, the models with ReWE have outperformed the LSTM and transformer baselines consistently. The LSTM did not benefit from using BPE, but the transformer+ReWE with BPE reached $36.30$ BLEU pp (a $+0.99$ pp improvement over the best model without ReWE). For this dataset we did not use ReSE because French was the target language."]}
{"question_id": "e5be900e70ea86c019efb06438ba200e11773a7c", "predicted_answer": "", "predicted_evidence": ["The model is trained by minimizing the negative log-likelihood (NLL) which can be expressed as:"]}
{"question_id": "b36a8a73b3457a94203eed43f063cb684a8366b7", "predicted_answer": "", "predicted_evidence": ["Table TABREF54 shows two examples of the translations made by the different LSTM models for eu-en and cs-en. A qualitative analysis of these examples shows that both ReWE and ReWE+ReSE have improved the quality of these translations. In the eu-en example, ReWE has correctly translated \u201cFile tab\u201d; and ReSE has correctly added \u201cclick Create\u201d. In the cs-en example, the model with ReWE has picked the correct subject \u201cthey\u201d, and only the model with ReWE and ReSE has correctly translated \u201cstudents\u201d and captured the opening phrase \u201cWhat was...about this...\u201d."]}
{"question_id": "3d73cb92d866448ec72a571331967da5d34dfbb1", "predicted_answer": "", "predicted_evidence": ["We conducted a head-to-head test (paired sample t-test) to compare the trained language model against the corresponding dummy regressor and found that the mean absolute error was significantly lower for the language model $\\textrm {\\textit {LM}}(D_\\textrm {\\textit {HR}})$, t(4) = 4.32, p = .02, as well as the $\\textrm {\\textit {LM}}(D_\\textrm {\\textit {LR}})$, t(4) = 4.47, p = .02. Thus, the trained language models performed significantly better than a dummy regressor. In light of these differences and the slightly lower mean absolute error $\\textrm {\\textit {LM}}(D_\\textrm {\\textit {HR}})$ compared to the $\\textrm {\\textit {LM}}(D_\\textrm {\\textit {LR}})$ [t(4) = 2.73, p = .05] and considering that $\\textrm {\\textit {LM}}(D_\\textrm {\\textit {HR}})$ is the best model in terms of $\\textrm {R}^2$ we take it for testing in the wild."]}
{"question_id": "708f5f83a3c356b23b27a9175f5c35ac00cdf5db", "predicted_answer": "", "predicted_evidence": ["The models were evaluated using 5-fold cross validation. The results for the cross validation is shown in table TABREF33 and TABREF34."]}
{"question_id": "9240ee584d4354349601aeca333f1bc92de2165e", "predicted_answer": "", "predicted_evidence": ["The intra-annotator reliability of both datasets $D_\\textrm {\\textit {LR}}$ and $D_\\textrm {\\textit {HR}}$ is shown in Table TABREF21. The reliability was calculated using the Krippendorff's alpha coefficient. Krippendorff's alpha can handle missing values, which in this case was necessary since many of the texts were annotated by only a few annotators."]}
{"question_id": "9133a85730c4090fe8b8d08eb3d9146efe7d7037", "predicted_answer": "", "predicted_evidence": ["Figures FIGREF29 and FIGREF30 aggregate the relative errors across all the datasets, for INLINEFORM0 and INLINEFORM1 , respectively. The proportion of errors is consistent between INLINEFORM2 and INLINEFORM3 , but there are more large errors when the performance is measured by INLINEFORM4 . This is due to smaller error magnitude when the performance is measured by INLINEFORM5 in contrast to INLINEFORM6 , since INLINEFORM7 takes classification by chance into account. With respect to individual estimation procedures, there is a considerable divergence of the random cross-validation. For both performance measures, INLINEFORM8 and INLINEFORM9 , it consistently incurs higher proportion of large errors and lower proportion of small errors in comparison to the rest of the estimation procedures."]}
{"question_id": "42279c3a202a93cfb4aef49212ccaf401a3f8761", "predicted_answer": "", "predicted_evidence": ["In sequential validation, a sample consists of the training set immediately followed by the test set. We vary the ratio of the training and test set sizes, and the number and distribution of samples taken from the in-set. The number of samples is 10 or 20, and they are distributed equidistantly or semi-equidistantly. In all variants, samples cover the whole in-set, but they are overlapping. See Figure FIGREF20 for illustration. We use the following abbreviations for sequential validations:"]}
{"question_id": "9ca85242ebeeafa88a0246986aa760014f6094f2", "predicted_answer": "", "predicted_evidence": ["The idea behind the INLINEFORM0 -fold cross-validation is to randomly shuffle the data and split it in INLINEFORM1 equally-sized folds. Each fold is a subset of the data randomly picked for testing. Models are trained on the INLINEFORM2 folds and their performance is estimated on the left-out fold. INLINEFORM3 -fold cross-validation has several practical advantages, such as an efficient use of all the data. However, it is also based on an assumption that the data is independent and identically distributed BIBREF9 which is often not true. For example, in time-ordered data, such as Twitter posts, the data are to some extent dependent due to the underlying temporal order of tweets. Therefore, using INLINEFORM4 -fold cross-validation means that one uses future information to predict past events, which might hinder the generalization ability of models."]}
{"question_id": "8641156c4d67e143ebbabbd79860349242a11451", "predicted_answer": "", "predicted_evidence": ["The differences between the estimation procedures are easier to detect when we aggregate the errors over all language datasets. The results are in Figures FIGREF25 and FIGREF26 , for INLINEFORM0 and INLINEFORM1 , respectively. In both cases we observe that the cross-validation procedures (xval) consistently overestimate the performance, while the sequential validations (seq) underestimate it. The largest overestimation errors are incurred by the random cross-validation, and the largest underestimations by the sequential validation with the training:test set ratio 2:1. We also observe high variability of errors, with many outliers. The conclusions are consistent for both measures, INLINEFORM2 and INLINEFORM3 ."]}
{"question_id": "2a120f358f50c377b5b63fb32633223fa4ee2149", "predicted_answer": "", "predicted_evidence": [" INLINEFORM0 implicitly takes into account the ordering of sentiment values, by considering only the extreme labels, negative INLINEFORM1 and positive INLINEFORM2 . The middle, neutral, is taken into account only indirectly. INLINEFORM3 is the harmonic mean of precision and recall for class INLINEFORM4 , INLINEFORM5 . INLINEFORM6 INLINEFORM7 implies that all negative and positive tweets were correctly classified, and as a consequence, all neutrals as well. INLINEFORM8 INLINEFORM9 indicates that all negative and positive tweets were incorrectly classified. INLINEFORM10 does not account for correct classification by chance."]}
